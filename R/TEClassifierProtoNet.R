#'@title Text embedding classifier with a ProtoNet
#'
#'@description Abstract class for neural nets with 'keras'/'tensorflow' and
#''pytorch'.
#'
#'This object represents in implementation of a prototypical network for
#'few-shot learning as described by Snell, Swersky, and Zemel (2017). The network
#'uses a multi way contrastive loss described by Zhang et al. (2019). The network
#'learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018)
#'
#'@return Objects of this class are used for assigning texts to classes/categories. For
#'the creation and training of a classifier an object of class \link{EmbeddedText} and a \code{factor}
#'are necessary. The object of class \link{EmbeddedText} contains the numerical text
#'representations (text embeddings) of the raw texts generated by an object of class
#'\link{TextEmbeddingModel}. The \code{factor} contains the classes/categories for every
#'text. Missing values (unlabeled cases) are supported. For predictions an object of class
#'\link{EmbeddedText} has to be used which was created with the same text embedding model as
#'for training.
#'
#'@references Oreshkin, B. N., Rodriguez, P. & Lacoste, A. (2018). TADAM:
#'Task dependent adaptive metric for improved few-shot learning. https://doi.org/10.48550/arXiv.1805.10123
#'@references Snell, J., Swersky, K. & Zemel, R. S. (2017). Prototypical Networks
#'for Few-shot Learning. https://doi.org/10.48550/arXiv.1703.05175
#'@references Zhang, X., Nie, J., Zong, L., Yu, H. & Liang, W. (2019). One Shot
#'Learning with Margin. In Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang & S.-J. Huang (Eds.),
#' Lecture Notes in Computer Science. Advances in Knowledge Discovery and Data Mining (Vol. 11440, pp. 305â€“317).
#' Springer International Publishing. https://doi.org/10.1007/978-3-030-16145-3_24
#'@family Classification
#'@export
TEClassifierProtoNet<-R6::R6Class(
  classname = "TEClassifierProtoNet",
  inherit = TEClassifierRegular,
  public = list(
    #New-----------------------------------------------------------------------
    #'@description Creating a new instance of this class.
    #'@param ml_framework \code{string} Framework to use for training and inference.
    #'\code{ml_framework="tensorflow"} for 'tensorflow' and \code{ml_framework="pytorch"}
    #'for 'pytorch'
    #'@param name \code{Character} Name of the new classifier. Please refer to
    #'common name conventions. Free text can be used with parameter \code{label}.
    #'@param label \code{Character} Label for the new classifier. Here you can use
    #'free text.
    #'@param text_embeddings An object of class\code{TextEmbeddingModel}.
    #'@param use_fe \code{bool} If \code{TRUE} a feature extractor is applied in
    #'order to reduce the dimensionality of the text embeddings.
    #'@param fe_features \code{int} determining the number of dimensions to which
    #'the dimension of the text embedding should be reduced.
    #'@param fe_method \code{string} Method to use for the feature extraction.
    #'\code{"lstm"} for an extractor based on LSTM-layers or \code{"dense"} for
    #'dense layers.
    #'@param fe_noise_factor \code{double} between 0 and a value lower 1 indicating
    #'how much noise should be added for the training of the feature extractor.
    #'@param targets \code{factor} containing the target values of the classifier.
    #'@param hidden \code{vector} containing the number of neurons for each dense layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec \code{vector} containing the number of neurons for each recurrent layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec_type \code{string} Type of the recurrent layers. \code{rec_type="gru"} for
    #'Gated Recurrent Unit and \code{rec_type="lstm"} for Long Short-Term Memory.
    #'@param rec_bidirectional \code{bool} If \code{TRUE} a bidirectional version of the reccurent
    #'layers is used.
    #'@param embedding_dim \code{Int} determining the dimensionality of the embedding.
    #'@param attention_type \code{string} Choose the relevant attention type. Possible values
    #'are \code{"fourier"} and \code{multihead}.
    #'@param self_attention_heads \code{integer} determining the number of attention heads
    #'for a self-attention layer. Only relevant if \code{attention_type="multihead"}
    #'@param repeat_encoder \code{int} determining how many times the encoder should be
    #'added to the network.
    #'@param intermediate_size \code{int} determining the size of the projection layer within
    #'a each transformer encoder.
    #'@param add_pos_embedding \code{bool} \code{TRUE} if positional embedding should be used.
    #'@param encoder_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout for the dense projection within the encoder layers.
    #'@param dense_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between dense layers.
    #'@param rec_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between bidirectional gru layers.
    #'@param recurrent_dropout \code{double} ranging between 0 and lower 1, determining the
    #'recurrent dropout for each recurrent layer. Only relevant for keras models.
    #'@param optimizer Object of class \code{keras.optimizers}.
    #'@return Returns an object of class \link{TextEmbeddingClassifierNeuralNet} which is ready for
    #'training.
    initialize=function(ml_framework=aifeducation_config$get_framework(),
                        name=NULL,
                        label=NULL,
                        text_embeddings=NULL,
                        use_fe=TRUE,
                        fe_features=128,
                        fe_method="lstm",
                        fe_noise_factor=0.2,
                        targets=NULL,
                        hidden=c(128),
                        rec=c(128),
                        rec_type="gru",
                        rec_bidirectional=FALSE,
                        embedding_dim=3,
                        self_attention_heads=0,
                        intermediate_size=NULL,
                        attention_type="fourier",
                        add_pos_embedding=TRUE,
                        rec_dropout=0.1,
                        repeat_encoder=1,
                        dense_dropout=0.4,
                        recurrent_dropout=0.4,
                        encoder_dropout=0.1,
                        optimizer="adam"
    ){
      #Checking of parameters--------------------------------------------------
      if(is.null(name)){
        stop("name is NULL but must be a character.")
      }
      if(is.null(label)){
        stop("label is NULL but must be a character.")
      }
      if(!("EmbeddedText" %in% class(text_embeddings))){
        stop("text_embeddings must be of class EmbeddedText.")
      }
      if(is.factor(targets)==FALSE){
        stop("targets must be of class factor.")
      }

      if(!(is.numeric(hidden)==TRUE | is.null(hidden)==TRUE)){
        stop("hidden must be a vector of integer or NULL.")
      }
      if(!(is.numeric(rec)==TRUE | is.null(rec)==TRUE)){
        stop("rec must be a vector of integer or NULL.")
      }
      if(is.integer(as.integer(self_attention_heads))==FALSE){
        stop("self_attention_heads must be an integer.")
      }

      if(optimizer %in% c("adam","rmsprop")==FALSE){
        stop("Optimzier must be 'adam' oder 'rmsprop'.")
      }

      if(attention_type %in% c("fourier","multihead")==FALSE){
        stop("Optimzier must be 'fourier' oder 'multihead'.")
      }
      if(repeat_encoder>0 & attention_type=="multihead" & self_attention_heads<=0){
        stop("Encoder layer is set to 'multihead'. This requires self_attention_heads>=1.")
      }

      if(embedding_dim<=0){
        stop("Embedding_dim must be an integer greater 0.")
      }

      #------------------------------------------------------------------------

      #Setting ML Framework
      if((ml_framework %in% c("tensorflow","pytorch"))==FALSE) {
        stop("ml_framework must be 'tensorflow' or 'pytorch'.")
      }

      private$ml_framework=ml_framework

      #Setting Label and Name-------------------------------------------------
      private$model_info$model_name_root=name
      private$model_info$model_name=paste0(private$model_info$model_name_root,"_ID_",generate_id(16))
      private$model_info$model_label=label

      #Basic Information of Input and Target Data
      variable_name_order<-dimnames(text_embeddings$embeddings)[[3]]
      target_levels_order<-levels(targets)

      model_info=text_embeddings$get_model_info()
      times=model_info$param_chunks
      features=dim(text_embeddings$embeddings)[3]

      private$text_embedding_model["model"]=list(model_info)
      private$text_embedding_model["times"]=times
      private$text_embedding_model["features"]=features

      if(is.null(rec) & self_attention_heads>0){
        if(features %% 2 !=0){
          stop("The number of features of the TextEmbeddingmodel is
               not a multiple of 2.")
        }
      }

      if(is.null(intermediate_size)==TRUE){
        if(attention_type=="fourier" & length(rec)>0){
          intermediate_size=2*rec[length(rec)]
        } else if(attention_type=="fourier" & length(rec)==0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)>0 & self_attention_heads>0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)==0 & self_attention_heads>0){
          intermediate_size=2*features
        } else {
          intermediate_size=NULL
        }
      }

      if(use_fe==TRUE){
        features=fe_features
      } else {
        features=private$text_embedding_model[["features"]]
      }

      #Saving Configuration
      config=list(
        use_fe=use_fe,
        fe_method=fe_method,
        fe_noise_factor=fe_noise_factor,
        features=features,
        times=private$text_embedding_model[["times"]],
        hidden=hidden,
        rec=rec,
        rec_type=rec_type,
        rec_bidirectional=rec_bidirectional,
        intermediate_size=intermediate_size,
        attention_type=attention_type,
        repeat_encoder=repeat_encoder,
        dense_dropout=dense_dropout,
        rec_dropout=rec_dropout,
        recurrent_dropout=recurrent_dropout,
        encoder_dropout=encoder_dropout,
        add_pos_embedding=add_pos_embedding,
        optimizer=optimizer,
        act_fct="gelu",
        rec_act_fct="tanh",
        embedding_dim=embedding_dim,
        self_attention_heads=self_attention_heads)

      if(length(target_levels_order)>2){
        #Multi Class
        config["act_fct_last"]="softmax"
        config["err_fct"]="categorical_crossentropy"
        config["metric"]="categorical_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      } else {
        #Binary Classification
        config["act_fct_last"]="sigmoid"
        config["err_fct"]="binary_crossentropy"
        config["metric"]="binary_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      }

      config["target_levels"]=list(target_levels_order)
      config["n_categories"]=list(length(target_levels_order))

      config["require_one_hot"]=list(FALSE)

      if(times>1|length(rec)>0|repeat_encoder>0){
        config["require_matrix_map"]=list(FALSE)
      } else {
        config["require_matrix_map"]=list(TRUE)
      }

      config["input_variables"]=list(variable_name_order)

      self$model_config=config

      #Create_Model------------------------------------------------------------
      private$create_reset_model()
      if(self$model_config$use_fe==TRUE){
        self$feature_extractor$model=private$create_feature_extractor()
      }

      private$model_info$model_date=date()

      private$r_package_versions$aifeducation<-packageVersion("aifeducation")
      private$r_package_versions$reticulate<-packageVersion("reticulate")

      private$py_package_versions$tensorflow<-tf$version$VERSION
      private$py_package_versions$torch<-torch["__version__"]
      private$py_package_versions$keras<-keras["__version__"]
      private$py_package_versions$numpy<-np$version$short_version
    },

    #-------------------------------------------------------------------------
    #'@description Method for training a neural net.
    #'@param data_embeddings Object of class \code{TextEmbeddingModel}.
    #'@param data_targets \code{Factor} containing the labels for cases
    #'stored in \code{data_embeddings}. Factor must be named and has to use the
    #'same names used in \code{data_embeddings}.
    #'@param data_folds \code{int} determining the number of cross-fold
    #'samples.
    #'@param data_val_size \code{double} between 0 and 1, indicating the proportion of cases of each class
    #'which should be used for the validation sample during the estimation of the baseline model.
    #'The remaining cases are part of the training data.
    #'@param fe_val_size \code{double} between 0 and 1, indicating the proportion of cases
    #'which should be used for the validation sample during the training of the feature extractor.
    #'@param balance_class_weights \code{bool} If \code{TRUE} class weights are
    #'generated based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each class has the weight 1.
    #'@param balance_sequence_length \code{bool} If \code{TRUE} sample weights are
    #'generated for the length of sequences based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each sequences length has the weight 1.
    #'@param use_sc \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced synthetic cases. \code{FALSE} if not.
    #'@param sc_method \code{vector} containing the methods for generating
    #'synthetic cases via 'smotefamily'. Multiple methods can
    #'be passed. Currently \code{sc_method=c("adas")}, \code{sc_method=c("smote")}
    #'and \code{sc_method=c("dbsmote")} are possible.
    #'@param sc_min_k \code{int} determining the minimal number of k which is used
    #'for creating synthetic units.
    #'@param sc_max_k \code{int} determining the maximal number of k which is used
    #'for creating synthetic units.
    #'@param use_pl \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced pseudo-labeling. \code{FALSE} if not.
    #'@param pl_max_steps \code{int} determining the maximum number of steps during
    #'pseudo-labeling.
    #'@param pl_anchor \code{double} between 0 and 1 indicating the reference
    #'point for sorting the new cases of every label. See notes for more details.
    #'@param pl_max \code{double} between 0 and 1, setting the maximal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param pl_min \code{double} between 0 and 1, setting the minimal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param sustain_track \code{bool} If \code{TRUE} energy consumption is tracked
    #'during training via the python library codecarbon.
    #'@param sustain_iso_code \code{string} ISO code (Alpha-3-Code) for the country. This variable
    #'must be set if sustainability should be tracked. A list can be found on
    #'Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.
    #'@param sustain_region Region within a country. Only available for USA and
    #'Canada See the documentation of codecarbon for more information.
    #'\url{https://mlco2.github.io/codecarbon/parameters.html}
    #'@param sustain_interval \code{integer} Interval in seconds for measuring power
    #'usage.
    #'@param batch_size \code{int} Size of the batches for the feature extractor.
    #'@param epochs \code{int} Number of training epochs.
    #'@param fe_epochs \code{int} Number of training epochs for the feature extractor.
    #'@param Ns \code{int} Number of cases for every class in the sample.
    #'@param Nq \code{int} Number of cases for every class in the query.
    #'@param loss_alpha \code{double} Value between 0 and 1 indicating how strong
    #'the loss should focus on pulling cases to its corresponding prototypes or
    #'pushing cases away from other prototypes. The higher the value the more the loss
    #'concentrates on pulling cases to its corresponding prototypes.
    #'@param loss_margin \code{dobule} Value greater 0 indicating the minimal
    #'distance of every case from prototypes of other classes.
    #'@param dir_checkpoint \code{string} Path to the directory where
    #'the checkpoint during training should be saved. If the directory does not
    #'exist, it is created.
    #'@param trace \code{bool} \code{TRUE}, if information about the estimation
    #'phase should be printed to the console.
    #'@param keras_trace \code{int} \code{keras_trace=0} does not print any
    #'information about the training process from keras on the console.
    #'@param pytorch_trace \code{int} \code{pytorch_trace=0} does not print any
    #'information about the training process from pytorch on the console.
    #'\code{pytorch_trace=1} prints a progress bar.
    #'@return Function does not return a value. It changes the object into a trained
    #'classifier.
    #'@details \itemize{
    #'
    #'\item{\code{sc_max_k: }All values from sc_min_k up to sc_max_k are successively used. If
    #'the number of sc_max_k is too high, the value is reduced to a number that
    #'allows the calculating of synthetic units.}
    #'
    #'\item{\code{pl_anchor: }With the help of this value, the new cases are sorted. For
    #'this aim, the distance from the anchor is calculated and all cases are arranged
    #'into an ascending order.
    #'}
    #'}
    #'@importFrom abind abind
    train=function(data_embeddings,
                   data_targets,
                   data_folds=5,
                   data_val_size=0.25,
                   use_sc=TRUE,
                   sc_method="dbsmote",
                   sc_min_k=1,
                   sc_max_k=10,
                   use_pl=TRUE,
                   pl_max_steps=3,
                   pl_max=1.00,
                   pl_anchor=1.00,
                   pl_min=0.00,
                   sustain_track=TRUE,
                   sustain_iso_code=NULL,
                   sustain_region=NULL,
                   sustain_interval=15,
                   epochs=40,
                   batch_size=35,
                   fe_epochs=1000,
                   fe_val_size=0.25,
                   Ns=5,
                   Nq=3,
                   loss_alpha,
                   loss_margin,
                   dir_checkpoint,
                   trace=TRUE,
                   keras_trace=2,
                   pytorch_trace=1){

      #Checking Arguments------------------------------------------------------
      if(!("EmbeddedText" %in% class(data_embeddings))){
        stop("data_embeddings must be an object of class EmbeddedText")
      }

      if(self$check_embedding_model(data_embeddings)==FALSE){
        stop("The TextEmbeddingModel that generated the data_embeddings is not
               the same as the TextEmbeddingModel when generating the classifier.")
      }

      if(is.factor(data_targets)==FALSE){
        stop("data_targets must be a factor.")
      }
      if(is.null(names(data_targets))){
        stop("data_targets must be a named factor.")
      }

      if(pl_anchor<pl_min){
        stop("pl_anchor must be at least pl_min.")
      }
      if(pl_anchor>pl_max){
        stop("pl_anchor must be lower or equal to pl_max.")
      }

      if(data_folds<2){
        stop("data_folds must be at least 2.")
      }

      #Saving training configuration-------------------------------------------
      self$last_training$config$data_val_size=data_val_size
      self$last_training$config$use_sc=use_sc
      self$last_training$config$sc_method=sc_method
      self$last_training$config$sc_min_k=sc_min_k
      self$last_training$config$sc_max_k=sc_max_k
      self$last_training$config$use_pl=use_pl
      self$last_training$config$pl_max_steps=pl_max_steps
      self$last_training$config$pl_max=pl_max
      self$last_training$config$pl_anchor=pl_anchor
      self$last_training$config$pl_min=pl_min
      self$last_training$config$sustain_track=sustain_track
      self$last_training$config$sustain_iso_code=sustain_iso_code
      self$last_training$config$sustain_region=sustain_region
      self$last_training$config$sustain_interval=sustain_interval
      self$last_training$config$epochs=epochs
      self$last_training$config$batch_size=batch_size
      self$last_training$config$Ns=Ns
      self$last_training$config$Nq=Nq
      self$last_training$config$loss_alpha=loss_alpha
      self$last_training$config$loss_margin=loss_margin
      self$last_training$config$dir_checkpoint=dir_checkpoint
      self$last_training$config$trace=trace
      self$last_training$config$keras_trace=keras_trace
      self$last_training$config$pytorch_trace=pytorch_trace

      self$feature_extractor$val_size=fe_val_size
      self$feature_extractor$epochs=fe_epochs

      #Start-------------------------------------------------------------------
      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                      "Start"))
      }

      #Create DataManager------------------------------------------------------
      if(self$model_config$use_fe==TRUE){
        private$train_feature_extractor(data_embeddings = data_embeddings)

        data_manager=DataManagerClassifier$new(
          data_embeddings=self$extract_features(data_embeddings = data_embeddings,
                                                as.integer(batch_size=self$last_training$config$batch_size),
                                                return_r_object = TRUE),
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      } else {
        data_manager=DataManagerClassifier$new(
          data_embeddings=data_embeddings,
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      }

      #Save Data Statistics
      self$last_training$data=data_manager$get_statistics()

      #Save the number of folds
      self$last_training$config$n_folds=data_manager$get_n_folds()

      #Init Training------------------------------------------------------------
      private$init_train()

      #disable Progress bars
      datasets$disable_progress_bars()

      #SetUp GUI----------------------------------------------------------------
      private$init_gui(data_manager = data_manager)
      private$gui_inc_progressbar()

      #Start Sustainability Tracking-------------------------------------------
      if(sustain_track==TRUE){
        if(is.null(sustain_iso_code)==TRUE){
          stop("Sustainability tracking is activated but iso code for the
               country is missing. Add iso code or deactivate tracking.")
        }
        sustainability_tracker<-codecarbon$OfflineEmissionsTracker(
          country_iso_code=sustain_iso_code,
          region=sustain_region,
          tracking_mode="machine",
          log_level="warning",
          measure_power_secs=sustain_interval,
          save_to_file=FALSE,
          save_to_api=FALSE
        )
        sustainability_tracker$start()
      }

      #Update Progressbar-------------------------------------------------------
      private$gui_inc_progressbar()

      #Start Training----------------------------------------------------------
      #Load Custom Model Scripts
      private$load_reload_python_scripts()

      #Start Loop inclusive final training
      for(iter in 1:(self$last_training$config$n_folds+1)){
        base::gc(verbose = FALSE,full = TRUE)

        if(self$last_training$config$use_pl==FALSE){
          private$train_standard(iteration = iter,
                                 data_manager = data_manager,
                                 inc_synthetic = self$last_training$config$use_sc)
        } else if(self$last_training$config$use_pl==TRUE) {
          private$train_with_pseudo_labels(init_train=TRUE,
                                           iteration=iter,
                                           data_manager=data_manager,
                                           inc_synthetic=self$last_training$config$use_sc)
        }

        #Calculate measures on categorical level
        private$calculate_measures_on_categorical_level(
          data_manager=data_manager,
          iteration=iter)
      }

      #Finalize Training
      private$finalize_train()

      #Stop sustainability tracking if requested
      if(sustain_track==TRUE){
        sustainability_tracker$stop()
        private$sustainability<-summarize_tracked_sustainability(sustainability_tracker)
      } else {
        private$sustainability=list(
          sustainability_tracked=FALSE,
          date=NA,
          sustainability_data=list(
            duration_sec=NA,
            co2eq_kg=NA,
            cpu_energy_kwh=NA,
            gpu_energy_kwh=NA,
            ram_energy_kwh=NA,
            total_energy_kwh=NA
          )
        )
      }

      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                    "Training Complete"))
      }
    },
  #---------------------------------------------------------------------------
  embed=function(embeddings_q=NULL,classes_q=NULL,embeddings_s=NULL,classes_s=NULL){
    #Parameter check
    if(is.null(embeddings_q)|is.null(classes_q)){
      stop("Embeddings and classes for the query are not set.")
    }
    if(is.null(embeddings_s)&!is.null(classes_s)){
      stop("classes_s is set but embeddings_s not. Please provide embeddings
           or set classes_s to NULL.in order to use the trained
           prototypes.")
    }
    if(!is.null(embeddings_s)&is.null(classes_s)){
      stop("embeddings_s is set but classes_s not. Please provide classes for
           the sample or set embeddings_s to NULL in order to use the trained
           prototypes.")
    }

    if(self$model_config$use_fe==TRUE){
      embeddings_q=self$extract_features(embeddings_q)
      if(!is.null(embeddings_s)){
        embeddings_s=self$extract_features(embeddings_q)
      }
    }

    if(self$model_config$use_fe==TRUE){
      #Prepare data set
      if("EmbeddedText" %in% class(data_embeddings)){
        if(nrow(data_embeddings$embeddings)>1){
          extractor_dataset=datasets$Dataset$from_dict(
            reticulate::dict(
              list(id=rownames(data_embeddings$embeddings),
                   input=np$squeeze(np$split(reticulate::np_array(data_embeddings$embeddings),as.integer(nrow(data_embeddings$embeddings)),axis=0L))),
              convert = FALSE))
        } else {
          extractor_dataset=data_embeddings$embeddings
        }

      } else if("array" %in% class(data_embeddings)){
        if(nrow(data_embeddings)>1){
          extractor_dataset=datasets$Dataset$from_dict(
            reticulate::dict(
              list(id=rownames(data_embeddings),
                   input=np$squeeze(np$split(reticulate::np_array(data_embeddings),as.integer(nrow(data_embeddings)),axis=0L))),
              convert = FALSE))
        } else {
          extractor_dataset=data_embeddings
        }
      }
    }

      #Extract features
      if(private$ml_framework=="pytorch"){
        if(torch$cuda$is_available()){
          device="cuda"
          dtype=torch$double

          if("datasets.arrow_dataset.Dataset"%in%class(extractor_dataset)){
            extractor_dataset$set_format("torch",device=device)
            self$feature_extractor$model$to(device,dtype=dtype)
            self$feature_extractor$model$eval()
            reduced_embeddings<-self$feature_extractor$model(extractor_dataset["input"],
                                                             encoder_mode=TRUE)$detach()$cpu()$numpy()
          } else {
            self$feature_extractor$model$to(device,dtype=dtype)
            self$feature_extractor$model$eval()
            input=torch$from_numpy(np$array(extractor_dataset))
            reduced_embeddings<-self$feature_extractor$model(input$to(device,dtype=dtype),
                                                             encoder_mode=TRUE)$detach()$cpu()$numpy()
          }
        } else {
          device="cpu"
          dtype=torch$float
          if("datasets.arrow_dataset.Dataset"%in%class(extractor_dataset)){
            extractor_dataset$set_format("torch",device=device)

            self$feature_extractor$model$to(device,dtype=dtype)
            self$feature_extractor$model$eval()
            reduced_embeddings<-self$feature_extractor$model(extractor_dataset["input"],
                                                             encoder_mode=TRUE)$detach()$numpy()
          } else {
            self$feature_extractor$model$to(device,dtype=dtype)
            self$feature_extractor$model$eval()
            input=torch$from_numpy(np$array(extractor_dataset))
            reduced_embeddings<-self$feature_extractor$model(input$to(device,dtype=dtype),
                                                             encoder_mode=TRUE)$detach()$numpy()
          }
        }
      }

    if(private$ml_framework=="pytorch"){

    }
  }
  ),
  private = list(
    #--------------------------------------------------------------------------
    load_reload_python_scripts=function(){
      if(private$ml_framework=="tensorflow"){
        reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/keras_callbacks.py",
                                            package = "aifeducation"))
      } else if(private$ml_framework=="pytorch"){
        reticulate::py_run_file(system.file("python/pytorch_te_classifier_V2.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/pytorch_te_protonet.py",
                                            package = "aifeducation"))
      }
    },
    #--------------------------------------------------------------------------
    create_reset_model=function(){
      if(private$ml_framework=="tensorflow"){

        #Load custom layers
        private$load_reload_python_scripts()

        #Defining basic keras model
        layer_list=NULL

        if(is.null(self$model_config$rec)==TRUE){
          n_rec=0
        } else {
          n_rec=length(self$model_config$rec)
        }

        if(is.null(self$model_config$hidden)==TRUE){
          n_hidden=0
        } else {
          n_hidden=length(self$model_config$hidden)
        }

        #Adding Input Layer

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          model_input<-keras$layers$Input(shape=list(as.integer(self$model_config$times),as.integer(self$model_config$features)),
                                          name="input_embeddings")
        } else {
          model_input<-keras$layers$Input(shape=as.integer(self$model_config$times*self$model_config$features),
                                          name="input_embeddings")
        }
        layer_list[1]<-list(model_input)

        #Adding a Mask-Layer
        if(n_rec>0 | self$model_config$repeat_encoder>0){
          masking_layer<-keras$layers$Masking(
            mask_value = 0.0,
            name="masking_layer",
            input_shape=c(self$model_config$times,self$model_config$features),
            trainable=FALSE)(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(masking_layer)

          if(self$model_config$add_pos_embedding==TRUE){
            positional_embedding<-py$AddPositionalEmbedding(sequence_length = as.integer(self$model_config$times),
                                                            name="add_positional_embedding")(layer_list[[length(layer_list)]])
            layer_list[length(layer_list)+1]<-list(positional_embedding)
          }

          norm_layer<-keras$layers$LayerNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        } else {
          norm_layer<-keras$layers$BatchNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        }

        if(self$model_config$repeat_encoder>0){
          for(r in 1:self$model_config$repeat_encoder){
            if(self$model_config$attention_type=="multihead"){
              layer_list[length(layer_list)+1]<-list(
                py$TransformerEncoder(embed_dim = as.integer(self$model_config$features),
                                      dense_dim= as.integer(self$model_config$intermediate_size),
                                      num_heads =as.integer(self$model_config$self_attention_heads),
                                      dropout_rate=self$model_config$encoder_dropout,
                                      name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            } else if(self$model_config$attention_type=="fourier"){
              layer_list[length(layer_list)+1]<-list(
                py$FourierEncoder(dense_dim=as.integer(self$model_config$intermediate_size),
                                  dropout_rate=self$model_config$encoder_dropout,
                                  name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            }
          }
        }

        #Adding rec layer
        if(n_rec>0){
          for(i in 1:n_rec){
            if(self$model_config$rec_type=="gru"){
              layer_list[length(layer_list)+1]<-list(
                keras$layers$Bidirectional(
                  layer=keras$layers$GRU(
                    units=as.integer(self$model_config$rec[i]),
                    input_shape=list(self$model_config$times,self$model_config$features),
                    return_sequences = TRUE,
                    dropout = 0,
                    recurrent_dropout = self$model_config$recurrent_dropout,
                    activation = "tanh",
                    name=paste0("gru_",i)),
                  name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
              if (i!=n_rec){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Dropout(
                    rate = self$model_config$rec_dropout,
                    name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
              }
            } else if (self$model_config$rec_type=="lstm"){
              layer_list[length(layer_list)+1]<-list(
                keras$layers$Bidirectional(
                  layer=keras$layers$LSTM(
                    units=as.integer(self$model_config$rec[i]),
                    input_shape=list(self$model_config$times,self$model_config$features),
                    return_sequences = TRUE,
                    dropout = 0,
                    recurrent_dropout = self$model_config$recurrent_dropout,
                    activation = "tanh",
                    name=paste0("lstm",i)),
                  name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
              if (i!=n_rec){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Dropout(
                    rate = self$model_config$rec_dropout,
                    name=paste0("lstm_dropout_",i))(layer_list[[length(layer_list)]]))
              }
            }
          }
        }

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$GlobalAveragePooling1D(
              name="global_average_pooling")(layer_list[[length(layer_list)]]))
        }

        #Adding standard layer
        if(n_hidden>0){
          for(i in 1:n_hidden){
            layer_list[length(layer_list)+1]<-list(
              keras$layers$Dense(
                units = as.integer(self$model_config$hidden[i]),
                activation = "gelu",
                name=paste0("dense_",i))(layer_list[[length(layer_list)]]))

            if(i!=n_hidden){
              #Add Dropout_Layer
              layer_list[length(layer_list)+1]<-list(
                keras$layers$Dropout(
                  rate = self$model_config$dense_dropout,
                  name=paste0("dense_dropout_",i))(layer_list[[length(layer_list)]]))
            }
          }
        }

        #Adding final Layer
        if(length(self$model_config$target_levels)>2){
          #Multi Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(length(self$model_config$target_levels)),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        } else {
          #Binary Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(1),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        }

        #Creating Model
        model<-keras$Model(
          inputs = model_input,
          outputs = layer_list[length(layer_list)],
          name = self$model_config$name)

        self$model=model
      } else {
        #--------------------------------------------------------------------------
        #Load Custom Pytorch Objects and Functions
        private$load_reload_python_scripts()

        self$model=py$TextEmbeddingClassifierProtoNet_PT(features=as.integer(self$model_config$features),
                                               times=as.integer(self$model_config$times),
                                               hidden=if(!is.null(self$model_config$hidden)){as.integer(self$model_config$hidden)}else{NULL},
                                               rec=if(!is.null(self$model_config$rec)){as.integer(self$model_config$rec)}else{NULL},
                                               rec_type=self$model_config$rec_type,
                                               rec_bidirectional=self$model_config$rec_bidirectional,
                                               intermediate_size=as.integer(self$model_config$intermediate_size),
                                               attention_type=self$model_config$attention_type,
                                               repeat_encoder=as.integer(self$model_config$repeat_encoder),
                                               dense_dropout=self$model_config$dense_dropout,
                                               rec_dropout=self$model_config$rec_dropout,
                                               encoder_dropout=self$model_config$encoder_dropout,
                                               add_pos_embedding=self$model_config$add_pos_embedding,
                                               self_attention_heads=as.integer(self$model_config$self_attention_heads),
                                               embedding_dim=as.integer(self$model_config$embedding_dim),
                                               target_levels=reticulate::np_array(seq(from=0,to=(length(self$model_config$target_levels)-1))))
      }
    },
    #--------------------------------------------------------------------------
    basic_train=function(train_data=NULL,
                         val_data=NULL,
                         test_data=NULL,
                         reset_model=FALSE,
                         use_callback=TRUE,
                         shiny_app_active=FALSE){

      #Clear session to provide enough resources for computations
      if(private$ml_framework=="tensorflow"){
        keras$backend$clear_session()
      } else if(private$ml_framework=="pytorch"){
        if(torch$cuda$is_available()){
          torch$cuda$empty_cache()
        }
      }

      #Reset model if requested
      if(reset_model==TRUE){
        private$create_reset_model()
      }

      #Set Optimizer
      if(private$ml_framework=="tensorflow"){
        balanced_metric=py$BalancedAccuracy(n_classes = as.integer(length(self$model_config$target_levels)))
        if(self$model_config$optimizer=="adam"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$Adam(),
            metrics=c(self$model_config$metric,balanced_metric))
        } else if (self$model_config$optimizer=="rmsprop"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$RMSprop(),
            metrics=c(self$model_config$metric,balanced_metric))
        }
      } else if(private$ml_framework=="pytorch"){
        loss_fct_name="CrossEntropyLoss"
        if(self$model_config$optimizer=="adam"){
          optimizer="adam"
        } else if (self$model_config$optimizer=="rmsprop"){
          optimizer="rmsprop"
        }
      }

      #Check directory for checkpoints
      if(dir.exists(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))==FALSE){
        if(self$last_training$config$trace==TRUE){
          message(paste(date(),"Creating Checkpoint Directory"))
        }
        dir.create(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))
      }

      #Set target column
      if(self$model_config$require_one_hot==FALSE){
        target_column="labels"
      } else {
        target_column="one_hot_encoding"
      }

      #Tensorflow - Callbacks and training
      if(private$ml_framework=="tensorflow"){
        if(use_callback==TRUE){
          callback=keras$callbacks$ModelCheckpoint(
            filepath = paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"),
            monitor = paste0("val_",self$model_config$balanced_metric),
            verbose = as.integer(min(self$last_training$config$keras_trace,1)),
            mode = "auto",
            save_best_only = TRUE,
            save_weights_only = TRUE)
        } else {
          callback=reticulate::py_none()
        }

        if(private$gui$shiny_app_active==TRUE){
          private$load_reload_python_scripts()

          callback=list(callback,py$ReportAiforeducationShiny())
        }

        data_set_weights=datasets$Dataset$from_dict(
          reticulate::dict(list(
            sample_weights=sample_weights)
          )
        )
        #inputs, targets, sample_weights
        dataset_tf=train_data$add_column("sample_weights",data_set_weights["sample_weights"])
        dataset_tf=dataset_tf$rename_column('input', 'input_embeddings')

        #Choose correct target column and rename
        dataset_tf=dataset_tf$rename_column(target_column, 'targets')

        dataset_tf$with_format("tf")
        tf_dataset_train=dataset_tf$to_tf_dataset(
          columns=c("input_embeddings","sample_weights"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=TRUE,
          label_cols="targets")
        #Add sample weights
        tf_dataset_train=tf_dataset_train$map(py$extract_sample_weight)

        dataset_tf_val=val_data$rename_column('input', 'input_embeddings')
        #Choose correct target column and rename
        dataset_tf_val=dataset_tf_val$rename_column(target_column, 'targets')

        tf_dataset_val=dataset_tf_val$to_tf_dataset(
          columns=c("input_embeddings"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=FALSE,
          label_cols="targets")

        history<-self$model$fit(
          verbose=as.integer(self$last_training$config$keras_trace),
          x=tf_dataset_train,
          validation_data=tf_dataset_val,
          epochs = as.integer(self$last_training$config$epochs),
          callbacks = callback,
          class_weight=reticulate::py_dict(keys = names(class_weights),values = class_weights))$history

        if(self$model_config$n_categories==2){
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$binary_accuracy,history$val_binary_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        } else {
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$categorical_accuracy,history$val_categorical_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        }

        if(use_callback==TRUE){
          self$model$load_weights(paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"))
        }

        #PyTorch - Callbacks and training
      } else if(private$ml_framework=="pytorch"){

        dataset_train=train_data$select_columns(c("input",target_column))
        if(self$model_config$require_one_hot==TRUE){
          dataset_train=dataset_train$rename_column(target_column, "labels")
        }

        pytorch_train_data=dataset_train$with_format("torch")

        pytorch_val_data=val_data$select_columns(c("input",target_column))
        if(self$model_config$require_one_hot==TRUE){
          pytorch_val_data=pytorch_val_data$rename_column(target_column, "labels")
        }
        pytorch_val_data=pytorch_val_data$with_format("torch")

        if(!is.null(test_data)){
          pytorch_test_data=test_data$select_columns(c("input",target_column))
          if(self$model_config$require_one_hot==TRUE){
            pytorch_test_data=pytorch_test_data$rename_column(target_column, "labels")
          }
          pytorch_test_data=pytorch_test_data$with_format("torch")
        } else {
          pytorch_test_data=NULL
        }

        history=py$TeClassifierProtoNetTrain_PT_with_Datasets(
          model=self$model,
          loss_fct_name=loss_fct_name,
          optimizer_method=self$model_config$optimizer,
          Ns=as.integer(self$last_training$config$Ns),
          Nq=as.integer(self$last_training$config$Nq),
          loss_alpha=self$last_training$config$loss_alpha,
          loss_margin=self$last_training$config$loss_margin,
          trace=as.integer(self$last_training$config$pytorch_trace),
          use_callback=use_callback,
          train_data=pytorch_train_data,
          val_data=pytorch_val_data,
          test_data=pytorch_test_data,
          epochs = as.integer(self$last_training$config$epochs),
          filepath=paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.pt"),
          n_classes=as.integer(length(self$model_config$target_levels)),
          shiny_app_active=self$gui$shiny_app_active)
      }

      #Provide rownames for the history
      for(i in 1:length(history)){
          if(!is.null(history[[i]])){
              if(nrow(history[[i]])==2){
                rownames(history[[i]])=c("train","val")
              } else {
                rownames(history[[i]])=c("train","val","test")
              }
          }
      }
      return(history)
    }
  )
)

#'@title Text embedding classifier with a neural net
#'
#'@description Abstract class for neural nets with 'keras'/'tensorflow' and
#''pytorch'.
#'
#'@return Objects of this class are used for assigning texts to classes/categories. For
#'the creation and training of a classifier an object of class \link{EmbeddedText} and a \code{factor}
#'are necessary. The object of class \link{EmbeddedText} contains the numerical text
#'representations (text embeddings) of the raw texts generated by an object of class
#'\link{TextEmbeddingModel}. The \code{factor} contains the classes/categories for every
#'text. Missing values (unlabeled cases) are supported. For predictions an object of class
#'\link{EmbeddedText} has to be used which was created with the same text embedding model as
#'for training.
#'@family Classification
#'@export
TEClassifierRegular<-R6::R6Class(
  classname = "TEClassifierRegular",
  inherit = AIFEBaseModel,
  public = list(
    #'@field feature_extractor ('list()')\cr
    #'List for storing information and objects about the feature_extractor.
    feature_extractor=list(),

    #'@field reliability ('list()')\cr
    #'List for storing central reliability measures of the last training.
    #'\itemize{
    #'\item{\code{reliability$test_metric: }Array containing the reliability measures for the validation data for
    #'every fold and step (in case of pseudo-labeling).}
    #'\item{\code{reliability$test_metric_mean: }Array containing the reliability measures for the validation data for..
    #'The values represent the mean values for every fold.}
    #'\item{\code{reliability$raw_iota_objects: }List containing all iota_object generated with the package \code{iotarelr}
    #'for every fold at the end of the last training.}
    #'}
    #'\itemize{
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold.}
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end_free: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold. Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'\item{\code{reliability$iota_object_end: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold.}
    #'\item{\code{reliability$iota_object_end_free: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold. Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'}
    #'\itemize{
    #'\item{\code{reliability$standard_measures_end: }Object of class \code{list} containing the final
    #'measures for precision, recall, and f1 for every fold.}
    #'\item{\code{reliability$standard_measures_mean: }\code{matrix} containing the mean
    #'measures for precision, recall, and f1.}
    #'}
    #'
    reliability=list(
      test_metric=NULL,
      test_metric_mean=NULL,
      raw_iota_objects=list(
        iota_objects_end=NULL,
        iota_objects_end_free=NULL),
      iota_object_end=NULL,
      iota_object_end_free=NULL,
      standard_measures_end=NULL,
      standard_measures_mean=NULL
    ),

    #New-----------------------------------------------------------------------
    #'@description Creating a new instance of this class.
    #'@param ml_framework \code{string} Framework to use for training and inference.
    #'\code{ml_framework="tensorflow"} for 'tensorflow' and \code{ml_framework="pytorch"}
    #'for 'pytorch'
    #'@param name \code{Character} Name of the new classifier. Please refer to
    #'common name conventions. Free text can be used with parameter \code{label}.
    #'@param label \code{Character} Label for the new classifier. Here you can use
    #'free text.
    #'@param text_embeddings An object of class\code{TextEmbeddingModel}.
    #'@param feature_extractor Object of class \code{TEFeatureExtractor} which should be used in
    #'order to reduce the dimensionality of the text embeddings. If no feature extractor should be
    #'applied set \code{NULL}.
    #'@param targets \code{factor} containing the target values of the classifier.
    #'@param hidden \code{vector} containing the number of neurons for each dense layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec \code{vector} containing the number of neurons for each recurrent layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec_type \code{string} Type of the recurrent layers. \code{rec_type="gru"} for
    #'Gated Recurrent Unit and \code{rec_type="lstm"} for Long Short-Term Memory.
    #'@param rec_bidirectional \code{bool} If \code{TRUE} a bidirectional version of the reccurent
    #'layers is used.
    #'@param attention_type \code{string} Choose the relevant attention type. Possible values
    #'are \code{"fourier"} and \code{multihead}.
    #'@param self_attention_heads \code{integer} determining the number of attention heads
    #'for a self-attention layer. Only relevant if \code{attention_type="multihead"}
    #'@param repeat_encoder \code{int} determining how many times the encoder should be
    #'added to the network.
    #'@param intermediate_size \code{int} determining the size of the projection layer within
    #'a each transformer encoder.
    #'@param add_pos_embedding \code{bool} \code{TRUE} if positional embedding should be used.
    #'@param encoder_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout for the dense projection within the encoder layers.
    #'@param dense_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between dense layers.
    #'@param rec_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between bidirectional gru layers.
    #'@param recurrent_dropout \code{double} ranging between 0 and lower 1, determining the
    #'recurrent dropout for each recurrent layer. Only relevant for keras models.
    #'@param optimizer Object of class \code{keras.optimizers}.
    #'@return Returns an object of class \link{TextEmbeddingClassifierNeuralNet} which is ready for
    #'training.
    initialize=function(ml_framework=aifeducation_config$get_framework(),
                        name=NULL,
                        label=NULL,
                        text_embeddings=NULL,
                        feature_extractor=NULL,
                        targets=NULL,
                        hidden=c(128),
                        rec=c(128),
                        rec_type="gru",
                        rec_bidirectional=FALSE,
                        self_attention_heads=0,
                        intermediate_size=NULL,
                        attention_type="fourier",
                        add_pos_embedding=TRUE,
                        rec_dropout=0.1,
                        repeat_encoder=1,
                        dense_dropout=0.4,
                        recurrent_dropout=0.4,
                        encoder_dropout=0.1,
                        optimizer="adam"
    ){
      #Checking of parameters--------------------------------------------------
      #Setting ML Framework
      if((ml_framework %in% c("tensorflow","pytorch"))==FALSE) {
        stop("ml_framework must be 'tensorflow' or 'pytorch'.")
      }

      if(is.null(name)){
        stop("name is NULL but must be a character.")
      }
      if(is.null(label)){
        stop("label is NULL but must be a character.")
      }
      if(!("EmbeddedText" %in% class(text_embeddings))){
        stop("text_embeddings must be of class EmbeddedText.")
      }
      if(is.factor(targets)==FALSE){
        stop("targets must be of class factor.")
      }

      if(!(is.numeric(hidden)==TRUE | is.null(hidden)==TRUE)){
        stop("hidden must be a vector of integer or NULL.")
      }
      if(!(is.numeric(rec)==TRUE | is.null(rec)==TRUE)){
        stop("rec must be a vector of integer or NULL.")
      }
      if(is.integer(as.integer(self_attention_heads))==FALSE){
        stop("self_attention_heads must be an integer.")
      }

      if(optimizer %in% c("adam","rmsprop")==FALSE){
        stop("Optimzier must be 'adam' oder 'rmsprop'.")
      }

      if(attention_type %in% c("fourier","multihead")==FALSE){
        stop("Optimzier must be 'fourier' oder 'multihead'.")
      }
      if(repeat_encoder>0 & attention_type=="multihead" & self_attention_heads<=0){
        stop("Encoder layer is set to 'multihead'. This requires self_attention_heads>=1.")
      }

      use_fe=FALSE
      if(!is.null(feature_extractor)){
        if("TEFeatureExtractor"%in% class(feature_extractor)==FALSE){
          stop("Object passed to feature_extractor must be an object of class
               TEFeatureExtractor.")
        } else {
          use_fe=TRUE
        }
      }

      #------------------------------------------------------------------------
      private$ml_framework=ml_framework

      #Setting Label and Name-------------------------------------------------
      private$model_info$model_name_root=name
      private$model_info$model_name=paste0(private$model_info$model_name_root,"_ID_",generate_id(16))
      private$model_info$model_label=label

      #Basic Information of Input and Target Data
      variable_name_order<-dimnames(text_embeddings$embeddings)[[3]]
      target_levels_order<-levels(targets)

      model_info=text_embeddings$get_model_info()
      times=model_info$param_chunks
      features=dim(text_embeddings$embeddings)[3]

      private$text_embedding_model["model"]=list(model_info)
      private$text_embedding_model["times"]=times
      private$text_embedding_model["features"]=features

      if(is.null(rec) & self_attention_heads>0){
        if(features %% 2 !=0){
          stop("The number of features of the TextEmbeddingmodel is
               not a multiple of 2.")
        }
      }

      if(is.null(intermediate_size)==TRUE){
        if(attention_type=="fourier" & length(rec)>0){
          intermediate_size=2*rec[length(rec)]
        } else if(attention_type=="fourier" & length(rec)==0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)>0 & self_attention_heads>0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)==0 & self_attention_heads>0){
          intermediate_size=2*features
        } else {
          intermediate_size=NULL
        }
      }

      if(use_fe==TRUE){
        features=feature_extractor$model_config$features
      } else {
        features=private$text_embedding_model[["features"]]
      }

      #Saving Configuration
      config=list(
        use_fe=use_fe,
        features=features,
        times=private$text_embedding_model[["times"]],
        hidden=hidden,
        rec=rec,
        rec_type=rec_type,
        rec_bidirectional=rec_bidirectional,
        intermediate_size=intermediate_size,
        attention_type=attention_type,
        repeat_encoder=repeat_encoder,
        dense_dropout=dense_dropout,
        rec_dropout=rec_dropout,
        recurrent_dropout=recurrent_dropout,
        encoder_dropout=encoder_dropout,
        add_pos_embedding=add_pos_embedding,
        optimizer=optimizer,
        act_fct="gelu",
        rec_act_fct="tanh",
        self_attention_heads=self_attention_heads)

      if(length(target_levels_order)>2){
        #Multi Class
        config["act_fct_last"]="softmax"
        config["err_fct"]="categorical_crossentropy"
        config["metric"]="categorical_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      } else {
        #Binary Classification
        config["act_fct_last"]="sigmoid"
        config["err_fct"]="binary_crossentropy"
        config["metric"]="binary_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      }

      config["target_levels"]=list(target_levels_order)
      config["n_categories"]=list(length(target_levels_order))
      if(ml_framework=="tensorflow"){
        if(length(target_levels_order)>2){
          config["require_one_hot"]=list(TRUE)
        } else {
          config["require_one_hot"]=list(FALSE)
        }
      } else {
        config["require_one_hot"]=list(TRUE)
      }

      if(times>1|length(rec)>0|repeat_encoder>0){
        config["require_matrix_map"]=list(FALSE)
      } else {
        config["require_matrix_map"]=list(TRUE)
      }

      config["input_variables"]=list(variable_name_order)

      self$model_config=config

      #Create_Model------------------------------------------------------------
      private$create_reset_model()
      if(self$model_config$use_fe==TRUE){
        self$feature_extractor=feature_extractor$clone(deep=TRUE)
        #private$check_feature_extractor()
      }

      private$model_info$model_date=date()

      private$r_package_versions$aifeducation<-packageVersion("aifeducation")
      private$r_package_versions$reticulate<-packageVersion("reticulate")

      private$py_package_versions$tensorflow<-tf$version$VERSION
      private$py_package_versions$torch<-torch["__version__"]
      private$py_package_versions$keras<-keras["__version__"]
      private$py_package_versions$numpy<-np$version$short_version
    },

    #-------------------------------------------------------------------------
    #'@description Method for training a neural net.
    #'@param data_embeddings Object of class \code{TextEmbeddingModel}.
    #'@param data_targets \code{Factor} containing the labels for cases
    #'stored in \code{data_embeddings}. Factor must be named and has to use the
    #'same names used in \code{data_embeddings}.
    #'@param data_folds \code{int} determining the number of cross-fold
    #'samples.
    #'@param data_val_size \code{double} between 0 and 1, indicating the proportion of cases of each class
    #'which should be used for the validation sample during the estimation of the baseline model.
    #'The remaining cases are part of the training data.
    #'@param fe_val_size \code{double} between 0 and 1, indicating the proportion of cases
    #'which should be used for the validation sample during the training of the feature extractor.
    #'@param balance_class_weights \code{bool} If \code{TRUE} class weights are
    #'generated based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each class has the weight 1.
    #'@param balance_sequence_length \code{bool} If \code{TRUE} sample weights are
    #'generated for the length of sequences based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each sequences length has the weight 1.
    #'@param use_sc \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced synthetic cases. \code{FALSE} if not.
    #'@param sc_method \code{vector} containing the methods for generating
    #'synthetic cases via 'smotefamily'. Multiple methods can
    #'be passed. Currently \code{sc_method=c("adas")}, \code{sc_method=c("smote")}
    #'and \code{sc_method=c("dbsmote")} are possible.
    #'@param sc_min_k \code{int} determining the minimal number of k which is used
    #'for creating synthetic units.
    #'@param sc_max_k \code{int} determining the maximal number of k which is used
    #'for creating synthetic units.
    #'@param use_pl \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced pseudo-labeling. \code{FALSE} if not.
    #'@param pl_max_steps \code{int} determining the maximum number of steps during
    #'pseudo-labeling.
    #'@param pl_anchor \code{double} between 0 and 1 indicating the reference
    #'point for sorting the new cases of every label. See notes for more details.
    #'@param pl_max \code{double} between 0 and 1, setting the maximal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param pl_min \code{double} between 0 and 1, setting the minimal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param sustain_track \code{bool} If \code{TRUE} energy consumption is tracked
    #'during training via the python library codecarbon.
    #'@param sustain_iso_code \code{string} ISO code (Alpha-3-Code) for the country. This variable
    #'must be set if sustainability should be tracked. A list can be found on
    #'Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.
    #'@param sustain_region Region within a country. Only available for USA and
    #'Canada See the documentation of codecarbon for more information.
    #'\url{https://mlco2.github.io/codecarbon/parameters.html}
    #'@param sustain_interval \code{integer} Interval in seconds for measuring power
    #'usage.
    #'@param epochs \code{int} Number of training epochs.
    #'@param dir_checkpoint \code{string} Path to the directory where
    #'the checkpoint during training should be saved. If the directory does not
    #'exist, it is created.
    #'@param trace \code{bool} \code{TRUE}, if information about the estimation
    #'phase should be printed to the console.
    #'@param keras_trace \code{int} \code{keras_trace=0} does not print any
    #'information about the training process from keras on the console.
    #'@param pytorch_trace \code{int} \code{pytorch_trace=0} does not print any
    #'information about the training process from pytorch on the console.
    #'\code{pytorch_trace=1} prints a progress bar.
    #'@return Function does not return a value. It changes the object into a trained
    #'classifier.
    #'@details \itemize{
    #'
    #'\item{\code{sc_max_k: }All values from sc_min_k up to sc_max_k are successively used. If
    #'the number of sc_max_k is too high, the value is reduced to a number that
    #'allows the calculating of synthetic units.}
    #'
    #'\item{\code{pl_anchor: }With the help of this value, the new cases are sorted. For
    #'this aim, the distance from the anchor is calculated and all cases are arranged
    #'into an ascending order.
    #'}
    #'}
    #'@importFrom abind abind
    train=function(data_embeddings,
                   data_targets,
                   data_folds=5,
                   data_val_size=0.25,
                   balance_class_weights=TRUE,
                   balance_sequence_length=TRUE,
                   use_sc=TRUE,
                   sc_method="dbsmote",
                   sc_min_k=1,
                   sc_max_k=10,
                   use_pl=TRUE,
                   pl_max_steps=3,
                   pl_max=1.00,
                   pl_anchor=1.00,
                   pl_min=0.00,
                   sustain_track=TRUE,
                   sustain_iso_code=NULL,
                   sustain_region=NULL,
                   sustain_interval=15,
                   epochs=40,
                   batch_size=32,
                   dir_checkpoint,
                   trace=TRUE,
                   keras_trace=2,
                   pytorch_trace=1){

      #Checking Arguments------------------------------------------------------
      if(!("EmbeddedText" %in% class(data_embeddings))){
        stop("data_embeddings must be an object of class EmbeddedText")
      }

      if(self$check_embedding_model(data_embeddings)==FALSE){
        stop("The TextEmbeddingModel that generated the data_embeddings is not
               the same as the TextEmbeddingModel when generating the classifier.")
      }

      if(is.factor(data_targets)==FALSE){
        stop("data_targets must be a factor.")
      }
      if(is.null(names(data_targets))){
        stop("data_targets must be a named factor.")
      }

      if(pl_anchor<pl_min){
        stop("pl_anchor must be at least pl_min.")
      }
      if(pl_anchor>pl_max){
        stop("pl_anchor must be lower or equal to pl_max.")
      }

      if(data_folds<2){
        stop("data_folds must be at least 2.")
      }

      #Saving training configuration-------------------------------------------
      self$last_training$config$balance_class_weights=balance_class_weights
      self$last_training$config$balance_sequence_length=balance_sequence_length
      self$last_training$config$data_val_size=data_val_size
      self$last_training$config$use_sc=use_sc
      self$last_training$config$sc_method=sc_method
      self$last_training$config$sc_min_k=sc_min_k
      self$last_training$config$sc_max_k=sc_max_k
      self$last_training$config$use_pl=use_pl
      self$last_training$config$pl_max_steps=pl_max_steps
      self$last_training$config$pl_max=pl_max
      self$last_training$config$pl_anchor=pl_anchor
      self$last_training$config$pl_min=pl_min
      self$last_training$config$sustain_track=sustain_track
      self$last_training$config$sustain_iso_code=sustain_iso_code
      self$last_training$config$sustain_region=sustain_region
      self$last_training$config$sustain_interval=sustain_interval
      self$last_training$config$epochs=epochs
      self$last_training$config$batch_size=batch_size
      self$last_training$config$dir_checkpoint=dir_checkpoint
      self$last_training$config$trace=trace
      self$last_training$config$keras_trace=keras_trace
      self$last_training$config$pytorch_trace=pytorch_trace

      #Start-------------------------------------------------------------------
      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                      "Start"))
      }

      #Create DataManager------------------------------------------------------
      if(self$model_config$use_fe==TRUE){

        data_manager=DataManagerClassifier$new(
          data_embeddings=self$feature_extractor$extract_features(data_embeddings = data_embeddings,
                                                as.integer(batch_size=self$last_training$config$batch_size),
                                                return_r_object = TRUE),
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      } else {
        data_manager=DataManagerClassifier$new(
          data_embeddings=data_embeddings,
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      }

      #Save Data Statistics
      self$last_training$data=data_manager$get_statistics()

      #Save the number of folds
      self$last_training$config$n_folds=data_manager$get_n_folds()

      #Init Training------------------------------------------------------------
      private$init_train()

      #config datasets
      datasets$disable_progress_bars()
      #datasets$disable_caching()

      #SetUp GUI----------------------------------------------------------------
      private$init_gui(data_manager = data_manager)
      private$gui_inc_progressbar()

      #Start Sustainability Tracking-------------------------------------------
      if(sustain_track==TRUE){
        if(is.null(sustain_iso_code)==TRUE){
          stop("Sustainability tracking is activated but iso code for the
               country is missing. Add iso code or deactivate tracking.")
        }
        sustainability_tracker<-codecarbon$OfflineEmissionsTracker(
          country_iso_code=sustain_iso_code,
          region=sustain_region,
          tracking_mode="machine",
          log_level="warning",
          measure_power_secs=sustain_interval,
          save_to_file=FALSE,
          save_to_api=FALSE
        )
        sustainability_tracker$start()
      }

      #Update Progressbar-------------------------------------------------------
      private$gui_inc_progressbar()

      #Start Training----------------------------------------------------------
      #Load Custom Model Scripts
      private$load_reload_python_scripts()

      #Start Loop inclusive final training
      for(iter in 1:(self$last_training$config$n_folds+1)){
        base::gc(verbose = FALSE,full = TRUE)

        if(self$last_training$config$use_pl==FALSE){
          private$train_standard(iteration = iter,
                                 data_manager = data_manager,
                                 inc_synthetic = self$last_training$config$use_sc)
        } else if(self$last_training$config$use_pl==TRUE) {
          private$train_with_pseudo_labels(init_train=TRUE,
                                           iteration=iter,
                                           data_manager=data_manager,
                                           inc_synthetic=self$last_training$config$use_sc)
        }

        #Calculate measures on categorical level
        private$calculate_measures_on_categorical_level(
          data_manager=data_manager,
          iteration=iter)
        gc()
      }

      #Finalize Training
      private$finalize_train()

      #Stop sustainability tracking if requested
      if(sustain_track==TRUE){
        sustainability_tracker$stop()
        private$sustainability<-summarize_tracked_sustainability(sustainability_tracker)
      } else {
        private$sustainability=list(
          sustainability_tracked=FALSE,
          date=NA,
          sustainability_data=list(
            duration_sec=NA,
            co2eq_kg=NA,
            cpu_energy_kwh=NA,
            gpu_energy_kwh=NA,
            ram_energy_kwh=NA,
            total_energy_kwh=NA
          )
        )
      }

      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                      "Training Complete"))
      }
    },
    #-------------------------------------------------------------------------
    #'@description Method for predicting new data with a trained neural net.
    #'@param newdata Object of class \code{TextEmbeddingModel} or
    #'\code{data.frame} for which predictions should be made.
    #'@param verbose \code{int} \code{verbose=0} does not cat any
    #'information about the training process from keras on the console.
    #'\code{verbose=1} prints a progress bar. \code{verbose=2} prints
    #'one line of information for every epoch.
    #'@param batch_size \code{int} Size of batches.
    #'@return Returns a \code{data.frame} containing the predictions and
    #'the probabilities of the different labels for each case.
    predict=function(newdata,
                     batch_size=32,
                     verbose=1){

      #Load Custom Model Scripts
      private$load_reload_python_scripts()

      if("datasets.arrow_dataset.Dataset"%in%class(newdata)){
        newdata$set_format("np")
        real_newdata=newdata["input"]
        rownames(real_newdata)=newdata["id"]
      } else {
        if("EmbeddedText" %in% class(newdata)){
          if(self$check_embedding_model(newdata)==FALSE){
            stop("The TextEmbeddingModel that generated the newdata is not
               the same as the TextEmbeddingModel when generating the classifier.")
          }
          real_newdata=newdata$embeddings
        } else {
          real_newdata=newdata
        }
      if(self$model_config$use_fe==TRUE){
        real_newdata=self$feature_extractor$extract_features(data_embedding = real_newdata,
                                      batch_size=as.integer(batch_size),
                                      return_r_object = TRUE)$embeddings
      }
    }

      #Ensuring the correct order of the variables for prediction
      current_row_names=rownames(real_newdata)

      if(is.null(self$model_config$rec)==TRUE){
        n_rec=0
      } else {
        n_rec=length(self$model_config$rec)
      }

      if(n_rec==0 & self$model_config$repeat_encoder==0 & self$model_config$times==1){
        real_newdata=array_to_matrix(real_newdata)
      }

      model<-self$model

      if(private$ml_framework=="tensorflow"){
        if(length(self$model_config$target_levels)>2){
          #Multi Class
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))
          predictions<-max.col(predictions_prob)-1
        } else {
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))

          #Add Column for the second characteristic
          predictions=vector(length = length(predictions_prob))
          predictions_binary_prob<-matrix(ncol=2,
                                          nrow=length(predictions_prob))

          for(i in 1:length(predictions_prob)){
            if(predictions_prob[i]>=0.5){
              predictions_binary_prob[i,1]=1-predictions_prob[i]
              predictions_binary_prob[i,2]=predictions_prob[i]
              predictions[i]=1
            } else {
              predictions_binary_prob[i,1]=1-predictions_prob[i]
              predictions_binary_prob[i,2]=predictions_prob[i]
              predictions[i]=0
            }
          }
          predictions_prob<-predictions_binary_prob
        }
      } else if (private$ml_framework=="pytorch"){
        pytorch_predict_data=torch$utils$data$TensorDataset(
          torch$from_numpy(np$array(real_newdata)))

        if(torch$cuda$is_available()){
          device="cuda"
          dtype=torch$double
          model$to(device,dtype=dtype)
          model$eval()
          input=torch$from_numpy(np$array(real_newdata))
          predictions_prob<-model(input$to(device,dtype=dtype),
                                  predication_mode=TRUE)$detach()$cpu()$numpy()
        } else {
          device="cpu"
          dtype=torch$float
          model$to(device,dtype=dtype)
          model$eval()
          input=torch$from_numpy(np$array(real_newdata))
          predictions_prob<-model(input$to(device,dtype=dtype),
                                  predication_mode=TRUE)$detach()$numpy()
        }
        predictions<-max.col(predictions_prob)-1
      }



      #Transforming predictions to target levels
      predictions<-as.character(as.vector(predictions))
      for(i in 0:(length(self$model_config$target_levels)-1)){
        predictions<-replace(x=predictions,
                             predictions==as.character(i),
                             values=self$model_config$target_levels[i+1])
      }

      #Transforming to a factor
      predictions=factor(predictions,levels = self$model_config$target_levels)

      colnames(predictions_prob)=self$model_config$target_levels
      predictions_prob<-as.data.frame(predictions_prob)
      predictions_prob$expected_category=predictions
      rownames(predictions_prob)=current_row_names

      return(predictions_prob)

    }
  ),
  private = list(
    #--------------------------------------------------------------------------
    load_reload_python_scripts=function(){
      if(private$ml_framework=="tensorflow"){
        reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/keras_callbacks.py",
                                            package = "aifeducation"))
      } else if(private$ml_framework=="pytorch"){
        reticulate::py_run_file(system.file("python/pytorch_te_classifier_V2.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/pytorch_autoencoder.py",
                                            package = "aifeducation"))

      }
    },
    #--------------------------------------------------------------------------
    create_reset_model=function(){
      if(private$ml_framework=="tensorflow"){

        #Load custom layers
        private$load_reload_python_scripts()

        #Defining basic keras model
        layer_list=NULL

        if(is.null(self$model_config$rec)==TRUE){
          n_rec=0
        } else {
          n_rec=length(self$model_config$rec)
        }

        if(is.null(self$model_config$hidden)==TRUE){
          n_hidden=0
        } else {
          n_hidden=length(self$model_config$hidden)
        }

        #Adding Input Layer

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          model_input<-keras$layers$Input(shape=list(as.integer(self$model_config$times),as.integer(self$model_config$features)),
                                          name="input_embeddings")
        } else {
          model_input<-keras$layers$Input(shape=as.integer(self$model_config$times*self$model_config$features),
                                          name="input_embeddings")
        }
        layer_list[1]<-list(model_input)

        #Adding a Mask-Layer
        if(n_rec>0 | self$model_config$repeat_encoder>0){
          masking_layer<-keras$layers$Masking(
            mask_value = 0.0,
            name="masking_layer",
            input_shape=c(self$model_config$times,self$model_config$features),
            trainable=FALSE)(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(masking_layer)

          if(self$model_config$add_pos_embedding==TRUE){
            positional_embedding<-py$AddPositionalEmbedding(sequence_length = as.integer(self$model_config$times),
                                                            name="add_positional_embedding")(layer_list[[length(layer_list)]])
            layer_list[length(layer_list)+1]<-list(positional_embedding)
          }

          norm_layer<-keras$layers$LayerNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        } else {
          norm_layer<-keras$layers$BatchNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        }

        if(self$model_config$repeat_encoder>0){
          for(r in 1:self$model_config$repeat_encoder){
            if(self$model_config$attention_type=="multihead"){
              layer_list[length(layer_list)+1]<-list(
                py$TransformerEncoder(embed_dim = as.integer(self$model_config$features),
                                      dense_dim= as.integer(self$model_config$intermediate_size),
                                      num_heads =as.integer(self$model_config$self_attention_heads),
                                      dropout_rate=self$model_config$encoder_dropout,
                                      name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            } else if(self$model_config$attention_type=="fourier"){
              layer_list[length(layer_list)+1]<-list(
                py$FourierEncoder(dense_dim=as.integer(self$model_config$intermediate_size),
                                  dropout_rate=self$model_config$encoder_dropout,
                                  name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            }
          }
        }

        #Adding rec layer
        if(n_rec>0){
          if(self$model_config$rec_bidirectional==TRUE){
            for(i in 1:n_rec){
              if(self$model_config$rec_type=="gru"){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Bidirectional(
                    layer=keras$layers$GRU(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("gru_",i)),
                    name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              } else if (self$model_config$rec_type=="lstm"){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Bidirectional(
                    layer=keras$layers$LSTM(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("lstm",i)),
                    name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("lstm_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              }
            }
          } else {
            for(i in 1:n_rec){
              if(self$model_config$rec_type=="gru"){
                layer_list[length(layer_list)+1]<-list(
                    layer=keras$layers$GRU(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("gru_",i)),
                    name=paste0("bidirectional_",i)(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              } else if (self$model_config$rec_type=="lstm"){
                layer_list[length(layer_list)+1]<-list(
                    layer=keras$layers$LSTM(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("lstm",i)),
                    name=paste0("bidirectional_",i)(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("lstm_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              }
            }
          }
        }

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$GlobalAveragePooling1D(
              name="global_average_pooling")(layer_list[[length(layer_list)]]))
        }

        #Adding standard layer
        if(n_hidden>0){
          for(i in 1:n_hidden){
            layer_list[length(layer_list)+1]<-list(
              keras$layers$Dense(
                units = as.integer(self$model_config$hidden[i]),
                activation = "gelu",
                name=paste0("dense_",i))(layer_list[[length(layer_list)]]))

            if(i!=n_hidden){
              #Add Dropout_Layer
              layer_list[length(layer_list)+1]<-list(
                keras$layers$Dropout(
                  rate = self$model_config$dense_dropout,
                  name=paste0("dense_dropout_",i))(layer_list[[length(layer_list)]]))
            }
          }
        }

        #Adding final Layer
        if(length(self$model_config$target_levels)>2){
          #Multi Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(length(self$model_config$target_levels)),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        } else {
          #Binary Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(1),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        }

        #Creating Model
        model<-keras$Model(
          inputs = model_input,
          outputs = layer_list[length(layer_list)],
          name = self$model_config$name)

        self$model=model
      } else {
        #--------------------------------------------------------------------------
        #Load Custom Pytorch Objects and Functions
        private$load_reload_python_scripts()

        self$model=py$TextEmbeddingClassifier_PT(features=as.integer(self$model_config$features),
                                                 times=as.integer(self$model_config$times),
                                                 hidden=if(!is.null(self$model_config$hidden)){as.integer(self$model_config$hidden)}else{NULL},
                                                 rec=if(!is.null(self$model_config$rec)){as.integer(self$model_config$rec)}else{NULL},
                                                 rec_type=self$model_config$rec_type,
                                                 rec_bidirectional=self$model_config$rec_bidirectional,
                                                 intermediate_size=as.integer(self$model_config$intermediate_size),
                                                 attention_type=self$model_config$attention_type,
                                                 repeat_encoder=as.integer(self$model_config$repeat_encoder),
                                                 dense_dropout=self$model_config$dense_dropout,
                                                 rec_dropout=self$model_config$rec_dropout,
                                                 encoder_dropout=self$model_config$encoder_dropout,
                                                 add_pos_embedding=self$model_config$add_pos_embedding,
                                                 self_attention_heads=as.integer(self$model_config$self_attention_heads),
                                                 target_levels=self$model_config$target_levels)


      }
    },
    #--------------------------------------------------------------------------
    init_train=function(){
      #Setting a new ID for the classifier
      private$model_info$model_name=paste0(
        private$model_info$model_name_root,
        "_id_",
        generate_id(16))

      #Initializing Objects for Saving Performance
      metric_names=get_coder_metrics(
        true_values=NULL,
        predicted_values=NULL,
        return_names_only=TRUE)

      self$reliability$test_metric=matrix(
        nrow=self$last_training$config$n_folds,
        ncol=length(metric_names),
        dimnames = list(iterations=NULL,
                        metrics=metric_names))

      self$reliability$test_metric_mean=NULL

      self$reliability$iota_objects_end=NULL
      self$reliability$iota_objects_end_free=NULL

      self$reliability$iota_object_end=NULL
      self$reliability$iota_object_end_free=NULL

      standard_measures_mean_table<-matrix(
        nrow = length(self$model_config$target_levels),
        ncol = 3,
        data = 0)
      colnames(standard_measures_mean_table)=c("precision","recall","f1")
      rownames(standard_measures_mean_table)<-self$model_config$target_levels

      self$reliability$standard_measures_mean=standard_measures_mean_table

      #Save start time of training
      self$last_training$start_time=Sys.time()
    },
    #--------------------------------------------------------------------------
    calculate_test_metric=function(test_data,iteration,type){
      test_predictions=self$predict(newdata = test_data,
                                    verbose = self$last_training$config$keras_trace,
                                    batch_size =self$last_training$config$batch_size)
      test_pred_cat=test_predictions$expected_category
      names(test_pred_cat)=rownames(test_predictions)
      test_pred_cat<-test_pred_cat[test_data["id"]]
      test_res=get_coder_metrics(true_values = factor(x=test_data["labels"],
                                                      levels=0:(length(self$model_config$target_levels)-1),
                                                      labels=self$model_config$target_levels),
                                 predicted_values = test_pred_cat)

      #Save results
      self$reliability$test_metric[iteration,]<-test_res

      #Update GUI
      private$gui_inc_progressbar()
    },
    #--------------------------------------------------------------------------
    calculate_measures_on_categorical_level=function(data_manager,iteration){
      #Get test data
      data_manager$set_state(iteration = iteration,
                             step = NULL)
      test_data=data_manager$get_test_dataset()

      if(!is.null(test_data)==TRUE){
        #Predict labels
        test_predictions=self$predict(newdata = test_data,
                                      verbose = self$last_training$config$keras_trace,
                                      batch_size =self$last_training$config$batch_size)
        test_pred_cat=test_predictions$expected_category
        names(test_pred_cat)=rownames(test_predictions)
        test_pred_cat<-test_pred_cat[test_data["id"]]

        #Calculate standard measures
        self$reliability$standard_measures_end[iteration]=list(
          calc_standard_classification_measures(
            true_values=factor(x=test_data["labels"],
                               levels=0:(length(self$model_config$target_levels)-1),
                               labels=self$model_config$target_levels),
            predicted_values=test_pred_cat))

        #Calculate iota objects
        self$reliability$iota_objects_end[iteration]=list(iotarelr::check_new_rater(true_values = factor(x=test_data["labels"],
                                                                                                         levels=0:(length(self$model_config$target_levels)-1),
                                                                                                         labels=self$model_config$target_levels),
                                                                                    assigned_values = test_pred_cat,
                                                                                    free_aem = FALSE))
        self$reliability$iota_objects_end_free[iteration]=list(iotarelr::check_new_rater(true_values = factor(x=test_data["labels"],
                                                                                                              levels=0:(length(self$model_config$target_levels)-1),
                                                                                                              labels=self$model_config$target_levels),
                                                                                         assigned_values = test_pred_cat,
                                                                                         free_aem = TRUE))
      } else if(iteration<=data_manager$get_n_folds()) {
        warning("Unable to calculate test scores. There is no test data.")
      }
    },
    #--------------------------------------------------------------------------
    finalize_train=function(){
      #Save Final Information
      self$last_training$date=date()

      #Finalize measures from content analysis
      test_metric_mean=vector(length = ncol(self$reliability$test_metric))
      test_metric_mean[]=0
      names(test_metric_mean)=colnames(self$reliability$test_metric)

      n_mean=vector(length = ncol(self$reliability$test_metric))
      n_mean[]=self$last_training$config$n_folds

      for(i in 1:self$last_training$config$n_folds){
        for(j in 1:ncol(self$reliability$test_metric)){
          if(is.na(self$reliability$test_metric[i,j])==FALSE){
            test_metric_mean[j]=test_metric_mean[j]+self$reliability$test_metric[i,j]
          } else {
            n_mean[j]=n_mean[j]-1
          }
        }
      }

      test_metric_mean=test_metric_mean/n_mean
      test_metric_mean[is.nan(test_metric_mean)]=NA
      self$reliability$test_metric_mean=test_metric_mean

      self$last_training$learning_time=as.numeric(
        difftime(Sys.time(),
                 self$last_training$start_time,
                 units="mins"))

      #Finalize iota objects
      if(is.null(self$reliability$iota_objects_end)==FALSE){
        self$reliability$iota_object_end=create_iota2_mean_object(
          iota2_list = self$reliability$iota_objects_end,
          original_cat_labels = self$model_config$target_levels,
          free_aem=FALSE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end=NULL
      }

      if(is.null(self$reliability$iota_objects_end_free)==FALSE){
        self$reliability$iota_object_end_free=create_iota2_mean_object(
          iota2_list = self$reliability$iota_objects_end_free,
          original_cat_labels = self$model_config$target_levels,
          free_aem=TRUE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end_free=NULL
      }

      #Finalize standard measures
      standard_measures=self$reliability$standard_measures_mean
      for(i in 1:self$last_training$config$n_folds){
        for(tmp_cat in self$model_config$target_levels){
          standard_measures[tmp_cat,"precision"]=standard_measures[tmp_cat,"precision"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"precision"]
          standard_measures[tmp_cat,"recall"]=standard_measures[tmp_cat,"recall"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"recall"]
          standard_measures[tmp_cat,"f1"]=standard_measures[tmp_cat,"f1"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"f1"]
        }
      }
      self$reliability$standard_measures_mean<-standard_measures/self$last_training$config$n_folds
    },
    #--------------------------------------------------------------------------
    init_gui=function(data_manager){
      #Check for a running Shiny App and set the configuration
      #The Gui functions must be set in the server function of shiny globally
      if(requireNamespace("shiny",quietly=TRUE) & requireNamespace("shinyWidgets",quietly=TRUE)){
        if(shiny::isRunning()){
          private$gui$shiny_app_active=TRUE
        } else {
          private$gui$shiny_app_active=FALSE
        }
      } else {
        private$gui$shiny_app_active=FALSE
      }

      #SetUp Progressbar for UI
      private$gui$pgr_value=-1
      private$gui$pgr_max_value=data_manager$get_n_folds()+1+
        (data_manager$get_n_folds()+1)*self$last_training$config$use_pl*self$last_training$config$pl_max_steps

    },
    #--------------------------------------------------------------------------
    gui_inc_progressbar=function(){
      private$gui$pgr_value=private$gui$pgr_value+1
      update_aifeducation_progress_bar(value = private$gui$pgr_value,
                                       total = private$gui$pgr_max_value,
                                       title = "Train Classifier")
    },
    #--------------------------------------------------------------------------
    train_standard=function(iteration=NULL,
                            data_manager=NULL,
                            inc_synthetic=FALSE){

      #Print status message to console
      if(self$last_training$config$trace==TRUE){
        if(iteration<=self$last_training$config$n_folds){
          message(paste(
            date(),
            "|","Iteration",iteration,"from",self$last_training$config$n_folds
          ))
        } else {
          message(paste(
            date(),
            "|","Final training"
          ))
        }
      }

      #Set the state of the DataManager
      data_manager$set_state(
        iteration = iteration,
        step = NULL)

      #Generate syntetic cases if requested
      if(inc_synthetic==TRUE){
        data_manager$create_synthetic(
          trace=self$last_training$config$trace,
          inc_pseudo_data=FALSE
        )
      }

      #Get the different DataSets
      train_data=data_manager$get_dataset(
        inc_labeled = TRUE,
        inc_synthetic = inc_synthetic,
        inc_pseudo_data = FALSE,
        inc_unlabeled = FALSE)
      val_data=data_manager$get_val_dataset()
      if(iteration!="final"){
        test_data=data_manager$get_test_dataset()
      } else {
        test_data=NULL
      }

      #Print status to console
      if(self$last_training$config$trace==TRUE){
        if(iteration<=self$last_training$config$n_folds){
          message(paste(
            date(),
            "|","Iteration",iteration,"from",self$last_training$config$n_folds,
            "|","Training"
          ))
        } else {
          message(paste(
            date(),
            "|","Final training",
            "|","Training"
          ))
        }
      }

      #Start training
      train_history=private$basic_train(
        train_data = train_data,
        val_data = val_data,
        test_data = test_data,
        shiny_app_active = private$gui$shiny_app_active,
        reset_model = TRUE,
        use_callback = TRUE)

      #Save history
      self$last_training$history[iteration]=list(train_history)

      #Calculate test metric
      if(!is.null(test_data)==TRUE){
        private$calculate_test_metric(test_data=test_data,
                                      iteration = iteration,
                                      type = (as.numeric(inc_synthetic))+1)

      }
    },
    #--------------------------------------------------------------------------
    train_with_pseudo_labels=function(init_train=TRUE,
                                      iteration=NULL,
                                      data_manager=NULL,
                                      inc_synthetic=FALSE){

      #If model is not trained than train for the first time
      #Necessary for estimating pseudo labels
      if(init_train==TRUE){
        private$train_standard(iteration = iteration,
                               data_manager = data_manager,
                               inc_synthetic = inc_synthetic)
      }

      #Get validation and test data for training loop
      val_data=data_manager$get_val_dataset()
      if(iteration!="final"){
        test_data=data_manager$get_test_dataset()
      } else {
        test_data=NULL
      }

      #Start training loop with pseudo labels
      data_manager$set_state(iteration = iteration,
                             step = NULL)

      #Create list for saving training histories per step
      step_histories=NULL

      for (step in 1:self$last_training$config$pl_max_steps) {
        #Print status message to console
        if(self$last_training$config$trace==TRUE){
          if(iteration<=self$last_training$config$n_folds){
            message(paste(
              date(),
              "|","Iteration",iteration,"from",self$last_training$config$n_folds,
              "|","Pseudo labeling","step",step,"from",self$last_training$config$pl_max_steps
            ))
          } else {
            message(paste(
              date(),
              "|","Final training",
              "|","Pseudo labeling","step",step,"from",self$last_training$config$pl_max_steps
            ))
          }
        }

        #Set correct state for the data_manager
        data_manager$set_state(
          iteration = iteration,
          step = step)

        #Generate pseudo labels
        pseudo_data=private$estimate_pseudo_labels(
          unlabeled_data=data_manager$get_unlabeled_data(),
          current_step = step)

        #Save pseudo labels in the data_manager
        data_manager$add_replace_pseudo_data(
          inputs = pseudo_data$input,
          labels = pseudo_data$labels)

        #Remove old pseudo data
        rm(pseudo_data)

        #Generate synthetic data if requested
        if(inc_synthetic==TRUE){
          data_manager$create_synthetic(
            trace = self$last_training$config$trace,
            inc_pseudo_data = TRUE)
        }

        #Request training data
        train_data=data_manager$get_dataset(
          inc_labeled = TRUE,
          inc_synthetic = inc_synthetic,
          inc_pseudo_data = TRUE,
          inc_unlabeled = FALSE)

        #Print status to console
        if(self$last_training$config$trace==TRUE){
          if(iteration<=self$last_training$config$n_folds){
            message(paste(
              date(),
              "|","Iteration",iteration,"from",self$last_training$config$n_folds,
              "|","Training"
            ))
          } else {
            message(paste(
              date(),
              "|","Final training",
              "|","Training"
            ))
          }
        }

        #Start training
        train_history=private$basic_train(
          train_data = train_data,
          val_data = val_data,
          test_data = test_data,
          shiny_app_active = private$gui$shiny_app_active,
          reset_model = TRUE,
          use_callback = TRUE)

        #Save history
        step_histories[step]=list(train_history)

        #Update GUI
        private$gui_inc_progressbar()
      }

      #Save the histories for the complete iteration
      self$last_training$history[iteration]=list(step_histories)

      #Calculate test metric
      if(!is.null(test_data)==TRUE){
        private$calculate_test_metric(test_data=test_data,
                                      iteration = iteration,
                                      type = 3)

      }
    },
    #--------------------------------------------------------------------------
    estimate_pseudo_labels=function(unlabeled_data,
                                    current_step){

      #Predict pseudo labels for unlabeled data
      predicted_labels=self$predict(
        newdata=unlabeled_data,
        verbose=self$last_training$config$keras_trace,
        batch_size = self$last_training$config$batch_size)

      #Create Matrix for saving the results
      new_categories<-matrix(nrow= nrow(predicted_labels),
                             ncol=2)
      rownames(new_categories)=rownames(predicted_labels)
      colnames(new_categories)=c("cat","prob")

      #Gather information for every case. That is the category with the
      #highest probability and save both
      for(i in 1:nrow(predicted_labels)){
        tmp_est_prob=predicted_labels[i,1:(ncol(predicted_labels)-1)]
        new_categories[i,1]=which.max(tmp_est_prob)-1
        new_categories[i,2]=max(tmp_est_prob)
      }
      new_categories<-as.data.frame(new_categories)

      #Transforming the probabilities to an information index
      new_categories[,2]=abs(
        self$last_training$config$pl_anchor-
          (as.numeric(new_categories[,2])-1/length(self$model_config$target_levels))/(1-1/length(self$model_config$target_levels)))
      new_categories=as.data.frame(new_categories)

      #Reducing the new categories to the desired range
      condition=(new_categories[,2]>=self$last_training$config$pl_min &
                   new_categories[,2]<=self$last_training$config$pl_max)
      new_categories=subset(new_categories,condition)

      #Calculate number of cases to include
      bpl_inc_ratio=current_step/self$last_training$config$pl_max_steps
      n_cases_to_include=nrow(new_categories)*bpl_inc_ratio

      #Order cases with increasing distance from maximal information
      new_categories=new_categories[order(new_categories$prob,decreasing = FALSE),]

      #Select the best cases
      names_final_new_categories=rownames(new_categories)[1:n_cases_to_include]

      #Get the labels for these cases
      targets_pseudo_labeled<-new_categories[names_final_new_categories,1]
      targets_pseudo_labeled=as.numeric(targets_pseudo_labeled)
      names(targets_pseudo_labeled)<-names_final_new_categories

      #get the corresponding input
      unlabeled_data$set_format("np")
      embeddings=unlabeled_data["input"]
      rownames(embeddings)=unlabeled_data["id"]
      embeddings=embeddings[names_final_new_categories,,]

      #Return results
      pseudo_data=list(
        input=embeddings,
        labels=targets_pseudo_labeled)

      return(pseudo_data)
    },
    #--------------------------------------------------------------------------
    basic_train=function(train_data=NULL,
                         val_data=NULL,
                         test_data=NULL,
                         reset_model=FALSE,
                         use_callback=TRUE,
                         shiny_app_active=FALSE){

      #Clear session to provide enough resources for computations
      if(private$ml_framework=="tensorflow"){
        keras$backend$clear_session()
      } else if(private$ml_framework=="pytorch"){
        if(torch$cuda$is_available()){
          torch$cuda$empty_cache()
        }
      }

      #Generating class weights
      if(self$last_training$config$balance_class_weights==TRUE){
        abs_freq_classes=table(train_data["labels"])
        class_weights=as.vector(sum(abs_freq_classes)/(length(abs_freq_classes)*abs_freq_classes))
      } else {
        class_weights=rep(x=1,times=length(self$model_config$target_levels))
      }

      #Generating weights for sequence length
      if(self$last_training$config$balance_sequence_length==TRUE){
        sequence_length=train_data["length"]
        abs_freq_length=table(sequence_length)

        sample_weight_per_sequence_length=as.vector(sum(abs_freq_length)/(length(abs_freq_length)*abs_freq_length))
        sequence_order=names(abs_freq_length)

        sample_weights=vector(length = length(sequence_length))
        for(i in 1:length(sample_weights)){
          idx=which(sequence_length[i]==sequence_order)
          sample_weights[i]=sample_weight_per_sequence_length[idx]
        }
      } else {
        sequence_length=train_data["length"]
        sample_weights=rep.int(x=1,times=length(sequence_length))
      }

      #Reset model if requested
      if(reset_model==TRUE){
        private$create_reset_model()
      }

      #Set Optimizer
      if(private$ml_framework=="tensorflow"){
        balanced_metric=py$BalancedAccuracy(n_classes = as.integer(length(self$model_config$target_levels)))
        if(self$model_config$optimizer=="adam"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$Adam(),
            metrics=c(self$model_config$metric,balanced_metric))
        } else if (self$model_config$optimizer=="rmsprop"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$RMSprop(),
            metrics=c(self$model_config$metric,balanced_metric))
        }
      } else if(private$ml_framework=="pytorch"){
        loss_fct_name="CrossEntropyLoss"
        if(self$model_config$optimizer=="adam"){
          optimizer="adam"
        } else if (self$model_config$optimizer=="rmsprop"){
          optimizer="rmsprop"
        }
      }

      #Check directory for checkpoints
      if(dir.exists(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))==FALSE){
        if(self$last_training$config$trace==TRUE){
          message(paste(date(),"Creating Checkpoint Directory"))
        }
        dir.create(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))
      }

      #Set target column
      if(self$model_config$require_one_hot==FALSE){
        target_column="labels"
      } else {
        target_column="one_hot_encoding"
      }

      #Tensorflow - Callbacks and training
      if(private$ml_framework=="tensorflow"){
        if(use_callback==TRUE){
          callback=keras$callbacks$ModelCheckpoint(
            filepath = paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"),
            monitor = paste0("val_",self$model_config$balanced_metric),
            verbose = as.integer(min(self$last_training$config$keras_trace,1)),
            mode = "auto",
            save_best_only = TRUE,
            save_weights_only = TRUE)
        } else {
          callback=reticulate::py_none()
        }

        if(private$gui$shiny_app_active==TRUE){
          private$load_reload_python_scripts()

          callback=list(callback,py$ReportAiforeducationShiny())
        }

        data_set_weights=datasets$Dataset$from_dict(
          reticulate::dict(list(
            sample_weights=sample_weights)
          )
        )
        #inputs, targets, sample_weights
        dataset_tf=train_data$add_column("sample_weights",data_set_weights["sample_weights"])
        dataset_tf=dataset_tf$rename_column('input', 'input_embeddings')

        #Choose correct target column and rename
        dataset_tf=dataset_tf$rename_column(target_column, 'targets')

        dataset_tf$with_format("tf")
        tf_dataset_train=dataset_tf$to_tf_dataset(
          columns=c("input_embeddings","sample_weights"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=TRUE,
          label_cols="targets")
        #Add sample weights
        tf_dataset_train=tf_dataset_train$map(py$extract_sample_weight)

        dataset_tf_val=val_data$rename_column('input', 'input_embeddings')
        #Choose correct target column and rename
        dataset_tf_val=dataset_tf_val$rename_column(target_column, 'targets')

        tf_dataset_val=dataset_tf_val$to_tf_dataset(
          columns=c("input_embeddings"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=FALSE,
          label_cols="targets")

        history<-self$model$fit(
          verbose=as.integer(self$last_training$config$keras_trace),
          x=tf_dataset_train,
          validation_data=tf_dataset_val,
          epochs = as.integer(self$last_training$config$epochs),
          callbacks = callback,
          class_weight=reticulate::py_dict(keys = names(class_weights),values = class_weights))$history

        if(self$model_config$n_categories==2){
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$binary_accuracy,history$val_binary_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        } else {
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$categorical_accuracy,history$val_categorical_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        }

        if(use_callback==TRUE){
          self$model$load_weights(paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"))
        }

        #PyTorch - Callbacks and training
      } else if(private$ml_framework=="pytorch"){

        data_set_weights=datasets$Dataset$from_dict(
          reticulate::dict(list(
            sample_weights=sample_weights)
          )
        )

        dataset_train=train_data$add_column("sample_weights",data_set_weights["sample_weights"])
        dataset_train=dataset_train$select_columns(c("input",target_column,"sample_weights"))
        if(self$model_config$require_one_hot==TRUE){
          dataset_train=dataset_train$rename_column(target_column, "labels")
        }

        pytorch_train_data=dataset_train$with_format("torch")

        pytorch_val_data=val_data$select_columns(c("input",target_column))
        if(self$model_config$require_one_hot==TRUE){
          pytorch_val_data=pytorch_val_data$rename_column(target_column, "labels")
        }
        pytorch_val_data=pytorch_val_data$with_format("torch")

        if(!is.null(test_data)){
          pytorch_test_data=test_data$select_columns(c("input",target_column))
          if(self$model_config$require_one_hot==TRUE){
            pytorch_test_data=pytorch_test_data$rename_column(target_column, "labels")
          }
          pytorch_test_data=pytorch_test_data$with_format("torch")
        } else {
          pytorch_test_data=NULL
        }

        history=py$TeClassifierTrain_PT_with_Datasets(
          model=self$model,
          loss_fct_name=loss_fct_name,
          optimizer_method=self$model_config$optimizer,
          epochs=as.integer(self$last_training$config$epochs),
          trace=as.integer(self$last_training$config$pytorch_trace),
          use_callback=use_callback,
          batch_size=as.integer(self$last_training$config$batch_size),
          train_data=pytorch_train_data,
          val_data=pytorch_val_data,
          test_data=pytorch_test_data,
          filepath=paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.pt"),
          n_classes=as.integer(length(self$model_config$target_levels)),
          shiny_app_active=self$gui$shiny_app_active,
          class_weights=torch$tensor(np$array(class_weights)))
      }

      #Provide rownames for the history
      for(i in 1:length(history)){
        if(!is.null(history[[i]])){
          if(nrow(history[[i]])==2){
            rownames(history[[i]])=c("train","val")
          } else {
            rownames(history[[i]])=c("train","val","test")
          }
        }
      }
      return(history)
    }
  )
)

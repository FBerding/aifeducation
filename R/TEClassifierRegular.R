#'@title Text embedding classifier with a neural net
#'
#'@description Abstract class for neural nets with 'keras'/'tensorflow' and
#''pytorch'.
#'
#'@return Objects of this class are used for assigning texts to classes/categories. For
#'the creation and training of a classifier an object of class \link{EmbeddedText} and a \code{factor}
#'are necessary. The object of class \link{EmbeddedText} contains the numerical text
#'representations (text embeddings) of the raw texts generated by an object of class
#'\link{TextEmbeddingModel}. The \code{factor} contains the classes/categories for every
#'text. Missing values (unlabeled cases) are supported. For predictions an object of class
#'\link{EmbeddedText} has to be used which was created with the same text embedding model as
#'for training.
#'@family Classification
#'@export
TEClassifierRegular<-R6::R6Class(
  classname = "TEClassifierRegular",
  public = list(
    #'@field model ('tensorflow_model' or 'pytorch_model')\cr
    #'Field for storing the tensorflow or pytorch model after loading.
    model=NULL,

    #'@field model_config ('list()')\cr
    #'List for storing information about the configuration of the model.
    model_config=list(),

    #'@field feature_extractor ('list()')\cr
    #'List for storing information and objects about the feature_extractor.
    feature_extractor=list(),

    #'@field last_training ('list()')\cr
    #'List for storing the history, the configuration, and the results of the last training. This
    #'information will be overwritten if a new training is started.
    #'\itemize{
    #'\item{\code{last_training$start_time: }Time point when training started.}
    #'\item{\code{last_training$learning_time: }Duration of the training process.}
    #'\item{\code{last_training$finish_time: }Time when the last training finished.}
    #'\item{\code{last_training$history: }History of the last training.}
    #'\item{\code{last_training$data: }Object of class \code{table} storing the initial frequencies of the passed data.}
    #'\item{\code{last_training$config: }List storing the configuration used for the last training.}
    #'}
    last_training=list(
      learning_time=NULL,
      start_time=NA,
      history=list(),
      data=NULL,
      finish_time=NULL,
      config=list()
    ),

    #'@field reliability ('list()')\cr
    #'List for storing central reliability measures of the last training.
    #'\itemize{
    #'\item{\code{reliability$test_metric: }Array containing the reliability measures for the validation data for
    #'every fold and step (in case of pseudo-labeling).}
    #'\item{\code{reliability$test_metric_mean: }Array containing the reliability measures for the validation data for..
    #'The values represent the mean values for every fold.}
    #'\item{\code{reliability$raw_iota_objects: }List containing all iota_object generated with the package \code{iotarelr}
    #'for every fold at the end of the last training.}
    #'}
    #'\itemize{
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold.}
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end_free: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold. Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'\item{\code{reliability$iota_object_end: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold.}
    #'\item{\code{reliability$iota_object_end_free: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold. Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'}
    #'\itemize{
    #'\item{\code{reliability$standard_measures_end: }Object of class \code{list} containing the final
    #'measures for precision, recall, and f1 for every fold.}
    #'\item{\code{reliability$standard_measures_mean: }\code{matrix} containing the mean
    #'measures for precision, recall, and f1.}
    #'}
    #'
    reliability=list(
      test_metric=NULL,
      test_metric_mean=NULL,
      raw_iota_objects=list(
        iota_objects_end=NULL,
        iota_objects_end_free=NULL),
      iota_object_end=NULL,
      iota_object_end_free=NULL,
      standard_measures_end=NULL,
      standard_measures_mean=NULL
    ),

    #New-----------------------------------------------------------------------
    #'@description Creating a new instance of this class.
    #'@param ml_framework \code{string} Framework to use for training and inference.
    #'\code{ml_framework="tensorflow"} for 'tensorflow' and \code{ml_framework="pytorch"}
    #'for 'pytorch'
    #'@param name \code{Character} Name of the new classifier. Please refer to
    #'common name conventions. Free text can be used with parameter \code{label}.
    #'@param label \code{Character} Label for the new classifier. Here you can use
    #'free text.
    #'@param text_embeddings An object of class\code{TextEmbeddingModel}.
    #'@param use_fe \code{bool} If \code{TRUE} a feature extractor is applied in
    #'order to reduce the dimensionality of the text embeddings.
    #'@param fe_features \code{int} determining the number of dimensions to which
    #'the dimension of the text embedding should be reduced.
    #'@param fe_method \code{string} Method to use for the feature extraction.
    #'\code{"lstm"} for an extractor based on LSTM-layers or \code{"dense"} for
    #'dense layers.
    #'@param fe_noise_factor \code{double} between 0 and a value lower 1 indicating
    #'how much noise should be added for the training of the feature extractor.
    #'@param targets \code{factor} containing the target values of the classifier.
    #'@param hidden \code{vector} containing the number of neurons for each dense layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec \code{vector} containing the number of neurons for each recurrent layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec_type \code{string} Type of the recurrent layers. \code{rec_type="gru"} for
    #'Gated Recurrent Unit and \code{rec_type="lstm"} for Long Short-Term Memory.
    #'@param rec_bidirectional \code{bool} If \code{TRUE} a bidirectional version of the reccurent
    #'layers is used.
    #'@param attention_type \code{string} Choose the relevant attention type. Possible values
    #'are \code{"fourier"} and \code{multihead}.
    #'@param self_attention_heads \code{integer} determining the number of attention heads
    #'for a self-attention layer. Only relevant if \code{attention_type="multihead"}
    #'@param repeat_encoder \code{int} determining how many times the encoder should be
    #'added to the network.
    #'@param intermediate_size \code{int} determining the size of the projection layer within
    #'a each transformer encoder.
    #'@param add_pos_embedding \code{bool} \code{TRUE} if positional embedding should be used.
    #'@param encoder_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout for the dense projection within the encoder layers.
    #'@param dense_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between dense layers.
    #'@param rec_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between bidirectional gru layers.
    #'@param recurrent_dropout \code{double} ranging between 0 and lower 1, determining the
    #'recurrent dropout for each recurrent layer. Only relevant for keras models.
    #'@param optimizer Object of class \code{keras.optimizers}.
    #'@return Returns an object of class \link{TextEmbeddingClassifierNeuralNet} which is ready for
    #'training.
    initialize=function(ml_framework=aifeducation_config$get_framework(),
                        name=NULL,
                        label=NULL,
                        text_embeddings=NULL,
                        use_fe=TRUE,
                        fe_features=128,
                        fe_method="lstm",
                        fe_noise_factor=0.2,
                        targets=NULL,
                        hidden=c(128),
                        rec=c(128),
                        rec_type="gru",
                        rec_bidirectional=FALSE,
                        self_attention_heads=0,
                        intermediate_size=NULL,
                        attention_type="fourier",
                        add_pos_embedding=TRUE,
                        rec_dropout=0.1,
                        repeat_encoder=1,
                        dense_dropout=0.4,
                        recurrent_dropout=0.4,
                        encoder_dropout=0.1,
                        optimizer="adam"
    ){
      #Checking of parameters--------------------------------------------------
      if(is.null(name)){
        stop("name is NULL but must be a character.")
      }
      if(is.null(label)){
        stop("label is NULL but must be a character.")
      }
      if(!("EmbeddedText" %in% class(text_embeddings))){
        stop("text_embeddings must be of class EmbeddedText.")
      }
      if(is.factor(targets)==FALSE){
        stop("targets must be of class factor.")
      }

      if(!(is.numeric(hidden)==TRUE | is.null(hidden)==TRUE)){
        stop("hidden must be a vector of integer or NULL.")
      }
      if(!(is.numeric(rec)==TRUE | is.null(rec)==TRUE)){
        stop("rec must be a vector of integer or NULL.")
      }
      if(is.integer(as.integer(self_attention_heads))==FALSE){
        stop("self_attention_heads must be an integer.")
      }

      if(optimizer %in% c("adam","rmsprop")==FALSE){
        stop("Optimzier must be 'adam' oder 'rmsprop'.")
      }

      if(attention_type %in% c("fourier","multihead")==FALSE){
        stop("Optimzier must be 'fourier' oder 'multihead'.")
      }
      if(repeat_encoder>0 & attention_type=="multihead" & self_attention_heads<=0){
        stop("Encoder layer is set to 'multihead'. This requires self_attention_heads>=1.")
      }

      #------------------------------------------------------------------------

      #Setting ML Framework
      if((ml_framework %in% c("tensorflow","pytorch"))==FALSE) {
        stop("ml_framework must be 'tensorflow' or 'pytorch'.")
      }

      private$ml_framework=ml_framework

      #Setting Label and Name-------------------------------------------------
      private$model_info$model_name_root=name
      private$model_info$model_name=paste0(private$model_info$model_name_root,"_ID_",generate_id(16))
      private$model_info$model_label=label

      #Basic Information of Input and Target Data
      variable_name_order<-dimnames(text_embeddings$embeddings)[[3]]
      target_levels_order<-levels(targets)

      model_info=text_embeddings$get_model_info()
      times=model_info$param_chunks
      features=dim(text_embeddings$embeddings)[3]

      private$text_embedding_model["model"]=list(model_info)
      private$text_embedding_model["times"]=times
      private$text_embedding_model["features"]=features

      if(is.null(rec) & self_attention_heads>0){
        if(features %% 2 !=0){
          stop("The number of features of the TextEmbeddingmodel is
               not a multiple of 2.")
        }
      }

      if(is.null(intermediate_size)==TRUE){
        if(attention_type=="fourier" & length(rec)>0){
          intermediate_size=2*rec[length(rec)]
        } else if(attention_type=="fourier" & length(rec)==0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)>0 & self_attention_heads>0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)==0 & self_attention_heads>0){
          intermediate_size=2*features
        } else {
          intermediate_size=NULL
        }
      }

      if(use_fe==TRUE){
        features=fe_features
      } else {
        features=private$text_embedding_model[["features"]]
      }

      #Saving Configuration
      config=list(
        use_fe=use_fe,
        fe_method=fe_method,
        fe_noise_factor=fe_noise_factor,
        features=features,
        times=private$text_embedding_model[["times"]],
        hidden=hidden,
        rec=rec,
        rec_type=rec_type,
        rec_bidirectional=rec_bidirectional,
        intermediate_size=intermediate_size,
        attention_type=attention_type,
        repeat_encoder=repeat_encoder,
        dense_dropout=dense_dropout,
        rec_dropout=rec_dropout,
        recurrent_dropout=recurrent_dropout,
        encoder_dropout=encoder_dropout,
        add_pos_embedding=add_pos_embedding,
        optimizer=optimizer,
        act_fct="gelu",
        rec_act_fct="tanh",
        self_attention_heads=self_attention_heads)

      if(length(target_levels_order)>2){
        #Multi Class
        config["act_fct_last"]="softmax"
        config["err_fct"]="categorical_crossentropy"
        config["metric"]="categorical_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      } else {
        #Binary Classification
        config["act_fct_last"]="sigmoid"
        config["err_fct"]="binary_crossentropy"
        config["metric"]="binary_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      }

      config["target_levels"]=list(target_levels_order)
      config["n_categories"]=list(length(target_levels_order))
      if(ml_framework=="tensorflow"){
        if(length(target_levels_order)>2){
          config["require_one_hot"]=list(TRUE)
        } else {
          config["require_one_hot"]=list(FALSE)
        }
      } else {
        config["require_one_hot"]=list(TRUE)
      }

      if(times>1|length(rec)>0|repeat_encoder>0){
        config["require_matrix_map"]=list(FALSE)
      } else {
        config["require_matrix_map"]=list(TRUE)
      }

      config["input_variables"]=list(variable_name_order)

      self$model_config=config

      #Create_Model------------------------------------------------------------
      private$create_reset_model()
      if(self$model_config$use_fe==TRUE){
        self$feature_extractor$model=private$create_feature_extractor()
      }

      private$model_info$model_date=date()

      private$r_package_versions$aifeducation<-packageVersion("aifeducation")
      private$r_package_versions$reticulate<-packageVersion("reticulate")

      private$py_package_versions$tensorflow<-tf$version$VERSION
      private$py_package_versions$torch<-torch["__version__"]
      private$py_package_versions$keras<-keras["__version__"]
      private$py_package_versions$numpy<-np$version$short_version
    },

    #-------------------------------------------------------------------------
    #'@description Method for training a neural net.
    #'@param data_embeddings Object of class \code{TextEmbeddingModel}.
    #'@param data_targets \code{Factor} containing the labels for cases
    #'stored in \code{data_embeddings}. Factor must be named and has to use the
    #'same names used in \code{data_embeddings}.
    #'@param data_folds \code{int} determining the number of cross-fold
    #'samples.
    #'@param data_val_size \code{double} between 0 and 1, indicating the proportion of cases of each class
    #'which should be used for the validation sample during the estimation of the baseline model.
    #'The remaining cases are part of the training data.
    #'@param fe_val_size \code{double} between 0 and 1, indicating the proportion of cases
    #'which should be used for the validation sample during the training of the feature extractor.
    #'@param balance_class_weights \code{bool} If \code{TRUE} class weights are
    #'generated based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each class has the weight 1.
    #'@param balance_sequence_length \code{bool} If \code{TRUE} sample weights are
    #'generated for the length of sequences based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each sequences length has the weight 1.
    #'@param use_sc \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced synthetic cases. \code{FALSE} if not.
    #'@param sc_method \code{vector} containing the methods for generating
    #'synthetic cases via 'smotefamily'. Multiple methods can
    #'be passed. Currently \code{sc_method=c("adas")}, \code{sc_method=c("smote")}
    #'and \code{sc_method=c("dbsmote")} are possible.
    #'@param sc_min_k \code{int} determining the minimal number of k which is used
    #'for creating synthetic units.
    #'@param sc_max_k \code{int} determining the maximal number of k which is used
    #'for creating synthetic units.
    #'@param use_pl \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced pseudo-labeling. \code{FALSE} if not.
    #'@param pl_max_steps \code{int} determining the maximum number of steps during
    #'pseudo-labeling.
    #'@param pl_anchor \code{double} between 0 and 1 indicating the reference
    #'point for sorting the new cases of every label. See notes for more details.
    #'@param pl_max \code{double} between 0 and 1, setting the maximal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param pl_min \code{double} between 0 and 1, setting the minimal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param sustain_track \code{bool} If \code{TRUE} energy consumption is tracked
    #'during training via the python library codecarbon.
    #'@param sustain_iso_code \code{string} ISO code (Alpha-3-Code) for the country. This variable
    #'must be set if sustainability should be tracked. A list can be found on
    #'Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.
    #'@param sustain_region Region within a country. Only available for USA and
    #'Canada See the documentation of codecarbon for more information.
    #'\url{https://mlco2.github.io/codecarbon/parameters.html}
    #'@param sustain_interval \code{integer} Interval in seconds for measuring power
    #'usage.
    #'@param epochs \code{int} Number of training epochs.
    #'@param fe_epochs \code{int} Number of training epochs for the feature extractor.
    #'@param batch_size \code{int} Size of batches.
    #'@param dir_checkpoint \code{string} Path to the directory where
    #'the checkpoint during training should be saved. If the directory does not
    #'exist, it is created.
    #'@param trace \code{bool} \code{TRUE}, if information about the estimation
    #'phase should be printed to the console.
    #'@param keras_trace \code{int} \code{keras_trace=0} does not print any
    #'information about the training process from keras on the console.
    #'@param pytorch_trace \code{int} \code{pytorch_trace=0} does not print any
    #'information about the training process from pytorch on the console.
    #'\code{pytorch_trace=1} prints a progress bar.
    #'@return Function does not return a value. It changes the object into a trained
    #'classifier.
    #'@details \itemize{
    #'
    #'\item{\code{sc_max_k: }All values from sc_min_k up to sc_max_k are successively used. If
    #'the number of sc_max_k is too high, the value is reduced to a number that
    #'allows the calculating of synthetic units.}
    #'
    #'\item{\code{pl_anchor: }With the help of this value, the new cases are sorted. For
    #'this aim, the distance from the anchor is calculated and all cases are arranged
    #'into an ascending order.
    #'}
    #'}
    #'@importFrom abind abind
    train=function(data_embeddings,
                   data_targets,
                   data_folds=5,
                   data_val_size=0.25,
                   balance_class_weights=TRUE,
                   balance_sequence_length=TRUE,
                   use_sc=TRUE,
                   sc_method="dbsmote",
                   sc_min_k=1,
                   sc_max_k=10,
                   use_pl=TRUE,
                   pl_max_steps=3,
                   pl_max=1.00,
                   pl_anchor=1.00,
                   pl_min=0.00,
                   sustain_track=TRUE,
                   sustain_iso_code=NULL,
                   sustain_region=NULL,
                   sustain_interval=15,
                   epochs=40,
                   fe_epochs=1000,
                   fe_val_size=0.25,
                   batch_size=32,
                   dir_checkpoint,
                   trace=TRUE,
                   keras_trace=2,
                   pytorch_trace=1){

      #Checking Arguments------------------------------------------------------
      if(!("EmbeddedText" %in% class(data_embeddings))){
        stop("data_embeddings must be an object of class EmbeddedText")
      }

      if(self$check_embedding_model(data_embeddings)==FALSE){
        stop("The TextEmbeddingModel that generated the data_embeddings is not
               the same as the TextEmbeddingModel when generating the classifier.")
      }

      if(is.factor(data_targets)==FALSE){
        stop("data_targets must be a factor.")
      }
      if(is.null(names(data_targets))){
        stop("data_targets must be a named factor.")
      }

      if(pl_anchor<pl_min){
        stop("pl_anchor must be at least pl_min.")
      }
      if(pl_anchor>pl_max){
        stop("pl_anchor must be lower or equal to pl_max.")
      }

      if(data_folds<2){
        stop("data_folds must be at least 2.")
      }

      #Saving training configuration-------------------------------------------
      self$last_training$config$balance_class_weights=balance_class_weights
      self$last_training$config$balance_sequence_length=balance_sequence_length
      self$last_training$config$data_val_size=data_val_size
      self$last_training$config$use_sc=use_sc
      self$last_training$config$sc_method=sc_method
      self$last_training$config$sc_min_k=sc_min_k
      self$last_training$config$sc_max_k=sc_max_k
      self$last_training$config$use_pl=use_pl
      self$last_training$config$pl_max_steps=pl_max_steps
      self$last_training$config$pl_max=pl_max
      self$last_training$config$pl_anchor=pl_anchor
      self$last_training$config$pl_min=pl_min
      self$last_training$config$sustain_track=sustain_track
      self$last_training$config$sustain_iso_code=sustain_iso_code
      self$last_training$config$sustain_region=sustain_region
      self$last_training$config$sustain_interval=sustain_interval
      self$last_training$config$epochs=epochs
      self$last_training$config$batch_size=batch_size
      self$last_training$config$dir_checkpoint=dir_checkpoint
      self$last_training$config$trace=trace
      self$last_training$config$keras_trace=keras_trace
      self$last_training$config$pytorch_trace=pytorch_trace

      self$feature_extractor$val_size=fe_val_size
      self$feature_extractor$epochs=fe_epochs

      #Start-------------------------------------------------------------------
      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                      "Start"))
      }

      #Create DataManager------------------------------------------------------
      if(self$model_config$use_fe==TRUE){
        private$train_feature_extractor(data_embeddings = data_embeddings)

        data_manager=DataManagerClassifier$new(
          data_embeddings=self$extract_features(data_embeddings = data_embeddings,
                                                as.integer(batch_size=self$last_training$config$batch_size),
                                                return_r_object = TRUE),
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      } else {
        data_manager=DataManagerClassifier$new(
          data_embeddings=data_embeddings,
          data_targets=data_targets,
          folds=data_folds,
          val_size=self$last_training$config$data_val_size,
          class_levels=self$model_config$target_levels,
          one_hot_encoding=self$model_config$require_one_hot,
          add_matrix_map=if(self$model_config$require_matrix_map==TRUE|self$last_training$config$use_sc==TRUE){TRUE}else{FALSE},
          sc_method=sc_method,
          sc_min_k=sc_min_k,
          sc_max_k=sc_max_k,
          trace=trace)
      }

      #Save Data Statistics
      self$last_training$data=data_manager$get_statistics()

      #Save the number of folds
      self$last_training$config$n_folds=data_manager$get_n_folds()

      #Init Training------------------------------------------------------------
      private$init_train()

      #config datasets
      datasets$disable_progress_bars()
      #datasets$disable_caching()

      #SetUp GUI----------------------------------------------------------------
      private$init_gui(data_manager = data_manager)
      private$gui_inc_progressbar()

      #Start Sustainability Tracking-------------------------------------------
      if(sustain_track==TRUE){
        if(is.null(sustain_iso_code)==TRUE){
          stop("Sustainability tracking is activated but iso code for the
               country is missing. Add iso code or deactivate tracking.")
        }
        sustainability_tracker<-codecarbon$OfflineEmissionsTracker(
          country_iso_code=sustain_iso_code,
          region=sustain_region,
          tracking_mode="machine",
          log_level="warning",
          measure_power_secs=sustain_interval,
          save_to_file=FALSE,
          save_to_api=FALSE
        )
        sustainability_tracker$start()
      }

      #Update Progressbar-------------------------------------------------------
      private$gui_inc_progressbar()

      #Start Training----------------------------------------------------------
      #Load Custom Model Scripts
      private$load_reload_python_scripts()

      #Start Loop inclusive final training
      for(iter in 1:(self$last_training$config$n_folds+1)){
        base::gc(verbose = FALSE,full = TRUE)

        if(self$last_training$config$use_pl==FALSE){
          private$train_standard(iteration = iter,
                                 data_manager = data_manager,
                                 inc_synthetic = self$last_training$config$use_sc)
        } else if(self$last_training$config$use_pl==TRUE) {
          private$train_with_pseudo_labels(init_train=TRUE,
                                           iteration=iter,
                                           data_manager=data_manager,
                                           inc_synthetic=self$last_training$config$use_sc)
        }

        #Calculate measures on categorical level
        private$calculate_measures_on_categorical_level(
          data_manager=data_manager,
          iteration=iter)
        gc()
      }

      #Finalize Training
      private$finalize_train()

      #Stop sustainability tracking if requested
      if(sustain_track==TRUE){
        sustainability_tracker$stop()
        private$sustainability<-summarize_tracked_sustainability(sustainability_tracker)
      } else {
        private$sustainability=list(
          sustainability_tracked=FALSE,
          date=NA,
          sustainability_data=list(
            duration_sec=NA,
            co2eq_kg=NA,
            cpu_energy_kwh=NA,
            gpu_energy_kwh=NA,
            ram_energy_kwh=NA,
            total_energy_kwh=NA
          )
        )
      }

      if(self$last_training$config$trace==TRUE){
        message(paste(date(),
                      "Training Complete"))
      }
    },
    #--------------------------------------------------------------------------
    extract_features=function(data_embeddings,batch_size,return_r_object=TRUE){
      if(self$model_config$use_fe==TRUE){
        #Prepare data set
        if("EmbeddedText" %in% class(data_embeddings)){
          if(nrow(data_embeddings$embeddings)>1){
            extractor_dataset=datasets$Dataset$from_dict(
              reticulate::dict(
                list(id=rownames(data_embeddings$embeddings),
                     input=np$squeeze(np$split(reticulate::np_array(data_embeddings$embeddings),as.integer(nrow(data_embeddings$embeddings)),axis=0L))),
                convert = FALSE))
          } else {
            extractor_dataset=data_embeddings$embeddings
          }

        } else if("array" %in% class(data_embeddings)){
          if(nrow(data_embeddings)>1){
            extractor_dataset=datasets$Dataset$from_dict(
              reticulate::dict(
                list(id=rownames(data_embeddings),
                     input=np$squeeze(np$split(reticulate::np_array(data_embeddings),as.integer(nrow(data_embeddings)),axis=0L))),
                convert = FALSE))
          } else {
            extractor_dataset=data_embeddings
          }
        }

        #Extract features
        if(private$ml_framework=="pytorch"){
          if(torch$cuda$is_available()){
            device="cuda"
            dtype=torch$double

            if("datasets.arrow_dataset.Dataset"%in%class(extractor_dataset)){
              extractor_dataset$set_format("torch",device=device)
              self$feature_extractor$model$to(device,dtype=dtype)
              self$feature_extractor$model$eval()
              reduced_embeddings<-self$feature_extractor$model(extractor_dataset["input"],
                                                               encoder_mode=TRUE)$detach()$cpu()$numpy()
            } else {
              self$feature_extractor$model$to(device,dtype=dtype)
              self$feature_extractor$model$eval()
              input=torch$from_numpy(np$array(extractor_dataset))
              reduced_embeddings<-self$feature_extractor$model(input$to(device,dtype=dtype),
                                                               encoder_mode=TRUE)$detach()$cpu()$numpy()
            }
          } else {
            device="cpu"
            dtype=torch$float
            if("datasets.arrow_dataset.Dataset"%in%class(extractor_dataset)){
              extractor_dataset$set_format("torch",device=device)

              self$feature_extractor$model$to(device,dtype=dtype)
              self$feature_extractor$model$eval()
              reduced_embeddings<-self$feature_extractor$model(extractor_dataset["input"],
                                                               encoder_mode=TRUE)$detach()$numpy()
            } else {
              self$feature_extractor$model$to(device,dtype=dtype)
              self$feature_extractor$model$eval()
              input=torch$from_numpy(np$array(extractor_dataset))
              reduced_embeddings<-self$feature_extractor$model(input$to(device,dtype=dtype),
                                                               encoder_mode=TRUE)$detach()$numpy()
            }
          }
        }

        #Prepare output
        if(return_r_object==TRUE){
          reduced_embeddings=reduced_embeddings
          if("datasets.arrow_dataset.Dataset"%in%class(extractor_dataset)){
            rownames(reduced_embeddings)=extractor_dataset["id"]
          } else {
            rownames(reduced_embeddings)=rownames(data_embeddings)
          }


          model_info=self$get_text_embedding_model()

          reduced_embeddings=EmbeddedText$new(
            model_name=paste0("feature_extracted_",model_info$model_name),
            model_label=model_info$model_label,
            model_date=model_info$model_date,
            model_method=model_info$model_method,
            model_version=model_info$model_version,
            model_language=model_info$model_language,
            param_seq_length=model_info$param_seq_length,
            param_chunks=model_info$param_chunks,
            param_overlap=model_info$param_overlap,
            param_emb_layer_min=model_info$param_emb_layer_min,
            param_emb_layer_max=model_info$param_emb_layer_max,
            param_emb_pool_type=model_info$param_emb_pool_type,
            param_aggregation=model_info$param_aggregation,
            embeddings=reduced_embeddings)
          return(reduced_embeddings)
        }
      } else {
        stop("A feature extractor is not part of the model.")
      }
    },
    #-------------------------------------------------------------------------
    #'@description Method for predicting new data with a trained neural net.
    #'@param newdata Object of class \code{TextEmbeddingModel} or
    #'\code{data.frame} for which predictions should be made.
    #'@param verbose \code{int} \code{verbose=0} does not cat any
    #'information about the training process from keras on the console.
    #'\code{verbose=1} prints a progress bar. \code{verbose=2} prints
    #'one line of information for every epoch.
    #'@param batch_size \code{int} Size of batches.
    #'@return Returns a \code{data.frame} containing the predictions and
    #'the probabilities of the different labels for each case.
    predict=function(newdata,
                     batch_size=32,
                     verbose=1){

      #Load Custom Model Scripts
      private$load_reload_python_scripts()

      if("datasets.arrow_dataset.Dataset"%in%class(newdata)){
        newdata$set_format("np")
        real_newdata=newdata["input"]
        rownames(real_newdata)=newdata["id"]
      } else {
        if("EmbeddedText" %in% class(newdata)){
          if(self$check_embedding_model(newdata)==FALSE){
            stop("The TextEmbeddingModel that generated the newdata is not
               the same as the TextEmbeddingModel when generating the classifier.")
          }
          real_newdata=newdata$embeddings
        } else {
          real_newdata=newdata
        }
      if(self$model_config$use_fe==TRUE){
        real_newdata=self$extract_features(data_embedding = real_newdata,
                                      batch_size=as.integer(batch_size),
                                      return_r_object = TRUE)$embeddings
      }
    }

      #Ensuring the correct order of the variables for prediction
      current_row_names=rownames(real_newdata)

      if(is.null(self$model_config$rec)==TRUE){
        n_rec=0
      } else {
        n_rec=length(self$model_config$rec)
      }

      if(n_rec==0 & self$model_config$repeat_encoder==0 & self$model_config$times==1){
        real_newdata=array_to_matrix(real_newdata)
      }

      model<-self$model

      if(private$ml_framework=="tensorflow"){
        if(length(self$model_config$target_levels)>2){
          #Multi Class
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))
          predictions<-max.col(predictions_prob)-1
        } else {
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))

          #Add Column for the second characteristic
          predictions=vector(length = length(predictions_prob))
          predictions_binary_prob<-matrix(ncol=2,
                                          nrow=length(predictions_prob))

          for(i in 1:length(predictions_prob)){
            if(predictions_prob[i]>=0.5){
              predictions_binary_prob[i,1]=1-predictions_prob[i]
              predictions_binary_prob[i,2]=predictions_prob[i]
              predictions[i]=1
            } else {
              predictions_binary_prob[i,1]=1-predictions_prob[i]
              predictions_binary_prob[i,2]=predictions_prob[i]
              predictions[i]=0
            }
          }
          predictions_prob<-predictions_binary_prob
        }
      } else if (private$ml_framework=="pytorch"){
        pytorch_predict_data=torch$utils$data$TensorDataset(
          torch$from_numpy(np$array(real_newdata)))

        if(torch$cuda$is_available()){
          device="cuda"
          dtype=torch$double
          model$to(device,dtype=dtype)
          model$eval()
          input=torch$from_numpy(np$array(real_newdata))
          predictions_prob<-model(input$to(device,dtype=dtype),
                                  predication_mode=TRUE)$detach()$cpu()$numpy()
        } else {
          device="cpu"
          dtype=torch$float
          model$to(device,dtype=dtype)
          model$eval()
          input=torch$from_numpy(np$array(real_newdata))
          predictions_prob<-model(input$to(device,dtype=dtype),
                                  predication_mode=TRUE)$detach()$numpy()
        }
        predictions<-max.col(predictions_prob)-1
      }



      #Transforming predictions to target levels
      predictions<-as.character(as.vector(predictions))
      for(i in 0:(length(self$model_config$target_levels)-1)){
        predictions<-replace(x=predictions,
                             predictions==as.character(i),
                             values=self$model_config$target_levels[i+1])
      }

      #Transforming to a factor
      predictions=factor(predictions,levels = self$model_config$target_levels)

      colnames(predictions_prob)=self$model_config$target_levels
      predictions_prob<-as.data.frame(predictions_prob)
      predictions_prob$expected_category=predictions
      rownames(predictions_prob)=current_row_names

      return(predictions_prob)

    },
    #Check Embedding Model compatibility of the text embedding
    #'@description Method for checking if the provided text embeddings are
    #'created with the same \link{TextEmbeddingModel} as the classifier.
    #'@param text_embeddings Object of class \link{EmbeddedText}.
    #'@return \code{TRUE} if the underlying \link{TextEmbeddingModel} are the same.
    #'\code{FALSE} if the models differ.
    check_embedding_model=function(text_embeddings){
      if(("EmbeddedText" %in% class(text_embeddings))==FALSE){
        stop("text_embeddings is not of class EmbeddedText.")
      }

      embedding_model_config<-text_embeddings$get_model_info()
      for(check in names(embedding_model_config)){
        if(!is.null_or_na(embedding_model_config[[check]]) &
           !is.null_or_na(private$text_embedding_model$model[[check]])){
          if(embedding_model_config[[check]]!=private$text_embedding_model$model[[check]]){
            return(FALSE)
          }
        } else if (!is.null_or_na(embedding_model_config[[check]]) &
                   is.null_or_na(private$text_embedding_model$model[[check]])){
          return(FALSE)
        } else if (is.null_or_na(embedding_model_config[[check]]) &
                   !is.null_or_na(private$text_embedding_model$model[[check]])){
          return(FALSE)
        }
      }
      return(TRUE)
    },
    #General Information set and get--------------------------------------------
    #'@description Method for requesting the model information
    #'@return \code{list} of all relevant model information
    get_model_info=function(){
      return(list(
        model_license=private$model_info$model_license,
        model_name=private$model_info$model_name,
        model_name_root=private$model_info$model_name_root,
        model_label=private$model_info$model_label,
        model_date=private$model_info$model_date
      )
      )
    },
    #'@description Method for requesting the text embedding model information
    #'@return \code{list} of all relevant model information on the text embedding model
    #'underlying the classifier
    get_text_embedding_model=function(){
      return(private$text_embedding_model)
    },
    #---------------------------------------------------------------------------
    #'@description Method for setting publication information of the classifier
    #'@param authors List of authors.
    #'@param citation Free text citation.
    #'@param url URL of a corresponding homepage.
    #'@return Function does not return a value. It is used for setting the private
    #'members for publication information.
    set_publication_info=function(authors ,
                                  citation,
                                  url=NULL){

      private$publication_info$developed_by$authors<-authors
      private$publication_info$developed_by$citation<-citation
      private$publication_info$developed_by$url<-url

    },
    #--------------------------------------------------------------------------
    #'@description Method for requesting the bibliographic information of the classifier.
    #'@return \code{list} with all saved bibliographic information.
    get_publication_info=function(){
      return(private$publication_info)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting the license of the classifier.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Function does not return a value. It is used for setting the private member for
    #'the software license of the model.
    set_software_license=function(license="GPL-3"){
      private$model_info$model_license<-license
    },
    #'@description Method for getting the license of the classifier.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return \code{string} representing the license for the software.
    get_software_license=function(){
      return(private$model_info$model_license)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting the license of the classifier's documentation.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Function does not return a value. It is used for setting the private member for
    #'the documentation license of the model.
    set_documentation_license=function(license="CC BY-SA"){
      private$model_description$license<-license
    },
    #'@description Method for getting the license of the classifier's documentation.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Returns the license as a \code{string}.
    get_documentation_license=function(){
      return(private$model_description$license)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting a description of the classifier.
    #'@param eng \code{string} A text describing the training of the learner,
    #'its theoretical and empirical background, and the different output labels
    #'in English.
    #'@param native \code{string} A text describing the training of the learner,
    #'its theoretical and empirical background, and the different output labels
    #'in the native language of the classifier.
    #'@param abstract_eng \code{string} A text providing a summary of the description
    #'in English.
    #'@param abstract_native \code{string} A text providing a summary of the description
    #'in the native language of the classifier.
    #'@param keywords_eng \code{vector} of keyword in English.
    #'@param keywords_native \code{vector} of keyword in the native language of the classifier.
    #'@return Function does not return a value. It is used for setting the private members for the
    #'description of the model.
    set_model_description=function(eng=NULL,
                                   native=NULL,
                                   abstract_eng=NULL,
                                   abstract_native=NULL,
                                   keywords_eng=NULL,
                                   keywords_native=NULL){
      if(!is.null(eng)){
        private$model_description$eng=eng
      }
      if(!is.null(native)){
        private$model_description$native=native
      }

      if(!is.null(abstract_eng)){
        private$model_description$abstract_eng=abstract_eng
      }
      if(!is.null(abstract_native)){
        private$model_description$abstract_native=abstract_native
      }

      if(!is.null(keywords_eng)){
        private$model_description$keywords_eng=keywords_eng
      }
      if(!is.null(keywords_native)){
        private$model_description$keywords_native=keywords_native
      }

    },
    #'@description Method for requesting the model description.
    #'@return \code{list} with the description of the classifier in English
    #'and the native language.
    get_model_description=function(){
      return(private$model_description)
    },
    #-------------------------------------------------------------------------
    #'@description Method for saving a model to 'Keras v3 format',
    #''tensorflow' SavedModel format or h5 format.
    #'@param dir_path \code{string()} Path of the directory where the model should be
    #'saved.
    #'@param save_format Format for saving the model. For 'tensorflow'/'keras' models
    #'\code{"keras"} for 'Keras v3 format',
    #'\code{"tf"} for SavedModel
    #'or \code{"h5"} for HDF5.
    #'For 'pytorch' models \code{"safetensors"} for 'safetensors' or
    #'\code{"pt"} for 'pytorch' via pickle.
    #'Use \code{"default"} for the standard format. This is keras for
    #''tensorflow'/'keras' models and safetensors for 'pytorch' models.
    #'@return Function does not return a value. It saves the model to disk.
    #'@importFrom utils write.csv
    save_model=function(dir_path,save_format="default"){
      if(private$ml_framework=="tensorflow"){
        if(save_format%in%c("safetensors","pt")){
          stop("'safetensors' and 'pt' are only supported for models based on
           pytorch.")
        }
      } else if(private$ml_framework=="pytorch"){
        if(save_format%in%c("keras","tf","h5")){
          stop("'keras','tf', and 'h5' are only supported for models based on
           tensorflow")
        }
      }

      if(save_format=="default"){
        if(private$ml_framework=="tensorflow"){
          save_format="keras"
        } else if(private$ml_framework=="pytorch"){
          save_format="safetensors"
        }
      }

      if(save_format=="safetensors" &
         reticulate::py_module_available("safetensors")==FALSE){
        warning("Python library 'safetensors' is not available. Using
                 standard save format for pytorch.")
        save_format="pt"
      }

      if(private$ml_framework=="tensorflow"){
        if(save_format=="keras"){
          extension=".keras"
        } else if(save_format=="tf"){
          extension=".tf"
        } else {
          extension=".h5"
        }
        file_path=paste0(dir_path,"/","model_data",extension)
        if(dir.exists(dir_path)==FALSE){
          dir.create(dir_path)
        }
        self$model$save(file_path)

        if(self$model_config$use_fe==TRUE){
          file_path_extractor=paste0(dir_path,"/","feature_extractor",extension)
          self$feature_extractor$model$save(file_path_extractor)
        }

      } else if(private$ml_framework=="pytorch"){
        if(dir.exists(dir_path)==FALSE){
          dir.create(dir_path)
        }
        self$model$to("cpu",dtype=torch$float)
        if(save_format=="safetensors"){
          file_path=paste0(dir_path,"/","model_data",".safetensors")
          safetensors$torch$save_model(model=self$model,filename=file_path)
        } else if (save_format=="pt"){
          file_path=paste0(dir_path,"/","model_data",".pt")
          torch$save(self$model$state_dict(),file_path)
        }

        if(self$model_config$use_fe==TRUE){
          self$feature_extractor$model$to("cpu",dtype=torch$float)
          if(save_format=="safetensors"){
            extension=".safetensors"
            file_path_extractor=paste0(dir_path,"/","feature_extractor",extension)
            safetensors$torch$save_model(model=self$feature_extractor$model,
                                         filename=file_path_extractor)
          } else if (save_format=="pt"){
            extension=".pt"
            file_path_extractor=paste0(dir_path,"/","feature_extractor",extension)
            torch$save(self$feature_extractor$model$state_dict(),file_path_extractor)
          }
        }
      }

      #Saving Sustainability Data
      sustain_matrix=t(as.matrix(unlist(private$sustainability)))
      write.csv(
        x=sustain_matrix,
        file=paste0(dir_path,"/","sustainability.csv"),
        row.names = FALSE
      )
    },
    #'@description Method for importing a model from 'Keras v3 format',
    #' 'tensorflow' SavedModel format or h5 format.
    #'@param dir_path \code{string()} Path of the directory where the model is
    #'saved.
    #'@param ml_framework \code{string} Determines the machine learning framework
    #'for using the model. Possible are \code{ml_framework="pytorch"} for 'pytorch',
    #'\code{ml_framework="tensorflow"} for 'tensorflow', and \code{ml_framework="auto"}.
    #'@return Function does not return a value. It is used to load the weights
    #'of a model.
    #'@importFrom utils compareVersion
    load_model=function(dir_path,
                        ml_framework="auto"){

      # Set the correct ml framework

      if((ml_framework %in%c("pytorch","tensorflow","auto","not_specified"))==FALSE){
        stop("ml_framework must be 'tensorflow', 'pytorch' or 'auto'.")
      }

      if(ml_framework=="not_specified"){
        stop("The global machine learning framework is not set. Please use
             aifeducation_config$set_global_ml_backend() directly after loading
             the library to set the global framework. ")
      }

      if(ml_framework!="auto"){
        private$ml_framework=ml_framework
      }

      #Load the model---------------------------------------------------------
      if(private$ml_framework=="tensorflow"){
        path=paste0(dir_path,"/","model_data",".keras")
        if(file.exists(paths = path)==TRUE){
          self$model<-keras$models$load_model(path)
        } else {
          path=paste0(dir_path,"/","model_data",".tf")
          if(dir.exists(paths = path)==TRUE){
            self$model<-keras$models$load_model(path)
          } else {
            path=paste0(dir_path,"/","model_data",".h5")
            if(file.exists(paths = path)==TRUE){
              self$model<-keras$models$load_model(paste0(dir_path,"/","model_data",".h5"))
            } else {
              stop("There is no compatible model file in the choosen directory.
                   Please check path. Please note that classifiers have to be loaded with
                   the same framework as during creation.")
            }
          }
        }
      } else if(private$ml_framework=="pytorch"){
        path_pt=paste0(dir_path,"/","model_data",".pt")
        path_safe_tensors=paste0(dir_path,"/","model_data",".safetensors")
        private$create_reset_model()
        if(file.exists(path_safe_tensors)){
          safetensors$torch$load_model(model=self$model,filename=path_safe_tensors)
        } else {
          if(file.exists(paths = path_pt)==TRUE){
            self$model$load_state_dict(torch$load(path_pt))
          } else {
            stop("There is no compatible model file in the choosen directory.
                     Please check path. Please note that classifiers have to be loaded with
                     the same framework as during creation.")
          }
        }
      }

      #Load feature extractor if necessary-------------------------------------
      if(self$model_config$use_fe==TRUE){
        if(private$ml_framework=="tensorflow"){
          path=paste0(dir_path,"/","feature_extractor",".keras")
          if(file.exists(paths = path)==TRUE){
            self$feature_extractor$model<-keras$models$load_model(path)
          } else {
            path=paste0(dir_path,"/","feature_extractor",".tf")
            if(dir.exists(paths = path)==TRUE){
              self$feature_extractor$model<-keras$models$load_model(path)
            } else {
              path=paste0(dir_path,"/","feature_extractor",".h5")
              if(file.exists(paths = path)==TRUE){
                self$feature_extractor$model<-keras$models$load_model(paste0(dir_path,"/","feature_extractor",".h5"))
              } else {
                stop("There is no compatible model file in the choosen directory.
                   Please check path. Please note that classifiers have to be loaded with
                   the same framework as during creation.")
              }
            }
          }
        } else if(private$ml_framework=="pytorch"){
          path_pt=paste0(dir_path,"/","feature_extractor",".pt")
          path_safe_tensors=paste0(dir_path,"/","feature_extractor",".safetensors")
          private$create_feature_extractor()
          if(file.exists(path_safe_tensors)){
            safetensors$torch$load_model(model=self$feature_extractor$model,
                                         filename=path_safe_tensors)
          } else {
            if(file.exists(paths = path_pt)==TRUE){
              self$feature_extractor$model$load_state_dict(torch$load(path_pt))
            } else {
              stop("There is no compatible model file in the choosen directory for the feature extractor.
                     Please check path. Please note that classifiers have to be loaded with
                     the same framework as during creation.")
            }
          }
        }
      }
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting a summary of the R and python packages'
    #'versions used for creating the classifier.
    #'@return Returns a \code{list} containing the versions of the relevant
    #'R and python packages.
    get_package_versions=function(){
      return(
        list(r_package_versions=private$r_package_versions,
             py_package_versions=private$py_package_versions)
      )
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting a summary of tracked energy consumption
    #'during training and an estimate of the resulting CO2 equivalents in kg.
    #'@return Returns a \code{list} containing the tracked energy consumption,
    #'CO2 equivalents in kg, information on the tracker used, and technical
    #'information on the training infrastructure.
    get_sustainability_data=function(){
      return(private$sustainability)
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting the machine learning framework used
    #'for the classifier.
    #'@return Returns a \code{string} describing the machine learning framework used
    #'for the classifier
    get_ml_framework=function(){
      return(private$ml_framework)
    }
  ),
  private = list(
    ml_framework=NA,

    #General Information-------------------------------------------------------
    model_info=list(
      model_license=NA,
      model_name=NA,
      name_root=NA,
      model_label=NA,
      model_date=NA
    ),

    text_embedding_model=list(
      model=list(),
      times=NA,
      features=NA
    ),


    publication_info=list(
      developed_by=list(
        authors =NULL,
        citation=NULL,
        url=NULL
      )
    ),

    model_description=list(
      eng=NULL,
      native=NULL,
      abstract_eng=NULL,
      abstract_native=NULL,
      keywords_eng=NULL,
      keywords_native=NULL,
      license=NA
    ),

    r_package_versions=list(
      aifeducation=NA,
      smotefamily=NA,
      reticulate=NA
    ),

    py_package_versions=list(
      tensorflow=NA,
      torch=NA,
      keras=NA,
      numpy=NA
    ),

    sustainability=list(
      sustainability_tracked=FALSE,
      date=NA,
      sustainability_data=list(
        duration_sec=NA,
        co2eq_kg=NA,
        cpu_energy_kwh=NA,
        gpu_energy_kwh=NA,
        ram_energy_kwh=NA,
        total_energy_kwh=NA
      ),
      technical=list(
        tracker=NA,
        py_package_version=NA,

        cpu_count=NA,
        cpu_model=NA,

        gpu_count=NA,
        gpu_model=NA,

        ram_total_size=NA
      ),
      region=list(
        country_name=NA,
        country_iso_code=NA,
        region=NA
      )
    ),

    gui=list(
      shiny_app_active=NA,
      pgr_value=0,
      pgr_max_value=0
    ),
    #--------------------------------------------------------------------------
    load_reload_python_scripts=function(){
      if(private$ml_framework=="tensorflow"){
        reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/keras_callbacks.py",
                                            package = "aifeducation"))
      } else if(private$ml_framework=="pytorch"){
        reticulate::py_run_file(system.file("python/pytorch_te_classifier_V2.py",
                                            package = "aifeducation"))
        reticulate::py_run_file(system.file("python/pytorch_autoencoder.py",
                                            package = "aifeducation"))

      }
    },
    #--------------------------------------------------------------------------
    create_reset_model=function(){
      if(private$ml_framework=="tensorflow"){

        #Load custom layers
        private$load_reload_python_scripts()

        #Defining basic keras model
        layer_list=NULL

        if(is.null(self$model_config$rec)==TRUE){
          n_rec=0
        } else {
          n_rec=length(self$model_config$rec)
        }

        if(is.null(self$model_config$hidden)==TRUE){
          n_hidden=0
        } else {
          n_hidden=length(self$model_config$hidden)
        }

        #Adding Input Layer

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          model_input<-keras$layers$Input(shape=list(as.integer(self$model_config$times),as.integer(self$model_config$features)),
                                          name="input_embeddings")
        } else {
          model_input<-keras$layers$Input(shape=as.integer(self$model_config$times*self$model_config$features),
                                          name="input_embeddings")
        }
        layer_list[1]<-list(model_input)

        #Adding a Mask-Layer
        if(n_rec>0 | self$model_config$repeat_encoder>0){
          masking_layer<-keras$layers$Masking(
            mask_value = 0.0,
            name="masking_layer",
            input_shape=c(self$model_config$times,self$model_config$features),
            trainable=FALSE)(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(masking_layer)

          if(self$model_config$add_pos_embedding==TRUE){
            positional_embedding<-py$AddPositionalEmbedding(sequence_length = as.integer(self$model_config$times),
                                                            name="add_positional_embedding")(layer_list[[length(layer_list)]])
            layer_list[length(layer_list)+1]<-list(positional_embedding)
          }

          norm_layer<-keras$layers$LayerNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        } else {
          norm_layer<-keras$layers$BatchNormalization(
            name = "normalizaion_layer")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(norm_layer)

        }

        if(self$model_config$repeat_encoder>0){
          for(r in 1:self$model_config$repeat_encoder){
            if(self$model_config$attention_type=="multihead"){
              layer_list[length(layer_list)+1]<-list(
                py$TransformerEncoder(embed_dim = as.integer(self$model_config$features),
                                      dense_dim= as.integer(self$model_config$intermediate_size),
                                      num_heads =as.integer(self$model_config$self_attention_heads),
                                      dropout_rate=self$model_config$encoder_dropout,
                                      name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            } else if(self$model_config$attention_type=="fourier"){
              layer_list[length(layer_list)+1]<-list(
                py$FourierEncoder(dense_dim=as.integer(self$model_config$intermediate_size),
                                  dropout_rate=self$model_config$encoder_dropout,
                                  name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
              )
            }
          }
        }

        #Adding rec layer
        if(n_rec>0){
          if(self$model_config$rec_bidirectional==TRUE){
            for(i in 1:n_rec){
              if(self$model_config$rec_type=="gru"){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Bidirectional(
                    layer=keras$layers$GRU(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("gru_",i)),
                    name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              } else if (self$model_config$rec_type=="lstm"){
                layer_list[length(layer_list)+1]<-list(
                  keras$layers$Bidirectional(
                    layer=keras$layers$LSTM(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("lstm",i)),
                    name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("lstm_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              }
            }
          } else {
            for(i in 1:n_rec){
              if(self$model_config$rec_type=="gru"){
                layer_list[length(layer_list)+1]<-list(
                    layer=keras$layers$GRU(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("gru_",i)),
                    name=paste0("bidirectional_",i)(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              } else if (self$model_config$rec_type=="lstm"){
                layer_list[length(layer_list)+1]<-list(
                    layer=keras$layers$LSTM(
                      units=as.integer(self$model_config$rec[i]),
                      input_shape=list(self$model_config$times,self$model_config$features),
                      return_sequences = TRUE,
                      dropout = 0,
                      recurrent_dropout = self$model_config$recurrent_dropout,
                      activation = "tanh",
                      name=paste0("lstm",i)),
                    name=paste0("bidirectional_",i)(layer_list[[length(layer_list)]]))
                if (i!=n_rec){
                  layer_list[length(layer_list)+1]<-list(
                    keras$layers$Dropout(
                      rate = self$model_config$rec_dropout,
                      name=paste0("lstm_dropout_",i))(layer_list[[length(layer_list)]]))
                }
              }
            }
          }
        }

        if(n_rec>0 | self$model_config$repeat_encoder>0 | self$model_config$times>1){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$GlobalAveragePooling1D(
              name="global_average_pooling")(layer_list[[length(layer_list)]]))
        }

        #Adding standard layer
        if(n_hidden>0){
          for(i in 1:n_hidden){
            layer_list[length(layer_list)+1]<-list(
              keras$layers$Dense(
                units = as.integer(self$model_config$hidden[i]),
                activation = "gelu",
                name=paste0("dense_",i))(layer_list[[length(layer_list)]]))

            if(i!=n_hidden){
              #Add Dropout_Layer
              layer_list[length(layer_list)+1]<-list(
                keras$layers$Dropout(
                  rate = self$model_config$dense_dropout,
                  name=paste0("dense_dropout_",i))(layer_list[[length(layer_list)]]))
            }
          }
        }

        #Adding final Layer
        if(length(self$model_config$target_levels)>2){
          #Multi Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(length(self$model_config$target_levels)),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        } else {
          #Binary Class
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(1),
              activation = self$model_config$act_fct_last,
              name="output_categories")(layer_list[[length(layer_list)]]))
        }

        #Creating Model
        model<-keras$Model(
          inputs = model_input,
          outputs = layer_list[length(layer_list)],
          name = self$model_config$name)

        self$model=model
      } else {
        #--------------------------------------------------------------------------
        #Load Custom Pytorch Objects and Functions
        private$load_reload_python_scripts()

        self$model=py$TextEmbeddingClassifier_PT(features=as.integer(self$model_config$features),
                                                 times=as.integer(self$model_config$times),
                                                 hidden=if(!is.null(self$model_config$hidden)){as.integer(self$model_config$hidden)}else{NULL},
                                                 rec=if(!is.null(self$model_config$rec)){as.integer(self$model_config$rec)}else{NULL},
                                                 rec_type=self$model_config$rec_type,
                                                 rec_bidirectional=self$model_config$rec_bidirectional,
                                                 intermediate_size=as.integer(self$model_config$intermediate_size),
                                                 attention_type=self$model_config$attention_type,
                                                 repeat_encoder=as.integer(self$model_config$repeat_encoder),
                                                 dense_dropout=self$model_config$dense_dropout,
                                                 rec_dropout=self$model_config$rec_dropout,
                                                 encoder_dropout=self$model_config$encoder_dropout,
                                                 add_pos_embedding=self$model_config$add_pos_embedding,
                                                 self_attention_heads=as.integer(self$model_config$self_attention_heads),
                                                 target_levels=self$model_config$target_levels)


      }
    },
    #--------------------------------------------------------------------------
    create_feature_extractor=function(){
      if(private$ml_framework=="pytorch"){
        if(self$model_config$fe_method=="lstm"){
          self$feature_extractor$model=feature_extractor=py$LSTMAutoencoder_with_Mask_PT(
            times=as.integer(private$text_embedding_model["times"]),
            features_in=as.integer(private$text_embedding_model["features"]),
            features_out=as.integer(self$model_config$features),
            noise_factor=self$model_config$fe_noise_factor)
        } else if(self$model_config$fe_method=="dense") {
          self$feature_extractor$model=feature_extractor=py$DenseAutoencoder_with_Mask_PT(
            #times=as.integer(private$text_embedding_model["times"]),
            features_in=as.integer(private$text_embedding_model["features"]),
            features_out=as.integer(self$model_config$features),
            noise_factor=self$model_config$fe_noise_factor)
        }
      } else if(private$ml_framework=="tensorflow"){
        features_in=as.integer(private$text_embedding_model["features"])
        features_out=as.integer(self$model_config$features)
        difference=features_in-features_out

        if(self$model_config$fe_method=="lstm"){

          LSTMAutoencoder_with_Mask<-NULL
          layer_list=NULL

          #Input layer
          layer_list[1]<-list(
            keras$layers$Input(shape=list(as.integer(private$text_embedding_model["times"]),features_in),
                               name="input_embeddings"))
          #Masking layer
          layer_list[length(layer_list)+1]=list(
            keras.layers.Masking(mask_value=0.0)
          )
          #Encoder
          layer_list[length(layer_list)+1]=list(
            keras$layers$LSTM(
              units=as.integer(ceiling(features_in-difference*(1/2))))(layer_list[length(layer_list)])
            )
          #Latent Space
          layer_list[length(layer_list)+1]=list(
            keras$layers$LSTM(
              units=as.integer(features_out))(layer_list[length(layer_list)])
          )
          #Decoder
          layer_list[length(layer_list)+1]=list(
            keras$layers$LSTM(
              units=as.integer(ceiling(features_in-difference*(1/2))))(layer_list[length(layer_list)])
          )
          layer_list[length(layer_list)+1]=list(
            keras$layers$LSTM(
              units=as.integer(features_in))(layer_list[length(layer_list)])
          )




        }
      }
    },
    #--------------------------------------------------------------------------
    train_feature_extractor=function(data_embeddings){
      private$load_reload_python_scripts()
      if(self$last_training$config$trace==TRUE){
        message(paste(date(),"Feature extractor | Start training"))
      }

      if("EmbeddedText" %in% class(data_embeddings)){
        #Reduce to unique cases for training
        data=unique(data_embeddings$embeddings)

        #create a data set
       extractor_dataset=datasets$Dataset$from_dict(
          reticulate::dict(
            list(id=rownames(data),
                 input=np$squeeze(np$split(reticulate::np_array(data),as.integer(nrow(data)),axis=0L))),
            convert = FALSE))

       #remove data
       rm(data)

       #Copy input as label for training
       extractor_dataset=extractor_dataset$add_column("labels",extractor_dataset["input"])
      }

      if(private$ml_framework=="pytorch"){
        #Set format
        extractor_dataset$set_format("torch")

        #Split into train and validation data
        extractor_dataset=extractor_dataset$train_test_split(self$feature_extractor$val_size)

        #Check directory for checkpoints
        if(dir.exists(paste0(self$last_training$config$dir_checkpoint,"/checkpoints_feature_extractor"))==FALSE){
          if(self$last_training$config$trace==TRUE){
            message(paste(date(),"Creating Checkpoint Directory"))
          }
          dir.create(paste0(self$last_training$config$dir_checkpoint,"/checkpoints_feature_extractor"))
        }
        #print(extractor_dataset$train)
        self$feature_extractor$history=py$AutoencoderTrain_PT_with_Datasets(
          model=self$feature_extractor$model,
          epochs=as.integer(self$feature_extractor$epochs),
          trace=as.integer(self$last_training$config$pytorch_trace),
          batch_size=as.integer(self$last_training$config$batch_size),
          train_data=extractor_dataset$train,
          val_data=extractor_dataset$test,
          filepath=paste0(self$last_training$config$dir_checkpoint,"/checkpoints_feature_extractor/best_weights.pt"),
          use_callback=TRUE,
          shiny_app_active=self$gui$shiny_app_active)$loss
      }

      rownames(self$feature_extractor$history)=c("train","val")

      if(self$last_training$config$trace==TRUE){
        message(paste(date(),"Feature extractor | Training finished"))
      }
    },
    #--------------------------------------------------------------------------
    init_train=function(){
      #Setting a new ID for the classifier
      private$model_info$model_name=paste0(
        private$model_info$model_name_root,
        "_id_",
        generate_id(16))

      #Initializing Objects for Saving Performance
      metric_names=get_coder_metrics(
        true_values=NULL,
        predicted_values=NULL,
        return_names_only=TRUE)

      self$reliability$test_metric=matrix(
        nrow=self$last_training$config$n_folds,
        ncol=length(metric_names),
        dimnames = list(iterations=NULL,
                        metrics=metric_names))

      self$reliability$test_metric_mean=NULL

      self$reliability$iota_objects_end=NULL
      self$reliability$iota_objects_end_free=NULL

      self$reliability$iota_object_end=NULL
      self$reliability$iota_object_end_free=NULL

      standard_measures_mean_table<-matrix(
        nrow = length(self$model_config$target_levels),
        ncol = 3,
        data = 0)
      colnames(standard_measures_mean_table)=c("precision","recall","f1")
      rownames(standard_measures_mean_table)<-self$model_config$target_levels

      self$reliability$standard_measures_mean=standard_measures_mean_table

      #Save start time of training
      self$last_training$start_time=Sys.time()
    },
    #--------------------------------------------------------------------------
    calculate_test_metric=function(test_data,iteration,type){
      test_predictions=self$predict(newdata = test_data,
                                    verbose = self$last_training$config$keras_trace,
                                    batch_size =self$last_training$config$batch_size)
      test_pred_cat=test_predictions$expected_category
      names(test_pred_cat)=rownames(test_predictions)
      test_pred_cat<-test_pred_cat[test_data["id"]]
      test_res=get_coder_metrics(true_values = factor(x=test_data["labels"],
                                                      levels=0:(length(self$model_config$target_levels)-1),
                                                      labels=self$model_config$target_levels),
                                 predicted_values = test_pred_cat)

      #Save results
      self$reliability$test_metric[iteration,]<-test_res

      #Update GUI
      private$gui_inc_progressbar()
    },
    #--------------------------------------------------------------------------
    calculate_measures_on_categorical_level=function(data_manager,iteration){
      #Get test data
      data_manager$set_state(iteration = iteration,
                             step = NULL)
      test_data=data_manager$get_test_dataset()

      if(!is.null(test_data)==TRUE){
        #Predict labels
        test_predictions=self$predict(newdata = test_data,
                                      verbose = self$last_training$config$keras_trace,
                                      batch_size =self$last_training$config$batch_size)
        test_pred_cat=test_predictions$expected_category
        names(test_pred_cat)=rownames(test_predictions)
        test_pred_cat<-test_pred_cat[test_data["id"]]

        #Calculate standard measures
        self$reliability$standard_measures_end[iteration]=list(
          calc_standard_classification_measures(
            true_values=factor(x=test_data["labels"],
                               levels=0:(length(self$model_config$target_levels)-1),
                               labels=self$model_config$target_levels),
            predicted_values=test_pred_cat))

        #Calculate iota objects
        self$reliability$iota_objects_end[iteration]=list(iotarelr::check_new_rater(true_values = factor(x=test_data["labels"],
                                                                                                         levels=0:(length(self$model_config$target_levels)-1),
                                                                                                         labels=self$model_config$target_levels),
                                                                                    assigned_values = test_pred_cat,
                                                                                    free_aem = FALSE))
        self$reliability$iota_objects_end_free[iteration]=list(iotarelr::check_new_rater(true_values = factor(x=test_data["labels"],
                                                                                                              levels=0:(length(self$model_config$target_levels)-1),
                                                                                                              labels=self$model_config$target_levels),
                                                                                         assigned_values = test_pred_cat,
                                                                                         free_aem = TRUE))
      } else if(iteration<=data_manager$get_n_folds()) {
        warning("Unable to calculate test scores. There is no test data.")
      }
    },
    #--------------------------------------------------------------------------
    finalize_train=function(){
      #Save Final Information
      self$last_training$date=date()

      #Finalize measures from content analysis
      test_metric_mean=vector(length = ncol(self$reliability$test_metric))
      test_metric_mean[]=0
      names(test_metric_mean)=colnames(self$reliability$test_metric)

      n_mean=vector(length = ncol(self$reliability$test_metric))
      n_mean[]=self$last_training$config$n_folds

      for(i in 1:self$last_training$config$n_folds){
        for(j in 1:ncol(self$reliability$test_metric)){
          if(is.na(self$reliability$test_metric[i,j])==FALSE){
            test_metric_mean[j]=test_metric_mean[j]+self$reliability$test_metric[i,j]
          } else {
            n_mean[j]=n_mean[j]-1
          }
        }
      }

      test_metric_mean=test_metric_mean/n_mean
      test_metric_mean[is.nan(test_metric_mean)]=NA
      self$reliability$test_metric_mean=test_metric_mean

      self$last_training$learning_time=as.numeric(
        difftime(Sys.time(),
                 self$last_training$start_time,
                 units="mins"))

      #Finalize iota objects
      if(is.null(self$reliability$iota_objects_end)==FALSE){
        self$reliability$iota_object_end=create_iota2_mean_object(
          iota2_list = self$reliability$iota_objects_end,
          original_cat_labels = self$model_config$target_levels,
          free_aem=FALSE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end=NULL
      }

      if(is.null(self$reliability$iota_objects_end_free)==FALSE){
        self$reliability$iota_object_end_free=create_iota2_mean_object(
          iota2_list = self$reliability$iota_objects_end_free,
          original_cat_labels = self$model_config$target_levels,
          free_aem=TRUE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end_free=NULL
      }

      #Finalize standard measures
      standard_measures=self$reliability$standard_measures_mean
      for(i in 1:self$last_training$config$n_folds){
        for(tmp_cat in self$model_config$target_levels){
          standard_measures[tmp_cat,"precision"]=standard_measures[tmp_cat,"precision"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"precision"]
          standard_measures[tmp_cat,"recall"]=standard_measures[tmp_cat,"recall"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"recall"]
          standard_measures[tmp_cat,"f1"]=standard_measures[tmp_cat,"f1"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"f1"]
        }
      }
      self$reliability$standard_measures_mean<-standard_measures/self$last_training$config$n_folds
    },
    #--------------------------------------------------------------------------
    init_gui=function(data_manager){
      #Check for a running Shiny App and set the configuration
      #The Gui functions must be set in the server function of shiny globally
      if(requireNamespace("shiny",quietly=TRUE) & requireNamespace("shinyWidgets",quietly=TRUE)){
        if(shiny::isRunning()){
          private$gui$shiny_app_active=TRUE
        } else {
          private$gui$shiny_app_active=FALSE
        }
      } else {
        private$gui$shiny_app_active=FALSE
      }

      #SetUp Progressbar for UI
      private$gui$pgr_value=-1
      private$gui$pgr_max_value=data_manager$get_n_folds()+1+
        (data_manager$get_n_folds()+1)*self$last_training$config$use_pl*self$last_training$config$pl_max_steps

    },
    #--------------------------------------------------------------------------
    gui_inc_progressbar=function(){
      private$gui$pgr_value=private$gui$pgr_value+1
      update_aifeducation_progress_bar(value = private$gui$pgr_value,
                                       total = private$gui$pgr_max_value,
                                       title = "Train Classifier")
    },
    #--------------------------------------------------------------------------
    train_standard=function(iteration=NULL,
                            data_manager=NULL,
                            inc_synthetic=FALSE){

      #Print status message to console
      if(self$last_training$config$trace==TRUE){
        if(iteration<=self$last_training$config$n_folds){
          message(paste(
            date(),
            "|","Iteration",iteration,"from",self$last_training$config$n_folds
          ))
        } else {
          message(paste(
            date(),
            "|","Final training"
          ))
        }
      }

      #Set the state of the DataManager
      data_manager$set_state(
        iteration = iteration,
        step = NULL)

      #Generate syntetic cases if requested
      if(inc_synthetic==TRUE){
        data_manager$create_synthetic(
          trace=self$last_training$config$trace,
          inc_pseudo_data=FALSE
        )
      }

      #Get the different DataSets
      train_data=data_manager$get_dataset(
        inc_labeled = TRUE,
        inc_synthetic = inc_synthetic,
        inc_pseudo_data = FALSE,
        inc_unlabeled = FALSE)
      val_data=data_manager$get_val_dataset()
      if(iteration!="final"){
        test_data=data_manager$get_test_dataset()
      } else {
        test_data=NULL
      }

      #Print status to console
      if(self$last_training$config$trace==TRUE){
        if(iteration<=self$last_training$config$n_folds){
          message(paste(
            date(),
            "|","Iteration",iteration,"from",self$last_training$config$n_folds,
            "|","Training"
          ))
        } else {
          message(paste(
            date(),
            "|","Final training",
            "|","Training"
          ))
        }
      }

      #Start training
      train_history=private$basic_train(
        train_data = train_data,
        val_data = val_data,
        test_data = test_data,
        shiny_app_active = private$gui$shiny_app_active,
        reset_model = TRUE,
        use_callback = TRUE)

      #Save history
      self$last_training$history[iteration]=list(train_history)

      #Calculate test metric
      if(!is.null(test_data)==TRUE){
        private$calculate_test_metric(test_data=test_data,
                                      iteration = iteration,
                                      type = (as.numeric(inc_synthetic))+1)

      }
    },
    #--------------------------------------------------------------------------
    train_with_pseudo_labels=function(init_train=TRUE,
                                      iteration=NULL,
                                      data_manager=NULL,
                                      inc_synthetic=FALSE){

      #If model is not trained than train for the first time
      #Necessary for estimating pseudo labels
      if(init_train==TRUE){
        private$train_standard(iteration = iteration,
                               data_manager = data_manager,
                               inc_synthetic = inc_synthetic)
      }

      #Get validation and test data for training loop
      val_data=data_manager$get_val_dataset()
      if(iteration!="final"){
        test_data=data_manager$get_test_dataset()
      } else {
        test_data=NULL
      }

      #Start training loop with pseudo labels
      data_manager$set_state(iteration = iteration,
                             step = NULL)

      #Create list for saving training histories per step
      step_histories=NULL

      for (step in 1:self$last_training$config$pl_max_steps) {
        #Print status message to console
        if(self$last_training$config$trace==TRUE){
          if(iteration<=self$last_training$config$n_folds){
            message(paste(
              date(),
              "|","Iteration",iteration,"from",self$last_training$config$n_folds,
              "|","Pseudo labeling","step",step,"from",self$last_training$config$pl_max_steps
            ))
          } else {
            message(paste(
              date(),
              "|","Final training",
              "|","Pseudo labeling","step",step,"from",self$last_training$config$pl_max_steps
            ))
          }
        }

        #Set correct state for the data_manager
        data_manager$set_state(
          iteration = iteration,
          step = step)

        #Generate pseudo labels
        pseudo_data=private$estimate_pseudo_labels(
          unlabeled_data=data_manager$get_unlabeled_data(),
          current_step = step)

        #Save pseudo labels in the data_manager
        data_manager$add_replace_pseudo_data(
          inputs = pseudo_data$input,
          labels = pseudo_data$labels)

        #Remove old pseudo data
        rm(pseudo_data)

        #Generate synthetic data if requested
        if(inc_synthetic==TRUE){
          data_manager$create_synthetic(
            trace = self$last_training$config$trace,
            inc_pseudo_data = TRUE)
        }

        #Request training data
        train_data=data_manager$get_dataset(
          inc_labeled = TRUE,
          inc_synthetic = inc_synthetic,
          inc_pseudo_data = TRUE,
          inc_unlabeled = FALSE)

        #Print status to console
        if(self$last_training$config$trace==TRUE){
          if(iteration<=self$last_training$config$n_folds){
            message(paste(
              date(),
              "|","Iteration",iteration,"from",self$last_training$config$n_folds,
              "|","Training"
            ))
          } else {
            message(paste(
              date(),
              "|","Final training",
              "|","Training"
            ))
          }
        }

        #Start training
        train_history=private$basic_train(
          train_data = train_data,
          val_data = val_data,
          test_data = test_data,
          shiny_app_active = private$gui$shiny_app_active,
          reset_model = TRUE,
          use_callback = TRUE)

        #Save history
        step_histories[step]=list(train_history)

        #Update GUI
        private$gui_inc_progressbar()
      }

      #Save the histories for the complete iteration
      self$last_training$history[iteration]=list(step_histories)

      #Calculate test metric
      if(!is.null(test_data)==TRUE){
        private$calculate_test_metric(test_data=test_data,
                                      iteration = iteration,
                                      type = 3)

      }
    },
    #--------------------------------------------------------------------------
    estimate_pseudo_labels=function(unlabeled_data,
                                    current_step){

      #Predict pseudo labels for unlabeled data
      predicted_labels=self$predict(
        newdata=unlabeled_data,
        verbose=self$last_training$config$keras_trace,
        batch_size = self$last_training$config$batch_size)

      #Create Matrix for saving the results
      new_categories<-matrix(nrow= nrow(predicted_labels),
                             ncol=2)
      rownames(new_categories)=rownames(predicted_labels)
      colnames(new_categories)=c("cat","prob")

      #Gather information for every case. That is the category with the
      #highest probability and save both
      for(i in 1:nrow(predicted_labels)){
        tmp_est_prob=predicted_labels[i,1:(ncol(predicted_labels)-1)]
        new_categories[i,1]=which.max(tmp_est_prob)-1
        new_categories[i,2]=max(tmp_est_prob)
      }
      new_categories<-as.data.frame(new_categories)

      #Transforming the probabilities to an information index
      new_categories[,2]=abs(
        self$last_training$config$pl_anchor-
          (as.numeric(new_categories[,2])-1/length(self$model_config$target_levels))/(1-1/length(self$model_config$target_levels)))
      new_categories=as.data.frame(new_categories)

      #Reducing the new categories to the desired range
      condition=(new_categories[,2]>=self$last_training$config$pl_min &
                   new_categories[,2]<=self$last_training$config$pl_max)
      new_categories=subset(new_categories,condition)

      #Calculate number of cases to include
      bpl_inc_ratio=current_step/self$last_training$config$pl_max_steps
      n_cases_to_include=nrow(new_categories)*bpl_inc_ratio

      #Order cases with increasing distance from maximal information
      new_categories=new_categories[order(new_categories$prob,decreasing = FALSE),]

      #Select the best cases
      names_final_new_categories=rownames(new_categories)[1:n_cases_to_include]

      #Get the labels for these cases
      targets_pseudo_labeled<-new_categories[names_final_new_categories,1]
      targets_pseudo_labeled=as.numeric(targets_pseudo_labeled)
      names(targets_pseudo_labeled)<-names_final_new_categories

      #get the corresponding input
      unlabeled_data$set_format("np")
      embeddings=unlabeled_data["input"]
      rownames(embeddings)=unlabeled_data["id"]
      embeddings=embeddings[names_final_new_categories,,]

      #Return results
      pseudo_data=list(
        input=embeddings,
        labels=targets_pseudo_labeled)

      return(pseudo_data)
    },
    #--------------------------------------------------------------------------
    basic_train=function(train_data=NULL,
                         val_data=NULL,
                         test_data=NULL,
                         reset_model=FALSE,
                         use_callback=TRUE,
                         shiny_app_active=FALSE){

      #Clear session to provide enough resources for computations
      if(private$ml_framework=="tensorflow"){
        keras$backend$clear_session()
      } else if(private$ml_framework=="pytorch"){
        if(torch$cuda$is_available()){
          torch$cuda$empty_cache()
        }
      }

      #Generating class weights
      if(self$last_training$config$balance_class_weights==TRUE){
        abs_freq_classes=table(train_data["labels"])
        class_weights=as.vector(sum(abs_freq_classes)/(length(abs_freq_classes)*abs_freq_classes))
      } else {
        class_weights=rep(x=1,times=length(self$model_config$target_levels))
      }

      #Generating weights for sequence length
      if(self$last_training$config$balance_sequence_length==TRUE){
        sequence_length=train_data["length"]
        abs_freq_length=table(sequence_length)

        sample_weight_per_sequence_length=as.vector(sum(abs_freq_length)/(length(abs_freq_length)*abs_freq_length))
        sequence_order=names(abs_freq_length)

        sample_weights=vector(length = length(sequence_length))
        for(i in 1:length(sample_weights)){
          idx=which(sequence_length[i]==sequence_order)
          sample_weights[i]=sample_weight_per_sequence_length[idx]
        }
      } else {
        sequence_length=train_data["length"]
        sample_weights=rep.int(x=1,times=length(sequence_length))
      }

      #Reset model if requested
      if(reset_model==TRUE){
        private$create_reset_model()
      }

      #Set Optimizer
      if(private$ml_framework=="tensorflow"){
        balanced_metric=py$BalancedAccuracy(n_classes = as.integer(length(self$model_config$target_levels)))
        if(self$model_config$optimizer=="adam"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$Adam(),
            metrics=c(self$model_config$metric,balanced_metric))
        } else if (self$model_config$optimizer=="rmsprop"){
          self$model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$RMSprop(),
            metrics=c(self$model_config$metric,balanced_metric))
        }
      } else if(private$ml_framework=="pytorch"){
        loss_fct_name="CrossEntropyLoss"
        if(self$model_config$optimizer=="adam"){
          optimizer="adam"
        } else if (self$model_config$optimizer=="rmsprop"){
          optimizer="rmsprop"
        }
      }

      #Check directory for checkpoints
      if(dir.exists(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))==FALSE){
        if(self$last_training$config$trace==TRUE){
          message(paste(date(),"Creating Checkpoint Directory"))
        }
        dir.create(paste0(self$last_training$config$dir_checkpoint,"/checkpoints"))
      }

      #Set target column
      if(self$model_config$require_one_hot==FALSE){
        target_column="labels"
      } else {
        target_column="one_hot_encoding"
      }

      #Tensorflow - Callbacks and training
      if(private$ml_framework=="tensorflow"){
        if(use_callback==TRUE){
          callback=keras$callbacks$ModelCheckpoint(
            filepath = paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"),
            monitor = paste0("val_",self$model_config$balanced_metric),
            verbose = as.integer(min(self$last_training$config$keras_trace,1)),
            mode = "auto",
            save_best_only = TRUE,
            save_weights_only = TRUE)
        } else {
          callback=reticulate::py_none()
        }

        if(private$gui$shiny_app_active==TRUE){
          private$load_reload_python_scripts()

          callback=list(callback,py$ReportAiforeducationShiny())
        }

        data_set_weights=datasets$Dataset$from_dict(
          reticulate::dict(list(
            sample_weights=sample_weights)
          )
        )
        #inputs, targets, sample_weights
        dataset_tf=train_data$add_column("sample_weights",data_set_weights["sample_weights"])
        dataset_tf=dataset_tf$rename_column('input', 'input_embeddings')

        #Choose correct target column and rename
        dataset_tf=dataset_tf$rename_column(target_column, 'targets')

        dataset_tf$with_format("tf")
        tf_dataset_train=dataset_tf$to_tf_dataset(
          columns=c("input_embeddings","sample_weights"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=TRUE,
          label_cols="targets")
        #Add sample weights
        tf_dataset_train=tf_dataset_train$map(py$extract_sample_weight)

        dataset_tf_val=val_data$rename_column('input', 'input_embeddings')
        #Choose correct target column and rename
        dataset_tf_val=dataset_tf_val$rename_column(target_column, 'targets')

        tf_dataset_val=dataset_tf_val$to_tf_dataset(
          columns=c("input_embeddings"),
          batch_size=as.integer(self$last_training$config$batch_size),
          shuffle=FALSE,
          label_cols="targets")

        history<-self$model$fit(
          verbose=as.integer(self$last_training$config$keras_trace),
          x=tf_dataset_train,
          validation_data=tf_dataset_val,
          epochs = as.integer(self$last_training$config$epochs),
          callbacks = callback,
          class_weight=reticulate::py_dict(keys = names(class_weights),values = class_weights))$history

        if(self$model_config$n_categories==2){
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$binary_accuracy,history$val_binary_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        } else {
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$categorical_accuracy,history$val_categorical_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        }

        if(use_callback==TRUE){
          self$model$load_weights(paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.h5"))
        }

        #PyTorch - Callbacks and training
      } else if(private$ml_framework=="pytorch"){

        data_set_weights=datasets$Dataset$from_dict(
          reticulate::dict(list(
            sample_weights=sample_weights)
          )
        )

        dataset_train=train_data$add_column("sample_weights",data_set_weights["sample_weights"])
        dataset_train=dataset_train$select_columns(c("input",target_column,"sample_weights"))
        if(self$model_config$require_one_hot==TRUE){
          dataset_train=dataset_train$rename_column(target_column, "labels")
        }

        pytorch_train_data=dataset_train$with_format("torch")

        pytorch_val_data=val_data$select_columns(c("input",target_column))
        if(self$model_config$require_one_hot==TRUE){
          pytorch_val_data=pytorch_val_data$rename_column(target_column, "labels")
        }
        pytorch_val_data=pytorch_val_data$with_format("torch")

        if(!is.null(test_data)){
          pytorch_test_data=test_data$select_columns(c("input",target_column))
          if(self$model_config$require_one_hot==TRUE){
            pytorch_test_data=pytorch_test_data$rename_column(target_column, "labels")
          }
          pytorch_test_data=pytorch_test_data$with_format("torch")
        } else {
          pytorch_test_data=NULL
        }

        history=py$TeClassifierTrain_PT_with_Datasets(
          model=self$model,
          loss_fct_name=loss_fct_name,
          optimizer_method=self$model_config$optimizer,
          epochs=as.integer(self$last_training$config$epochs),
          trace=as.integer(self$last_training$config$pytorch_trace),
          use_callback=use_callback,
          batch_size=as.integer(self$last_training$config$batch_size),
          train_data=pytorch_train_data,
          val_data=pytorch_val_data,
          test_data=pytorch_test_data,
          filepath=paste0(self$last_training$config$dir_checkpoint,"/checkpoints/best_weights.pt"),
          n_classes=as.integer(length(self$model_config$target_levels)),
          shiny_app_active=self$gui$shiny_app_active,
          class_weights=torch$tensor(np$array(class_weights)))
      }

      #Provide rownames for the history
      for(i in 1:length(history)){
        if(!is.null(history[[i]])){
          if(nrow(history[[i]])==2){
            rownames(history[[i]])=c("train","val")
          } else {
            rownames(history[[i]])=c("train","val","test")
          }
        }
      }
      return(history)
    },
    #--------------------------------------------------------------------------
    #Method for summarizing sustainability data for this classifier
    #List for results must correspond to the private fields of the classifier
    summarize_tracked_sustainability=function(sustainability_tracker){

      results<-list(
        sustainability_tracked=TRUE,
        sustainability_data=list(
          co2eq_kg=sustainability_tracker$final_emissions_data$emissions,
          cpu_energy_kwh=sustainability_tracker$final_emissions_data$cpu_energy,
          gpu_energy_kwh=sustainability_tracker$final_emissions_data$gpu_energy,
          ram_energy_kwh=sustainability_tracker$final_emissions_data$ram_energy,
          total_energy_kwh=sustainability_tracker$final_emissions_data$energy_consumed
        ),
        technical=list(
          tracker="codecarbon",
          py_package_version=codecarbon$"__version__",

          cpu_count=sustainability_tracker$final_emissions_data$cpu_count,
          cpu_model=sustainability_tracker$final_emissions_data$cpu_model,

          gpu_count=sustainability_tracker$final_emissions_data$gpu_count,
          gpu_model=sustainability_tracker$final_emissions_data$gpu_model,

          ram_total_size=sustainability_tracker$final_emissions_data$ram_total_size
        ),
        region=list(
          country_name=sustainability_tracker$final_emissions_data$country_name,
          country_iso_code=sustainability_tracker$final_emissions_data$country_iso_code,
          region=sustainability_tracker$final_emissions_data$region
        )
      )
      return(results)
    }
  )
)

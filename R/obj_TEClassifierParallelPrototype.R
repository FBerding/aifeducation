# This file is part of the R package "aifeducation".
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 3 as published by
# the Free Software Foundation.
#
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>

#' @title Text embedding classifier with a ProtoNet
#' @description Abstract class for neural nets with 'pytorch'.
#'
#'   This object represents in implementation of a prototypical network for few-shot learning as described by Snell,
#'   Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
#'   network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018)
#'
#' @return Objects of this class are used for assigning texts to classes/categories. For the creation and training of a
#'   classifier an object of class [EmbeddedText] or [LargeDataSetForTextEmbeddings] and a `factor` are necessary. The
#'   object of class [EmbeddedText] or [LargeDataSetForTextEmbeddings] contains the numerical text representations (text
#'   embeddings) of the raw texts generated by an object of class [TextEmbeddingModel]. The `factor` contains the
#'   classes/categories for every text. Missing values (unlabeled cases) are supported. For predictions an object of
#'   class [EmbeddedText] or [LargeDataSetForTextEmbeddings] has to be used which was created with the same
#'   [TextEmbeddingModel] as for training.
#'
#'
#' @references Oreshkin, B. N., Rodriguez, P. & Lacoste, A. (2018). TADAM: Task dependent adaptive metric for improved
#'   few-shot learning. https://doi.org/10.48550/arXiv.1805.10123
#' @references Snell, J., Swersky, K. & Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning.
#'   https://doi.org/10.48550/arXiv.1703.05175
#' @references Zhang, X., Nie, J., Zong, L., Yu, H. & Liang, W. (2019). One Shot Learning with Margin. In Q. Yang, Z.-H.
#'   Zhou, Z. Gong, M.-L. Zhang & S.-J. Huang (Eds.), Lecture Notes in Computer Science. Advances in Knowledge Discovery
#'   and Data Mining (Vol. 11440, pp. 305â€“317). Springer International Publishing.
#'   https://doi.org/10.1007/978-3-030-16145-3_24
#' @family Classification
#' @export
TEClassifierParallelPrototype <- R6::R6Class(
  classname = "TEClassifierParallelPrototype",
  inherit = TEClassifiersBasedOnProtoNet,
  public = list(
    # New-----------------------------------------------------------------------
    #' @description Creating a new instance of this class.
    #' @param name `r get_param_doc_desc("name")`
    #' @param label `r get_param_doc_desc("label")`
    #' @param text_embeddings `r get_param_doc_desc("text_embeddings")`
    #' @param feature_extractor `r get_param_doc_desc("feature_extractor")`
    #' @param target_levels `r get_param_doc_desc("target_levels")`
    #' @param intermediate_features `r get_param_doc_desc("intermediate_features")`
    #' @param cls_pooling_type `r get_param_doc_desc("cls_pooling_type")`
    #' @param residual_type `r get_param_doc_desc("residual_type")`
    #' @param normalization_type `r get_param_doc_desc("normalization_type")`
    #' @param feat_act_fct `r get_param_doc_desc("feat_act_fct")`
    #' @param feat_size `r get_param_doc_desc("feat_size")`
    #' @param feat_bias `r get_param_doc_desc("feat_bias")`
    #' @param feat_dropout `r get_param_doc_desc("feat_dropout")`
    #' @param feat_parametrizations `r get_param_doc_desc("feat_parametrizations")`
    #' @param conv_act_fct `r get_param_doc_desc("conv_act_fct")`
    #' @param conv_n_layers `r get_param_doc_desc("conv_n_layers")`
    #' @param conv_ks_min `r get_param_doc_desc("conv_ks_min")`
    #' @param conv_ks_max `r get_param_doc_desc("conv_ks_max")`
    #' @param conv_bias `r get_param_doc_desc("conv_bias")`
    #' @param conv_parametrizations `r get_param_doc_desc("conv_parametrizations")`
    #' @param dense_act_fct `r get_param_doc_desc("dense_act_fct")`
    #' @param dense_n_layers `r get_param_doc_desc("dense_n_layers")`
    #' @param dense_dropout `r get_param_doc_desc("dense_dropout")`
    #' @param dense_bias `r get_param_doc_desc("dense_bias")`
    #' @param dense_parametrizations `r get_param_doc_desc("dense_parametrizations")`
    #' @param rec_act_fct `r get_param_doc_desc("rec_act_fct")`
    #' @param rec_n_layers `r get_param_doc_desc("rec_n_layers")`
    #' @param rec_type `r get_param_doc_desc("rec_type")`
    #' @param rec_bidirectional `r get_param_doc_desc("rec_bidirectional")`
    #' @param rec_dropout `r get_param_doc_desc("rec_dropout")`
    #' @param rec_bias `r get_param_doc_desc("rec_bias")`
    #' @param rec_parametrizations `r get_param_doc_desc("rec_parametrizations")`
    #' @param tf_act_fct `r get_param_doc_desc("tf_act_fct")`
    #' @param tf_dense_dim `r get_param_doc_desc("tf_dense_dim")`
    #' @param tf_n_layers `r get_param_doc_desc("tf_n_layers")`
    #' @param tf_dropout_rate_1 `r get_param_doc_desc("tf_dropout_rate_1")`
    #' @param tf_dropout_rate_2 `r get_param_doc_desc("tf_dropout_rate_2")`
    #' @param tf_attention_type `r get_param_doc_desc("tf_attention_type")`
    #' @param tf_embedding_type `r get_param_doc_desc("tf_embedding_type")`
    #' @param tf_num_heads `r get_param_doc_desc("tf_num_heads")`
    #' @param tf_bias `r get_param_doc_desc("tf_bias")`
    #' @param tf_parametrizations `r get_param_doc_desc("tf_parametrizations")`
    #' @param optimizer `r get_param_doc_desc("optimizer")`
    #' @param merge_attention_type `r get_param_doc_desc("merge_attention_type")`
    #' @param merge_num_heads `r get_param_doc_desc("merge_num_heads")`
    #' @param metric_type `r get_param_doc_desc("metric_type")`
    #' @param embedding_dim `r get_param_doc_desc("embedding_dim")`
    #' @note This model requires `pad_value=0`. If this condition is not met the
    #' padding value is switched automatically.
    configure = function(name = NULL,
                         label = NULL,
                         text_embeddings = NULL,
                         feature_extractor = NULL,
                         target_levels = NULL,
                         intermediate_features = NULL,
                         cls_pooling_type = "min_max",
                         residual_type = "residual_gate",
                         normalization_type = "layer_nom",
                         metric_type = "euclidean",
                         feat_act_fct = "elu",
                         feat_size = 50,
                         feat_bias = TRUE,
                         feat_dropout = 0.0,
                         feat_parametrizations = "None",
                         conv_act_fct = "elu",
                         conv_n_layers = 1,
                         conv_ks_min = 2,
                         conv_ks_max = 4,
                         conv_bias = FALSE,
                         conv_parametrizations = "None",
                         dense_act_fct = "elu",
                         dense_n_layers = 1,
                         dense_dropout = 0.0,
                         dense_bias = FALSE,
                         dense_parametrizations = "None",
                         rec_act_fct = "tanh",
                         rec_n_layers = 1,
                         rec_type = "gru",
                         rec_bidirectional = FALSE,
                         rec_dropout = 0.0,
                         rec_bias = FALSE,
                         rec_parametrizations = "None",
                         tf_act_fct = "elu",
                         tf_dense_dim = 50,
                         tf_n_layers = 1,
                         tf_dropout_rate_1 = 0.0,
                         tf_dropout_rate_2 = 0.0,
                         tf_attention_type = "multihead",
                         tf_embedding_type = "absolute",
                         tf_num_heads = 1,
                         tf_bias = FALSE,
                         tf_parametrizations = "None",
                         merge_attention_type = "multi_head",
                         merge_num_heads = 1,
                         optimizer = "adamw",
                         embedding_dim = 2) {
      arguments <- get_called_args(n = 1)
      arguments$core_net_type <- "parallel"
      private$do_configuration(args = arguments, one_hot_encoding = FALSE)
    }
  ),
  private = list(
    # Private--------------------------------------------------------------------------
    create_reset_model = function() {
      private$check_config_for_TRUE()

      private$load_reload_python_scripts()

      self$model <- py$TEClassifierPrototype(
        features = as.integer(self$model_config$features),
        times = as.integer(self$model_config$times),
        target_levels = reticulate::np_array(seq(from = 0, to = (length(self$model_config$target_levels) - 1))),
        pad_value = as.integer(private$text_embedding_model$pad_value),
        intermediate_features = as.integer(self$model_config$intermediate_features),
        pooling_type = self$model_config$cls_pooling_type,
        residual_type = self$model_config$residual_type,
        normalization_type = self$model_config$normalization_type,
        metric_type = self$model_config$metric_type,
        feat_act_fct = self$model_config$feat_act_fct,
        feat_size = as.integer(self$model_config$feat_size),
        feat_bias = self$model_config$feat_bias,
        feat_dropout = self$model_config$feat_dropout,
        feat_parametrizations = self$model_config$feat_parametrizations,
        conv_act_fct = self$model_config$conv_act_fct,
        conv_n_layers = as.integer(self$model_config$conv_n_layers),
        conv_ks_min = as.integer(self$model_config$conv_ks_min),
        conv_ks_max = as.integer(self$model_config$conv_ks_max),
        conv_bias = self$model_config$conv_bias,
        conv_parametrizations = self$model_config$conv_parametrizations,
        dense_act_fct = self$model_config$dense_act_fct,
        dense_n_layers = as.integer(self$model_config$dense_n_layers),
        dense_dropout = self$model_config$dense_dropout,
        dense_bias = self$model_config$dense_bias,
        dense_parametrizations = self$model_config$dense_parametrizations,
        rec_act_fct = self$model_config$rec_act_fct,
        rec_n_layers = as.integer(self$model_config$rec_n_layers),
        rec_type = self$model_config$rec_type,
        rec_bidirectional = self$model_config$rec_bidirectional,
        rec_dropout = self$model_config$rec_dropout,
        rec_bias = self$model_config$rec_bias,
        rec_parametrizations = self$model_config$rec_parametrizations,
        tf_act_fct = self$model_config$tf_act_fct,
        tf_dense_dim = as.integer(self$model_config$tf_dense_dim),
        tf_n_layers = as.integer(self$model_config$tf_n_layers),
        tf_dropout_rate_1 = self$model_config$tf_dropout_rate_1,
        tf_dropout_rate_2 = self$model_config$tf_dropout_rate_2,
        tf_attention_type = self$model_config$tf_attention_type,
        tf_embedding_type = self$model_config$tf_embedding_type,
        tf_num_heads = as.integer(self$model_config$tf_num_heads),
        tf_bias = self$model_config$tf_bias,
        tf_parametrizations = self$model_config$tf_parametrizations,
        merge_attention_type = self$model_config$merge_attention_type,
        merge_num_heads = as.integer(self$model_config$merge_num_heads),
        embedding_dim = as.integer(self$model_config$embedding_dim),
        core_net_type = self$model_config$core_net_type
      )

      private$set_random_prototypes()
    },
    #--------------------------------------------------------------------------
    check_param_combinations_configuration = function() {},
    #--------------------------------------------------------------------------
    adjust_configuration = function() {
    }
  )
)

# Add Classifier to central index
TEClassifiers_class_names <- append(x = TEClassifiers_class_names, values = "TEClassifierParallelPrototype")

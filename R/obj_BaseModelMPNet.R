# This file is part of the R package "aifeducation".
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 3 as published by
# the Free Software Foundation.
#
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>

#' @title MPNet
#' @description Represents models based on MPNet.
#' @references Song,K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. (2020). MPNet: Masked and Permuted Pre-training for
#'   Language Understanding. \doi{10.48550/arXiv.2004.09297}
#' @return `r get_description("return_object")`
#' @family Base Model
#' @export
BaseModelMPNet <- R6::R6Class(
  classname = "BaseModelMPNet",
  inherit = BaseModelCore,
  private = list(
    model_type = "mpnet",

    adjust_max_sequence_length=2,

    create_model=function(args){
      configuration <- transformers$MPNetConfig(
        vocab_size = as.integer(length(args$tokenizer$get_tokenizer()$get_vocab())+length(unique(args$tokenizer$get_tokenizer()$special_tokens_map))),
        hidden_size = as.integer(args$hidden_size),
        num_hidden_layers = as.integer(args$num_hidden_layers),
        num_attention_heads = as.integer(args$num_attention_heads),
        intermediate_size = as.integer(args$intermediate_size),
        hidden_act = tolower(args$hidden_act),
        hidden_dropout_prob = args$hidden_dropout_prob,
        attention_probs_dropout_prob = args$attention_probs_dropout_prob,
        max_position_embeddings = as.integer(args$max_position_embeddings),
        initializer_range = 0.02,
        layer_norm_eps = 1e-12
      )

      run_py_file("MPNetForMPLM_PT.py")
      device <- ifelse(torch$cuda$is_available(), "cuda", "cpu")
      private$model <- py$MPNetForMPLM_PT(configuration)$to(device)

    },
    #--------------------------------------------------------------------------
    create_data_collator = function() {
      collator_maker <- NULL
      run_py_file("DataCollatorForMPLM_PT.py")
      collator_maker <- py$CollatorMaker_PT(
        tokenizer = self$Tokenizer$get_tokenizer(),
        mlm = TRUE,
        mlm_probability = self$last_training$config$p_mask,
        plm_probability =  self$last_training$config$p_perm,
        mask_whole_words =  self$last_training$config$whole_word
      )
      return(collator_maker$collator$collate_batch)
    },
    #--------------------------------------------------------------------------
    load_BaseModel=function(dir_path){
      private$model <- py$MPNetForMPLM_PT$from_pretrained(
        dir_path,
        from_tf = FALSE,
        use_safetensors = TRUE)
    }
  ),
  public = list(
    #---------------------------------------------------------------------------
    #' @description Configures a new object of this class.
    #' @param tokenizer `r get_param_doc_desc("tokenizer")`
    #' @param max_position_embeddings `r get_param_doc_desc("max_position_embeddings")`
    #' @param hidden_size `r get_param_doc_desc("hidden_size")`
    #' @param num_hidden_layers `r get_param_doc_desc("num_hidden_layers")`
    #' @param num_attention_heads `r get_param_doc_desc("num_attention_heads")`
    #' @param intermediate_size `r get_param_doc_desc("intermediate_size")`
    #' @param hidden_act `r get_param_doc_desc("hidden_act")`
    #' @param hidden_dropout_prob `r get_param_doc_desc("hidden_dropout_prob")`
    #' @param attention_probs_dropout_prob `r get_param_doc_desc("attention_probs_dropout_prob")`
    #' @return `r get_description("return_nothing")`
    configure = function(tokenizer,
                         max_position_embeddings = 512,
                         hidden_size = 768,
                         num_hidden_layers = 12,
                         num_attention_heads = 12,
                         intermediate_size = 3072,
                         hidden_act = "GELU",
                         hidden_dropout_prob = 0.1,
                         attention_probs_dropout_prob = 0.1) {
      arguments <- get_called_args(n = 1)
      private$do_configuration(args = arguments)
    },
    #--------------------------------------------------------------------------
    #' @description Traines a BaseModel
    #' @param text_dataset `r get_description("text_dataset")`
    #' @param p_mask `r get_description("p_mask")`
    #' @param p_perm `r get_description("p_perm")`
    #' @param whole_word `r get_description("whole_word")`
    #' @param val_size `r get_description("val_size")`
    #' @param n_epoch `r get_description("n_epoch")`
    #' @param batch_size `r get_description("batch_size")`
    #' @param max_sequence_length `r get_description("max_sequence_length")`
    #' @param full_sequences_only `r get_description("full_sequences_only")`
    #' @param min_seq_len `r get_description("min_seq_len")`
    #' @param learning_rate `r get_description("learning_rate")`
    #' @param sustain_track `r get_description("sustain_track")`
    #' @param sustain_iso_code `r get_description("sustain_iso_code")`
    #' @param sustain_region `r get_description("sustain_region")`
    #' @param sustain_interval `r get_description("sustain_interval")`
    #' @param trace `r get_description("trace")`
    #' @param pytorch_trace `r get_description("pytorch_trace")`
    #' @param log_dir `r get_description("log_dir")`
    #' @param log_write_interval `r get_description("log_write_interval")`
    #' @return `r get_description("return_nothing")`
    train=function(text_dataset,
                   p_mask = 0.15,
                   p_perm = 0.15,
                   whole_word = TRUE,
                   val_size = 0.1,
                   n_epoch = 1,
                   batch_size = 12,
                   max_sequence_length = 250,
                   full_sequences_only = FALSE,
                   min_seq_len = 50,
                   learning_rate = 3e-3,
                   sustain_track = FALSE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_trace = 1,
                   log_dir = NULL,
                   log_write_interval = 2){
      private$do_training(args=get_called_args(n=1))
    }
  )
)

#Add the model to the user list
BaseModelsIndex$MPNet=("BaseModelMPNet")

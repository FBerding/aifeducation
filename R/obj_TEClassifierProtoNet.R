# This file is part of the R package "aifeducation".
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 3 as published by
# the Free Software Foundation.
#
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>

#' @title Text embedding classifier with a ProtoNet
#' @description Abstract class for neural nets with 'pytorch'.
#'
#'   This object represents in implementation of a prototypical network for few-shot learning as described by Snell,
#'   Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
#'   network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018)
#'
#' @return Objects of this class are used for assigning texts to classes/categories. For the creation and training of a
#'   classifier an object of class [EmbeddedText] or [LargeDataSetForTextEmbeddings] and a `factor` are necessary. The
#'   object of class [EmbeddedText] or [LargeDataSetForTextEmbeddings] contains the numerical text representations (text
#'   embeddings) of the raw texts generated by an object of class [TextEmbeddingModel]. The `factor` contains the
#'   classes/categories for every text. Missing values (unlabeled cases) are supported. For predictions an object of
#'   class [EmbeddedText] or [LargeDataSetForTextEmbeddings] has to be used which was created with the same
#'   [TextEmbeddingModel] as for training.
#'
#' @references Oreshkin, B. N., Rodriguez, P. & Lacoste, A. (2018). TADAM: Task dependent adaptive metric for improved
#'   few-shot learning. https://doi.org/10.48550/arXiv.1805.10123
#' @references Snell, J., Swersky, K. & Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning.
#'   https://doi.org/10.48550/arXiv.1703.05175
#' @references Zhang, X., Nie, J., Zong, L., Yu, H. & Liang, W. (2019). One Shot Learning with Margin. In Q. Yang, Z.-H.
#'   Zhou, Z. Gong, M.-L. Zhang & S.-J. Huang (Eds.), Lecture Notes in Computer Science. Advances in Knowledge Discovery
#'   and Data Mining (Vol. 11440, pp. 305â€“317). Springer International Publishing.
#'   https://doi.org/10.1007/978-3-030-16145-3_24
#' @family Classification
#' @export
TEClassifierProtoNet <- R6::R6Class(
  classname = "TextEmbeddingClassifiersProtoNet",
  inherit = TEClassifiersBasedOnProtoNet,
  public = list(
    # New-----------------------------------------------------------------------
    #' @description Creating a new instance of this class.
    #' @param name `r get_param_doc_desc("name")`
    #' @param label `r get_param_doc_desc("label")`
    #' @param text_embeddings `r get_param_doc_desc("text_embeddings")`
    #' @param feature_extractor `r get_param_doc_desc("feature_extractor")`
    #' @param bias `r get_param_doc_desc("bias")`
    #' @param target_levels `r get_param_doc_desc("target_levels")`
    #' @param dense_layers `r get_param_doc_desc("dense_layers")`
    #' @param dense_size `r get_param_doc_desc("dense_size")`
    #' @param rec_layers `r get_param_doc_desc("rec_layers")`
    #' @param rec_size `r get_param_doc_desc("rec_size")`
    #' @param rec_type `r get_param_doc_desc("rec_type")`
    #' @param rec_bidirectional `r get_param_doc_desc("rec_bidirectional")`
    #' @param attention_type `r get_param_doc_desc("attention_type")`
    #' @param self_attention_heads `r get_param_doc_desc("self_attention_heads")`
    #' @param repeat_encoder `r get_param_doc_desc("repeat_encoder")`
    #' @param intermediate_size `r get_param_doc_desc("intermediate_size")`
    #' @param add_pos_embedding `r get_param_doc_desc("add_pos_embedding")`
    #' @param act_fct `r get_param_doc_desc("act_fct")`
    #' @param parametrizations `r get_param_doc_desc("parametrizations")`
    #' @param encoder_dropout `r get_param_doc_desc("encoder_dropout")`
    #' @param dense_dropout `r get_param_doc_desc("dense_dropout")`
    #' @param rec_dropout `r get_param_doc_desc("rec_dropout")`
    #' @param optimizer `r get_param_doc_desc("optimizer")`
    #' @param embedding_dim `r get_param_doc_desc("optimizer")`
    configure = function(name = NULL,
                         label = NULL,
                         text_embeddings = NULL,
                         feature_extractor = NULL,
                         target_levels = NULL,
                         dense_size = 4,
                         dense_layers = 0,
                         rec_size = 4,
                         rec_layers = 2,
                         rec_type = "gru",
                         rec_bidirectional = FALSE,
                         embedding_dim = 2,
                         self_attention_heads = 0,
                         intermediate_size = NULL,
                         attention_type = "fourier",
                         add_pos_embedding = TRUE,
                         act_fct="elu",
                         parametrizations="None",
                         rec_dropout = 0.1,
                         repeat_encoder = 1,
                         dense_dropout = 0.4,
                         encoder_dropout = 0.1,
                         optimizer = "adamw") {
      private$do_configuration(args=get_called_args(n=1))
    }
  ),
  private = list(
    #Private--------------------------------------------------------------------------
    create_reset_model = function() {

      private$check_config_for_TRUE()

      private$load_reload_python_scripts()

      self$model <- py$TextEmbeddingClassifierProtoNet_PT(
        features = as.integer(self$model_config$features),
        times = as.integer(self$model_config$times),
        dense_size = as.integer(self$model_config$dense_size),
        dense_layers = as.integer(self$model_config$dense_layers),
        rec_size = as.integer(self$model_config$rec_size),
        rec_layers = as.integer(self$model_config$rec_layers),
        rec_type = self$model_config$rec_type,
        rec_bidirectional = self$model_config$rec_bidirectional,
        intermediate_size = as.integer(self$model_config$intermediate_size),
        attention_type = self$model_config$attention_type,
        repeat_encoder = as.integer(self$model_config$repeat_encoder),
        dense_dropout = self$model_config$dense_dropout,
        rec_dropout = self$model_config$rec_dropout,
        encoder_dropout = self$model_config$encoder_dropout,
        add_pos_embedding = self$model_config$add_pos_embedding,
        self_attention_heads = as.integer(self$model_config$self_attention_heads),
        embedding_dim = as.integer(self$model_config$embedding_dim),
        target_levels = reticulate::np_array(seq(from = 0, to = (length(self$model_config$target_levels) - 1))),
        act_fct=self$model_config$act_fct,
        parametrizations=self$model_config$parametrizations
      )
    },
    #--------------------------------------------------------------------------
    check_param_combinations_configuration=function(){
      if (self$model_config$dense_layers > 0) {
        if (self$model_config$dense_size < 1) {
          stop("Dense layers added. Size for dense layers must be at least 1.")
        }
      }

      if (self$model_config$rec_layers > 0) {
        if (self$model_config$rec_size < 1) {
          stop("Recurrent  layers added. Size for recurrent layers must be at least 1.")
        }
      }

      if (self$model_config$repeat_encoder > 0 &
          self$model_config$attention_type == "multihead" &
          self$model_config$self_attention_heads <= 0) {
        stop("Encoder layer is set to 'multihead'. This requires self_attention_heads>=1.")
      }

      if (self$model_config$rec_layers != 0 & self$model_config$self_attention_heads > 0) {
        if (self$model_config$features %% 2 != 0) {
          stop("The number of features of the TextEmbeddingmodel is
               not a multiple of 2.")
        }
      }
    },
    #--------------------------------------------------------------------------
    adjust_configuration = function() {
      if (is.null(self$model_config$intermediate_size) == TRUE) {
        if (self$model_config$attention_type == "fourier" & self$model_config$rec_layers > 0) {
          self$model_config$intermediate_size <- 2 * self$model_config$rec_size
        } else if (self$model_config$attention_type == "fourier" & self$model_config$rec_layers == 0) {
          self$model_config$intermediate_size <- 2 * self$model_config$features
        } else if (
          self$model_config$attention_type == "multihead" &
          self$model_config$rec_layers > 0 &
          self$model_config$self_attention_heads > 0
        ) {
          self$model_config$intermediate_size <- 2 * self$model_config$features
        } else if (
          self$model_config$attention_type == "multihead" &
          self$model_config$rec_layers == 0 &
          self$model_config$self_attention_heads > 0
        ) {
          self$model_config$intermediate_size <- 2 * self$model_config$features
        } else {
          self$model_config$intermediate_size <- NULL
        }
      }

      if (self$model_config$rec_layers <= 1) {
        self$model_config$rec_dropout <- 0.0
      }
      if (self$model_config$rec_layers <= 0) {
        self$model_config$rec_size <- 0
      }

      if (self$model_config$dense_layers <= 1) {
        self$model_config$dense_dropout <- 0.0
      }
      if (self$model_config$dense_layers <= 0) {
        self$model_config$dense_size <- 0
      }
    }
  )
)

<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Text embedding classifier with a ProtoNet — TEClassifierSequentialPrototype • aifeducation</title><!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png"><link rel="icon" type="”image/svg+xml”" href="../favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" sizes="any" href="../favicon.ico"><link rel="manifest" href="../site.webmanifest"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Text embedding classifier with a ProtoNet — TEClassifierSequentialPrototype"><meta name="description" content="Classification Type
This object is a metric based classifer and represents in implementation of a prototypical network for
few-shot learning as described by Snell,
Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018).
Sequential Core Architecture
This model is based on a sequential architecture.
The input is passed to a specific number of layers step by step.
All layers are grouped by their kind into stacks.
Transformer Encoder Layers
Description
The transformer encoder layers follow the structure of the encoder layers
used in transformer models. A single layer is designed as described by Chollet, Kalinowski, and Allaire (2022, p. 373) with
the exception that single components of the layers (such as the activation function,
the kind of residual connection, the kind of normalization or the kind of attention) can be customized. All parameters with the prefix tf_ can be used to configure this layer.
Feature Layer
Description
The feature layer is a dense layer that can be used to
increase or decrease the number of features of the input data before passing the
data into your model. The aim of this layer is to increase or reduce the complexity of the data for your model.
The output size of this layer determines the number of features for all following layers. In the special case that
the requested number of features equals the number of features of the text embeddings this layer
is reduced to a dropout layer with masking capabilities. All parameters with the prefix feat_ can be used to configure this layer.
Dense Layers
Description
A fully connected layer. The layer is applied to every step of a sequence. All parameters with the prefix dense_ can be used to configure this layer.
Multiple N-Gram Layers
Description
This type of layer focuses on sub-sequence and performs an 1d convolutional operation. On a word and token level
these sub-sequences can be interpreted as n-grams (Jacovi, Shalom &amp;amp; Goldberg 2018). The convolution is done across all features.
The number of filters equals the number of features of the input tensor. Thus, the shape of the tensor is retained (Pham, Kruszewski &amp;amp; Boleda 2016).
The layer is able to consider multiple n-grams at the same time. In this case the convolution of the n-grams is done
seprately and the resulting tensors are concatenated along the feature dimension. The number of filters for every n-gram
is set to num_features/num_n-grams. Thus, the resulting tensor has the same shape as the input tensor.
Sub-sequences that are masked in the input are
also masked in the output.
The output of this layer can be understand as the results of the n-gram filters. Stacking this layer
allows the model to perform n-gram detection of n-grams (meta perspective). All parameters with the prefix ng_conv_ can be used to configure this layer.
Recurrent Layers
Description
A regular recurrent layer either as Gated Recurrent Unit (GRU) or Long Short-Term Memory (LSTM) layer. Uses
PyTorchs implementation. All parameters with the prefix rec_ can be used to configure this layer.
Classifiction Pooling Layer
Description
Layer transforms sequences into a lower dimensional space that can be passed to dense layers. It
performs two types of pooling. First, it extractes features across the time dimension selecting the maximal
and/or minimal features. Second, it performs pooling over the remaining features selecting a speficifc number of
the heighest and/or lowest features.
In the case of selecting the minmal and maximal features at the same time the minmal
features are concatenated to the tensor of the maximal features resulting the in the shape $(Batch, Times, 2*Features)$ at the end of the first step.
In the second step the
number of requested features is halved. The first half is used for the maximal features and the second for the minimal
features. All parameters with the prefix cls_pooling_ can be used to configure this layer.
Training and Prediction
For the creation and training of a
classifier an object of class EmbeddedText or LargeDataSetForTextEmbeddings on the one hand and a factor on
the other hand are necessary.
The object of class EmbeddedText or LargeDataSetForTextEmbeddings  contains the numerical text representations
(text embeddings) of the raw texts generated by an object of class TextEmbeddingModel. For supporting large data
sets it is recommended to use LargeDataSetForTextEmbeddings instead of EmbeddedText.
The factor contains the classes/categories for every text. Missing values (unlabeled cases) are supported and can
be used for pseudo labeling.
For predictions an object of class EmbeddedText or LargeDataSetForTextEmbeddings has to be used which was
created with the same TextEmbeddingModel as for training.."><meta property="og:description" content="Classification Type
This object is a metric based classifer and represents in implementation of a prototypical network for
few-shot learning as described by Snell,
Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018).
Sequential Core Architecture
This model is based on a sequential architecture.
The input is passed to a specific number of layers step by step.
All layers are grouped by their kind into stacks.
Transformer Encoder Layers
Description
The transformer encoder layers follow the structure of the encoder layers
used in transformer models. A single layer is designed as described by Chollet, Kalinowski, and Allaire (2022, p. 373) with
the exception that single components of the layers (such as the activation function,
the kind of residual connection, the kind of normalization or the kind of attention) can be customized. All parameters with the prefix tf_ can be used to configure this layer.
Feature Layer
Description
The feature layer is a dense layer that can be used to
increase or decrease the number of features of the input data before passing the
data into your model. The aim of this layer is to increase or reduce the complexity of the data for your model.
The output size of this layer determines the number of features for all following layers. In the special case that
the requested number of features equals the number of features of the text embeddings this layer
is reduced to a dropout layer with masking capabilities. All parameters with the prefix feat_ can be used to configure this layer.
Dense Layers
Description
A fully connected layer. The layer is applied to every step of a sequence. All parameters with the prefix dense_ can be used to configure this layer.
Multiple N-Gram Layers
Description
This type of layer focuses on sub-sequence and performs an 1d convolutional operation. On a word and token level
these sub-sequences can be interpreted as n-grams (Jacovi, Shalom &amp;amp; Goldberg 2018). The convolution is done across all features.
The number of filters equals the number of features of the input tensor. Thus, the shape of the tensor is retained (Pham, Kruszewski &amp;amp; Boleda 2016).
The layer is able to consider multiple n-grams at the same time. In this case the convolution of the n-grams is done
seprately and the resulting tensors are concatenated along the feature dimension. The number of filters for every n-gram
is set to num_features/num_n-grams. Thus, the resulting tensor has the same shape as the input tensor.
Sub-sequences that are masked in the input are
also masked in the output.
The output of this layer can be understand as the results of the n-gram filters. Stacking this layer
allows the model to perform n-gram detection of n-grams (meta perspective). All parameters with the prefix ng_conv_ can be used to configure this layer.
Recurrent Layers
Description
A regular recurrent layer either as Gated Recurrent Unit (GRU) or Long Short-Term Memory (LSTM) layer. Uses
PyTorchs implementation. All parameters with the prefix rec_ can be used to configure this layer.
Classifiction Pooling Layer
Description
Layer transforms sequences into a lower dimensional space that can be passed to dense layers. It
performs two types of pooling. First, it extractes features across the time dimension selecting the maximal
and/or minimal features. Second, it performs pooling over the remaining features selecting a speficifc number of
the heighest and/or lowest features.
In the case of selecting the minmal and maximal features at the same time the minmal
features are concatenated to the tensor of the maximal features resulting the in the shape $(Batch, Times, 2*Features)$ at the end of the first step.
In the second step the
number of requested features is halved. The first half is used for the maximal features and the second for the minimal
features. All parameters with the prefix cls_pooling_ can be used to configure this layer.
Training and Prediction
For the creation and training of a
classifier an object of class EmbeddedText or LargeDataSetForTextEmbeddings on the one hand and a factor on
the other hand are necessary.
The object of class EmbeddedText or LargeDataSetForTextEmbeddings  contains the numerical text representations
(text embeddings) of the raw texts generated by an object of class TextEmbeddingModel. For supporting large data
sets it is recommended to use LargeDataSetForTextEmbeddings instead of EmbeddedText.
The factor contains the classes/categories for every text. Missing values (unlabeled cases) are supported and can
be used for pseudo labeling.
For predictions an object of class EmbeddedText or LargeDataSetForTextEmbeddings has to be used which was
created with the same TextEmbeddingModel as for training.."><meta property="og:image" content="https://fberding.github.io/aifeducation/logo.png"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/aifeducation.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><h6 class="dropdown-header" data-toc-skip>For users</h6></li>
    <li><a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a></li>
    <li><a class="dropdown-item" href="../articles/gui_aife_studio.html">02 Aifeducation Studio</a></li>
    <li><a class="dropdown-item" href="../articles/classification_tasks.html">03 Using the Package without Studio</a></li>
    <li><a class="dropdown-item" href="../articles/model_configuration.html">04 Model configuration</a></li>
    <li><a class="dropdown-item" href="../articles/sharing_and_publishing.html">05 Sharing and Using Trained AI/Models</a></li>
    <li><a class="dropdown-item" href="../articles/a01_layers_stacks.html">Appendix 01 Layers and Stacks</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>For developers</h6></li>
    <li><a class="dropdown-item" href="../articles/transformers.html">01 Transformers</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Text embedding classifier with a ProtoNet</h1>
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/R/obj_TEClassifierSequentialPrototype.R" class="external-link"><code>R/obj_TEClassifierSequentialPrototype.R</code></a></small>
      <div class="d-none name"><code>TEClassifierSequentialPrototype.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><strong>Classification Type</strong></p>
<p>This object is a metric based classifer and represents in implementation of a prototypical network for
few-shot learning as described by Snell,
Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018).</p>
<p><strong>Sequential Core Architecture</strong></p>
<p>This model is based on a sequential architecture.
The input is passed to a specific number of layers step by step.
All layers are grouped by their kind into stacks.</p>
<p><strong>Transformer Encoder Layers</strong></p>
<p><em>Description</em></p>
<p>The transformer encoder layers follow the structure of the encoder layers
used in transformer models. A single layer is designed as described by Chollet, Kalinowski, and Allaire (2022, p. 373) with
the exception that single components of the layers (such as the activation function,
the kind of residual connection, the kind of normalization or the kind of attention) can be customized. All parameters with the prefix <em>tf_</em> can be used to configure this layer.</p>
<p><strong>Feature Layer</strong></p>
<p><em>Description</em></p>
<p>The feature layer is a dense layer that can be used to
increase or decrease the number of features of the input data before passing the
data into your model. The aim of this layer is to increase or reduce the complexity of the data for your model.
The output size of this layer determines the number of features for all following layers. In the special case that
the requested number of features equals the number of features of the text embeddings this layer
is reduced to a dropout layer with masking capabilities. All parameters with the prefix <em>feat_</em> can be used to configure this layer.</p>
<p><strong>Dense Layers</strong></p>
<p><em>Description</em></p>
<p>A fully connected layer. The layer is applied to every step of a sequence. All parameters with the prefix <em>dense_</em> can be used to configure this layer.</p>
<p><strong>Multiple N-Gram Layers</strong></p>
<p><em>Description</em></p>
<p>This type of layer focuses on sub-sequence and performs an 1d convolutional operation. On a word and token level
these sub-sequences can be interpreted as n-grams (Jacovi, Shalom &amp; Goldberg 2018). The convolution is done across all features.
The number of filters equals the number of features of the input tensor. Thus, the shape of the tensor is retained (Pham, Kruszewski &amp; Boleda 2016).</p>
<p>The layer is able to consider multiple n-grams at the same time. In this case the convolution of the n-grams is done
seprately and the resulting tensors are concatenated along the feature dimension. The number of filters for every n-gram
is set to num_features/num_n-grams. Thus, the resulting tensor has the same shape as the input tensor.</p>
<p>Sub-sequences that are masked in the input are
also masked in the output.</p>
<p>The output of this layer can be understand as the results of the n-gram filters. Stacking this layer
allows the model to perform n-gram detection of n-grams (meta perspective). All parameters with the prefix <em>ng_conv_</em> can be used to configure this layer.</p>
<p><strong>Recurrent Layers</strong></p>
<p><em>Description</em></p>
<p>A regular recurrent layer either as Gated Recurrent Unit (GRU) or Long Short-Term Memory (LSTM) layer. Uses
PyTorchs implementation. All parameters with the prefix <em>rec_</em> can be used to configure this layer.</p>
<p><strong>Classifiction Pooling Layer</strong></p>
<p><em>Description</em></p>
<p>Layer transforms sequences into a lower dimensional space that can be passed to dense layers. It
performs two types of pooling. First, it extractes features across the time dimension selecting the maximal
and/or minimal features. Second, it performs pooling over the remaining features selecting a speficifc number of
the heighest and/or lowest features.</p>
<p>In the case of selecting the minmal <em>and</em> maximal features at the same time the minmal
features are concatenated to the tensor of the maximal features resulting the in the shape $(Batch, Times, 2*Features)$ at the end of the first step.
In the second step the
number of requested features is halved. The first half is used for the maximal features and the second for the minimal
features. All parameters with the prefix <em>cls_pooling_</em> can be used to configure this layer.</p>
<p><strong>Training and Prediction</strong></p>
<p>For the creation and training of a
classifier an object of class <a href="EmbeddedText.html">EmbeddedText</a> or <a href="LargeDataSetForTextEmbeddings.html">LargeDataSetForTextEmbeddings</a> on the one hand and a <a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a> on
the other hand are necessary.</p>
<p>The object of class <a href="EmbeddedText.html">EmbeddedText</a> or <a href="LargeDataSetForTextEmbeddings.html">LargeDataSetForTextEmbeddings</a>  contains the numerical text representations
(text embeddings) of the raw texts generated by an object of class <a href="TextEmbeddingModel.html">TextEmbeddingModel</a>. For supporting large data
sets it is recommended to use <a href="LargeDataSetForTextEmbeddings.html">LargeDataSetForTextEmbeddings</a> instead of <a href="EmbeddedText.html">EmbeddedText</a>.</p>
<p>The <code>factor</code> contains the classes/categories for every text. Missing values (unlabeled cases) are supported and can
be used for pseudo labeling.</p>
<p>For predictions an object of class <a href="EmbeddedText.html">EmbeddedText</a> or <a href="LargeDataSetForTextEmbeddings.html">LargeDataSetForTextEmbeddings</a> has to be used which was
created with the same <a href="TextEmbeddingModel.html">TextEmbeddingModel</a> as for training..</p>
    </div>


    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>Returns a new object of this class ready for configuration or for loading
a saved classifier.</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Oreshkin, B. N., Rodriguez, P. &amp; Lacoste, A. (2018). TADAM: Task dependent adaptive metric for improved
few-shot learning. https://doi.org/10.48550/arXiv.1805.10123</p>
<p>Snell, J., Swersky, K. &amp; Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning.
https://doi.org/10.48550/arXiv.1703.05175</p>
<p>Zhang, X., Nie, J., Zong, L., Yu, H. &amp; Liang, W. (2019). One Shot Learning with Margin. In Q. Yang, Z.-H.
Zhou, Z. Gong, M.-L. Zhang &amp; S.-J. Huang (Eds.), Lecture Notes in Computer Science. Advances in Knowledge Discovery
and Data Mining (Vol. 11440, pp. 305–317). Springer International Publishing.
https://doi.org/10.1007/978-3-030-16145-3_24</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>Other Classification:
<code><a href="TEClassifierParallel.html">TEClassifierParallel</a></code>,
<code><a href="TEClassifierParallelPrototype.html">TEClassifierParallelPrototype</a></code>,
<code><a href="TEClassifierProtoNet.html">TEClassifierProtoNet</a></code>,
<code><a href="TEClassifierRegular.html">TEClassifierRegular</a></code>,
<code><a href="TEClassifierSequential.html">TEClassifierSequential</a></code></p></div>
    </div>
    <div class="section level2">
    <h2 id="super-classes">Super classes<a class="anchor" aria-label="anchor" href="#super-classes"></a></h2>
    <p><code><a href="AIFEBaseModel.html">aifeducation::AIFEBaseModel</a></code> -&gt; <code><a href="ModelsBasedOnTextEmbeddings.html">aifeducation::ModelsBasedOnTextEmbeddings</a></code> -&gt; <code><a href="ClassifiersBasedOnTextEmbeddings.html">aifeducation::ClassifiersBasedOnTextEmbeddings</a></code> -&gt; <code><a href="TEClassifiersBasedOnProtoNet.html">aifeducation::TEClassifiersBasedOnProtoNet</a></code> -&gt; <code>TEClassifierSequentialPrototype</code></p>
    </div>
    <div class="section level2">
    <h2 id="methods">Methods<a class="anchor" aria-label="anchor" href="#methods"></a></h2>

<div class="section">
<h3 id="public-methods">Public methods<a class="anchor" aria-label="anchor" href="#public-methods"></a></h3>

<ul><li><p><a href="#method-TEClassifierSequentialPrototype-configure"><code>TEClassifierSequentialPrototype$configure()</code></a></p></li>
<li><p><a href="#method-TEClassifierSequentialPrototype-clone"><code>TEClassifierSequentialPrototype$clone()</code></a></p></li>
</ul></div><p><details><summary>Inherited methods</summary><ul><li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="count_parameter"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-count_parameter"><code>aifeducation::AIFEBaseModel$count_parameter()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_all_fields"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_all_fields"><code>aifeducation::AIFEBaseModel$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_documentation_license"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_documentation_license"><code>aifeducation::AIFEBaseModel$get_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_ml_framework"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_ml_framework"><code>aifeducation::AIFEBaseModel$get_ml_framework()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_description"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_description"><code>aifeducation::AIFEBaseModel$get_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_info"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_info"><code>aifeducation::AIFEBaseModel$get_model_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_license"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_license"><code>aifeducation::AIFEBaseModel$get_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_package_versions"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_package_versions"><code>aifeducation::AIFEBaseModel$get_package_versions()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_private"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_private"><code>aifeducation::AIFEBaseModel$get_private()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_publication_info"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_publication_info"><code>aifeducation::AIFEBaseModel$get_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_sustainability_data"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_sustainability_data"><code>aifeducation::AIFEBaseModel$get_sustainability_data()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="is_configured"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-is_configured"><code>aifeducation::AIFEBaseModel$is_configured()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="is_trained"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-is_trained"><code>aifeducation::AIFEBaseModel$is_trained()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="load"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-load"><code>aifeducation::AIFEBaseModel$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_documentation_license"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_documentation_license"><code>aifeducation::AIFEBaseModel$set_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_description"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_description"><code>aifeducation::AIFEBaseModel$set_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_license"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_license"><code>aifeducation::AIFEBaseModel$set_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_publication_info"><a href="../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_publication_info"><code>aifeducation::AIFEBaseModel$set_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ModelsBasedOnTextEmbeddings" data-id="get_text_embedding_model"><a href="../../aifeducation/html/ModelsBasedOnTextEmbeddings.html#method-ModelsBasedOnTextEmbeddings-get_text_embedding_model"><code>aifeducation::ModelsBasedOnTextEmbeddings$get_text_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ModelsBasedOnTextEmbeddings" data-id="get_text_embedding_model_name"><a href="../../aifeducation/html/ModelsBasedOnTextEmbeddings.html#method-ModelsBasedOnTextEmbeddings-get_text_embedding_model_name"><code>aifeducation::ModelsBasedOnTextEmbeddings$get_text_embedding_model_name()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="adjust_target_levels"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-adjust_target_levels"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$adjust_target_levels()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="check_embedding_model"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-check_embedding_model"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$check_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="check_feature_extractor_object_type"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-check_feature_extractor_object_type"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$check_feature_extractor_object_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="load_from_disk"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-load_from_disk"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$load_from_disk()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="plot_coding_stream"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-plot_coding_stream"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$plot_coding_stream()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="plot_training_history"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-plot_training_history"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$plot_training_history()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="predict"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-predict"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="requires_compression"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-requires_compression"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$requires_compression()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="ClassifiersBasedOnTextEmbeddings" data-id="save"><a href="../../aifeducation/html/ClassifiersBasedOnTextEmbeddings.html#method-ClassifiersBasedOnTextEmbeddings-save"><code>aifeducation::ClassifiersBasedOnTextEmbeddings$save()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifiersBasedOnProtoNet" data-id="embed"><a href="../../aifeducation/html/TEClassifiersBasedOnProtoNet.html#method-TEClassifiersBasedOnProtoNet-embed"><code>aifeducation::TEClassifiersBasedOnProtoNet$embed()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifiersBasedOnProtoNet" data-id="get_metric_scale_factor"><a href="../../aifeducation/html/TEClassifiersBasedOnProtoNet.html#method-TEClassifiersBasedOnProtoNet-get_metric_scale_factor"><code>aifeducation::TEClassifiersBasedOnProtoNet$get_metric_scale_factor()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifiersBasedOnProtoNet" data-id="plot_embeddings"><a href="../../aifeducation/html/TEClassifiersBasedOnProtoNet.html#method-TEClassifiersBasedOnProtoNet-plot_embeddings"><code>aifeducation::TEClassifiersBasedOnProtoNet$plot_embeddings()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifiersBasedOnProtoNet" data-id="predict_with_samples"><a href="../../aifeducation/html/TEClassifiersBasedOnProtoNet.html#method-TEClassifiersBasedOnProtoNet-predict_with_samples"><code>aifeducation::TEClassifiersBasedOnProtoNet$predict_with_samples()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifiersBasedOnProtoNet" data-id="train"><a href="../../aifeducation/html/TEClassifiersBasedOnProtoNet.html#method-TEClassifiersBasedOnProtoNet-train"><code>aifeducation::TEClassifiersBasedOnProtoNet$train()</code></a></span></li>
</ul></details></p><hr><a id="method-TEClassifierSequentialPrototype-configure"></a><div class="section">
<h3 id="method-configure-">Method <code>configure()</code><a class="anchor" aria-label="anchor" href="#method-configure-"></a></h3>
<p>Creating a new instance of this class.</p><div class="section">
<h4 id="usage">Usage<a class="anchor" aria-label="anchor" href="#usage"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">TEClassifierSequentialPrototype</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  label <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  text_embeddings <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  feature_extractor <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  target_levels <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  skip_connection_type <span class="op">=</span> <span class="st">"ResidualGate"</span>,</span>
<span>  cls_pooling_features <span class="op">=</span> <span class="fl">50</span>,</span>
<span>  cls_pooling_type <span class="op">=</span> <span class="st">"MinMax"</span>,</span>
<span>  metric_type <span class="op">=</span> <span class="st">"Euclidean"</span>,</span>
<span>  feat_act_fct <span class="op">=</span> <span class="st">"ELU"</span>,</span>
<span>  feat_size <span class="op">=</span> <span class="fl">50</span>,</span>
<span>  feat_bias <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  feat_dropout <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  feat_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  feat_normalization_type <span class="op">=</span> <span class="st">"LayerNorm"</span>,</span>
<span>  ng_conv_act_fct <span class="op">=</span> <span class="st">"ELU"</span>,</span>
<span>  ng_conv_n_layers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  ng_conv_ks_min <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  ng_conv_ks_max <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  ng_conv_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  ng_conv_dropout <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  ng_conv_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  ng_conv_normalization_type <span class="op">=</span> <span class="st">"LayerNorm"</span>,</span>
<span>  ng_conv_residual_type <span class="op">=</span> <span class="st">"ResidualGate"</span>,</span>
<span>  dense_act_fct <span class="op">=</span> <span class="st">"ELU"</span>,</span>
<span>  dense_n_layers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  dense_dropout <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  dense_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  dense_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  dense_normalization_type <span class="op">=</span> <span class="st">"LayerNorm"</span>,</span>
<span>  dense_residual_type <span class="op">=</span> <span class="st">"ResidualGate"</span>,</span>
<span>  rec_act_fct <span class="op">=</span> <span class="st">"Tanh"</span>,</span>
<span>  rec_n_layers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  rec_type <span class="op">=</span> <span class="st">"GRU"</span>,</span>
<span>  rec_bidirectional <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  rec_dropout <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  rec_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  rec_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  rec_normalization_type <span class="op">=</span> <span class="st">"LayerNorm"</span>,</span>
<span>  rec_residual_type <span class="op">=</span> <span class="st">"ResidualGate"</span>,</span>
<span>  tf_act_fct <span class="op">=</span> <span class="st">"ELU"</span>,</span>
<span>  tf_dense_dim <span class="op">=</span> <span class="fl">50</span>,</span>
<span>  tf_n_layers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  tf_dropout_rate_1 <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  tf_dropout_rate_2 <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  tf_attention_type <span class="op">=</span> <span class="st">"MultiHead"</span>,</span>
<span>  tf_positional_type <span class="op">=</span> <span class="st">"absolute"</span>,</span>
<span>  tf_num_heads <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  tf_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  tf_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  tf_normalization_type <span class="op">=</span> <span class="st">"LayerNorm"</span>,</span>
<span>  tf_residual_type <span class="op">=</span> <span class="st">"ResidualGate"</span>,</span>
<span>  embedding_dim <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h4>
<p></p><div class="arguments"><dl><dt><code>name</code></dt>
<dd><p><code>string</code> Name of the new model. Please refer to common name conventions.
Free text can be used with parameter <code>label</code>. If set to <code>NULL</code> a unique ID
is generated automatically. Allowed values: any</p></dd>


<dt><code>label</code></dt>
<dd><p><code>string</code> Label for the new model. Here you can use free text. Allowed values: any</p></dd>


<dt><code>text_embeddings</code></dt>
<dd><p><code>EmbeddedText, LargeDataSetForTextEmbeddings</code> Object of class <a href="EmbeddedText.html">EmbeddedText</a> or <a href="LargeDataSetForTextEmbeddings.html">LargeDataSetForTextEmbeddings</a>.</p></dd>


<dt><code>feature_extractor</code></dt>
<dd><p><code>TEFeatureExtractor</code> Object of class <a href="TEFeatureExtractor.html">TEFeatureExtractor</a> which should be used in order to reduce
the number of dimensions of the text embeddings. If no feature extractor should be applied set <code>NULL</code>.</p></dd>


<dt><code>target_levels</code></dt>
<dd><p><code>vector</code> containing the levels (categories or classes) within the target data. Please
note that order matters. For ordinal data please ensure that the levels are sorted correctly with later levels
indicating a higher category/class. For nominal data the order does not matter.</p></dd>


<dt><code>skip_connection_type</code></dt>
<dd><p><code>string</code> Type of residual connenction for all layers and stack of layers. Allowed values: 'ResidualGate', 'Addition', 'None'</p></dd>


<dt><code>cls_pooling_features</code></dt>
<dd><p><code>int</code> Number of features to be extracted at the end of the model. Allowed values: <code>1 &lt;= x </code></p></dd>


<dt><code>cls_pooling_type</code></dt>
<dd><p><code>string</code> Type of extracting intermediate features. Allowed values: 'Max', 'Min', 'MinMax'</p></dd>


<dt><code>metric_type</code></dt>
<dd><p><code>string</code> Type of metric used for calculating the distance. Allowed values: 'Euclidean'</p></dd>


<dt><code>feat_act_fct</code></dt>
<dd><p><code>string</code> Activation function for all layers. Allowed values: 'ELU', 'LeakyReLU', 'ReLU', 'GELU', 'Sigmoid', 'Tanh', 'PReLU'</p></dd>


<dt><code>feat_size</code></dt>
<dd><p><code>int</code> Number of neurons for each dense layer. Allowed values: <code>2 &lt;= x </code></p></dd>


<dt><code>feat_bias</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bias term is added to all layers. If <code>FALSE</code> no bias term is added to the layers.</p></dd>


<dt><code>feat_dropout</code></dt>
<dd><p><code>double</code> determining the dropout for the dense projection of the feature layer. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>feat_parametrizations</code></dt>
<dd><p><code>string</code> Re-Parametrizations of the weights of layers. Allowed values: 'None', 'OrthogonalWeights', 'WeightNorm', 'SpectralNorm'</p></dd>


<dt><code>feat_normalization_type</code></dt>
<dd><p><code>string</code> Type of normalization applied to all layers and stack layers. Allowed values: 'LayerNorm', 'None'</p></dd>


<dt><code>ng_conv_act_fct</code></dt>
<dd><p><code>string</code> Activation function for all layers. Allowed values: 'ELU', 'LeakyReLU', 'ReLU', 'GELU', 'Sigmoid', 'Tanh', 'PReLU'</p></dd>


<dt><code>ng_conv_n_layers</code></dt>
<dd><p><code>int</code> determining how many times the n-gram layers should be added to the network. Allowed values: <code>0 &lt;= x </code></p></dd>


<dt><code>ng_conv_ks_min</code></dt>
<dd><p><code>int</code> determining the minimal window size for n-grams. Allowed values: <code>2 &lt;= x </code></p></dd>


<dt><code>ng_conv_ks_max</code></dt>
<dd><p><code>int</code> determining the maximal window size for n-grams. Allowed values: <code>2 &lt;= x </code></p></dd>


<dt><code>ng_conv_bias</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bias term is added to all layers. If <code>FALSE</code> no bias term is added to the layers.</p></dd>


<dt><code>ng_conv_dropout</code></dt>
<dd><p><code>double</code> determining the dropout for n-gram convolution layers. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>ng_conv_parametrizations</code></dt>
<dd><p><code>string</code> Re-Parametrizations of the weights of layers. Allowed values: 'None', 'OrthogonalWeights', 'WeightNorm', 'SpectralNorm'</p></dd>


<dt><code>ng_conv_normalization_type</code></dt>
<dd><p><code>string</code> Type of normalization applied to all layers and stack layers. Allowed values: 'LayerNorm', 'None'</p></dd>


<dt><code>ng_conv_residual_type</code></dt>
<dd><p><code>string</code> Type of residual connenction for all layers and stack of layers. Allowed values: 'ResidualGate', 'Addition', 'None'</p></dd>


<dt><code>dense_act_fct</code></dt>
<dd><p><code>string</code> Activation function for all layers. Allowed values: 'ELU', 'LeakyReLU', 'ReLU', 'GELU', 'Sigmoid', 'Tanh', 'PReLU'</p></dd>


<dt><code>dense_n_layers</code></dt>
<dd><p><code>int</code> Number of dense layers. Allowed values: <code>0 &lt;= x </code></p></dd>


<dt><code>dense_dropout</code></dt>
<dd><p><code>double</code> determining the dropout between dense layers. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>dense_bias</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bias term is added to all layers. If <code>FALSE</code> no bias term is added to the layers.</p></dd>


<dt><code>dense_parametrizations</code></dt>
<dd><p><code>string</code> Re-Parametrizations of the weights of layers. Allowed values: 'None', 'OrthogonalWeights', 'WeightNorm', 'SpectralNorm'</p></dd>


<dt><code>dense_normalization_type</code></dt>
<dd><p><code>string</code> Type of normalization applied to all layers and stack layers. Allowed values: 'LayerNorm', 'None'</p></dd>


<dt><code>dense_residual_type</code></dt>
<dd><p><code>string</code> Type of residual connenction for all layers and stack of layers. Allowed values: 'ResidualGate', 'Addition', 'None'</p></dd>


<dt><code>rec_act_fct</code></dt>
<dd><p><code>string</code> Activation function for all layers. Allowed values: 'Tanh'</p></dd>


<dt><code>rec_n_layers</code></dt>
<dd><p><code>int</code> Number of recurrent layers. Allowed values: <code>0 &lt;= x </code></p></dd>


<dt><code>rec_type</code></dt>
<dd><p><code>string</code> Type of the recurrent layers. <code>rec_type='GRU'</code> for Gated Recurrent Unit and <code>rec_type='LSTM'</code> for Long Short-Term Memory. Allowed values: 'GRU', 'LSTM'</p></dd>


<dt><code>rec_bidirectional</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bidirectional version of the recurrent layers is used.</p></dd>


<dt><code>rec_dropout</code></dt>
<dd><p><code>double</code> determining the dropout between recurrent layers. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>rec_bias</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bias term is added to all layers. If <code>FALSE</code> no bias term is added to the layers.</p></dd>


<dt><code>rec_parametrizations</code></dt>
<dd><p><code>string</code> Re-Parametrizations of the weights of layers. Allowed values: 'None'</p></dd>


<dt><code>rec_normalization_type</code></dt>
<dd><p><code>string</code> Type of normalization applied to all layers and stack layers. Allowed values: 'LayerNorm', 'None'</p></dd>


<dt><code>rec_residual_type</code></dt>
<dd><p><code>string</code> Type of residual connenction for all layers and stack of layers. Allowed values: 'ResidualGate', 'Addition', 'None'</p></dd>


<dt><code>tf_act_fct</code></dt>
<dd><p><code>string</code> Activation function for all layers. Allowed values: 'ELU', 'LeakyReLU', 'ReLU', 'GELU', 'Sigmoid', 'Tanh', 'PReLU'</p></dd>


<dt><code>tf_dense_dim</code></dt>
<dd><p><code>int</code> determining the size of the projection layer within a each transformer encoder. Allowed values: <code>1 &lt;= x </code></p></dd>


<dt><code>tf_n_layers</code></dt>
<dd><p><code>int</code> determining how many times the encoder should be added to the network. Allowed values: <code>0 &lt;= x </code></p></dd>


<dt><code>tf_dropout_rate_1</code></dt>
<dd><p><code>double</code> determining the dropout after the attention mechanism within the transformer encoder layers. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>tf_dropout_rate_2</code></dt>
<dd><p><code>double</code> determining the dropout for the dense projection within the transformer encoder layers. Allowed values: <code>0 &lt;= x &lt;= 0.6</code></p></dd>


<dt><code>tf_attention_type</code></dt>
<dd><p><code>string</code> Choose the attention type. Allowed values: 'Fourier', 'MultiHead'</p></dd>


<dt><code>tf_positional_type</code></dt>
<dd><p><code>string</code> Type of processing positional information. Allowed values: 'absolute'</p></dd>


<dt><code>tf_num_heads</code></dt>
<dd><p><code>int</code> determining the number of attention heads for a self-attention layer. Only relevant if <code>attention_type='multihead'</code> Allowed values: <code>0 &lt;= x </code></p></dd>


<dt><code>tf_bias</code></dt>
<dd><p><code>bool</code> If <code>TRUE</code> a bias term is added to all layers. If <code>FALSE</code> no bias term is added to the layers.</p></dd>


<dt><code>tf_parametrizations</code></dt>
<dd><p><code>string</code> Re-Parametrizations of the weights of layers. Allowed values: 'None', 'OrthogonalWeights', 'WeightNorm', 'SpectralNorm'</p></dd>


<dt><code>tf_normalization_type</code></dt>
<dd><p><code>string</code> Type of normalization applied to all layers and stack layers. Allowed values: 'LayerNorm', 'None'</p></dd>


<dt><code>tf_residual_type</code></dt>
<dd><p><code>string</code> Type of residual connenction for all layers and stack of layers. Allowed values: 'ResidualGate', 'Addition', 'None'</p></dd>


<dt><code>embedding_dim</code></dt>
<dd><p><code>int</code>  determining the number of dimensions for the embedding. Allowed values: <code>2 &lt;= x </code></p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns">Returns<a class="anchor" aria-label="anchor" href="#returns"></a></h4>
<p>Function does nothing return. It modifies the current object.</p>
</div>

</div><p></p><hr><a id="method-TEClassifierSequentialPrototype-clone"></a><div class="section">
<h3 id="method-clone-">Method <code>clone()</code><a class="anchor" aria-label="anchor" href="#method-clone-"></a></h3>
<p>The objects of this class are cloneable with this method.</p><div class="section">
<h4 id="usage-1">Usage<a class="anchor" aria-label="anchor" href="#usage-1"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">TEClassifierSequentialPrototype</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span>deep <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-1">Arguments<a class="anchor" aria-label="anchor" href="#arguments-1"></a></h4>
<p></p><div class="arguments"><dl><dt><code>deep</code></dt>
<dd><p>Whether to make a deep clone.</p></dd>


</dl><p></p></div>
</div>

</div>

    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Berding Florian, Tykhonova Yuliia.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>


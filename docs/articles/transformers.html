<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>01 Transformers • aifeducation</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="01 Transformers">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/aifeducation.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>For users</h6></li>
    <li><a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a></li>
    <li><a class="dropdown-item" href="../articles/gui_aife_studio.html">02 Aifeducation Studio</a></li>
    <li><a class="dropdown-item" href="../articles/classification_tasks.html">03 Using the Package without Studio</a></li>
    <li><a class="dropdown-item" href="../articles/model_configuration.html">04 Model configuration</a></li>
    <li><a class="dropdown-item" href="../articles/sharing_and_publishing.html">05 Sharing and Using Trained AI/Models</a></li>
    <li><a class="dropdown-item" href="../articles/a01_layers_stacks.html">Appendix 01 Layers and Stacks</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>For developers</h6></li>
    <li><a class="dropdown-item" href="../articles/transformers.html">01 Transformers</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>01 Transformers</h1>
                        <h4 data-toc-skip class="author">Yuliia
Tykhonova, Florian Berding</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/transformers.Rmd" class="external-link"><code>vignettes/transformers.Rmd</code></a></small>
      <div class="d-none name"><code>transformers.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="base-transformer-class">1 Base Transformer Class<a class="anchor" aria-label="anchor" href="#base-transformer-class"></a>
</h2>
<div class="section level3">
<h3 id="overview">1.1 Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h3>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html">Base
Transformer Class Documentation</a> for details.</p>
<p>UML-diagram of Base Transformer Class:</p>
<div class="float">
<img src="img_articles/transformer_base_class.png" style="width:50.0%" alt="Figure 1.1: UML-diagram of Base Transformer Class"><div class="figcaption">Figure 1.1: UML-diagram of Base Transformer
Class</div>
</div>
<p>This class has:</p>
<ul>
<li><p>private attributes: <code>title</code>,
<code>sustainability_tracker</code>, <code>steps_for_creation</code> and
<code>steps_for_training</code> (see 1.2 Private attributes).</p></li>
<li><p>public attributes: <code>params</code> and <code>temp</code>
lists (see 1.3 Public attributes).</p></li>
<li>
<p>private methods:</p>
<ul>
<li><p><code>clear_variables</code> method sets the <code>params</code>,
<code>temp</code> and <code>sustainability_tracker</code> attributes to
<code>NULL</code>.</p></li>
<li><p><code>check_required_SFC</code> and
<code>check_required_SFT</code> methods check the required steps
(functions) in SFC and SFT lists for <code>NULL</code> respectively.
Steps set to <code>NULL</code> by default and must be
functions.</p></li>
<li><p><code>init_common_model_params</code> method contains
<code>set_model_params</code> method calls to set common model’s
parameters (listed as ‘static’ parameters in p. 1.3 Public attributes
-&gt; <code>param</code> list).</p></li>
<li><p><code>define_required_SFC_functions</code> and
<code>define_required_SFT_functions</code> methods contains the
definitions for the required creation/training steps (functions)
respectively.</p></li>
</ul>
</li>
<li><p>public methods: <code>initialize</code>, setters for
<code>title</code> attribute, <code>params</code> and <code>temp</code>
lists elements, steps for creation (SFC) and training (STF) functions,
main <code>create</code> and <code>train</code> user-methods. See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html">documentation</a>
for details.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="private-attributes">1.2 Private attributes<a class="anchor" aria-label="anchor" href="#private-attributes"></a>
</h3>
<p>Developers should know the purpose of all private attributes.</p>
<div class="section level4">
<h4 id="attribute-title">
<strong>Attribute <code>title</code></strong><a class="anchor" aria-label="anchor" href="#attribute-title"></a>
</h4>
<p><code>string</code> The title for a transformer. This title is
displayed in the progress bar. By default title is set to
<code>"Transformer Model"</code>.</p>
<p>Can be set with <code>set_title()</code> method (for example when
implementing a new child-transformer in
<code>initialize</code>-method):</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>,</span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_title</span><span class="op">(</span><span class="st">"Custom Transformer"</span><span class="op">)</span> <span class="co"># (1)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Use <code>super</code> to access public methods of the base class
<strong>(1)</strong>.</p>
</div>
<div class="section level4">
<h4 id="attribute-sustainability_tracker">
<strong>Attribute <code>sustainability_tracker</code></strong><a class="anchor" aria-label="anchor" href="#attribute-sustainability_tracker"></a>
</h4>
<p><code>codecarbon.OfflineEmissionsTracker</code> object is used to
track sustainability on demand. It can be created with the private
<code>create_sustain_tracker()</code> method.</p>
</div>
<div class="section level4">
<h4 id="attribute-steps_for_creation">
<strong>Attribute <code>steps_for_creation</code></strong><a class="anchor" aria-label="anchor" href="#attribute-steps_for_creation"></a>
</h4>
<p><code><a href="https://rdrr.io/r/base/list.html" class="external-link">list()</a></code> that stores required and optional steps
(functions) for creating a new transformer.</p>
<p>To access (input) parameters of the transformer, use the
<code>params</code> list (e.g. <code>params$ml_framework</code>). To
access a local variable outside of a function, put it in the
<code>temp</code> list.</p>
<p>Use the <code>set_SFC_*()</code> methods to set required/optional
steps for creation, where * is the name of the step.</p>
<p>Use the <code>set_required_SFC()</code> method to set all required
steps at once.</p>
<div class="section level5">
<h5 id="required">Required<a class="anchor" aria-label="anchor" href="#required"></a>
</h5>
<p>The <strong>required steps</strong> to be defined in each child
transformer are:</p>
<ul>
<li>
<p><code>create_tokenizer_draft</code>: <code>function()</code> that
creates a tokenizer draft.</p>
<p>In this function a new tokenizer must be created and stored as an
element of a list <code>temp</code> (e.g. <code>temp$tok_new</code>).
This function can include the definition of special tokens and/or
trainers (<code>tokenizers.trainers.WordPieceTrainer</code>).</p>
<p>See the <code>create_WordPiece_tokenizer()</code> and
<code>create_ByteLevelBPE_tokenizer()</code> functions to create a new
tokenizer object (<code>tokenizers.Tokenizer</code>) based on the
<code>tokenizers.models.WordPiece</code> and
<code>tokenizers.models.ByteLevel</code> models respectively.</p>
</li>
<li>
<p><code>calculate_vocab</code>: <code>function()</code> for
calculating a vocabulary.</p>
<p>The tokenizer created in the <code>create_tokenizer_draft()</code>
function is trained.</p>
<p>See <code>tokenizers.Tokenizer.train_from_iterator()</code> for
details.</p>
</li>
<li>
<p><code>save_tokenizer_draft</code>: <code>function()</code> that
saves a tokenizer draft to a model directory (e.g. to a
<code>vocab.txt</code> file).</p>
<p>See <code>tokenizers.Tokenizer.save_model()</code> for
details.</p>
</li>
<li>
<p><code>create_final_tokenizer</code>: <code>function()</code> that
creates a new transformer tokenizer object.</p>
<p>The tokenizer must be stored in the <code>tokenizer</code> parameter
of the <code>temp</code> list.</p>
<p>See <code>transformers.PreTrainedTokenizerFast</code>,
<code>transformers.LongformerTokenizerFast</code> and
<code>transformers.RobertaTokenizerFast</code> for details.</p>
</li>
<li>
<p><code>create_transformer_model</code>: <code>function()</code>
that creates a transformer model.</p>
<p>The model must be passed to the <code>model</code> parameter of the
<code>temp</code> list.</p>
<p>See <code>transformers.(TF)BertModel</code>,
<code>transformers.(TF)DebertaV2ForMaskedLM</code>,
<code>transformers.(TF)FunnelModel</code>,
<code>transformers.(TF)LongformerModel</code>,
<code>transformers.(TF)RobertaModel</code>, etc. for details.</p>
</li>
</ul>
</div>
<div class="section level5">
<h5 id="optional">Optional<a class="anchor" aria-label="anchor" href="#optional"></a>
</h5>
<p><strong>Optional step</strong> is:</p>
<ul>
<li>
<p><code>check_max_pos_emb</code>: <code>function()</code> that
checks transformer parameter <code>max_position_embeddings</code>.</p>
<p>Leave <code>NULL</code> to skip the check.</p>
</li>
</ul>
</div>
<div class="section level5">
<h5 id="other">Other<a class="anchor" aria-label="anchor" href="#other"></a>
</h5>
<p><strong>Required and already defined step</strong> is:</p>
<ul>
<li>
<p><code>save_transformer_model</code>: <code>function()</code> that
saves a newly created transformer.</p>
<p>Uses the temporary <code>model</code> and <code>pt_safe_save</code>
parameters of the <code>temp</code> list.</p>
<p>See <code>transformers.(TF)PreTrainedModel.save_pretrained()</code>
for details.</p>
</li>
</ul>
</div>
</div>
<div class="section level4">
<h4 id="attribute-steps_for_training">
<strong>Attribute</strong> <code>steps_for_training</code><a class="anchor" aria-label="anchor" href="#attribute-steps_for_training"></a>
</h4>
<p><code><a href="https://rdrr.io/r/base/list.html" class="external-link">list()</a></code> that stores required and optional steps
(functions) for training the transformer.</p>
<p>To access (input) parameters of the transformer, use the
<code>params</code> list (e.g. <code>params$ml_framework</code>). To
access a local variable outside of a function, put it in the
<code>temp</code> list.</p>
<p>Use the <code>set_SFT_*()</code> methods to set required/optional
steps for creation, where * is the name of the step.</p>
<div class="section level5">
<h5 id="required-1">Required<a class="anchor" aria-label="anchor" href="#required-1"></a>
</h5>
<p>The <strong>required step</strong> in each child transformer is:</p>
<ul>
<li>
<p><code>load_existing_model</code>: <code>function()</code> that
loads the model and its tokenizer.</p>
<p>The model and the transformer must be stored to the
<code>model</code> and <code>tokenizer</code> parameters respectively of
the <code>temp</code> list.</p>
<p>See <code>transformers.(TF)PreTrainedModel</code> for
details.</p>
</li>
</ul>
</div>
<div class="section level5">
<h5 id="optional-1">Optional<a class="anchor" aria-label="anchor" href="#optional-1"></a>
</h5>
<p><strong>Optional step</strong> is:</p>
<ul>
<li>
<code>cuda_empty_cache</code>: <code>function()</code> to empty the
cache if <code>torch.cuda</code> is available.</li>
</ul>
</div>
<div class="section level5">
<h5 id="other-1">Other<a class="anchor" aria-label="anchor" href="#other-1"></a>
</h5>
<p><strong>Required and already defined steps</strong> are:</p>
<ul>
<li>
<p><code>check_chunk_size</code>: <code>function()</code> that
checks transformer’s parameter <code>chunk_size</code> and adjusts
it.</p>
<p>Uses the <code>model</code> parameter of the <code>temp</code> list
and modifies the <code>chunk_size</code> parameter of the
<code>params</code> list.</p>
</li>
<li>
<p><code>create_chunks_for_training</code>: <code>function()</code>
that creates chunks of the sequenses for the trainining.</p>
<p>Uses the <code>tokenizer</code> parameter and adds
<code>tokenized_dataset</code> parameter to the <code>temp</code>
list.</p>
</li>
<li>
<p><code>prepare_train_tune</code>: <code>function()</code> that
prepares the data for the training.</p>
<p>For <code>tensorflow</code>: uses the <code>model</code> and
<code>tokenizer</code> parameters, adds the
<code>tf_train_dataset</code>, <code>tf_test_dataset</code>,
<code>callbacks</code> parameters to the <code>temp</code> list.</p>
<p>For <code>pytorch</code>: uses the <code>model</code>,
<code>tokenizer</code> parameters, adds the <code>trainer</code>
parameter to the <code>temp</code> list.</p>
</li>
<li>
<p><code>start_training</code>: <code>function()</code> that starts
the training.</p>
<p>For <code>tensorflow</code>: uses the <code>model</code>,
<code>tf_train_dataset</code>, <code>tf_test_dataset</code>,
<code>callbacks</code> parameters of the <code>temp</code> list.</p>
<p>For <code>pytorch</code>: uses the <code>trainer</code> parameter of
the <code>temp</code> list.</p>
</li>
<li>
<p><code>save_model</code>: <code>function()</code> that saves the
model.</p>
<p>For <code>tensorflow</code>: uses the <code>model</code> parameter of
the <code>temp</code> list.</p>
<p>For <code>pytorch</code>: uses the <code>model</code>,
<code>pt_safe_save</code> and <code>trainer</code> parameters of the
<code>temp</code> list.</p>
</li>
</ul>
<p><strong>Required and already defined step</strong>, but can be
overwritten with a custom version:</p>
<ul>
<li>
<p><code>create_data_collator</code>: <code>function()</code> that
creates the data collator for the model.</p>
<p>From the <code>temp</code> list uses the <code>tokenizer</code> and
<code>return_tensors</code> parameters, adds <code>data_collator</code>
parameter to this list.</p>
<p>From the <code>params</code> list uses <code>whole_word</code> and
<code>p_mask</code>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="section level3">
<h3 id="public-attributes">1.3 Public attributes<a class="anchor" aria-label="anchor" href="#public-attributes"></a>
</h3>
<div class="section level4">
<h4 id="params-list">
<code>params</code> list<a class="anchor" aria-label="anchor" href="#params-list"></a>
</h4>
<p>A list containing all the transformer’s parameters (‘static’,
‘dynamic’ and ‘dependent’ parameters). Can be set with
<code>set_model_param()</code>.</p>
<div class="section level5">
<h5 id="static-parameters">
<strong>‘Static’ parameters</strong><a class="anchor" aria-label="anchor" href="#static-parameters"></a>
</h5>
<p>Regardless of the transformer, the following parameters are always
included:</p>
<ul>
<li><p><code>ml_framework</code></p></li>
<li><p><code>text_dataset</code></p></li>
<li><p><code>sustain_track</code></p></li>
<li><p><code>sustain_iso_code</code></p></li>
<li><p><code>sustain_region</code></p></li>
<li><p><code>sustain_interval</code></p></li>
<li><p><code>trace</code></p></li>
<li><p><code>pytorch_safetensors</code></p></li>
<li><p><code>log_dir</code></p></li>
<li><p><code>log_write_interval</code></p></li>
</ul>
</div>
<div class="section level5">
<h5 id="dynamic-parameters">
<strong>‘Dynamic’ parameters</strong><a class="anchor" aria-label="anchor" href="#dynamic-parameters"></a>
</h5>
<p>In the case of <strong>create</strong> it also contains (see
<code>create</code>-method for details):</p>
<ul>
<li><p><code>model_dir</code></p></li>
<li><p><code>vocab_size</code></p></li>
<li><p><code>max_position_embeddings</code></p></li>
<li><p><code>hidden_size</code></p></li>
<li><p><code>hidden_act</code></p></li>
<li><p><code>hidden_dropout_prob</code></p></li>
<li><p><code>attention_probs_dropout_prob</code></p></li>
<li><p><code>intermediate_size</code></p></li>
<li><p><code>num_attention_heads</code></p></li>
</ul>
<p>In the case of <strong>train</strong> it also contains (see
<code>train</code>-method for details):</p>
<ul>
<li><p><code>output_dir</code></p></li>
<li><p><code>model_dir_path</code></p></li>
<li><p><code>p_mask</code></p></li>
<li><p><code>whole_word</code></p></li>
<li><p><code>val_size</code></p></li>
<li><p><code>n_epoch</code></p></li>
<li><p><code>batch_size</code></p></li>
<li><p><code>chunk_size</code></p></li>
<li><p><code>min_seq_len</code></p></li>
<li><p><code>full_sequences_only</code></p></li>
<li><p><code>learning_rate</code></p></li>
<li><p><code>n_workers</code></p></li>
<li><p><code>multi_process</code></p></li>
<li><p><code>keras_trace</code></p></li>
<li><p><code>pytorch_trace</code></p></li>
</ul>
</div>
<div class="section level5">
<h5 id="dependent-parameters">
<strong>‘Dependent’ parameters</strong><a class="anchor" aria-label="anchor" href="#dependent-parameters"></a>
</h5>
<p>Depending on the transformer and the method used, class may contain
different parameters:</p>
<ul>
<li><p><code>vocab_do_lower_case</code></p></li>
<li><p><code>num_hidden_layer</code></p></li>
<li><p><code>add_prefix_space</code></p></li>
<li><p>etc.</p></li>
</ul>
<div class="float">
<img src="img_articles/transformer_params_list.png" style="width:50.0%" alt="Figure 1.2: Possible parameters in the params list"><div class="figcaption">Figure 1.2: Possible parameters in the params
list</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="temp-list">
<code>temp</code> list<a class="anchor" aria-label="anchor" href="#temp-list"></a>
</h4>
<p>A list containing temporary transformer’s parameters</p>
<p><code><a href="https://rdrr.io/r/base/list.html" class="external-link">list()</a></code> containing all the temporary local variables that
need to be accessed between the step functions. Can be set with
<code>set_model_temp()</code>.</p>
<p>For example, it can be a variable <code>tok_new</code> that stores
the tokenizer from
<code>steps_for_creation$create_tokenizer_draft</code>. To train the
tokenizer, access the variable <code>tok_new</code> in
<code>steps_for_creation$calculate_vocab</code> through the
<code>temp</code> list of this class.</p>
<div class="float">
<img src="img_articles/transformer_temp_list.png" style="width:50.0%" alt="Figure 1.3: Possible parameters in the temp list"><div class="figcaption">Figure 1.3: Possible parameters in the temp
list</div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="allowed-transformers">2 Allowed Transformers<a class="anchor" aria-label="anchor" href="#allowed-transformers"></a>
</h2>
<div class="section level3">
<h3 id="overview-1">2.1 Overview<a class="anchor" aria-label="anchor" href="#overview-1"></a>
</h3>
<p>UML-diagram of Transformer Classes:</p>
<div class="float">
<img src="img_articles/transformer_allowed_classes.png" style="width:75.0%" alt="Figure 2.1: UML-diagram of Base Transformer Class"><div class="figcaption">Figure 2.1: UML-diagram of Base Transformer
Class</div>
</div>
<p>The Base Transformer Class has the following child-classes:</p>
<ul>
<li><p>.AIFEBertTransformer</p></li>
<li><p>.AIFEDebertaTransformer</p></li>
<li><p>.AIFEFunnelTransformer</p></li>
<li><p>.AIFELongformerTransformer</p></li>
<li><p>.AIFERobertaTransformer</p></li>
<li><p>.AIFEMpnetTransformer</p></li>
</ul>
<p>The object of the Base Transformer Class cannot be created, thus the
<code>create</code> and <code>train</code> methods cannot be called
directly. However objects of the child classes can be created with
<code>new</code>-method <strong>(1)</strong>. Use <code>create</code>
<strong>(2)</strong> and <code>train</code> <strong>(3)</strong> methods
to create/train the concrete transformer respectively:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example of using .AIFEBertTransformer class</span></span>
<span><span class="co"># For the other one - analogically</span></span>
<span></span>
<span><span class="va">bert_transformer</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/dot-AIFEBertTransformer.html">.AIFEBertTransformer</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span> <span class="co"># (1)</span></span>
<span></span>
<span><span class="co"># See .AIFEBertTransformer documentation to get input parameters</span></span>
<span><span class="co"># for create and train methods instead of ...</span></span>
<span><span class="va">bert_transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span><span class="va">...</span><span class="op">)</span>                   <span class="co"># (2)</span></span>
<span><span class="va">bert_transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">...</span><span class="op">)</span>                    <span class="co"># (3)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="transformer-parameters">2.2 Transformer Parameters<a class="anchor" aria-label="anchor" href="#transformer-parameters"></a>
</h3>
<div class="section level4">
<h4 id="static-parameters-1">
<strong>‘Static’ parameters</strong><a class="anchor" aria-label="anchor" href="#static-parameters-1"></a>
</h4>
<table class="table">
<colgroup>
<col width="19%">
<col width="19%">
<col width="61%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Type</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>ml_framework</td>
<td><code>string</code></td>
<td>Framework to use for training and inference<sup>1</sup>
</td>
</tr>
<tr class="odd">
<td>text_dataset</td>
<td><code>object</code></td>
<td>Object of the class <code>LargeDataSetForText</code>
</td>
</tr>
<tr class="even">
<td>sustain_track</td>
<td><code>bool</code></td>
<td>If <code>TRUE</code> energy consumption is tracked during
training<sup>2</sup>
</td>
</tr>
<tr class="odd">
<td>sustain_iso_code</td>
<td><code>string</code></td>
<td>ISO code (Alpha-3-Code) for the country<sup>3</sup>
</td>
</tr>
<tr class="even">
<td>sustain_region</td>
<td><code>string</code></td>
<td>Region within a country. Only available for USA and
Canada<sup>4</sup>
</td>
</tr>
<tr class="odd">
<td>sustain_interval</td>
<td><code>integer</code></td>
<td>Interval in seconds for measuring power usage</td>
</tr>
<tr class="even">
<td>trace</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> if information about the progress should be
printed to the console</td>
</tr>
<tr class="odd">
<td>pytorch_safetensors</td>
<td><code>bool</code></td>
<td>Choose between safetensors and standard pytorch
format<sup>5</sup>
</td>
</tr>
<tr class="even">
<td>log_dir</td>
<td><code>string</code></td>
<td>Path to the directory where the log files should be saved</td>
</tr>
<tr class="odd">
<td>log_write_interval</td>
<td><code>integer</code></td>
<td>Time in seconds for updating the log files<sup>6</sup>
</td>
</tr>
</tbody>
</table>
<p><sup>1 Available frameworks are “tensorflow” and “pytorch”</sup></p>
<p><sup>2 Via the python library codecarbon</sup></p>
<p><sup>3 This variable must be set if sustainability should be tracked. A list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes" class="external-link uri">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a></sup></p>
<p><sup>4 See the documentation of codecarbon for more information <a href="https://mlco2.github.io/codecarbon/parameters.html" class="external-link uri">https://mlco2.github.io/codecarbon/parameters.html</a></sup></p>
<p><sup>5 Only relevant for pytorch models. <code>TRUE</code>: a ‘pytorch’ model is saved in safetensors format; <code>FALSE</code> (or ‘safetensors’ is not available): model is saved in the standard pytorch format (.bin)</sup></p>
<p><sup>6 Only relevant if <code>log_dir</code> is not <code>NULL</code></sup></p>
</div>
<div class="section level4">
<h4 id="dynamic-parameters-for-creation">
<strong>‘Dynamic’ parameters for creation</strong><a class="anchor" aria-label="anchor" href="#dynamic-parameters-for-creation"></a>
</h4>
<table class="table">
<colgroup>
<col width="20%">
<col width="20%">
<col width="58%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Type</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>model_dir</td>
<td><code>string</code></td>
<td>Path to the directory where the model should be saved</td>
</tr>
<tr class="odd">
<td>vocab_size</td>
<td><code>int</code></td>
<td>Size of the vocabulary</td>
</tr>
<tr class="even">
<td>max_position_embeddings</td>
<td><code>int</code></td>
<td>Number of maximum position embeddings<sup>1</sup>
</td>
</tr>
<tr class="odd">
<td>hidden_size</td>
<td><code>int</code></td>
<td>Number of neurons in each layer<sup>2</sup>
</td>
</tr>
<tr class="even">
<td>hidden_act</td>
<td><code>string</code></td>
<td>Name of the activation function</td>
</tr>
<tr class="odd">
<td>hidden_dropout_prob</td>
<td><code>double</code></td>
<td>Ratio of dropout</td>
</tr>
<tr class="even">
<td>attention_probs_dropout_prob</td>
<td><code>double</code></td>
<td>Ratio of dropout for attention probabilities</td>
</tr>
<tr class="odd">
<td>intermediate_size</td>
<td><code>int</code></td>
<td>Number of neurons in the intermediate layer of the attention
mechanism</td>
</tr>
<tr class="even">
<td>num_attention_heads</td>
<td><code>int</code></td>
<td>Number of attention heads</td>
</tr>
<tr class="odd">
<td>vocab_do_lower_case</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> if all words/tokens should be lower case</td>
</tr>
<tr class="even">
<td>num_hidden_layer</td>
<td><code>int</code></td>
<td>Number of hidden layers</td>
</tr>
<tr class="odd">
<td>target_hidden_size</td>
<td><code>int</code></td>
<td>Number of neurons in the final layer<sup>2</sup>
</td>
</tr>
<tr class="even">
<td>block_sizes</td>
<td><code>vector</code></td>
<td>Contains <code>int</code>s that determine the number and sizes of
each block</td>
</tr>
<tr class="odd">
<td>num_decoder_layers</td>
<td><code>int</code></td>
<td>Number of decoding layers</td>
</tr>
<tr class="even">
<td>activation_dropout</td>
<td><code>float</code></td>
<td>Dropout probability between the layers of the feed-forward
blocks</td>
</tr>
<tr class="odd">
<td>pooling_type</td>
<td><code>string</code></td>
<td>Type of pooling<sup>3</sup>
</td>
</tr>
<tr class="even">
<td>add_prefix_space</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> if an additional space should be inserted to the
leading words</td>
</tr>
<tr class="odd">
<td>trim_offsets</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> trims the whitespaces from the produced
offsets</td>
</tr>
<tr class="even">
<td>attention_window</td>
<td><code>int</code></td>
<td>Size of the window around each token for attention mechanism in
every layer</td>
</tr>
</tbody>
</table>
<p><sup>1 This parameter also determines the maximum length of a sequence which can be processed with the model</sup></p>
<p><sup>2 This parameter determines the dimensionality of the resulting text embedding</sup></p>
<p><sup>3 <code>"Mean"</code> and <code>"Max"</code> for pooling with mean and maximum values respectively</sup></p>
</div>
<div class="section level4">
<h4 id="dynamic-parameters-for-training">
<strong>‘Dynamic’ parameters for training</strong><a class="anchor" aria-label="anchor" href="#dynamic-parameters-for-training"></a>
</h4>
<table class="table">
<colgroup>
<col width="18%">
<col width="18%">
<col width="63%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Name</strong></td>
<td><strong>Type</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td>output_dir</td>
<td><code>string</code></td>
<td>Path to the directory where the final model should be
saved<sup>1</sup>
</td>
</tr>
<tr class="odd">
<td>model_dir_path</td>
<td><code>string</code></td>
<td>Path to the directory where the original model is stored</td>
</tr>
<tr class="even">
<td>p_mask</td>
<td><code>double</code></td>
<td>Ratio that determines the number of words/tokens used for
masking</td>
</tr>
<tr class="odd">
<td>whole_word</td>
<td><code>bool</code></td>
<td>Choose a type of masking<sup>2</sup>
</td>
</tr>
<tr class="even">
<td>val_size</td>
<td><code>double</code></td>
<td>Ratio that determines the amount of token chunks used for
validation</td>
</tr>
<tr class="odd">
<td>n_epoch</td>
<td><code>int</code></td>
<td>Number of epochs for training</td>
</tr>
<tr class="even">
<td>batch_size</td>
<td><code>int</code></td>
<td>Size of batches</td>
</tr>
<tr class="odd">
<td>chunk_size</td>
<td><code>int</code></td>
<td>Size of every chunk for training</td>
</tr>
<tr class="even">
<td>min_seq_len</td>
<td><code>int</code></td>
<td>Value determines the minimal sequence length included in training
process<sup>3</sup>
</td>
</tr>
<tr class="odd">
<td>full_sequences_only</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> for using only chunks with a sequence length equal
to <code>chunk_size</code>
</td>
</tr>
<tr class="even">
<td>learning_rate</td>
<td><code>double</code></td>
<td>Learning rate for adam optimizer</td>
</tr>
<tr class="odd">
<td>n_workers</td>
<td><code>int</code></td>
<td>Number of workers<sup>4</sup>
</td>
</tr>
<tr class="even">
<td>multi_process</td>
<td><code>bool</code></td>
<td>
<code>TRUE</code> if multiple processes should be
activated<sup>4</sup>
</td>
</tr>
<tr class="odd">
<td>keras_trace</td>
<td><code>int</code></td>
<td>Controls the information about the training process from keras on
the console<sup>4,5</sup>
</td>
</tr>
<tr class="even">
<td>pytorch_trace</td>
<td><code>int</code></td>
<td>Controls the information about the training process from pytorch on
the console</td>
</tr>
</tbody>
</table>
<p><sup>1 If the directory does not exist, it will be created</sup></p>
<p><sup>2 <code>TRUE</code>: whole word masking should be applied; <code>FALSE</code>: token masking is used</sup></p>
<p><sup>3 Only relevant if <code>full_sequences_only = FALSE</code></sup></p>
<p><sup>4 Only relevant if <code>ml_framework = "tensorflow"</code></sup></p>
<p><sup>5 <code>keras_trace = 0</code>: does not print any information; <code>keras_trace = 1</code>: prints a progress bar; <code>keras_trace = 2</code>: prints one line of information for every epoch</sup></p>
<p><sup>6 <code>pytorch_trace = 0</code>: does not print any information; <code>pytorch_trace = 1</code>: prints a progress bar</sup></p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="transformer-maker">3 Transformer Maker<a class="anchor" aria-label="anchor" href="#transformer-maker"></a>
</h2>
<div class="section level3">
<h3 id="overview-2">3.1 Overview<a class="anchor" aria-label="anchor" href="#overview-2"></a>
</h3>
<p>See <a href="https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html">Transformer
Maker Class</a> for details.</p>
<p>Transformer Maker Class has a <code>make</code>-method to create a
Transformer object.</p>
<p>UML-diagram of the Transformer Maker Class:</p>
<div class="float">
<img src="img_articles/transformer_maker_class.png" style="width:85.0%" alt="Figure 3.1: UML-diagram of Transformer Maker Class"><div class="figcaption">Figure 3.1: UML-diagram of Transformer Maker
Class</div>
</div>
</div>
<div class="section level3">
<h3 id="usage">3.2 Usage<a class="anchor" aria-label="anchor" href="#usage"></a>
</h3>
<div class="section level4">
<h4 id="bert">
<code>BERT</code><a class="anchor" aria-label="anchor" href="#bert"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html">BERT
Transformer for Developers</a> for details.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">bert</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("bert")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create">Create<a class="anchor" aria-label="anchor" href="#create"></a>
</h5>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>                   vocab_do_lower_case <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>                   num_hidden_layer <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train">Train<a class="anchor" aria-label="anchor" href="#train"></a>
</h5>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  whole_word <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-3</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="deberta-v2">
<code>DeBERTa-v2</code><a class="anchor" aria-label="anchor" href="#deberta-v2"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html">DeBERTa-v2
Transformer for Develovers</a> for details.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">deberta_v2</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("deberta_v2")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create-1">Create<a class="anchor" aria-label="anchor" href="#create-1"></a>
</h5>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">128100</span>,</span>
<span>                   vocab_do_lower_case <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">1536</span>,</span>
<span>                   num_hidden_layer <span class="op">=</span> <span class="fl">24</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">24</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">6144</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train-1">Train<a class="anchor" aria-label="anchor" href="#train-1"></a>
</h5>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  whole_word <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-2</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="roberta">
<code>RoBERTa</code><a class="anchor" aria-label="anchor" href="#roberta"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html">RoBERTa
Transformer for Develovers</a> for details.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">roberta</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("roberta")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create-2">Create<a class="anchor" aria-label="anchor" href="#create-2"></a>
</h5>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>                   add_prefix_space <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   trim_offsets <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>                   num_hidden_layer <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train-2">Train<a class="anchor" aria-label="anchor" href="#train-2"></a>
</h5>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-2</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="funnel">
<code>Funnel</code><a class="anchor" aria-label="anchor" href="#funnel"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html">Funnel
Transformer for Develovers</a> for details.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">funnel</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("funnel")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create-3">Create<a class="anchor" aria-label="anchor" href="#create-3"></a>
</h5>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>                   vocab_do_lower_case <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>                   target_hidden_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>                   block_sizes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>                   num_decoder_layers <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                   pooling_type <span class="op">=</span> <span class="st">"Mean"</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   activation_dropout <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train-3">Train<a class="anchor" aria-label="anchor" href="#train-3"></a>
</h5>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  whole_word <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-3</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="longformer">
<code>Longformer</code><a class="anchor" aria-label="anchor" href="#longformer"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html">Longformer
Transformer for Develovers</a> for details.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">longformer</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("longformer")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create-4">Create<a class="anchor" aria-label="anchor" href="#create-4"></a>
</h5>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>                   add_prefix_space <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   trim_offsets <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>                   num_hidden_layer <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_window <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train-4">Train<a class="anchor" aria-label="anchor" href="#train-4"></a>
</h5>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-2</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="mpnet">
<code>MPNet</code><a class="anchor" aria-label="anchor" href="#mpnet"></a>
</h4>
<p>See <a href="https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html">MPNet
Transformer for Develovers</a> for details.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span> <span class="op">&lt;-</span> <span class="va">aife_transformer_maker</span><span class="op">$</span><span class="fu">make</span><span class="op">(</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">mpnet</span><span class="op">)</span></span>
<span><span class="co"># or</span></span>
<span><span class="co"># transformer &lt;- aife_transformer_maker$make("mpnet")</span></span></code></pre></div>
<div class="section level5">
<h5 id="create-5">Create<a class="anchor" aria-label="anchor" href="#create-5"></a>
</h5>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span>model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>                   text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                   vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>                   vocab_do_lower_case <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>                   hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>                   num_hidden_layer <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                   intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>                   hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>                   hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   attention_probs_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                   trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                   log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="train-5">Train<a class="anchor" aria-label="anchor" href="#train-5"></a>
</h5>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">transformer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>output_dir <span class="op">=</span> <span class="va">output_dir</span>,</span>
<span>                  model_dir_path <span class="op">=</span> <span class="va">model_dir_path</span>,</span>
<span>                  text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>                  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  p_perm <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>                  whole_word <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>                  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>                  full_sequences_only <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  min_seq_len <span class="op">=</span> <span class="fl">50</span>,</span>
<span>                  learning_rate <span class="op">=</span> <span class="fl">3e-3</span>,</span>
<span>                  n_workers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  multi_process <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  sustain_iso_code <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>                  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  keras_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_trace <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                  pytorch_safetensors <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>                  log_write_interval <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="implement-a-custom-transformer">4 Implement A Custom Transformer<a class="anchor" aria-label="anchor" href="#implement-a-custom-transformer"></a>
</h2>
<div class="section level3">
<h3 id="overview-3">4.1 Overview<a class="anchor" aria-label="anchor" href="#overview-3"></a>
</h3>
<p>A Custom Transformer template (“dotAIIFECustomTransformer.R”) is
located in the <code>R</code>-folder of the project.</p>
</div>
<div class="section level3">
<h3 id="implementation-steps">4.2 Implementation Steps<a class="anchor" aria-label="anchor" href="#implementation-steps"></a>
</h3>
<p>To implement a new transformer, do the following steps:</p>
<ol style="list-style-type: decimal">
<li><p>Create a new <code>R</code>-file with a name like
<code>dotAIFECustomTransformer</code>.</p></li>
<li>
<p>Open the file and write the creation of a new
<code><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6::R6Class()</a></code> inside of it (see the code below). The name
of the class must be defined here <strong>(1)</strong>. Remember to
inherit the base transformer class <strong>(2)</strong>. Use the
<code>private</code> list for the private attributes
<strong>(3)</strong> like <code>title</code>,
<code>steps_for_creation</code>, etc. (which will be explained later)
and the <code>public</code> list <strong>(4)</strong> for the
<code>initialize</code>, <code>create</code>, <code>train</code>
methods.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>, <span class="co"># (1)</span></span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>, <span class="co"># (2)</span></span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span>, <span class="co"># (3)</span></span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># (4)</span></span>
<span><span class="op">)</span></span></code></pre></div>
</li>
<li>
<p>Define the private <code>title</code> attribute
<strong>(1)</strong> and set it in the <code>initialize</code> method
<strong>(2)</strong> using the inherited <code>super$set_title()</code>
base method <strong>(3)</strong> in the base class.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>,</span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Custom Model"</span> <span class="co"># (1)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="co"># (2)</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_title</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">title</span><span class="op">)</span> <span class="co"># (3)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
</li>
<li>
<p>Define the private <code>steps_for_creation</code> list
<strong>(1)</strong> to implement the required steps (functions)
<strong>(2)</strong>-<strong>(6)</strong>, and <strong>(7)</strong> if
needed. Do not forget to pass <code>self</code> as input parameter of
the functions.</p>
<blockquote>
<p><strong>Note</strong> that local variables created inside of
functions can be used through the inherited <code>temp</code> list. Put
the local <code>tok_new</code> variable <strong>(8)</strong> in the
<code>temp</code> list in the <code>create_tokenizer_draft</code> step
<strong>(2)</strong> and use it in the <code>calculate_vocab</code> step
<strong>(3)</strong> like <strong>(9)</strong>.</p>
</blockquote>
<p>Similarly, use the input parameters of the transformer such as
<code>ml_framework</code> using the inherited <code>params</code> list
like <strong>(10)</strong>.</p>
<blockquote>
<p><strong>Important!</strong></p>
<p>In the <code>create_final_tokenizer</code> step <strong>(5)</strong>
store the tokenizer in the <code>self$temp$tokenizer</code> variable
<strong>(11)</strong>.</p>
<p>In the <code>create_transformer_model</code> step
<strong>(6)</strong> store the transformer model in the
<code>self$temp$model</code> variable <strong>(12)</strong>.</p>
</blockquote>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">{</span><span class="va">r</span><span class="op">}</span></span>
<span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>,</span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Custom Model"</span>,</span>
<span>    steps_for_creation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>                   <span class="co"># (1)</span></span>
<span>      <span class="co"># required</span></span>
<span>      create_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>      <span class="co"># (2)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>        <span class="co"># self$temp$tok_new &lt;- ...         # (8)</span></span>
<span>      <span class="op">}</span>,</span>
<span>      calculate_vocab <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>             <span class="co"># (3)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>        <span class="co"># ... self$temp$tok_new ...        # (9)</span></span>
<span>      <span class="op">}</span>,</span>
<span>      save_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>        <span class="co"># (4)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>      <span class="op">}</span>,</span>
<span>      create_final_tokenizer <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>      <span class="co"># (5)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>        <span class="co"># self$temp$tokenizer &lt;- ... # (!!!) (11)</span></span>
<span>      <span class="op">}</span>,</span>
<span>      create_transformer_model <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>    <span class="co"># (6)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>        <span class="co"># ... self$params$ml_framework ... # (10)</span></span>
<span>        <span class="co"># self$temp$model &lt;- ...     # (!!!) (12)</span></span>
<span>      <span class="op">}</span>,</span>
<span>      <span class="co"># optional: omit this element if do not needed</span></span>
<span>      check_max_pos_emb <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span>           <span class="co"># (7)</span></span>
<span>        <span class="co"># The implementation must be here</span></span>
<span>      <span class="op">}</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_title</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">title</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
</li>
<li>
<p>Define the <code>create</code> method <strong>(1)</strong> with
all the input parameters <strong>(2)</strong> of the <code>create</code>
method of the base class. Add all the dependent parameters of the custom
transformer to the input parameters <strong>(3)</strong>. Dependent
parameters are parameters that depend on the transformer and are not
present in the base class. Set these dependent parameters to the base
class using the <code>super$set_model_param()</code> method
<strong>(4)</strong>. Set required and optional steps to the base class
using the <code>super$set_required_SFC()</code> and
<code>super$set_SFC_check_max_pos_emb()</code> methods respectively
<strong>(5)</strong>. Finally run the basic <code>create</code>
algorithm using <code>super$create()</code> <strong>(6)</strong> with
all the input parameters <strong>(2)</strong>.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">{</span><span class="va">r</span><span class="op">}</span></span>
<span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>,</span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Custom Model"</span>,</span>
<span>    steps_for_creation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="co"># required</span></span>
<span>      create_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      calculate_vocab <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      save_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      create_final_tokenizer <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      create_transformer_model <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      <span class="co"># optional: omit this element if do not needed</span></span>
<span>      check_max_pos_emb <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>    <span class="co"># (1)</span></span>
<span>    create <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="co"># (2) --------------------------</span></span>
<span>                      <span class="va">model_dir</span>,</span>
<span>                      <span class="va">text_dataset</span>,</span>
<span>                      <span class="va">vocab_size</span>,</span>
<span>                      <span class="co"># ...</span></span>
<span>                      <span class="va">trace</span>,</span>
<span>                      <span class="va">pytorch_safetensors</span>,</span>
<span>                      <span class="co"># ...</span></span>
<span></span>
<span>                      <span class="co"># (3) --------------------------</span></span>
<span>                      <span class="va">dep_param1</span>,</span>
<span>                      <span class="va">dep_param2</span>,</span>
<span>                      <span class="co"># ...</span></span>
<span>                      <span class="va">dep_paramN</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># (4) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_model_param</span><span class="op">(</span><span class="st">"dep_param1"</span>, <span class="va">dep_param1</span><span class="op">)</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_model_param</span><span class="op">(</span><span class="st">"dep_param2"</span>, <span class="va">dep_param2</span><span class="op">)</span></span>
<span>      <span class="co"># ...</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_model_param</span><span class="op">(</span><span class="st">"dep_paramN"</span>, <span class="va">dep_paramN</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># (5) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_required_SFC</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">steps_for_creation</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># optional, can be omitted if do not needed</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_SFC_check_max_pos_emb</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">steps_for_creation</span><span class="op">$</span><span class="va">check_max_pos_emb</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># (6) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span></span>
<span>        model_dir <span class="op">=</span> <span class="va">model_dir</span>,</span>
<span>        text_dataset <span class="op">=</span> <span class="va">text_dataset</span>,</span>
<span>        vocab_size <span class="op">=</span> <span class="va">vocab_size</span>,</span>
<span>        <span class="co"># ...</span></span>
<span>        trace <span class="op">=</span> <span class="va">trace</span>,</span>
<span>        pytorch_safetensors <span class="op">=</span> <span class="va">pytorch_safetensors</span></span>
<span>        <span class="co"># ...</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
</li>
<li>
<p>Define <code>train</code> method <strong>(1)</strong> similarly
to the step 5. Implement steps (functions) in the private
<code>steps_for_training</code> list <strong>(2)</strong>. Do not forget
to pass <code>self</code> as input parameter of the required function.
Set the dependent parameters <strong>(4)</strong> in the base class
using <code>super$set_model_param()</code> method <strong>(5)</strong>.
Set the implemented steps for training in the base class using
<code>super$set_SFT_*()</code> methods <strong>(6)</strong>. Finally run
the basic <code>train</code> algorithm <strong>(7)</strong> of the base
class with all the (input) parameters <strong>(3)</strong>.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">{</span><span class="va">r</span><span class="op">}</span></span>
<span><span class="va">.AIFECustomTransformer</span> <span class="op">&lt;-</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html" class="external-link">R6Class</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">".AIFECustomTransformer"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="va">.AIFEBaseTransformer</span>,</span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Custom Model"</span>,</span>
<span>    steps_for_creation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="co"># required</span></span>
<span>      create_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      calculate_vocab <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      save_tokenizer_draft <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      create_final_tokenizer <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      create_transformer_model <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      <span class="co"># optional: omit this element if do not needed</span></span>
<span>      check_max_pos_emb <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="co"># (2)</span></span>
<span>    steps_for_training <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="co"># required</span></span>
<span>      load_existing_model <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      <span class="co"># optional</span></span>
<span>      cuda_empty_cache <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>      create_data_collator <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="op">}</span>,</span>
<span>    create <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># ---------------------------</span></span>
<span>      <span class="co"># super$set_model_param(...)</span></span>
<span>      <span class="co"># ...</span></span>
<span>      <span class="co"># ---------------------------</span></span>
<span>      <span class="co"># super$set_required_SFC(...)</span></span>
<span>      <span class="co"># super$set_SFC_*(...)</span></span>
<span>      <span class="co"># ...</span></span>
<span>      <span class="co"># ---------------------------</span></span>
<span>      <span class="co"># super$create(...)</span></span>
<span>    <span class="op">}</span>,</span>
<span></span>
<span>    <span class="co"># (1)</span></span>
<span>    train <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="co"># (3) --------</span></span>
<span>                     <span class="co"># ...</span></span>
<span></span>
<span>                     <span class="co"># (4) --------</span></span>
<span>                     <span class="va">dep_param1</span>,</span>
<span>                     <span class="co"># ...</span></span>
<span>                     <span class="va">dep_paramN</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># (5) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_model_param</span><span class="op">(</span><span class="st">"dep_param1"</span>, <span class="va">dep_param1</span><span class="op">)</span></span>
<span>      <span class="co"># ...</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_model_param</span><span class="op">(</span><span class="st">"dep_paramN"</span>, <span class="va">dep_paramN</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># (6) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_SFT_load_existing_model</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">steps_for_training</span><span class="op">$</span><span class="va">load_existing_model</span><span class="op">)</span></span>
<span>      <span class="co"># optional</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_SFT_cuda_empty_cache</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">steps_for_training</span><span class="op">$</span><span class="va">cuda_empty_cache</span><span class="op">)</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">set_SFT_create_data_collator</span><span class="op">(</span><span class="va">private</span><span class="op">$</span><span class="va">steps_for_training</span><span class="op">$</span><span class="va">create_data_collator</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># (7) -----------------------------------------</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>        <span class="co"># ...</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<blockquote>
<p>Now use the transformer (created in p. 4.2) like in the code example
from p. 2.1.</p>
</blockquote>
</li>
<li>
<p>In the file “R/AIFETransformerMaker.R” find the definition of the
<code>AIFETrType</code> list and add a new element to the end of it
<strong>(1)</strong>:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">AIFETrType</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  bert <span class="op">=</span> <span class="st">"bert"</span>,</span>
<span>  roberta <span class="op">=</span> <span class="st">"roberta"</span>,</span>
<span>  <span class="co"># ...</span></span>
<span>  mpnet <span class="op">=</span> <span class="st">"mpnet"</span>,</span>
<span>  custom <span class="op">=</span> <span class="st">"custom"</span>   <span class="co"># (1)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>In the end of the file “R/dotAIFECustomTransformer.R” put the
following line to use this transformer with Transformer Maker:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">.AIFETrObj</span><span class="op">[[</span><span class="va">AIFETrType</span><span class="op">$</span><span class="va">custom</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">.AIFECustomTransformer</span><span class="op">$</span><span class="va">new</span></span></code></pre></div>
<blockquote>
<p>Now use the Transformer Maker class to create a custom transformer
like in p. 3.2.</p>
</blockquote>
</li>
</ol>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Berding Florian, Tykhonova Yuliia.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>

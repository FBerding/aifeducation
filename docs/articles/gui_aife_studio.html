<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="aifeducation">
<title>02 Using the graphical user interface Aifeducation - Studio • aifeducation</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="02 Using the graphical user interface Aifeducation - Studio">
<meta property="og:description" content="aifeducation">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.4</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/aifeducation.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a>
    <a class="dropdown-item" href="../articles/gui_aife_studio.html">02a Aifeducation Studio</a>
    <a class="dropdown-item" href="../articles/classification_tasks.html">02b Classification tasks</a>
    <a class="dropdown-item" href="../articles/sharing_and_publishing.html">03 Sharing and Using Trained AI/Models</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>02 Using the graphical user interface Aifeducation - Studio</h1>
                        <h4 data-toc-skip class="author">Florian
Berding, Julia Pargmann, Andreas Slopinski, Elisabeth Riebenbauer, Karin
Rebmann</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/gui_aife_studio.Rmd" class="external-link"><code>vignettes/gui_aife_studio.Rmd</code></a></small>
      <div class="d-none name"><code>gui_aife_studio.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-and-overview">1 Introduction and Overview<a class="anchor" aria-label="anchor" href="#introduction-and-overview"></a>
</h2>
<div class="section level3">
<h3 id="preface">1.1 Preface<a class="anchor" aria-label="anchor" href="#preface"></a>
</h3>
<p>This vignette introduces <em>Aifeducation - Studio</em> which is a
graphical user interface for creating, training, documenting, analyzing,
and applying artificial intelligence (AI). It is made for users
unfamiliar with <em>R</em> or those who do not have coding skills in
relevant languages (e.g., python). For more experienced users, the
interface provides a convenient way for working with AI in an
educational context.</p>
<p>This article overlaps with vignette <a href="classification_tasks.html">03 Using R syntax</a>, which explains
how to use the package with <em>R</em> syntax. We assume that
<em>aifeducation</em> is installed as described in vignette <a href="https://fberding.github.io/aifeducation/articles/aifeducation.html" class="external-link">01
Get Started</a>. The introduction starts with a brief explanation of
basic concepts, which are necessary to work with this package.</p>
</div>
<div class="section level3">
<h3 id="basic-concepts">1.2 Basic Concepts<a class="anchor" aria-label="anchor" href="#basic-concepts"></a>
</h3>
<p>In the educational and social sciences, assigning an observation to
scientific concepts is an important task that allows researchers to
understand an observation, to generate new insights, and to derive
recommendations for research and practice.</p>
<p>In educational science, several areas deal with this kind of task.
For example, diagnosing students’ characteristics is an important aspect
of a teachers’ profession and necessary to understand and promote
learning. Another example is the use of learning analytics, where data
about students is used to provide learning environments adapted to their
individual needs. On another level, educational institutions such as
schools and universities can use this information for data-driven
performance decisions (Laurusson &amp; White 2014) as well as where and
how to improve it. In any case, a real-world observation is aligned with
scientific models to use scientific knowledge as a technology for
improved learning and instruction.</p>
<p>Supervised machine learning is one concept that allows a link between
real-world observations and existing scientific models and theories
(Berding et al. 2022). For the educational sciences, this is a great
advantage because it allows researchers to use the existing knowledge
and insights to apply AI. The drawback of this approach is that the
training of AI requires both information about the real world
observations and information on the corresponding alignment with
scientific models and theories.</p>
<p>A valuable source of data in educational science are written texts,
since textual data can be found almost everywhere in the realm of
learning and teaching (Berding et al. 2022). For example, teachers often
require students to solve a task which they provide in a written form.
Students have to create a solution for the tasks which they often
document with a short written essay or a presentation. This data can be
used to analyze learning and teaching. Teachers’ written tasks for their
students may provide insights into the quality of instruction while
students’ solutions may provide insights into their learning outcomes
and prerequisites.</p>
<p>AI can be a helpful assistant in analyzing textual data since the
analysis of textual data is a challenging and time-consuming task for
humans.</p>
<blockquote>
<p>Please note that an introduction to content analysis, natural
language processing or machine learning is beyond the scope of this
vignette. If you would like to learn more, please refer to the cited
literature.</p>
</blockquote>
<p>Before we start, it is necessary to introduce a definition of our
understanding of some basic concepts, since applying AI to educational
contexts means to combine the knowledge of different scientific
disciplines using different, sometimes overlapping concepts. Even within
a single research area, concepts are not unified. Figure 1 illustrates
this package’s understanding.</p>
<div class="float">
<img src="img_articles/classif_fig_01.png" style="width:100.0%" alt="Figure 1: Understanding of Central Concepts"><div class="figcaption">Figure 1: Understanding of Central
Concepts</div>
</div>
<p>Since <em>aifeducation</em> looks at the application of AI for
classification tasks from the perspective of the empirical method of
content analysis, there is some overlapping between the concepts of
content analysis and machine learning. In content analysis, a phenomenon
like performance or colors can be described as a scale/dimension which
is made up by several categories (e.g. Schreier 2012, pp. 59). In our
example, an exam’s performance (scale/dimension) could be “good”,
“average” or “poor”. In terms of colors (scale/dimension) categories
could be “blue”, “green”, etc. Machine learning literature uses other
words to describe this kind of data. In machine learning, “scale” and
“dimension” correspond to the term “label” while “categories” refer to
the term “classes” (Chollet, Kalinowski &amp; Allaire 2022, p. 114).</p>
<p>With these clarifications, classification means that a text is
assigned to the correct category of a scale or, respectively, that the
text is labeled with the correct class. As Figure 2 illustrates, two
kinds of data are necessary to train an AI to classify text in line with
supervised machine learning principles.</p>
<div class="float">
<img src="img_articles/classif_fig_02.png" style="width:100.0%" alt="Figure 2: Basic Structure of Supervised Machine Learning"><div class="figcaption">Figure 2: Basic Structure of Supervised Machine
Learning</div>
</div>
<p>By providing AI with both the textual data as input data and the
corresponding information about the class as target data, AI can learn
which texts imply a specific class or category. In the above exam
example, AI can learn which texts imply a “good”, an “average” or a
“poor” judgment. After training, AI can be applied to new texts and
predict the most likely class of every new text. The generated class can
be used for further statistical analysis or to derive recommendations
about learning and teaching.</p>
<p>In use cases as described in this vignette, AI has to “understand”
natural language: „Natural language processing is an area of research in
computer science and artificial intelligence (AI) concerned with
processing natural languages such as English and Mandarin. This
processing generally involves translating natural language into data
(numbers) that a computer can use to learn about the world. (…)” (Lane ,
Howard &amp; Hapke 2019, p. 4)</p>
<p>Thus, the first step is to transform raw texts into a a form that is
usable for a computer, hence raw texts must be transformed into numbers.
In modern approaches, this is usually done through word embeddings.
Campesato (2021, p. 102) describes them as “the collective name for a
set of language modeling and feature learning techniques (…) where words
or phrases from the vocabulary are mapped to vectors of real numbers.”
The definition of a word vector is similar: „Word vectors represent the
semantic meaning of words as vectors in the context of the training
corpus.” (Lane, Howard &amp; Hapke 2019, p. 191). In the next step, the
words or text embeddings can be used as input data and the labels as
target data for training AI to classify a text.</p>
<p>In <em>aifeducation,</em> these steps are covered with three
different types of models, as shown in Figure 3.</p>
<div class="float">
<img src="img_articles/classif_model_hierachy.png" style="width:100.0%" alt="Figure 3: Modells Types in aifeducation"><div class="figcaption">Figure 3: Modells Types in aifeducation</div>
</div>
<ul>
<li><p><strong>Base Models:</strong> The base models are the models
which contain the capacities to understand natural language. In general,
these are transformers such as BERT, RoBERTa, etc. A huge number of
pre-trained models can be found on <a href="https://huggingface.co/" class="external-link">Huggingface</a>.</p></li>
<li><p><strong>Text Embedding Models:</strong> The modes are built on
top of base models and store directions on how to use these base models
for converting raw texts into sequences of numbers. Please note that the
same base model can be used to create different text embedding
models.</p></li>
<li><p><strong>Classifiers:</strong> Classifiers are used on top of a
text embedding model. They are used to classify a text into
categories/classes based on the numeric representation provided by the
corresponding text embedding model. Please note that a text embedding
model can be used to create different classifiers (e.g. one classifier
for colors, one classifier to estimate the quality of a text,
etc.).</p></li>
</ul>
<p>With the help of this overview we can start the introduction of
<em>Aifeducation Studio</em>.</p>
</div>
</div>
<div class="section level2">
<h2 id="starting-aifeducation-studio">2 Starting Aifeducation Studio<a class="anchor" aria-label="anchor" href="#starting-aifeducation-studio"></a>
</h2>
<p>We recommend to start with a clean <em>R</em> session. Then you can
start Aifeducation Studio by entering the following into the
console:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/aifeducation/" class="external-link">aifeducation</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/start_aifeducation_studio.html">start_aifeducation_studio</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Please note that this can take a moment.</p>
<p>At the beginning you will see the start page (Figure 4). Here you can
configure your current session. First, it is very important that you
choose the machine learning framework that you would like to use during
the session (box <em>Machine Learning Framework</em>). This choice
cannot be changed after the session starts. To change the framework you
have to restart <em>Aifeducation Studio</em>.</p>
<div class="float">
<img src="img_articles/gui_aife_studio_config.jpg" style="width:100.0%" alt="Figure 4: Start Page"><div class="figcaption">Figure 4: Start Page</div>
</div>
<p>Depending on the chosen framework, you can activate some further
settings (box <em>Machine Learning Framework</em>). If you would like to
use <em>tensorflow</em> and your computer has a graphic device with low
memory, we recommend to activate the option for low memory. For
<em>pytorch</em>, no configuration is necessary.</p>
<p>On the right side of the start page, you can decide if energy
consumption should be recorded during training AI (box
<em>Sustainability Tracking</em>). Tracking energy consumption allows to
estimate the CO2 emissions of using AI. Since our world faces the
challenge of climate change, we recommend to enable this option. In this
case you have to choose your country in order to allow a more accurate
estimation of the model’s sustainability impact.</p>
<p>If you are ready you can press the start button (box <em>Start
Session</em>), which directs you to the home page.</p>
</div>
<div class="section level2">
<h2 id="using-aifeducation-studio">3 Using Aifeducation Studio<a class="anchor" aria-label="anchor" href="#using-aifeducation-studio"></a>
</h2>
<div class="section level3">
<h3 id="preparing-data">3.1 Preparing Data<a class="anchor" aria-label="anchor" href="#preparing-data"></a>
</h3>
<div class="section level4">
<h4 id="collection-of-raw-texts">3.1.1 Collection of Raw Texts<a class="anchor" aria-label="anchor" href="#collection-of-raw-texts"></a>
</h4>
<p>The fist step in working with AI is to gather and to structure data.
In the scope of <em>aifeducation,</em> data can be either a collection
of raw texts, sequences of numbers representing the texts (text
embeddings) or texts’ labels.</p>
<p>Collections of raw texts are necessary in two cases: First, to train
or to fine-tune base models. Second, to transform texts into texts
embeddings which can be used as input for training a classifier or for
predicting the texts’ labels via classifier.</p>
<p>To create a collection of raw texts, you have to choose the <em>Data
Preparation</em> page on the right side as shown in Figure 5.</p>
<div class="float">
<img src="img_articles/gui_aife_studio_home.jpg" style="width:100.0%" alt="Figure 5: Start Page"><div class="figcaption">Figure 5: Start Page</div>
</div>
<p>On the resulting page (see Figure 6), you first have to choose the
directory where the texts are stored (box <em>Text Sources</em>). We
recommend that you store all texts you would like to use in a single
folder. Within this folder, you can structure your data with
sub-folders. In case you use sub-folders, please ensure that you include
them for creating a collection of raw texts.</p>
<div class="float">
<img src="img_articles/gui_aife_studio_create_text_collection.jpg" style="width:100.0%" alt="Figure 6: Data Preparation Page"><div class="figcaption">Figure 6: Data Preparation Page</div>
</div>
<p>In the next step, you can decide which file formats should be
included (box <em>File Types</em>). Currently, <em>aifeducation</em>
supports .pdf, .csv, and .xlsx files. If enabled, all files of the
requested file format are included in your data collection.</p>
<p>In case you would like to consider .xlsx files, <strong>all</strong>
files must have one column containing the texts and one column for the
texts’ IDs as shown in Figure 7. The name for the corresponding columns
must be identical for all files and you have to provide the column names
(first row in Figure 7).</p>
<div class="float">
<img src="img_articles/gui_excel_raw_texts.png" style="width:50.0%" alt="Figure 7: Structure of Raw Texts within an Excel Sheet"><div class="figcaption">Figure 7: Structure of Raw Texts within an Excel
Sheet</div>
</div>
<p>The last step is to choose a folder where your collection of raw
texts should be saved. Please select a folder and provide a name for
your file (box <em>Text Output</em>).</p>
<p>Finally, you can start creating the collection (box <em>Start
Process</em>). Please note that this can take some time.</p>
<p>When the process finishes, you have a single file which can be used
for further tasks. The file contains a <code>data.table</code> that
stores the texts together with their IDs. In case of .xlsx files, the
texts’ IDs are set to the IDs stored in the corresponding column for ID.
In the case of .pdf and .csv files, the file names are used as ID
(without the file extension, see Figure 8).</p>
<div class="float">
<img src="img_articles/gui_excel_raw_texts_ids.png" style="width:50.0%" alt="Figure 8: Filnames as IDs"><div class="figcaption">Figure 8: Filnames as IDs</div>
</div>
<blockquote>
<p>Please note that a consequence of this is that two files
<em>text_01.csv</em> and <em>text_01.pdf</em> have the same ID, which is
not allowed. Please ensure that you use unique IDs across file
formats.</p>
</blockquote>
<p>The IDs are very important since they are used to match the
corresponding class/category if available.</p>
</div>
<div class="section level4">
<h4 id="collections-of-texts-labels">3.1.2 Collections of Texts’ Labels<a class="anchor" aria-label="anchor" href="#collections-of-texts-labels"></a>
</h4>
<p>Labels are necessary if you would like to train a classifier. The
easiest way is to create a table that contains a column for the texts’
ID and one or multiple columns that contain the texts’
categories/classes. Supported file formats are .xlsx, .csv, and
.rda/.rdata. Figure 9 illustrates an example for a .xslx file.</p>
<div class="float">
<img src="img_articles/gui_excel_target_data.png" style="width:50.0%" alt="Figure 9: Filnames as IDs"><div class="figcaption">Figure 9: Filnames as IDs</div>
</div>
<blockquote>
<p>In any case, the table must contain a column with the name “id” which
contains the texts’ IDs. All other columns must also have unique names.
Please pay attention to use “id” and not “ID” or “Id”.</p>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="base-models">3.2 Base Models<a class="anchor" aria-label="anchor" href="#base-models"></a>
</h3>
<div class="section level4">
<h4 id="overview">3.2.1 Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h4>
<p>Base models are the foundation of all further models in
<em>aifeducation</em>. At the moment, these are transformer models such
as BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version
2 (He et al. 2020), Funnel-Transformer (Dai et al. 2020), and Longformer
(Beltagy, Peters &amp; Cohan 2020). In general, these models are trained
on a large corpus of general texts in the first step. In the next step,
the models are fine-tuned to domain-specific texts and/or fine-tuned for
specific tasks. Since the creation of base models requires a huge number
of texts resulting in high computational time, it is recommended to use
pre-trained models. These can be found on <a href="https://huggingface.co/" class="external-link">Huggingface</a>. Sometimes, however, it
is more straightforward to create a new model to fit a specific purpose.
Aifeducation Studio supports the opportunity to both create and
train/fine-tune base models.</p>
</div>
<div class="section level4">
<h4 id="creation-of-base-models">3.2.2 Creation of Base Models<a class="anchor" aria-label="anchor" href="#creation-of-base-models"></a>
</h4>
<p>In order to create a new base model you have to choose the option
<em>Create</em> on the tab <em>Language Modeling</em> for the <em>Base
Models</em> on the right side of the app (Figure 5). Figure 10 shows the
corresponding page.</p>
<p><a href="img_articles/gui_aife_studio_create_transformer.jpg"><img src="img_articles/gui_aife_studio_create_transformer.jpg" style="width:100.0%" alt="Figure 10: Language Modeling - Create Transformer"></a> Figure
10: Language Modeling - Create Transformer (click image to enlarge)</p>
<p>Every transformer model is composed of two parts: 1) the tokenizer
which splits raw texts into smaller pieces to model a large number of
words with a limited, small number of tokens and 2) the neural network
that is used to model the capabilities for understanding natural
language.</p>
<p>At the beginning you can choose between the different supported
transformer architectures (box <em>Model Architecture</em>). Depending
on the architecture, you have different options determining the shape of
your neural network.</p>
<p>In the middle, you find a box named <em>Vocabulary</em>. Here you
must provide the path to a file which contains a collection of raw
texts. These raw texts are used to calculate the vocabulary of the
transformer. This file should be created with <em>Aifeducation
Studio</em> to ensure compatibility. See section 3.1.1 for more details.
It is very important that you provide a number of how many tokens the
vocabulary should include. Depending on the transformer method, you can
set additional options affecting a transformer’s vocabulary.</p>
<ul>
<li><p><strong>Transform to Lower:</strong> If this option is enabled,
all words in a raw text are transformed to lower cases. For instance,
the resulting token of <em>Learners</em> and <em>learners</em> are the
same. If disabled, <em>Learners</em> and <em>learners</em> will have a
different tokenization.</p></li>
<li><p><strong>Add Prefix Spaces:</strong> If enabled, a space is added
to the first word if there is not already one. Thus, enabling this
option leads to a similar tokenization for the word <em>learners</em> in
both cases: 1) “<em>learners</em> need a high motivation for high
achievement.” and 2) “A high motivation is necessary for
<em>learners</em> to achieve high performance.”.</p></li>
<li><p><strong>Trim Offsets:</strong> If this option is enabled, the
white spaces of the produced offsets are trimmed.</p></li>
</ul>
<p>The last step is to choose a folder where the new base model should
be saved (box <em>Creation</em>). Finally, you can start the creation of
your model by clicking on the button “Start Creation”. The creation of
the model may take some time.</p>
</div>
<div class="section level4">
<h4 id="traintune-a-base-model">3.2.3 Train/Tune a Base Model<a class="anchor" aria-label="anchor" href="#traintune-a-base-model"></a>
</h4>
<p>If you would like to train a new base model (see section 3.2.2) for
the first time or want to adapt a pre-trained model to a domain-specific
language or task, you have to click on “Train/Tune” on the right side of
the app. You can find this option via “Language Modeling” as shown in
Figure 5.</p>
<p><a href="img_articles/gui_aife_studio_train_transformer.jpg"><img src="img_articles/gui_aife_studio_train_transformer.jpg" style="width:100.0%" alt="Figure 11: Language Modeling - Train/Tune Transformer"></a>
Figure 11: Language Modeling - Train/Tune Transformer (click image to
enlarge)</p>
<p>In the first step, you have to choose the base model you would like
to train/tune (box <em>Base Model</em>). Please note that every base
model consists of several files. Thus, you cannot provide neither single
nor multiple files. Instead you have to provide the folder that stores
the entire model.</p>
<p>Compatible models are all base models that you have created with
<em>Aifeducation Studio</em>. In addition you can use any model from <a href="https://huggingface.co/" class="external-link">Huggingface</a> that uses an architecture
implemented in <em>aifeducation</em> such as BERT, DeBERTa, etc.</p>
<p>After choosing a base model, new boxes appear as shown in Figure 11.
To train a model, you must first provide a collection of raw texts (box
<em>Raw Texts</em>). We recommend that you create this collection of
texts as described in section 3.1.1.</p>
<p>Next you can configure the training of your base model (box <em>Train
and Tune Settings</em>). Possible options depend on the kind of
model.</p>
<ul>
<li><p><strong>Chunk Size:</strong> For training and validating a base
model, the raw texts are split into several smaller texts. This value
determines the maximum length of these smaller text pieces in number of
tokens. This value cannot exceed the maximum size set during creation of
the base model.</p></li>
<li><p><strong>Minimal Sequence Length:</strong> This value determines
the minimal length of a text chunk in order to be part of the training
and validation data.</p></li>
<li><p><strong>Full Sequences Only</strong>: If this option is enabled,
only text chunks with a number of tokens equal to “chunk size” are
included in the data. Disable this option if you have a lot of small
text chunks you would like to use for training and validation.</p></li>
<li><p><strong>Probability of Token Masking:</strong> This option
determines how many tokens of every sequence should be masked.</p></li>
<li><p><strong>Whole Word Masking:</strong> If this option is activated,
all tokens belonging to a single word are masked. If this options is
disabled or not available token masking is used.</p></li>
<li><p><strong>Validation Size:</strong> This option determines how many
sequences should be used for validating the performance of the base
model. Sequences used for validation are not available for
training.</p></li>
<li><p><strong>Batch Size:</strong> This option determines how many
sequences should be processed at the same time. Please adjust this value
to the computation capacities of your machine.</p></li>
<li><p><strong>n Epochs:</strong> The maximum number of epochs for
training. During training, the model that has the best validation loss
is saved on disk an will be used for the final model.</p></li>
<li><p><strong>Learning Rate:</strong> The initial learning
rate.</p></li>
</ul>
<p>In the last step, you have to provide the directory where your
trained model should be saved after training (box <em>Start
Training/Tuning</em>). The corresponding folder will also contain the
checkpoints during training. It is very important that this directory is
not the same directory as the one you stored the original model in. By
clicking the button “Start Training/Tuning”, the training starts. Please
note that the training of a base model can last days or even weeks,
depending on the size and the kind of model, the amount of data, and the
capacities of your machine.</p>
</div>
</div>
<div class="section level3">
<h3 id="text-embedding-models">3.3 Text Embedding Models<a class="anchor" aria-label="anchor" href="#text-embedding-models"></a>
</h3>
<div class="section level4">
<h4 id="create-a-text-embedding-model">3.3.1 Create a Text Embedding Model<a class="anchor" aria-label="anchor" href="#create-a-text-embedding-model"></a>
</h4>
<p>The text embedding model is the interface to <em>R</em> in
<em>aifeducation</em>. In order to create a new model, you need a base
model that provides the ability to understand natural language. This
object contains all relevant information for transforming raw texts into
a numeric representation that can be used for machine learning.</p>
<p>In <em>aifedcuation</em>, the transformation of raw texts into
numbers is a separate step from downstream tasks such as classification.
This is to reduce computational time on machines with low performance.
By separating text embedding from other tasks, the text embedding has to
be calculated only once and can be used for different tasks at the same
time. Another advantage is that the training of the downstream tasks
involves only the downstream tasks an not the parameters of the
embedding model, making training less time-consuming, thus decreasing
computational intensity. Finally, this approach allows the analysis of
long documents by applying the same algorithm to different parts.</p>
<p>You can open the creation page by clicking on “Create” in the section
“Text Embedding Model” via “Language Modeling” (see Figure 5). Figure 12
shows the corresponding page.</p>
<p><a href="img_articles/gui_aife_studio_emb_interface_create.jpg"><img src="img_articles/gui_aife_studio_emb_interface_create.jpg" style="width:100.0%" alt="Figure 12: Text Embedding Model - Create"></a> Figure 12: Text
Embedding Model - Create (click image to enlarge)</p>
<p>First you have to choose the base model that should form the
foundation of your new text embedding model. Please select the folder
that contains the entire model and not single files (box <em>Base
Model</em>).</p>
<p>After choosing a model, new boxes appear which allow you to customize
your interface (box <em>Interface Setting</em>). Here it is very
important that you give your model a unique name and label. The
difference between <em>Name</em> and <em>Label</em> is that
<em>Name</em> is used for the computer and <em>Label</em> for users.
Thus, <em>Name</em> should not contain any spaces or special characters.
<em>Label</em> does not have any restrictions. Think of <em>Label</em>
as the title of a book or paper. With <em>Version</em> you can provide a
version number if you create a newer version of your model. In case you
create a new model, we recommend to use “0.0.1”. For <em>Language,</em>
it is necessary that you choose which language your model is created
for, such as English, French, German, etc.</p>
<p>On the right side of the box <em>Interface Setting</em> you can set
how the interface should process raw text:</p>
<ul>
<li><p><strong>N chunks:</strong> Sometimes texts are very long. With
this value, you can decide into how many chunks longer texts should be
divided. The maximum length of every chunk is determined by the value
provided at “Maximum Sequence Length”.</p></li>
<li><p><strong>Maximal Sequence Length:</strong> This value determines
the maximum number of tokens your model processes for every
chunk.</p></li>
<li><p><strong>N Token Overlap:</strong> This value determines how many
tokens form the prior chunk should be included in the current chunk. An
overlap can be useful to provide the correct context for every
chunk.</p></li>
<li><p><strong>Layers for Embeddings - Min</strong>: Base models
transform raw data into a sequence of numbers by using the different
layers’ hidden states. With this option you can decide which is the
first layer to use.</p></li>
<li><p><strong>Layers for Embeddings - Max</strong>: With this option
you can decide which is the last layer to use. The hidden states of all
layers between min and max are averaged to form the embedding of a text
chunk.</p></li>
<li><p><strong>Pooling Type</strong>: With this option you can decide if
only the hidden states of the cls-token should be used for the
embedding. If you set thisn option to “average” the hidden states of all
tokens are averaged within each layer except the hidden states for
padding tokens.</p></li>
</ul>
<p>The maximum number of tokens your model can process and provide for
downstream tasks can be calculated as <span class="math display">\[Max
Tokens = NChunks*MaximalSequenceLength-(NChunks-1)*NOverlap\]</span> If
the text is longer, the remaining tokens are ignored and lost for
further analysis.</p>
<blockquote>
<p>Please note that you can create multiple text embedding models with a
different configuration based on the same base model.</p>
</blockquote>
<p>The last step is to provide and name a folder to save the model to
(box <em>Creation</em>).</p>
</div>
<div class="section level4">
<h4 id="using-a-text-embedding-model">3.3.2 Using a Text Embedding Model<a class="anchor" aria-label="anchor" href="#using-a-text-embedding-model"></a>
</h4>
<p>Using a text embedding model is a central aspect of applying
artificial intelligence in <em>aifeducation</em>. The corresponding page
can be found by clicking on “Use” in the tab “Language Modeling”. At the
start you have to choose the model that you would like to use. Please
select the folder that contains the entire model instead of selecting
single files. After selecting and loading a model, a new box appears
that shows you the different aspects of the model and how you can use
it.</p>
<p>The tab <em>Model Description</em> (Figure 13) provides you with the
documentation of the model.</p>
<p><a href="img_articles/gui_aife_studio_emb_interface_use_desc.jpg"><img src="img_articles/gui_aife_studio_emb_interface_use_desc.jpg" style="width:100.0%" alt="Figure 13: Text Embedding Model - Description"></a> Figure 13:
Text Embedding Model - Description (click image to enlarge)</p>
<p>The tab <em>Training</em> shows the development of the loss and the
validation loss during the last training of the corresponding base
model. If no plot is displayed no history data is available.</p>
<p>The tab <em>Create Text Embeddings</em> (Figure 14) allows you to
transform raw texts into a numerical representation of these texts,
called text embeddings. These text embeddings can be used in downstream
tasks such as classifying texts. In order to transform raw texts into
embedded texts, you first have to select a collection of raw texts. We
recommend that you create this collection according to section 3.1.1.
Next you have to provide the folder where the embeddings should be
stored and name it. With <em>Batch Size</em> you can determine how many
raw texts should be processed simultaneously. Please adjust this value
to your machine’s capacities. By clicking on the button “Start Embed”
the transformation of texts begins.</p>
<p><a href="img_articles/gui_aife_studio_emb_interface_use_embed.jpg"><img src="img_articles/gui_aife_studio_emb_interface_use_embed.jpg" style="width:100.0%" alt="Figure 14: Text Embedding Model - Embeddings"></a> Figure 14:
Text Embedding Model - Embeddings (click image to enlarge)</p>
<p>The tab <em>Encode/Decode/Tokenize</em> (Figure 15) offers you
insights into the way your text embedding model processes data. In the
box <em>Encode</em> you can insert a raw text and after clicking on
<em>Encode</em> you can see how the text is divided into tokens and
their corresponding IDs. The IDs are passed to the base model and are
used to generate the numeric representation of a text. The box
<em>Decode</em> allows you to reverse this process. There you can insert
a sequence of numbers (separated with a comma or with spaces) and after
clicking on <em>Decode,</em> the corresponding tokens and the raw text
appear.</p>
<p><a href="img_articles/gui_aife_studio_emb_interface_use_encode_decode.jpg"><img src="img_articles/gui_aife_studio_emb_interface_use_encode_decode.jpg" style="width:100.0%" alt="Figure 15: Text Embedding Model - Encode/Decode/Tokenize"></a>
Figure 15: Text Embedding Model - Encode/Decode/Tokenize (click image to
enlarge)</p>
<p>Finally, the tab <em>Fill Mask</em> (Figure 16) allows you to request
the underlying base model of your text embedding model to calculate a
solution to a fill-in-the-blank text. In the box <em>Text</em> you can
insert a raw text. A gap is signaled by insert the corresponding masking
token. This token can be found in the table above in the row for
“mask_token”. If you insert a gap/mask_token please ensure correct
spelling. With “N Solutions per Mask<em>”</em> you can determine how
many tokens the model should calculate for every gap/mask_token. After
clicking”Calculate Tokens”, you will find an image on the right side of
the box, showing you the most reasonable token for the selected gap. The
tokens are ordered by certainty; from the perspective of the model, the
most reasonable tokens are at the top and the less reasonable tokens are
at the bottom.</p>
<p><a href="img_articles/gui_aife_studio_emb_interface_use_fill_mask.jpg"><img src="img_articles/gui_aife_studio_emb_interface_use_fill_mask.jpg" style="width:100.0%" alt="Figure 16: Text Embedding Model - Fill Mask"></a> Figure 16: Text
Embedding Model - Fill Mask (click image to enlarge)</p>
</div>
<div class="section level4">
<h4 id="documenting-a-text-embedding-model">3.3.3 Documenting a Text Embedding Model<a class="anchor" aria-label="anchor" href="#documenting-a-text-embedding-model"></a>
</h4>
<p>Creating “good” AI models requires a lot of effort. Thus, sharing
work with other users is very important to support progress in a
discipline. Thus, meaningful documentation is required. In addition, a
well written documentation makes an AI model more transparent, allowing
others to understand how the AI model generated a solution. This is also
very important in order to judge the limitations of a model.</p>
<p>To support developers in documenting their work, <em>Aifeducation
Studio</em> provides an easy way to add a comprehensive description. You
find this part of the app by clicking on “Document” in the tab
<em>Language Modeling</em>. First, you have to choose the text embedding
model you would like to document (not the base model!).</p>
<p>After choosing a model, a new box appears, allowing you to insert the
necessary information. Via the tabs <em>Developers</em> and
<em>Modifiers,</em> you can provide the names and email addresses of all
relevant contributors. <em>Developers</em> refer to the people who
created a model, while <em>Modifiers</em> refers to the people who
adapted a pre-trained model to another domain or task.</p>
<p><a href="img_articles/gui_aife_studio_emb_doc.jpg"><img src="img_articles/gui_aife_studio_emb_doc.jpg" style="width:100.0%" alt="Figure 17: Text Embedding Model - Documentation"></a> Figure 17:
Text Embedding Model - Documentation (click image to enlarge)</p>
<p>In the tabs <em>Abstract</em> and <em>Description</em>, you can
provide an abstract and a detailed description of your work in English
and/or in the native language of your text embedding model (e.g.,
French, German, etc.), allowing you to reach a broader audience (Figure
17). In all four tabs you can provide your documentation in plain text,
html, and/or markdown allowing you to insert tables or to highlight
parts of your documentation. If you would like to see what your
documentation will look like on the internet, you can click on the
button “Preview”. Saving your changes is possible by clicking on
<em>Save</em>.</p>
<blockquote>
<p>For more information on how to document your model, please refer to
the vignette <a href="sharing_and_publishing.html">03 Sharing and Using
Trained AI/Models</a>.</p>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="classifiers">3.4 Classifiers<a class="anchor" aria-label="anchor" href="#classifiers"></a>
</h3>
<div class="section level4">
<h4 id="create-a-classifier">3.4.1 Create a Classifier<a class="anchor" aria-label="anchor" href="#create-a-classifier"></a>
</h4>
<p>Classifiers are built on top of a text embedding model. To create a
classifier, click on “Create and Train” in the tab
<em>Classification</em> (see Figure 5). Figure 18 shows the
corresponding page.</p>
<p><a href="img_articles/gui_aife_studio_classifier_create_part_1.jpg"><img src="img_articles/gui_aife_studio_classifier_create_part_1.jpg" style="width:100.0%" alt="Figure 18: Classifier - Creation Part 1"></a> Figure 18:
Classifier - Creation Part 1 (click image to enlarge)</p>
<p>Creating a classifier requires two kinds of data. First, a text
embedding of a collection of texts. These embeddings should be created
with a text embedding model as described in section 3.3.2. Second, a
table with the labels of every text. This kind of data should be created
as described in section 3.1.2.</p>
<p>You can provide the text embeddings by opening the corresponding file
in the first box (<em>Input Data</em>). After selecting the embeddings,
you will see a summary of the underlying text embedding model that
generated the embeddings. In addition, you can see how many documents
are in the file.</p>
<blockquote>
<p>Please note that the classifier is bound to the text embedding model
that generated the embeddings. That is, the classifier can only be used
if you have access to the corresponding text embedding model. The text
embedding model is necessary to transform raw texts into a format that
the classifier can understand.</p>
</blockquote>
<p>In the second box (<em>Target Data</em>), you can select the file
that contains the corresponding labels. After loading the file, you can
select the column of the table that you would like to use as target data
for training. In addition, you can see a short summary of the absolute
frequencies of the single classes/categories.</p>
<blockquote>
<p>Please note that you can create multiple classifiers for different
target data based on the same text embedding model. Thus, there is no
need to create a new text embedding model for a new classifier. In
particular, you can use the same text embeddings for training different
classifiers.</p>
</blockquote>
<p>In the third box (<em>Architecture</em>) you create the architecture
of your neural network (Figure 19). It is very important that you
provide the model’s name and label in the section <em>General</em>. The
<em>Model Name</em> is used for internal purposes by the machine while
the <em>Model Label</em> is used as a title of your classifiers for
other users. Thus, the <em>Model Name</em> should not contain any spaces
or special characters. For <em>Model Label,</em> there are no
restrictions.</p>
<p><a href="img_articles/gui_aife_studio_classifier_create_part_2.jpg"><img src="img_articles/gui_aife_studio_classifier_create_part_2.jpg" style="width:100.0%" alt="Figure 19: Classifier - Creation Part 2"></a> Figure 19:
Classifier - Creation Part 2 (click image to enlarge)</p>
<p>To expand the different sections you can click on the “+” on the
right side, since a detailed explanation of every option is beyond the
scope of this introduction. Here we can only provide an overview.</p>
<ul>
<li><p><strong>Positional Embedding:</strong> By activating this option,
you add positional embedding to your classifier. This provides the
neural network with the ability to take the order within a sequence into
account.</p></li>
<li><p><strong>Encoding Layers:</strong> These layers are similar to
encoding layers used in some transformer models, allowing to calculate
context-sensitive text embeddings. That is, they provide your classifier
with the ability to take the surrounding text chunks (see section 3.3.1)
of the sequences into account.</p></li>
<li><p><strong>Recurrent Layers:</strong> This section allows you to add
recurrent layers to your classifier. These layers are able to account
for the order within a sequence. In order to add these layers, just pass
the numbers to the input field <em>Reccurent Layers</em> and separate
them with a comma or space. Every number represents a layer and the
number determines the number of neurons. Below the field you can see
what <em>Aifeducation Studio</em> does with your input. This is helpful
to avoid invalid specifications of layers.</p></li>
<li><p><strong>Dense Layers:</strong> In this section you can add dense
layers to your network. The process to add layers is similar to the
process of recurrent layers.</p></li>
<li><p><strong>Optimizer:</strong> Here you can choose between different
optimizers for training.</p></li>
</ul>
<p>The next box (<em>Training Settings</em>) contains the setting for
training your classifier (Figure 19). Going into detail is again beyond
the scope of this introduction. Here we can only provide an
overview.</p>
<p><strong>Section: General Setting</strong></p>
<ul>
<li><p><strong>Balance Class Weights</strong>: If this option is
enabled, the loss is adjusted to the absolute frequencies of the
classes/categories according to the ‘Inverse Class Frequency’ method.
This option should be activated if you have to deal with imbalanced
data.</p></li>
<li><p><strong>Balance Sequence Length</strong>: Activating this option
can increase performance if you have to deal with texts that differ in
their lengths and have an imbalanced frequency. If this option is
enabled, the loss is adjusted to the absolute frequencies of length of
your texts according to the ‘Inverse Class Frequency’ method.</p></li>
<li><p><strong>Number of Folds:</strong> The number of folds used for
estimating the performance of your classifier.</p></li>
<li><p><strong>Proportion for Validation Sample:</strong> The percentage
of cases within each fold used as a validation sample. This sample is
used to determine the state of the model that generalizes best.</p></li>
<li><p><strong>Epochs:</strong> Maximal number of epochs. During
training, the model with the best balanced accuracy is saved and
used.</p></li>
<li><p><strong>Batch Size:</strong> The number of cases that should be
processed simultaneously. Please adjust this value to your machine’s
capacities. Please note that the batch size can have an impact on the
classifier’s performance.</p></li>
</ul>
<p><strong>Section: Synthetic Cases</strong></p>
<ul>
<li><p><strong>Add Synthetic Cases:</strong> If active, the creation of
additional synthetic cases is applied during training. They are added to
the train data. The way these cases are generated can be configured with
the following parameters:</p></li>
<li><p><strong>Method:</strong> The method which should be used for
generating the cases.</p></li>
<li><p><strong>Min k:</strong> The minimal number of neighbors used for
generating the synthetic cases.</p></li>
<li><p><strong>Max k:</strong> The maximum number of neighbors used for
generating the synthetic cases.</p></li>
</ul>
<p>The algorithm will generate a number of synthetic cases for every
class to ensure that the number of cases for every class equals the
number of cases of the majority class. The synthetic cases for every
class a generated for all <em>k</em> between <em>Min k</em> and <em>Max
k</em>. Every <em>k</em> contributes proportional to the synthetic
cases.</p>
<p><strong>Section: Pseudo-Labeling</strong></p>
<ul>
<li><p><strong>Add Pseudo Labeling:</strong> If activated,
pseudo-labeling is used during training as described by Cascante-Bonilla
et al. (2020). The way pseudo-labeling is applied can be configured with
the following parameters:</p></li>
<li><p><strong>Max Steps:</strong> The number of steps during
pseudo-labeling. For example, in the first step, 1/Max Steps
pseudo-labeled cases are added, in the second step, 2/Max Steps
pseudo-labeled cases are added, etc.. Which cases are added can be
influenced by <em>Balance Pseudo-Labels</em>, <em>Certainty Anchor</em>,
<em>Max Certainty Value</em>, and <em>Min Certainty Value</em>.</p></li>
<li><p><strong>Certainty Anchor:</strong> This value determines the
reference point for choosing pseudo-labeled cases. 1 refers to perfect
certainty, 0 refers to a certainty similar to random guessing. Selected
are the cases that are closest to this value.</p></li>
<li><p><strong>Max Certainty Value:</strong> Pseudo-labeled cases
exceeding this value are not included during training.</p></li>
<li><p><strong>Min Certainty Value:</strong> Pseudo-labeled cases
falling bellow this value are not included during training.</p></li>
</ul>
<p>We recommend to use pseudo-labeling as described by Cascante-Bonilla
et al. (2020). Therefore, the following parameters have to be set:</p>
<ul>
<li>Max Steps = 5</li>
<li>Max Certainty Value=1.00</li>
<li>Certainty Anchor=1.00</li>
<li>Min Certainty Value=0.00</li>
</ul>
<p><a href="img_articles/gui_aife_studio_classifier_create_part_3.jpg"><img src="img_articles/gui_aife_studio_classifier_create_part_3.jpg" style="width:100.0%" alt="Figure 20: Classifier - Creation Part 3"></a> Figure 20:
Classifier - Creation Part 3 (click image to enlarge)</p>
<p>In the last box (Figure 20), you have to provide the directory where
you would like to save the model. The name of the folder created within
that directory can be set with <em>Folder Name</em>.</p>
<p>Before you start training, you can check how many cases can be
matched between the text embeddings and the target data by clicking on
the button <em>Test Data Matching</em> (box <em>Model Saving</em>). This
allows you to check if the structure of the data is working. If
everything is okay you can start training the model by clicking on
<em>Start Training</em>. Please note that training a classifier can take
up to several hours.</p>
</div>
<div class="section level4">
<h4 id="using-a-classifier">3.4.2 Using a Classifier<a class="anchor" aria-label="anchor" href="#using-a-classifier"></a>
</h4>
<p>In case you have trained a classifier or are using a classifier
trained by other users, you can analyze the model’s performance or use
the model to classify new texts. To do so, you have to select “Use” in
the tab <em>Classification</em>.</p>
<p>Similar to the other functions of this app, you first have to select
the classifier by providing the folder that contains the entire model.
Please note that a classifier is made up of several files. Thus,
<em>Aifeducation Studio</em> asks you to select the folder containing
these files and not single files. After loading a classifier, a new box
appears.</p>
<p>In the first tab, <em>Model Description</em>, (Figure 21) you will
find the documentation of the model.</p>
<p><a href="img_articles/gui_aife_studio_classifier_use_desc.jpg"><img src="img_articles/gui_aife_studio_classifier_use_desc.jpg" style="width:100.0%" alt="Figure 21: Classifier - Description"></a>
Figure 21: Classifier - Description (click image to enlarge)</p>
<p>In the second tab, <em>Training</em> (Figure 22) you receive a
summary of the training process of the model. This includes a
visualization of the loss, the accuracy, and the balanced accuracy for
every fold and every epoch. Depending on the applied training techniques
(such as <em>Balanced Pseudo-Labeling</em>), you can request additional
images.</p>
<p><a href="img_articles/gui_aife_studio_classifier_use_train.jpg"><img src="img_articles/gui_aife_studio_classifier_use_train.jpg" style="width:100.0%" alt="Figure 22: Classifier - Training"></a>
Figure 22: Classifier - Training (click image to enlarge)</p>
<p>The third tab, <em>Reliability</em>, (Figure 23) provides you with
information on the quality of the model. Here you find visualizations
giving you insights into how the classifier is able to generate reliable
results. In addition, measures from content analysis as well as machine
learning allow you to analyze specific aspects of the model’s
performance.</p>
<p><a href="img_articles/gui_aife_studio_classifier_use_reliability.jpg"><img src="img_articles/gui_aife_studio_classifier_use_reliability.jpg" style="width:100.0%" alt="Figure 23: Classifier - Reliability"></a>
Figure 23: Classifier - Reliability (click image to enlarge)</p>
<p>The last tab, <em>Prediction</em> (Figure 24) allows you to apply a
trained model to new data. Here you can use the trained model to assign
classes/categories to new texts. For this purpose, you must first
provide the file that contains the text embeddings of the documents you
would like to classify. You can create these embeddings with the text
embedding model used for providing the training data of the classifier.
The necessary steps are described in section 3.3.2.</p>
<p><a href="img_articles/gui_aife_studio_classifier_use_predictions.jpg"><img src="img_articles/gui_aife_studio_classifier_use_predictions.jpg" style="width:100.0%" alt="Figure 24: Classifier - Prediction"></a>
Figure 24: Classifier - Prediction (click image to enlarge)</p>
<blockquote>
<p>These embeddings must be created with the same text embedding model
that created the text embeddings for training. If not, an error will
occur. See section 3.4.1 and 3.3.2 for more details.</p>
</blockquote>
<p>The next step is to provide the folder where you would like to save
the predictions and to provide a file name. The default case is to store
the predictions in an .rda file, allowing you to load the data directly
into <em>R</em> for further analysis. However, you can additionally save
the results in a .csv file, allowing you to export the predictions to
other programs. The resulting data table may look like shown in Figure
25.</p>
<p><a href="img_articles/gui_aife_studio_classifier_predictions.jpg"><img src="img_articles/gui_aife_studio_classifier_predictions.jpg" style="width:100.0%" alt="Figure 25: Classifier - Prediction Results"></a> Figure 25:
Classifier - Prediction Results (click image to enlarge)</p>
</div>
<div class="section level4">
<h4 id="documenting-a-classifier">3.4.3 Documenting a Classifier<a class="anchor" aria-label="anchor" href="#documenting-a-classifier"></a>
</h4>
<p>Documenting a classifier is similar to the documentation of a text
embedding model (section 3.3.3, see Figure 18).</p>
<p>To support developers in documenting their work, <em>Aifeducation
Studio</em> provides an easy way to add a comprehensive description. You
find this part of the app by clicking on “Document” in the tab
<em>Classification</em>. First, you have to choose the classifier you
would like to document.</p>
<p>After choosing a model, a new box appears, allowing you to insert the
necessary information. Via the tab <em>Developers,</em> you can provide
the names and email addressees of all relevant contributors.</p>
<p>In the tabs <em>Abstract</em> and <em>Description,</em> you can
provide an abstract and a detailed description of your work in English
and/or in the native language of your classifier (e.g., French, German,
etc.), allowing you to reach a broader audience. In all four tabs you
can provide your documentation in plain text, html, and/or markdown
allowing you to insert tables or to highlight parts of your
documentation. If you would like to see what your documentation will
look like on the internet, you can click on the button “Preview”. Saving
your changes is possible by clicking on <em>Save</em></p>
<blockquote>
<p>For more information on how to document your model, please refer to
the vignette <a href="sharing_and_publishing.html">03 Sharing and Using
Trained AI/Models</a>.</p>
</blockquote>
</div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The
Long-Document Transformer. <a href="https://doi.org/10.48550/arXiv.2004.05150" class="external-link uri">https://doi.org/10.48550/arXiv.2004.05150</a></p>
<p>Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A.,
&amp; Rebmann, K. (2022). Performance and Configuration of Artificial
Intelligence in Educational Settings.: Introducing a New Reliability
Concept Based on Content Analysis. Frontiers in Education, 1–21. <a href="https://doi.org/10.3389/feduc.2022.818365" class="external-link uri">https://doi.org/10.3389/feduc.2022.818365</a></p>
<p>Campesato, O. (2021). Natural Language Processing Fundamentals for
Developers. Mercury Learning &amp; Information. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713</a></p>
<p>Cascante-Bonilla, P., Tan, F., Qi, Y. &amp; Ordonez, V. (2020).
Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised
Learning. <a href="https://doi.org/10.48550/arXiv.2001.06001" class="external-link uri">https://doi.org/10.48550/arXiv.2001.06001</a></p>
<p>Chollet, F., Kalinowski, T., &amp; Allaire, J. J. (2022). Deep
learning with R (Second edition). Manning Publications Co. <a href="https://learning.oreilly.com/library/view/-/9781633439849/?ar" class="external-link uri">https://learning.oreilly.com/library/view/-/9781633439849/?ar</a></p>
<p>Dai, Z., Lai, G., Yang, Y. &amp; Le, Q. V. (2020).
Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing. <a href="https://doi.org/10.48550/arXiv.2006.03236" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03236</a></p>
<p>Devlin, J., Chang, M.‑W., Lee, K., &amp; Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171–4186).
Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423" class="external-link uri">https://doi.org/10.18653/v1/N19-1423</a></p>
<p>He, P., Liu, X., Gao, J. &amp; Chen, W. (2020). DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. <a href="https://doi.org/10.48550/arXiv.2006.03654" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03654</a></p>
<p>Lane, H., Howard, C., &amp; Hapke, H. M. (2019). Natural language
processing in action: Understanding, analyzing, and generating text with
Python. Shelter Island: Manning.</p>
<p>Larusson, J. A., &amp; White, B. (Eds.). (2014). Learning Analytics:
From Research to Practice. New York: Springer. <a href="https://doi.org/10.1007/978-1-4614-3305-7" class="external-link uri">https://doi.org/10.1007/978-1-4614-3305-7</a></p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). RoBERTa: A
Robustly Optimized BERT Pretraining Approach. <a href="https://doi.org/10.48550/arXiv.1907.11692" class="external-link uri">https://doi.org/10.48550/arXiv.1907.11692</a></p>
<p>Schreier, M. (2012). Qualitative Content Analysis in Practice. Los
Angeles: SAGE.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Berding Florian.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>

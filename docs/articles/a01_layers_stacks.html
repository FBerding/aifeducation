<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A01 Layers and Stacks • aifeducation</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="A01 Layers and Stacks">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/aifeducation.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>For users</h6></li>
    <li><a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a></li>
    <li><a class="dropdown-item" href="../articles/gui_aife_studio.html">02 Aifeducation Studio</a></li>
    <li><a class="dropdown-item" href="../articles/classification_tasks.html">03 Using the Package without Studio</a></li>
    <li><a class="dropdown-item" href="../articles/model_configuration.html">04 Model configuration</a></li>
    <li><a class="dropdown-item" href="../articles/sharing_and_publishing.html">05 Sharing and Using Trained AI/Models</a></li>
    <li><a class="dropdown-item" href="../articles/a01_layers_stacks.html">Appendix 01 Layers and Stacks</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>For developers</h6></li>
    <li><a class="dropdown-item" href="../articles/transformers.html">01 Transformers</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>A01 Layers and Stacks</h1>
                        <h4 data-toc-skip class="author">Florian
Berding, Yuliia Tykhonova, Julia Pargmann, Andreas Slopinski, Elisabeth
Riebenbauer, Karin Rebmann</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/a01_layers_stacks.Rmd" class="external-link"><code>vignettes/a01_layers_stacks.Rmd</code></a></small>
      <div class="d-none name"><code>a01_layers_stacks.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/aifeducation/">aifeducation</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="layers">Layers<a class="anchor" aria-label="anchor" href="#layers"></a>
</h2>
<div class="section level3">
<h3 id="transformer-encoder-layers">Transformer Encoder Layers<a class="anchor" aria-label="anchor" href="#transformer-encoder-layers"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_tf_encoder.png" style="width:100.0%" alt="Figure: Transformer Encoder Layers"><div class="figcaption">Figure: Transformer Encoder Layers</div>
</div>
<p><strong>Description</strong></p>
<p>The transformer encoder layers follow the structure of the encoder
layers used in transformer models. A single layer is designed as
described by Chollet, Kalinowski, and Allaire (2022, p. 373) with the
exception that single components of the layers (such as the activation
function, the kind of residual connection, the kind of normalization or
the kind of attention) can be customized. All parameters with the prefix
<em>tf_</em> can be used to configure this layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>tf_residual_type</em>: Type of residual connenction for all
layers and stack of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Add no residual connection.</li>
<li>
<code>'Addition'</code>: Adds a residual connection by adding the
original input to the output.</li>
<li>
<code>'ResidualGate'</code>: Adds a residucal connection by creating
a weightes sum from the original input and the output. The weight is a
learnable parameter. This type of residual connection is described by <a href="https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf" class="external-link">Savarese
and Figueiredo (2017)</a>.</li>
</ul>
</li>
<li>
<p><em>tf_normalization_type</em>: Type of normalization applied to
all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li>
<p><em>tf_parametrizations</em>: Re-Parametrizations of the weights
of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Does not apply any re-parametrizations.</li>
<li>
<code>'OrthogonalWeights'</code>: Applies an orthogonal
re-parametrizations of the weights with PyTorchs implemented function
using orthogonal_map=‘matrix_exp’.</li>
<li>
<code>'WeightNorm'</code>: Applies a weight norm with the default
settings of PyTorch’s corresponding function. Weight norm is described
by <a href="https://doi.org/10.48550/arXiv.1602.07868" class="external-link">Salimans and
Kingma 2016</a>.</li>
<li>
<code>'SpectralNorm'</code>: Applies a spectral norm with the
default settings of PyTorch’s corresponding function. The norm is
described by <a href="https://doi.org/10.48550/arXiv.1802.05957" class="external-link">Miyato
et al. 2018</a>.</li>
</ul>
</li>
<li><p><em>tf_bias</em>: If <code>TRUE</code> a bias term is added to
all layers. If <code>FALSE</code> no bias term is added to the
layers.</p></li>
<li><p><em>tf_act_fct</em>: Activation function for all layers.</p></li>
<li><p><em>tf_dropout_rate_1</em>: determining the dropout after the
attention mechanism within the transformer encoder layers.</p></li>
<li><p><em>tf_dropout_rate_2</em>: determining the dropout for the dense
projection within the transformer encoder layers.</p></li>
<li><p><em>tf_num_heads</em>: determining the number of attention heads
for a self-attention layer. Only relevant if
<code>attention_type='multihead'</code></p></li>
<li><p><em>tf_dense_dim</em>: determining the size of the projection
layer within a each transformer encoder.</p></li>
<li>
<p><em>tf_attention_type</em>: Choose the attention type. Allowed
values:</p>
<ul>
<li>
<code>'multihead'</code>: The original multi-head attention as
described by <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link">Vaswani
et al. (2017)</a>.</li>
<li>
<code>'fourier'</code>: Attention with fourier transformation as
described by <a href="https://doi.org/10.48550/arXiv.2105.03824" class="external-link">Lee-Thorp et
al. (2021)</a>.</li>
</ul>
</li>
<li>
<p><em>tf_positional_type</em>: Type of processing positional
information. Allowed values:</p>
<ul>
<li>
<code>'absolute'</code>: Adds positional information by using an
embedding matrix as described by Chollet, Kalinowski, and Allaire (2022,
pp. 378-379). This implementation is different to the original work by
<a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link">Vaswani et
al. (2017)</a>.</li>
</ul>
</li>
<li><p><em>tf_n_layers</em>: determining how many times the encoder
should be added to the network.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="feature-layer">Feature Layer<a class="anchor" aria-label="anchor" href="#feature-layer"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_features.png" style="width:100.0%" alt="Figure: Feature Layer"><div class="figcaption">Figure: Feature Layer</div>
</div>
<p><strong>Description</strong></p>
<p>The feature layer is a dense layer that can be used to increase or
decrease the number of features of the input data before passing the
data into your model. The aim of this layer is to increase or reduce the
complexity of the data for your model. The output size of this layer
determines the number of features for all following layers. In the
special case that the requested number of features equals the number of
features of the text embeddings this layer is reduced to a dropout layer
with masking capabilities. All parameters with the prefix <em>feat_</em>
can be used to configure this layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>feat_normalization_type</em>: Type of normalization applied
to all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li>
<p><em>feat_parametrizations</em>: Re-Parametrizations of the
weights of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Does not apply any re-parametrizations.</li>
<li>
<code>'OrthogonalWeights'</code>: Applies an orthogonal
re-parametrizations of the weights with PyTorchs implemented function
using orthogonal_map=‘matrix_exp’.</li>
<li>
<code>'WeightNorm'</code>: Applies a weight norm with the default
settings of PyTorch’s corresponding function. Weight norm is described
by <a href="https://doi.org/10.48550/arXiv.1602.07868" class="external-link">Salimans and
Kingma 2016</a>.</li>
<li>
<code>'SpectralNorm'</code>: Applies a spectral norm with the
default settings of PyTorch’s corresponding function. The norm is
described by <a href="https://doi.org/10.48550/arXiv.1802.05957" class="external-link">Miyato
et al. 2018</a>.</li>
</ul>
</li>
<li><p><em>feat_bias</em>: If <code>TRUE</code> a bias term is added to
all layers. If <code>FALSE</code> no bias term is added to the
layers.</p></li>
<li><p><em>feat_act_fct</em>: Activation function for all
layers.</p></li>
<li><p><em>feat_dropout</em>: determining the dropout for the dense
projection of the feature layer.</p></li>
<li><p><em>feat_size</em>: Number of neurons for each dense
layer.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="dense-layers">Dense Layers<a class="anchor" aria-label="anchor" href="#dense-layers"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_dense.png" style="width:100.0%" alt="Figure: Dense Layers"><div class="figcaption">Figure: Dense Layers</div>
</div>
<p><strong>Description</strong></p>
<p>A fully connected layer. The layer is applied to every step of a
sequence. All parameters with the prefix <em>dense_</em> can be used to
configure this layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>dense_residual_type</em>: Type of residual connenction for
all layers and stack of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Add no residual connection.</li>
<li>
<code>'Addition'</code>: Adds a residual connection by adding the
original input to the output.</li>
<li>
<code>'ResidualGate'</code>: Adds a residucal connection by creating
a weightes sum from the original input and the output. The weight is a
learnable parameter. This type of residual connection is described by <a href="https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf" class="external-link">Savarese
and Figueiredo (2017)</a>.</li>
</ul>
</li>
<li>
<p><em>dense_normalization_type</em>: Type of normalization applied
to all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li>
<p><em>dense_parametrizations</em>: Re-Parametrizations of the
weights of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Does not apply any re-parametrizations.</li>
<li>
<code>'OrthogonalWeights'</code>: Applies an orthogonal
re-parametrizations of the weights with PyTorchs implemented function
using orthogonal_map=‘matrix_exp’.</li>
<li>
<code>'WeightNorm'</code>: Applies a weight norm with the default
settings of PyTorch’s corresponding function. Weight norm is described
by <a href="https://doi.org/10.48550/arXiv.1602.07868" class="external-link">Salimans and
Kingma 2016</a>.</li>
<li>
<code>'SpectralNorm'</code>: Applies a spectral norm with the
default settings of PyTorch’s corresponding function. The norm is
described by <a href="https://doi.org/10.48550/arXiv.1802.05957" class="external-link">Miyato
et al. 2018</a>.</li>
</ul>
</li>
<li><p><em>dense_bias</em>: If <code>TRUE</code> a bias term is added to
all layers. If <code>FALSE</code> no bias term is added to the
layers.</p></li>
<li><p><em>dense_act_fct</em>: Activation function for all
layers.</p></li>
<li><p><em>dense_dropout</em>: determining the dropout between dense
layers.</p></li>
<li><p><em>dense_size</em>: Number of neurons for each dense
layer.</p></li>
<li><p><em>dense_layers</em>: Number of dense layers.</p></li>
<li><p><em>dense_n_layers</em>: Number of dense layers.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="multiple-n-gram-layers">Multiple N-Gram Layers<a class="anchor" aria-label="anchor" href="#multiple-n-gram-layers"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_ng_conv.png" style="width:100.0%" alt="Figure: Multiple N-Gram Layers"><div class="figcaption">Figure: Multiple N-Gram Layers</div>
</div>
<p><strong>Description</strong></p>
<p>This type of layer focuses on sub-sequence and performs an 1d
convolutional operation. On a word and token level these sub-sequences
can be interpreted as n-grams (Jacovi, Shalom &amp; Goldberg 2018). The
convolution is done across all features. The number of filters equals
the number of features of the input tensor. Thus, the shape of the
tensor is retained (Pham, Kruszewski &amp; Boleda 2016).</p>
<p>The layer is able to consider multiple n-grams at the same time. In
this case the convolution of the n-grams is done seprately and the
resulting tensors are concatenated along the feature dimension. The
number of filters for every n-gram is set to num_features/num_n-grams.
Thus, the resulting tensor has the same shape as the input tensor.</p>
<p>Sub-sequences that are masked in the input are also masked in the
output.</p>
<p>The output of this layer can be understand as the results of the
n-gram filters. Stacking this layer allows the model to perform n-gram
detection of n-grams (meta perspective). All parameters with the prefix
<em>ng_conv_</em> can be used to configure this layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>ng_conv_residual_type</em>: Type of residual connenction for
all layers and stack of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Add no residual connection.</li>
<li>
<code>'Addition'</code>: Adds a residual connection by adding the
original input to the output.</li>
<li>
<code>'ResidualGate'</code>: Adds a residucal connection by creating
a weightes sum from the original input and the output. The weight is a
learnable parameter. This type of residual connection is described by <a href="https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf" class="external-link">Savarese
and Figueiredo (2017)</a>.</li>
</ul>
</li>
<li>
<p><em>ng_conv_normalization_type</em>: Type of normalization
applied to all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li>
<p><em>ng_conv_parametrizations</em>: Re-Parametrizations of the
weights of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Does not apply any re-parametrizations.</li>
<li>
<code>'OrthogonalWeights'</code>: Applies an orthogonal
re-parametrizations of the weights with PyTorchs implemented function
using orthogonal_map=‘matrix_exp’.</li>
<li>
<code>'WeightNorm'</code>: Applies a weight norm with the default
settings of PyTorch’s corresponding function. Weight norm is described
by <a href="https://doi.org/10.48550/arXiv.1602.07868" class="external-link">Salimans and
Kingma 2016</a>.</li>
<li>
<code>'SpectralNorm'</code>: Applies a spectral norm with the
default settings of PyTorch’s corresponding function. The norm is
described by <a href="https://doi.org/10.48550/arXiv.1802.05957" class="external-link">Miyato
et al. 2018</a>.</li>
</ul>
</li>
<li><p><em>ng_conv_bias</em>: If <code>TRUE</code> a bias term is added
to all layers. If <code>FALSE</code> no bias term is added to the
layers.</p></li>
<li><p><em>ng_conv_act_fct</em>: Activation function for all
layers.</p></li>
<li><p><em>ng_conv_dropout</em>: determining the dropout for n-gram
convolution layers.</p></li>
<li><p><em>ng_conv_n_layers</em>: determining how many times the n-gram
layers should be added to the network.</p></li>
<li><p><em>ng_conv_ks_min</em>: determining the minimal window size for
n-grams.</p></li>
<li><p><em>ng_conv_ks_max</em>: determining the maximal window size for
n-grams.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="recurrent-layers">Recurrent Layers<a class="anchor" aria-label="anchor" href="#recurrent-layers"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="" style="width:100.0%" alt="Figure: Recurrent Layers"><div class="figcaption">Figure: Recurrent Layers</div>
</div>
<p><strong>Description</strong></p>
<p>A regular recurrent layer either as Gated Recurrent Unit (GRU) or
Long Short-Term Memory (LSTM) layer. Uses PyTorchs implementation. All
parameters with the prefix <em>rec_</em> can be used to configure this
layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>rec_residual_type</em>: Type of residual connenction for all
layers and stack of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Add no residual connection.</li>
<li>
<code>'Addition'</code>: Adds a residual connection by adding the
original input to the output.</li>
<li>
<code>'ResidualGate'</code>: Adds a residucal connection by creating
a weightes sum from the original input and the output. The weight is a
learnable parameter. This type of residual connection is described by <a href="https://home.ttic.edu/~savarese/savarese_files/Residual_Gates.pdf" class="external-link">Savarese
and Figueiredo (2017)</a>.</li>
</ul>
</li>
<li>
<p><em>rec_normalization_type</em>: Type of normalization applied to
all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li>
<p><em>rec_parametrizations</em>: Re-Parametrizations of the weights
of layers. Allowed values:</p>
<ul>
<li>
<code>'None'</code>: Does not apply any re-parametrizations.</li>
<li>
<code>'OrthogonalWeights'</code>: Applies an orthogonal
re-parametrizations of the weights with PyTorchs implemented function
using orthogonal_map=‘matrix_exp’.</li>
<li>
<code>'WeightNorm'</code>: Applies a weight norm with the default
settings of PyTorch’s corresponding function. Weight norm is described
by <a href="https://doi.org/10.48550/arXiv.1602.07868" class="external-link">Salimans and
Kingma 2016</a>.</li>
<li>
<code>'SpectralNorm'</code>: Applies a spectral norm with the
default settings of PyTorch’s corresponding function. The norm is
described by <a href="https://doi.org/10.48550/arXiv.1802.05957" class="external-link">Miyato
et al. 2018</a>.</li>
</ul>
</li>
<li><p><em>rec_bias</em>: If <code>TRUE</code> a bias term is added to
all layers. If <code>FALSE</code> no bias term is added to the
layers.</p></li>
<li><p><em>rec_act_fct</em>: Activation function for all
layers.</p></li>
<li><p><em>rec_dropout</em>: determining the dropout between recurrent
layers.</p></li>
<li><p><em>rec_type</em>: Type of the recurrent layers.
<code>rec_type='GRU'</code> for Gated Recurrent Unit and
<code>rec_type='LSTM'</code> for Long Short-Term Memory.</p></li>
<li><p><em>rec_bidirectional</em>: If <code>TRUE</code> a bidirectional
version of the recurrent layers is used.</p></li>
<li><p><em>rec_size</em>: Number of neurons for each recurrent
layer.</p></li>
<li><p><em>rec_layers</em>: Number of recurrent layers.</p></li>
<li><p><em>rec_n_layers</em>: Number of recurrent layers.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="classifiction-pooling-layer">Classifiction Pooling Layer<a class="anchor" aria-label="anchor" href="#classifiction-pooling-layer"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_cls_pooling.png" style="width:100.0%" alt="Figure: Classifiction Pooling Layer"><div class="figcaption">Figure: Classifiction Pooling Layer</div>
</div>
<p><strong>Description</strong></p>
<p>Layer transforms sequences into a lower dimensional space that can be
passed to dense layers. It performs two types of pooling. First, it
extractes features across the time dimension selecting the maximal
and/or minimal features. Second, it performs pooling over the remaining
features selecting a speficifc number of the heighest and/or lowest
features.</p>
<p>In the case of selecting the minmal <em>and</em> maximal features at
the same time the minmal features are concatenated to the tensor of the
maximal features resulting the in the shape
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo>,</mo><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mo>,</mo><mn>2</mn><mo>*</mo><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Batch, Times, 2*Features)</annotation></semantics></math>
at the end of the first step. In the second step the number of requested
features is halved. The first half is used for the maximal features and
the second for the minimal features. All parameters with the prefix
<em>cls_pooling_</em> can be used to configure this layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><p><em>cls_pooling_features</em>: Number of features to be extracted
at the end of the model.</p></li>
<li><p><em>cls_pooling_type</em>: Type of extracting intermediate
features.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="merge-layer">Merge Layer<a class="anchor" aria-label="anchor" href="#merge-layer"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="layers_merge.png" style="width:100.0%" alt="Figure: Merge Layer"><div class="figcaption">Figure: Merge Layer</div>
</div>
<p><strong>Description</strong></p>
<p>Layer for combining the output of different layers. All inputs must
be sequential data of shape (Batch, Times, Features). First, pooling
over time is applied extracting the minimal and/or maximal features.
Second, the pooled tensors are combined by calculating their weighted
sum. Different attention mechanism can be used to dynamically calculate
the corresponding weights. This allows the model to decide which part of
the data is most usefull. Finally, pooling over features is applied
extracting a specific number of maximal and/or minimal features. A
normalization of all input at the begining of the layer is possible. All
parameters with the prefix <em>merge_</em> can be used to configure this
layer.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><em>merge_normalization_type</em>: Type of normalization applied
to all layers and stack layers. Allowed values:</p>
<ul>
<li>
<code>'LayerNorm'</code>: Applies normalization as described by <a href="https://doi.org/10.48550/arXiv.1607.06450" class="external-link">Ba, Kiros, and Hinton
(2016)</a>.</li>
<li>
<code>'None'</code>: Applies no normalization.</li>
</ul>
</li>
<li><p><em>merge_pooling_features</em>: Number of features to be
extracted at the end of the model.</p></li>
<li><p><em>merge_pooling_type</em>: Type of extracting intermediate
features.</p></li>
<li>
<p><em>merge_attention_type</em>: Choose the attention type. Allowed
values:</p>
<ul>
<li>
<code>'multihead'</code>: The original multi-head attention as
described by <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link">Vaswani
et al. (2017)</a>.</li>
<li>
<code>'fourier'</code>: Attention with fourier transformation as
described by <a href="https://doi.org/10.48550/arXiv.2105.03824" class="external-link">Lee-Thorp et
al. (2021)</a>.</li>
</ul>
</li>
<li><p><em>merge_num_heads</em>: determining the number of attention
heads for a self-attention layer. Only relevant if
<code>attention_type='multihead'</code></p></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="core-models">Core Models<a class="anchor" aria-label="anchor" href="#core-models"></a>
</h2>
<div class="section level3">
<h3 id="sequential-core-architecture">Sequential Core Architecture<a class="anchor" aria-label="anchor" href="#sequential-core-architecture"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="core_arch_sequential.png" style="width:100.0%" alt="Sequential Core Architecture"><div class="figcaption">Sequential Core Architecture</div>
</div>
<p>This model is based on a sequential architecture. The input is passed
to a specific number of layers step by step. All layers are grouped by
their kind into stacks.</p>
</div>
<div class="section level3">
<h3 id="parallel-core-architecture">Parallel Core Architecture<a class="anchor" aria-label="anchor" href="#parallel-core-architecture"></a>
</h3>
<p><strong>Visualization</strong></p>
<div class="float">
<img src="core_arch_parallel.png" style="width:100.0%" alt="Parallel Core Architecture"><div class="figcaption">Parallel Core Architecture</div>
</div>
<p>This model is based on a parallel architecture. An input is passed to
different types of layers separately. At the end the outputs are
combined to create the final output of the whole model.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Berding Florian, Tykhonova Yuliia.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>

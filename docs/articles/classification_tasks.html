<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="aifeducation">
<title>02b Text Embedding and Classification Tasks • aifeducation</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="02b Text Embedding and Classification Tasks">
<meta property="og:description" content="aifeducation">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.3</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/aifeducation.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a>
    <a class="dropdown-item" href="../articles/gui_aife_studio.html">02a Aifeducation Studio</a>
    <a class="dropdown-item" href="../articles/classification_tasks.html">02b Classification tasks</a>
    <a class="dropdown-item" href="../articles/sharing_and_publishing.html">03 Sharing and Using Trained AI/Models</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>02b Text Embedding and Classification Tasks</h1>
                        <h4 data-toc-skip class="author">Florian
Berding, Julia Pargmann, Andreas Slopinski, Elisabeth Riebenbauer, Karin
Rebmann</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/classification_tasks.Rmd" class="external-link"><code>vignettes/classification_tasks.Rmd</code></a></small>
      <div class="d-none name"><code>classification_tasks.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-and-overview">1 Introduction and Overview<a class="anchor" aria-label="anchor" href="#introduction-and-overview"></a>
</h2>
<p>In the educational and social sciences, the assignment of an
observation to scientific concepts is an important task that allows
researchers to understand an observation, to generate new insights, and
to derive recommendations for research and practice.</p>
<p>In educational science, several areas deal with this kind of task.
For example, diagnosing students’ characteristics is an important aspect
of a teachers’ profession and necessary to understand and promote
learning. Another example is the use of learning analytics, where data
about students is used to provide learning environments adapted to their
individual needs. On another level, educational institutions such as
schools and universities can use this information for data-driven
performance decisions (Laurusson &amp; White 2014) as well as where and
how to improve it. In any case, a real-world observation is aligned to
scientific models to use scientific knowledge as a technology for
improved learning and instruction.</p>
<p>Supervised machine learning is one concept that allows a link between
real-world observations and existing scientific models and theories
(Berding et al. 2022). For educational sciences this is a great
advantage because it allows researchers to use the existing knowledge
and insights for applications of AI. The drawback of this approach is
that the training of AI requires both information about the real world
observations and information on the corresponding alignment with
scientific models and theories.</p>
<p>A valuable source of data in educational science are written texts,
since textual data can be found almost everywhere in the realm of
learning and teaching (Berding et al. 2022). For example, teachers often
require students to solve a task which they provide in a written form.
Students have to create a solution for the tasks which they often
document with a short-written essay or a presentation. This data can be
used to analyze learning and teaching. Teachers’ written tasks for their
students may provide insights into the quality of instruction while
students’ solutions may provide insights into their learning outcomes
and prerequisites.</p>
<p>AI can be a helpful assistant in analyzing textual data since the
analysis of textual data is a challenging and time-consuming task for
humans. In this vignette, we would like to show how to create an AI that
can help you with such tasks by using the package
<em>aifedcuation</em>.</p>
<blockquote>
<p>Please note that an introduction to content analysis, natural
language processing or machine learning is beyond the scope of this
vignette. If you would like to learn more, please refer to the cited
literature.</p>
</blockquote>
<p>Before we start it is necessary to introduce a definition of our
understanding of some basic concepts since applying AI to educational
contexts means to combine the knowledge of different scientific
disciplines using different, sometimes overlapping concepts. Even within
a research area, concepts are not unified. Figure 1 illustrates this
package’s understanding.</p>
<div class="float">
<img src="img_articles/classif_fig_01.png" style="width:100.0%" alt="Figure 1: Understanding of Central Concepts"><div class="figcaption">Figure 1: Understanding of Central
Concepts</div>
</div>
<p>Since <em>aifeducation</em> looks at the application of AI for
classification tasks from the perspective of the empirical method of
content analysis, there is some overlapping between the concepts of
content analysis and machine learning. In content analysis, a phenomenon
like performance and colors can be described as a scale/dimension which
is made up by several categories (e.g. Schreier 2012 pp. 59). In our
example, an exam’s performance (scale/dimension) could be “good”,
“average” or “poor”. In terms of colors (scale/dimension) categories
could be “blue”, “green”, etc. Machine learning literature uses other
words to describe this kind of data. In machine learning, “scale” and
“dimension” correspond to the term “label” while “categories” refer to
the term “classes” (Chollet, Kalinowski &amp; Allaire 2022, p. 114).</p>
<p>With these clarifications, classification means that a text is
assigned to the correct category of a scale or that the text is labeled
with the correct class. As Figure 2 illustrates, two kinds of data are
necessary to train an AI to classify text in line with supervised
machine learning principles.</p>
<div class="float">
<img src="img_articles/classif_fig_02.png" style="width:100.0%" alt="Figure 2: Basic Structure of Supervised Machine Learning"><div class="figcaption">Figure 2: Basic Structure of Supervised Machine
Learning</div>
</div>
<p>By providing AI with both the textual data as input data and the
corresponding information about the class as target data, AI can learn
which texts imply a specific class or category. In the above exam
example, AI can learn which texts imply a “good”, an “average” or a
“poor” judgment. After training, AI can be applied to new texts and
predict the most likely class of every new text. The generated class can
be used for further statistical analysis or to derive recommendations
about learning and teaching.</p>
<p>To achieve this support by an artificial intelligence, several steps
are necessary. Figure 3 provides an overview integrating the functions
and objects of <em>aifeducation</em>.</p>
<div class="float">
<img src="img_articles/classif_fig_03.png" style="width:100.0%" alt="Figure 3: Overview of the Steps to Perform a Classification"><div class="figcaption">Figure 3: Overview of the Steps to Perform a
Classification</div>
</div>
<p>The first step is to transform raw texts into a form computers can
use. That is, the raw texts must be transformed into numbers. In modern
approaches, this is usually done through word embeddings. Campesato
(2021, p. 102) describes them as “the collective name for a set of
language modeling and feature learning techniques (…) where words or
phrases from the vocabulary are mapped to vectors of real numbers.” The
definition of a word vector is similar: „Word vectors represent the
semantic meaning of words as vectors in the context of the training
corpus.” (Lane, Howard &amp; Hapke 2019, p. 191)</p>
<p>Campesato (2021, pp. 112) clusters approaches for creating word
embeddings into three groups, reflecting their ability to provide
context-sensitive numerical representations. Approaches in group one do
not account for any context. Typical methods rely on bag-of-words
assumptions. Thus, they are normally not able to provide a word
embedding for single words. Group two consists of approaches such as
word2vec, GloVe (Pennington, Socher &amp; Manning 2014) or fastText,
which are able to provide one embedding for each word regardless of its
context. Thus, they only account for one context. The last group
consists of approaches such as BERT (Devlin et al. 2019), which are able
to produce multiple word embeddings depending on the context of the
words.</p>
<p>From these different groups, <em>aifedcuation</em> implements several
methods.</p>
<ul>
<li>
<strong>Topic Modeling:</strong> Topic modeling is an approach that
uses frequencies of tokens within a text. The frequencies of the tokens
are models as the observable variables of one more latent topic
(Campesato 2021, p. 113). The estimation of a topic model is often based
on a Latent Dirichlet Analysis (LDA) which describes each text by a
distribution of topics. The topics themselves are described by a
distribution of words/tokens (Campesato 2021, p. 114). This relationship
between texts, words, and topics can be used to create a text embedding
by computing the relative amount of every topic in a text based on every
token in a text.</li>
<li>
<strong>GlobalVectorClusters:</strong> GlobalVectors is a newer
approach which utilizes the co-occurrence of words/tokens to compute
GlobalVectors (Campesato 2021, p. 110). These vectors are generated in a
way that tokens/words with a similar meaning are located close to each
other (Pennington, Socher &amp; Manning 2014). In order to create a
<em>text</em> embedding from <em>word</em> embeddings,
<em>aifeducation</em> groups tokens into clusters based on their
vectors. Thus, tokens with a similar meaning are members of the same
cluster. For the <em>text</em> embedding, the tokens of a text are
counted for every cluster and the frequencies of every cluster for that
text are used as a numerical representation of that text.</li>
<li>
<strong>Transformers:</strong> Transformers are the current
state-of-the-art approach for many natural language tasks (Tunstall, von
Werra &amp; Wolf 2022, p. xv). With the help of the self-attention
mechanism (Vaswani et al. 2017), they are able to produce
context-sensitive <em>word</em> embeddings (Chollet, Kalinowski &amp;
Allaire, 2022, pp. 366).</li>
</ul>
<p>All the approaches are managed and used with a unified interface
provided by the object <code>TextEmbeddingModel</code>. With this object
you can easily convert raw texts into a numerical representation, which
you can use for different classification tasks at the same time. This
makes it possible to reduce computational time. The created text
embedding is stored in an object of class <code>EmbeddedText</code>.
This object additionally contains information about the text embedding
model that created this object.</p>
<p>In the very best case you can apply an existing text embedding model
by using a transformer from <a href="https://huggingface.co/" class="external-link">Huggingface</a> or by using a model from
colleagues. If not, <em>aifeducation</em> provides several functions
allowing you to create your own models. Depending on the approach you
would like to use, different steps are necessary. In the case of Topic
Modeling or GlobalVectorClusters, you must first create a draft of a
vocabulary with the two functions
<code><a href="../reference/bow_pp_create_vocab_draft.html">bow_pp_create_vocab_draft()</a></code> and
<code><a href="../reference/bow_pp_create_basic_text_rep.html">bow_pp_create_basic_text_rep()</a></code>. When calling these
functions, you determine central properties of the resulting model. In
the case of transformers, you first have to configure and to train a
vocabulary with <code>create_xxx_model()</code> and in a next step you
can train your model with <code>train_tune_xxx_model()</code>. Every
step will be explained in the next chapters. Please note that
<code>xxx</code> stands for different architectures of transformers that
are supported with <em>aifedcuation</em>.</p>
<p>With an object of class <code>TextEmbeddingModel</code> you can
create the input data for the supervised machine learning process.
Additionally, you need the target data which must be a named factor
containing the classes/categories of each text.</p>
<p>With both kinds of data, you are able to create a new object of class
<code>TextEmbeddingClassifierNeuralNet</code> which is the classifier.
To train the classifier you have several options which we will cover in
detail in chapter 3. After training the classifier you can share it with
other researchers and apply it to new texts. Please note that the
application to new texts requires the text to be transformed into
numbers with <em>exactly the same text embedding model</em> before
passing the text to the classifier. Please note: Do not pass the raw
texts to the classifier, only embedded texts work!</p>
<p>In the next chapters, we will guide you through the complete process,
starting with the creation of the text embedding models.</p>
<p><strong>Please note that the creation of a new text embedding model
is only necessary if you cannot rely on an existing model or if you
cannot rely on a pre-trained transformer.</strong></p>
</div>
<div class="section level2">
<h2 id="start-working">2 Start Working<a class="anchor" aria-label="anchor" href="#start-working"></a>
</h2>
</div>
<div class="section level2">
<h2 id="starting-a-new-session">2.1 Starting a New Session<a class="anchor" aria-label="anchor" href="#starting-a-new-session"></a>
</h2>
<p>Before you can work with <em>aifeducation</em> you must set up a new
<em>R</em> session. First, it is necessary that you load the library.
Second, you must set up python via reticulate. In case you installed
python as suggested in vignette <a href="aifeducation.html">01 Get
started</a> you may start a new session like this:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/use_python.html" class="external-link">use_condaenv</a></span><span class="op">(</span>condaenv <span class="op">=</span> <span class="st">"aifeducation"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/aifeducation/" class="external-link">aifeducation</a></span><span class="op">)</span></span></code></pre></div>
<p>Next you have to choose the machine learning framework you would like
to use. You can set the framework for the complete session with</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#For tensorflow</span></span>
<span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">set_global_ml_backend</span><span class="op">(</span><span class="st">"tensorflow"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/set_transformers_logger.html">set_transformers_logger</a></span><span class="op">(</span><span class="st">"ERROR"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#For PyTorch</span></span>
<span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">set_global_ml_backend</span><span class="op">(</span><span class="st">"pytorch"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/set_transformers_logger.html">set_transformers_logger</a></span><span class="op">(</span><span class="st">"ERROR"</span><span class="op">)</span></span></code></pre></div>
<p>Setting the global machine learning framework is only for
convenience. You can change the framework at any time during a session
by calling this method again or by setting the argument ‘ml_framework’
of methods and functions manually.</p>
<p>In the case that you would like to use tensorflow now is a good time
to configure that backend, since some configurations can only be done
<strong>before</strong> tensorflow is used the first time.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#if you would like to use only cpus</span></span>
<span><span class="fu"><a href="../reference/set_config_cpu_only.html">set_config_cpu_only</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#if you have a graphic device with low memory</span></span>
<span><span class="fu"><a href="../reference/set_config_gpu_low_memory.html">set_config_gpu_low_memory</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#if you would like to reduce the tensorflow output to errors</span></span>
<span><span class="fu"><a href="../reference/set_config_os_environ_logger.html">set_config_os_environ_logger</a></span><span class="op">(</span>level <span class="op">=</span> <span class="st">"ERROR"</span><span class="op">)</span></span></code></pre></div>
<blockquote>
<p><strong>Note:</strong> Please remember: Every time you start a new
session in <em>R</em> you have to to set the correct conda environment,
to load the library <em>aifeducation</em>, and to chose your machine
learning framework.</p>
</blockquote>
</div>
<div class="section level2">
<h2 id="reading-texts-into-r">2.2 Reading Texts into <em>R</em><a class="anchor" aria-label="anchor" href="#reading-texts-into-r"></a>
</h2>
<p>For most applications of <em>aifeducation</em> it’s necessary to read
the text you would like to use into <em>R</em>. For this task, several
packages are available on CRAN. Our experience has been good with the
package <a href="https://cran.r-project.org/package=readtext" class="external-link">readtext</a> since it
allows you to process different kind of sources for textual data. Please
refer to readtext’s documentation for more details. If you have not
installed this package on your machine, you can request it by</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"readtext"</span><span class="op">)</span></span></code></pre></div>
<p>For example, if you have stored your texts in an excel sheet with two
columns (<em>texts</em> for the texts and <em>id</em> for the texts’ id)
you can read the data by</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#for excel files</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"text_data.xlsx"</span>,</span>
<span>  text_field <span class="op">=</span> <span class="st">"texts"</span>,</span>
<span>  docid_field <span class="op">=</span> <span class="st">"id"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Here it is crucial that you pass the file path to <code>file</code>
and the name of the column for the texts to <code>text_field</code> and
the name of the column for the id to <code>docid_field</code>.</p>
<p>In other cases you may have stored each text in a separate file
(e.g., .txt or .pdf). For these cases you can pass the directory of the
files and read the data. In the following example the files are stored
in the directory “data”.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#read all files with the extension .txt in the directory data</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"data/*.txt"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#read all files with the extension .pdf in the directory data</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"data/*.pdf"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>If you read texts for sever files you do not need to specify the
arguments <code>docid_field</code> and <code>text_field</code>. The id
of the texts is automatically set to the file names.</p>
<p>After the text is read we recommend to do some text cleaning.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#remove multiple spaces and new lines</span></span>
<span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span><span class="op">=</span><span class="fu">stringr</span><span class="fu">::</span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_replace.html" class="external-link">str_replace_all</a></span><span class="op">(</span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span>,pattern <span class="op">=</span> <span class="st">"[:space:]{1,}"</span>,replacement <span class="op">=</span> <span class="st">" "</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#remove hyphenation</span></span>
<span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span><span class="op">=</span><span class="fu">stringr</span><span class="fu">::</span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_replace.html" class="external-link">str_replace_all</a></span><span class="op">(</span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span>,pattern <span class="op">=</span> <span class="st">"-(?=[:space:])"</span>,replacement <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span></code></pre></div>
<p>Please refer to the documentation of the function readtext within the
readtext library for more information.</p>
<p>Now everything is ready to start the preparation tasks.</p>
</div>
<div class="section level2">
<h2 id="preparation-tasks">3 Preparation Tasks<a class="anchor" aria-label="anchor" href="#preparation-tasks"></a>
</h2>
<div class="section level3">
<h3 id="example-data-for-this-vignette">3.1 Example Data for this Vignette<a class="anchor" aria-label="anchor" href="#example-data-for-this-vignette"></a>
</h3>
<p>To illustrate the steps in this vignette, we cannot use data from
educational settings since these data is generally protected by privacy
policies. Therefore, we use the data set
<code>data_corpus_moviereviews</code> from the package
quanteda.textmodels to illustrate the usage of this package. This
quanteda.textmodels is automatically installed when you install
<em>aifeducation</em>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  id<span class="op">=</span><span class="fu">quanteda</span><span class="fu">::</span><span class="fu"><a href="https://quanteda.io/reference/docvars.html" class="external-link">docvars</a></span><span class="op">(</span><span class="fu">quanteda.textmodels</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.textmodels/man/data_corpus_moviereviews.html" class="external-link">data_corpus_moviereviews</a></span><span class="op">)</span><span class="op">$</span><span class="va">id2</span>,</span>
<span>  label<span class="op">=</span><span class="fu">quanteda</span><span class="fu">::</span><span class="fu"><a href="https://quanteda.io/reference/docvars.html" class="external-link">docvars</a></span><span class="op">(</span><span class="fu">quanteda.textmodels</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.textmodels/man/data_corpus_moviereviews.html" class="external-link">data_corpus_moviereviews</a></span><span class="op">)</span><span class="op">$</span><span class="va">sentiment</span><span class="op">)</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="fu">quanteda.textmodels</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.textmodels/man/data_corpus_moviereviews.html" class="external-link">data_corpus_moviereviews</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  neg  pos </span></span>
<span><span class="co">#&gt; 1000 1000</span></span></code></pre></div>
<p>We now have a data set with three columns. The first contains the ID
of the movie review, the second contains the rating of the movie
(positive or negative), and the third column contains the raw texts. As
you can see, the data is balanced. About 1,000 reviews imply a positive
rating of a movie and about 1,000 imply a negative rating.</p>
<p>For this tutorial, we modify this data set by setting about half of
the negative and positive reviews to <code>NA</code>, indicating that
these reviews are not labeled.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">500</span>,<span class="fl">1001</span><span class="op">:</span><span class="fl">1500</span><span class="op">)</span><span class="op">]</span><span class="op">=</span><span class="cn">NA</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="co">#&gt;  neg  pos NA's </span></span>
<span><span class="co">#&gt;  500  500 1000</span></span></code></pre></div>
<p>Furthermore, we will bring some imbalance by setting 250 positive
reviews to <code>NA</code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fl">1501</span><span class="op">:</span><span class="fl">1750</span><span class="op">]</span><span class="op">=</span><span class="cn">NA</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="co">#&gt;  neg  pos NA's </span></span>
<span><span class="co">#&gt;  500  250 1250</span></span></code></pre></div>
<p>We will now use this data to show you how to use the different
objects and functions in <em>aifeducation</em>.</p>
</div>
<div class="section level3">
<h3 id="topic-modeling-and-globalvectorclusters">3.2 Topic Modeling and GlobalVectorClusters<a class="anchor" aria-label="anchor" href="#topic-modeling-and-globalvectorclusters"></a>
</h3>
<p>If you would like to create a new text embedding model with Topic
Modeling or GlobalVectorClusters, you first have to create a draft of a
vocabulary. You can do this by calling the function
<code><a href="../reference/bow_pp_create_vocab_draft.html">bow_pp_create_vocab_draft()</a></code>. The main input of this
function is a vector of texts. The function’s aims are</p>
<ul>
<li>to create a list of all tokens of the texts,</li>
<li>to reduce the tokens to tokens that carry semantic meaning,</li>
<li>to provide the lemma of every token.</li>
</ul>
<p>Since Topic Modeling depends on a bag-of-word approach, the reason
for this pre-process step is to reduce the tokens to tokens that really
carry semantic meaning. In general, these are tokens of words that are
either nouns, verbs or adjectives (Papilloud &amp; Hinneburg 2018,
p. 32). With our example data, an application of that function could
be:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vocab_draft</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/bow_pp_create_vocab_draft.html">bow_pp_create_vocab_draft</a></span><span class="op">(</span></span>
<span>  path_language_model<span class="op">=</span><span class="st">"language_model/english-gum-ud-2.5-191206.udpipe"</span>,</span>
<span>  data<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  upos<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"NOUN"</span>, <span class="st">"ADJ"</span>,<span class="st">"VERB"</span><span class="op">)</span>,</span>
<span>  label_language_model<span class="op">=</span><span class="st">"english-gum-ud-2.5-191206"</span>,</span>
<span>  language<span class="op">=</span><span class="st">"english"</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>As you can see, there is an additional parameter:
<code>path_language_model</code>. Here you must insert the path to an
udpipe pre-trained language model since this function uses the
<em>udpipe</em> package for part-of-speech tagging and lemmataziation. A
collection of pre-trained models for about 65 languages can be found
here [<a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131" class="external-link uri">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131</a>].
Just download the relevant model to your machine and provide the path to
the model.</p>
<p>With the parameter <code>upos</code> you can select which tokens
should be selected. In this example, only tokens that represent a noun,
an adjective or a verb will remain after the analysis. A list of
possible tags can be found here: [<a href="https://universaldependencies.org/u/pos/index.html" class="external-link uri">https://universaldependencies.org/u/pos/index.html</a>].</p>
<p>Please do not forget do provide a label for the udpipe model you use
and please also provide the language you are analyzing. This information
is important since this will be transferred to the text embedding model.
Other researchers/users will need this information to decide if this
model could help with their own work.</p>
<p>In the next step, we can use our draft of a vocabulary to create a
basic text representation with the function
<code><a href="../reference/bow_pp_create_basic_text_rep.html">bow_pp_create_basic_text_rep()</a></code>. This function takes raw
texts and the draft of a vocabulary as main input. The function aims</p>
<ul>
<li>to remove tokens referring to stopwords,</li>
<li>to clean the data (e.g., removing punctuation, numbers),</li>
<li>to lower case all tokens if requested,</li>
<li>to remove tokens with a specific minimal frequency,</li>
<li>to remove tokens that occur in too few or too many documents</li>
<li>to create a document-feature-matrix (dfm),</li>
<li>to create a feature-co-occurrence-matrix (fcm).</li>
</ul>
<p>Applied to the example, the call of the function could look like
this:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">basic_text_rep</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/bow_pp_create_basic_text_rep.html">bow_pp_create_basic_text_rep</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  vocab_draft <span class="op">=</span> <span class="va">vocab_draft</span>,</span>
<span>  remove_punct <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  remove_symbols <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  remove_numbers <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  remove_url <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  remove_separators <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  split_hyphens <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  split_tags <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  language_stopwords<span class="op">=</span><span class="st">"en"</span>,</span>
<span>  use_lemmata <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  to_lower<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  min_termfreq <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  min_docfreq<span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  max_docfreq<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  window <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  weights <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><code>data</code> takes the raw texts while <code>vocab_draft</code>
takes the draft of a vocabulary we created in the first step.</p>
<p>The main goal is to create a document-feature-matrix(dfm) and a
feature-co- occurrence-matrix (fcm). The dfm is a matrix that reports
the texts in the rows and the number of tokens in the columns. This
matrix is later used to create a text embedding model based on topic
modeling. The dfm is reduced to tokens that correspond to the
part-of-speech tags of the vocabulary draft. Punctuation, symbols,
numbers etc. are removed from this matrix if you set the corresponding
parameter to <code>TRUE</code>. If you set
<code>use_lemmata = TRUE</code> you can reduce the dimensionality of
this matrix further by using the lemmas instead of the tokens (Papilloud
&amp; Hinneburg 2018, p.33). If you set <code>to_lower = TRUE</code> all
tokens are transformed to lower case. At the end you get a matrix that
tries to represent the semantic meaning of the text with the smallest
possible number of tokens.</p>
<p>The same applies for the fcm. Here, the tokens/features are reduced
in the same way. However, before the features are reduced, the token’s
co-occurrence is calculated. For this aim a window is used and shifted
across the text, counting the tokens left and right from the token under
investigation. The size of this window can be determined with
<code>window</code>. With <code>weights</code> you can provide weights
for counting. For example, the tokens which are far away from the token
under investigation count less than tokens that are closer to the token
under investigation. The fcm is later used to create a text embedding
model based on GlobalVectorClusters.</p>
<p>As you may notice, the dfm only counts the words in a text. Thus,
their position in the text or within a sentence does not matter. If you
further lower-case tokens or use lemmas, more syntactic information is
lost for the advantage that the dfm has a lower dimensionality while
losing only little semantic meaning. In contrast, the fcm is a matrix
that describes how often different tokens occur together. Thus, an fcm
recovers part of the position of words in a sentence and in a text.</p>
<p>Now, everything is ready to create a new text embedding model based
on Topic Modeling or GlobalVectorClusters. Before we show you how to
create the new model, we will have a look on the preparation of a new
transformer.</p>
</div>
<div class="section level3">
<h3 id="creating-a-new-transformer">3.3 Creating a New Transformer<a class="anchor" aria-label="anchor" href="#creating-a-new-transformer"></a>
</h3>
<p>In general, it is recommended to use a pre-trained model since the
creation of a new transformer requires a large data set of texts and is
computationally intensive. In this vignette we will illustrate the
process with a BERT model. However, for many other transformers, the
process is the same.</p>
<p>The creation of a new transformer requires at least two steps. First,
you must decide about the architecture of your transformer. This
includes the creation of a set of vocabulary. In <em>aifedcuation</em>
you can do this by calling the function
<code><a href="../reference/create_bert_model.html">create_bert_model()</a></code>. For our example this could look like
this:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/create_bert_model.html">create_bert_model</a></span><span class="op">(</span></span>
<span>    ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>    model_dir <span class="op">=</span> <span class="st">"my_own_transformer"</span>,</span>
<span>    vocab_raw_texts<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>    vocab_size<span class="op">=</span><span class="fl">30522</span>,</span>
<span>    vocab_do_lower_case<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>    max_position_embeddings<span class="op">=</span><span class="fl">512</span>,</span>
<span>    hidden_size<span class="op">=</span><span class="fl">768</span>,</span>
<span>    num_hidden_layer<span class="op">=</span><span class="fl">12</span>,</span>
<span>    num_attention_heads<span class="op">=</span><span class="fl">12</span>,</span>
<span>    intermediate_size<span class="op">=</span><span class="fl">3072</span>,</span>
<span>    hidden_act<span class="op">=</span><span class="st">"gelu"</span>,</span>
<span>    hidden_dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>    sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>    sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>    sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>    sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>    trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>First, the function receives the machine learning framework you chose
at the start of the session. However, you can change this by setting
<code>ml_framework="tensorflow"</code> or by
<code>ml_framework="pytorch"</code>.</p>
<p>For this function to work, you must provide a path to a directory
where your new transformer should be saved. Furthermore, you must
provide raw texts. These texts are <strong>not</strong> used for
training the transformer but for training the vocabulary. The maximum
size of the vocabulary is determined by <code>vocab_size</code>. Please
do not provide a size above 50,000 to 60,000 since this kind of
vocabulary works differently than the approaches described in section
2.2. Modern tokenizers such as <em>WordPiece</em> (Wu et al. 2016) use
algorithms that splits tokens into smaller elements, allowing them to
build a huge number of words with a small number of elements. Thus, even
with only small number of about 30,000 tokens, they are able to
represent a very large number of words. As a consequence, these kinds of
vocabularies are many times smaller than the vocabularies build in
section 2.2.</p>
<p>The other parameters allow you to customize your BERT model. For
example, you could increase the number of hidden layers from 12 to 24 or
reduce the hidden size from 768 to 256, allowing you to build and to
test larger or smaller transformers.</p>
<p>Please note that with <code>max_position_embeddings</code> you
determine how many tokens your transformer can process. If your text has
more tokens <em>after</em> tokenization, these tokens are ignored.
However, if you would like to analyze long documents, please avoid to
increase this number too significantly because the computational time
does not increase in a linear way but quadratic (Beltagy, Peters &amp;
Cohan 2020). For long documents you can use another architecture of BERT
(e.g. Longformer from Beltagy, Peters &amp; Cohan 2020) or split a long
document into several chunks which are used sequentially for
classification (e.g., Pappagari et al. 2019). Using chunks is supported
by <em>aifedcuation</em>.</p>
<p>Since creating a transformer model is energy consuming
<em>aifeducation</em> allows you to estimate its ecological impact with
help of the python library <code>codecarbon</code>. Thus,
<code>sustain_track</code> is set to <code>TRUE</code> by default. If
you use the sustainability tracker you must provide the alpha-3 code for
the country where your computer is located (e.g., “CAN”=“Canada”,
“Deu”=“Germany”). A list with the codes can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For USA and Canada you can additionally specify a region
by setting <code>sustain_region</code>. Please refer to the
documentation of codecarbon for more information.</p>
<p>After calling the function, you will find your new model in your
model directory. The next step is to train your model by calling
<code><a href="../reference/train_tune_bert_model.html">train_tune_bert_model()</a></code>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/train_tune_bert_model.html">train_tune_bert_model</a></span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"my_own_transformer_trained"</span>,</span>
<span>  model_dir_path <span class="op">=</span> <span class="st">"my_own_transformer"</span>,</span>
<span>  raw_texts <span class="op">=</span> <span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  p_mask<span class="op">=</span><span class="fl">0.15</span>,</span>
<span>  whole_word<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  val_size<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  n_epoch<span class="op">=</span><span class="fl">1</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">12</span>,</span>
<span>  chunk_size<span class="op">=</span><span class="fl">250</span>,</span>
<span>  n_workers<span class="op">=</span><span class="fl">1</span>,</span>
<span>  multi_process<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>  sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>Here it is important that you provide the path to the directory where
your new transformer is stored. Furthermore, it is important that you
provide <em>another</em> directory where your trained transformer should
be saved to avoid reading and writing collisions.</p>
<p>Now, the provided raw data is used to train your model by using
Masked Language Modeling. First, you can set the length of token
sequences with <code>chunk_size</code>. With <code>whole_word</code> you
can choose between masking single tokens or masking complete words
(Please remember that modern tokenizers split words into several tokens.
Thus, tokens and words are not forced to match each other directly).
With <code>p_mask</code> you can determine how many tokens should be
masked. Finally, with <code>val_size</code>, you set how many chunks
should be used for the validation sample.</p>
<p>Please remember to set the correct alpha-3 code for tracking the
ecological impact of training your model
(<code>sustain_iso_code</code>).</p>
<p>If you work on a machine and your graphic device only has small
memory, please reduce the batch size significantly. We also recommend to
change the usage of memory with <code><a href="../reference/set_config_gpu_low_memory.html">set_config_gpu_low_memory()</a></code>
at the beginning of the session.</p>
<p>After the training finishes, you can find the transformer ready to
use in your output_directory. Now you are able to create a text
embedding model.</p>
<p>Again you can change the machine learning framework by setting
<code>ml_framework="tensorflow"</code> or by
<code>ml_framework="pytorch"</code>. If you do not change this argument
the framework you chose at the beginning is used.</p>
</div>
</div>
<div class="section level2">
<h2 id="text-embedding">4 Text Embedding<a class="anchor" aria-label="anchor" href="#text-embedding"></a>
</h2>
<div class="section level3">
<h3 id="introduction">4.1 Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>In <em>aifedcuation</em>, a text embedding model is stored as an
object of the class <code>TextEmbeddingModel</code>. This object
contains all relevant information for transforming raw texts into a
numeric representation that can be used for machine learning.</p>
<p>In <em>aifedcuation</em>, the transformation of raw texts into
numbers is a separate step from downstream tasks such as classification.
This is to reduce computational time on machines with low performance.
By separating text embedding from other tasks, the text embedding has to
be calculated only once and can be used for different tasks at the same
time. Another advantage is that the training of the downstream tasks
involves only the downstream tasks an not the parameters of the
embedding model, making training less time-consuming, thus decreasing
computational intensity. Finally, this approach allows the analysis of
long documents by applying the same algorithm to different parts.</p>
<p>The text embedding model provides a unified interface: After creating
the model with different methods, the handling of the model is always
the same.</p>
<p>In the following we will show you how to use this object. We start
with Topic Modeling.</p>
</div>
<div class="section level3">
<h3 id="creating-text-embedding-models">4.2 Creating Text Embedding Models<a class="anchor" aria-label="anchor" href="#creating-text-embedding-models"></a>
</h3>
<div class="section level4">
<h4 id="topic-modeling">4.2.1 Topic Modeling<a class="anchor" aria-label="anchor" href="#topic-modeling"></a>
</h4>
<p>For creating a new text embedding model based on Topic Modeling, you
only need a basic text representation generated with the function
<code><a href="../reference/bow_pp_create_basic_text_rep.html">bow_pp_create_basic_text_rep()</a></code> (see section 2.2). Now you
can create a new instance of a text embedding model by calling
<code>TextEmbeddingModel$new()</code>.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_modeling</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TextEmbeddingModel.html">TextEmbeddingModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  model_name<span class="op">=</span><span class="st">"topic_model_embedding"</span>,</span>
<span>  model_label<span class="op">=</span><span class="st">"Text Embedding via Topic Modeling"</span>,</span>
<span>  model_version<span class="op">=</span><span class="st">"0.0.1"</span>,</span>
<span>  model_language<span class="op">=</span><span class="st">"english"</span>,</span>
<span>  method<span class="op">=</span><span class="st">"lda"</span>,</span>
<span>  bow_basic_text_rep<span class="op">=</span><span class="va">basic_text_rep</span>,</span>
<span>  bow_n_dim<span class="op">=</span><span class="fl">12</span>,</span>
<span>  bow_max_iter<span class="op">=</span><span class="fl">500</span>,</span>
<span>  bow_cr_criterion<span class="op">=</span><span class="fl">1e-8</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>First you have to provide a name for your new model
(<code>model_name</code>). This should be a unique but short name
without any spaces. With <code>model_label</code> you can provide a
label for your model with more freedom. It is important that you provide
a version for your model in case you want to create an improved version
in the future. With <code>model_language</code> you provide users the
information for which language your model is designed. This is very
important if you plan to share your model to a wider community.</p>
<p>With <code>method</code> you determine which approach should be used
for your model. If you would like to use Topic Modeling, you have to set
<code>method = "lda"</code>. the number of topics is set via
<code>bow_n_dim</code>. In this example we would like to create a topic
model with twelve topics. The number of topics also determines the
dimensionality for our text embedding. Consequently, every text will be
characterized by these twelve topics.</p>
<p>Please do not forget to pass your basic text representation to
<code>bow_basic_text_rep</code>.</p>
<p>After the model is estimated, it is stored as
<code>topic_modeling</code> in our example.</p>
</div>
<div class="section level4">
<h4 id="globalvectorclusters">4.2.2 GlobalVectorClusters<a class="anchor" aria-label="anchor" href="#globalvectorclusters"></a>
</h4>
<p>The creation of a text embedding model based on GlobalVectorClusters
is very similar to a model based on Topic Modeling. There are only two
differences.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">global_vector_clusters_modeling</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TextEmbeddingModel.html">TextEmbeddingModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  model_name<span class="op">=</span><span class="st">"global_vector_clusters_embedding"</span>,</span>
<span>  model_label<span class="op">=</span><span class="st">"Text Embedding via Clusters of GlobalVectors"</span>,</span>
<span>  model_version<span class="op">=</span><span class="st">"0.0.1"</span>,</span>
<span>  model_language<span class="op">=</span><span class="st">"english"</span>,</span>
<span>  method<span class="op">=</span><span class="st">"glove_cluster"</span>,</span>
<span>  bow_basic_text_rep<span class="op">=</span><span class="va">basic_text_rep</span>,</span>
<span>  bow_n_dim<span class="op">=</span><span class="fl">96</span>,</span>
<span>  bow_n_cluster<span class="op">=</span><span class="fl">384</span>,</span>
<span>  bow_max_iter<span class="op">=</span><span class="fl">500</span>,</span>
<span>  bow_max_iter_cluster<span class="op">=</span><span class="fl">500</span>,</span>
<span>  bow_cr_criterion<span class="op">=</span><span class="fl">1e-8</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>First, you request a model based on GlobalVectorCluster by setting
<code>method="glove_cluster"</code>. Second, you have to determine the
dimensionality of the global vectors with <code>bow_n_dim</code> and the
number of clusters by <code>bow_n_cluster</code>. When creating a new
text embedding model, the global vector of each token is calculated
based on the feature-co-occurrence-matrix (fcm) you provide with
<code>basic_text_rep</code>. For very token, a vector is calculated with
the length of <code>bow_n_dim</code>. Since these vectors are
<strong>word</strong> embeddings and not <strong>text</strong>
embeddings, an additional step is necessary to create text embeddings.
In <em>aifedcuation</em> the word embeddings are used to group the words
into clusters. The number of clusters is set with
<code>bow_n_cluster</code>. Now, the text embedding is produced by
counting the tokens of every cluster for every text.</p>
<p>The final model is stored as
<code>global_vector_clusters_modeling</code>.</p>
</div>
<div class="section level4">
<h4 id="transformers">4.2.3 Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h4>
<p>Using a transformer for creating a text embedding model is similar to
the other two approaches.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TextEmbeddingModel.html">TextEmbeddingModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  model_name<span class="op">=</span><span class="st">"bert_embedding"</span>,</span>
<span>  model_label<span class="op">=</span><span class="st">"Text Embedding via BERT"</span>,</span>
<span>  model_version<span class="op">=</span><span class="st">"0.0.1"</span>,</span>
<span>  model_language<span class="op">=</span><span class="st">"english"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"bert"</span>,</span>
<span>  max_length <span class="op">=</span> <span class="fl">512</span>,</span>
<span>  chunks<span class="op">=</span><span class="fl">4</span>,</span>
<span>  overlap<span class="op">=</span><span class="fl">30</span>,</span>
<span>  emb_layer_min<span class="op">=</span><span class="st">"middle"</span>,</span>
<span>  emb_layer_max<span class="op">=</span><span class="st">"2_3_layer"</span>,</span>
<span>  emb_pool_type<span class="op">=</span><span class="st">"average"</span>,</span>
<span>  model_dir<span class="op">=</span><span class="st">"my_own_transformer_trained"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>To request a model based on a transformer you must set
<code>method</code> accordingly. Since we use a BERT model in our
example, we have to set <code>method = "bert"</code>. Next, you have to
provide the directory where your model is stored. In this example this
would be <code>bert_model_dir_path="my_own_transformer_trained</code>.
Of course you can use any other pre-trained model from Huggingface which
addresses your needs.</p>
<p>Using a BERT model for text embedding is not a problem since your
text does not provide more tokens than the transformer can process. This
maximal value is set in the configuration of the transformer (see
section 2.3). If the text produces more tokens the last tokens are
ignored. In some instances you might want to analyze long texts. In
these situations, reducing the text to the first tokens (e.g. only the
first 512 tokens) could result in a problematic loss of information. To
deal with these situations you can configure a text embedding model in
<em>aifecuation</em> to split long texts into several chunks which are
processed by the transformer. The maximal number of chunks is set with
<code>chunks</code>. In our example above, the text embedding model
would split a text consisting of 1024 tokens into two chunks with every
chunk consisting of 512 tokens. For every chunk a text embedding is
calculated. As a result, you receive a sequence of embeddings. The first
embeddings characterizes the first part of the text and the second
embedding characterizes the second part of the text (and so on). Thus,
our example text embedding model is able to process texts with about
4*512=2048 tokens. This approach is inspired by the work by Pappagari et
al. (2019).</p>
<p>Since transformers are able to account for the context, it may be
useful to interconnect every chunk to bring context into the
calculations. This can be done with <code>overlap</code> to determine
how many tokens of the end of a prior chunk should be added to the
next.In our example the last 30 tokens of the prior chunks are added at
the beginning of the following chunk. This can help to add the correct
context of the text sections into the analysis. Altogether, this example
model can analyse a maximum of 512+(4-1)*(512-30)=1958 tokens of a
text.</p>
<p>Finally, you have to decide from which hidden layer or layers the
embeddings should be drawn. With <code>emb_layer_min</code> and
<code>emb_layer_max</code> you can decide over which layers the average
value for every token should be calculated. Please note that the
calculation considers all layers between <code>emb_layer_min</code> and
<code>emb_layer_max</code>. In their initial work, Devlin et al. (2019)
used the hidden states of different layers for classification.</p>
<p>With <code>emb_pool_type</code> you decide which tokens are used for
pooling within every layer. In the case of
<code>emb_pool_type="cls"</code> only the cls token is used. In the case
of <code>emb_pool_type="average"</code> all tokens within a layer are
averaged except padding tokens.</p>
<p>After deciding about the configuration, you can use your model.</p>
<blockquote>
<p><strong>Note:</strong> With version 0.3.1 of aifeducation every
transformer can be used with both machine learning frameworks. Even the
pre-trained weights can be used across backends. However, in the future
models my be implemented that are available only for a specific
framework.</p>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="transforming-raw-texts-into-embedded-texts">4.3 Transforming Raw Texts into Embedded Texts<a class="anchor" aria-label="anchor" href="#transforming-raw-texts-into-embedded-texts"></a>
</h3>
<p>Although the mechanics within a text embedding model are different,
the usage is always the same. To transform raw text into a numeric
representation you only have to use the <code>embed</code> method of
your model. To do this, you must provide the raw texts to
<code>raw_text</code>. In addition, it is necessary that you provide a
character vector containing the ID of every text. The IDs must be
unique.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_embeddings</span><span class="op">&lt;-</span><span class="va">topic_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  doc_id<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span>, </span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cluster_embeddings</span><span class="op">&lt;-</span><span class="va">global_vector_clusters_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  doc_id<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span>, </span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">bert_embeddings</span><span class="op">&lt;-</span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  doc_id<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span>, </span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>The method <code>embed</code>creates an object of class
<code>EmbeddedText</code>. This is just a data.frame consisting the
embedding of every text. Depending on the method, the data.frame has a
different meaning:</p>
<ul>
<li>
<strong>Topic Modeling:</strong> Regarding topic modeling, the rows
represent the texts and the columns represent the percentage of every
topic within a text.</li>
<li>
<strong>GlobalVectorClusters:</strong> Here, the rows represent the
texts and the columns represent the absolute frequencies of tokens
belonging to a semantic cluster.</li>
<li>
<strong>Transformer - Bert:</strong> With BERT, the rows represent
the texts and the columns represents the contextualized text embedding
or BERT’s understanding of the relevant text chunk.</li>
</ul>
<p>Please note that in the case of transformer models, the embeddings of
every chunks are interlinked.</p>
<p>With the embedded texts you now have the input to train a new
classifier or to apply a pre-trained classifier for predicting
categories/classes. In the next chapter we will show you how to use
these classifiers. But before we start, we will show you how to save and
load your model.</p>
</div>
<div class="section level3">
<h3 id="saving-and-loading-text-embedding-models">4.4 Saving and Loading Text Embedding Models<a class="anchor" aria-label="anchor" href="#saving-and-loading-text-embedding-models"></a>
</h3>
<p>Saving a created text embedding model is very easy in aifeducation by
using the function <code>save_ai_model</code>. This function provides a
unique interface for all text embedding models. For saving your work you
can pass your model to <code>model</code> and the directory where to
save the model to <code>model_dir</code>. Please do only pass the path
of a directory and not the path of a file to this function. Internally
the function creates a new folder in the directory where all files
belonging to a model are stored.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">topic_modeling</span>, </span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models"</span>,</span>
<span>  dir_name<span class="op">=</span><span class="st">"model_topic_modeling"</span>,</span>
<span>  save_format<span class="op">=</span><span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">global_vector_clusters_modeling</span>, </span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models"</span>,</span>
<span>    dir_name<span class="op">=</span><span class="st">"model_global_vectors"</span>,</span>
<span>  save_format<span class="op">=</span><span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">bert_modeling</span>, </span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models"</span>,</span>
<span>  dir_name<span class="op">=</span><span class="st">"model_transformer_bert"</span>,</span>
<span>  save_format<span class="op">=</span><span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>As you can see all three text embedding models are saved within the
same directory named “text_embedding_models”. Within this directory the
function creates a unique folder for every model. The name of this
folder is specified with <code>dir_name</code>.</p>
<p>If you set <code>dir_name=NULL</code> and
<code>append_ID=FALSE</code> the the name of the folder is created by
using the models’ names. If you change the argument
<code>append_ID</code> to <code>append_ID=TRUE</code> and set
<code>dir_name=NULL</code> the unique ID of the model is added to the
directory. The ID is added automatically to ensure that every model has
a unique name. This is important if you would like to share your work
with other persons.</p>
<blockquote>
<p>Since the files are stored with a special structure please do
<strong>not</strong> change the files manually.</p>
</blockquote>
<p>If you want to load your model, just call the function
<code>load_ai_model</code> and you can continue using your model.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/model_topic_modeling"</span>,</span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">global_vector_clusters_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/model_global_vectors"</span>,</span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/model_transformer_bert"</span>,</span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>With <code>ml_framework</code> you can decide which framework the
model should use. If you set <code>ml_framework="auto"</code> the models
will be initialized with the same framework during saving the model.
Please note that at the moment all implemented text embedding models can
be used with both frameworks. However, this may change in the
future.</p>
<p><strong>Please note that you have to add the name of the model to the
directory path.</strong> In our example we have stored three models in
the directory “text_embedding_models”. Each model is saved within its
own folder. The folder’s name is created automatically with the help of
the name of the model. Thus, for loading a model you must specify which
model you want to load by adding the model’s name to the directory path
as shown above.</p>
<p>Now you can use your text embedding model.</p>
</div>
<div class="section level3">
<h3 id="sustainability">4.5 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability"></a>
</h3>
<p>In the case the underlying model was trained with an active
sustainability tracker (section 3.3) you can receive a table showing you
the energy consumption, CO2 emissions, and hardware used during training
by calling <code>bert_modeling$get_sustainability_data()</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="using-ai-for-classification">5 Using AI for Classification<a class="anchor" aria-label="anchor" href="#using-ai-for-classification"></a>
</h2>
<div class="section level3">
<h3 id="creating-a-new-classifier">5.1 Creating a New Classifier<a class="anchor" aria-label="anchor" href="#creating-a-new-classifier"></a>
</h3>
<p>In <em>aifedcuation</em>, classifiers are based on neural nets and
stored in objects of the class
<code>TextEmbeddingClassifierNeuralNet</code>. You can create a new
classifier by calling
<code>TextEmbeddingClassifierNeuralNet$new()</code>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_targets</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">example_targets</span><span class="op">)</span><span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span></span>
<span></span>
<span><span class="va">classifier</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TextEmbeddingClassifierNeuralNet.html">TextEmbeddingClassifierNeuralNet</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  name<span class="op">=</span><span class="st">"movie_review_classifier"</span>,</span>
<span>  label<span class="op">=</span><span class="st">"Classifier for Estimating a Postive or Negative Rating of Movie Reviews"</span>,</span>
<span>  text_embeddings<span class="op">=</span><span class="va">bert_embeddings</span>,</span>
<span>  targets<span class="op">=</span><span class="va">example_targets</span>,</span>
<span>  hidden<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  rec<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">256</span><span class="op">)</span>,</span>
<span>  self_attention_heads<span class="op">=</span><span class="fl">2</span>,</span>
<span>  intermediate_size<span class="op">=</span><span class="fl">512</span>,</span>
<span>  attention_type<span class="op">=</span><span class="st">"fourier"</span>,</span>
<span>  add_pos_embedding<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  rec_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  repeat_encoder<span class="op">=</span><span class="fl">1</span>,</span>
<span>  dense_dropout<span class="op">=</span><span class="fl">0.4</span>,</span>
<span>  recurrent_dropout<span class="op">=</span><span class="fl">0.4</span>,</span>
<span>  encoder_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  optimizer<span class="op">=</span><span class="st">"adam"</span><span class="op">)</span></span></code></pre></div>
<p>Similar to the text embedding model you should provide a name
(<code>name</code>) and a label (<code>label</code>) for your new
classifier. With <code>text_embeddings</code> you have to provide an
embedded text. We would like to recommend that you use the embedding you
would like to use for training. We here continue our example and use the
embedding produced by our BERT model.</p>
<p><code>targets</code> takes the target data for the supervised
learning. Please do not omit cases which have no category/class since
they can be used with a special training technique we will show you
later. It is very important that you provide the target data as factors.
Otherwise an error will occur. It is also important that you name your
factor. That is, the entries of the factor mus have names that
correspond to the IDs of the corresponding texts. Without these names
the method cannot match the input data (text embeddings) to the target
data.</p>
<p>With the other parameters you decide about the structure of your
classifier. Figure 4 illustrates this.</p>
<div class="float">
<img src="img_articles/classif_fig_04.png" style="width:100.0%" alt="Figure 4: Overview of Possible Structure of a Classifier"><div class="figcaption">Figure 4: Overview of Possible Structure of a
Classifier</div>
</div>
<p><code>hidden</code> takes a vector of integers, determining the
number of layers and the number of neurons. In our example, there are no
dense layers. <code>rec</code> also takes a vector of integers
determining the number and size of the Gated Recurrent Unit (gru). In
this example, we use one layer with 256 neurons.</p>
<p>Since the classifiers in <em>aifeducation</em> use a standardized
scheme for their creation, dense layers are used after the gru layers.
If you want to omit gru layers or dense layers, set the corresponding
argument to <code>NULL</code>.</p>
<p>If you use a text embedding model that processes more than one chunk
we would like to recommend to use recurrent layers since they are able
to use the sequential structure of your data. In all other cases you can
rely on dense layers only.</p>
<p>If you use text embeddings with more than one chunk, it is a good
idea to try self-attention layering in order to take the context of all
chunks into account. To add self-attention you have two choices: - You
can use the attention mechanism used in classic transformer models as
multihead attention (Vaswani et al. 2017). For this variant you have to
set <code>attention_type="multihead"</code>, <code>repeat_encoder</code>
to a value of at least 1, and
self_attention_heads<code>to a value of at least 1.  - Furthermore you can use the attention mechanism described in Lee-Thorp et al. (2021)  of the FNet model which allows much fast computations at low accuracy costs. To use this kind of attention you have to set</code>attention_type=“fourier<code>and</code>repeat_encoder`
to a value of at least 1.</p>
<p>With <code>repeat_encoder</code> you can chose how many times an
encoder layer should be added. The encoder is implemented as described
by Chollet, Kalinowski, and Allaire (2022, pp. 373) for both variants of
attention.</p>
<p>You can further extend the abilities of your network by adding
positional embeddings. Positional embeddings take care for the order of
your chunks. Thus, adding such a layer may increase performance if the
order of information is important. You can add this layer by setting
<code>add_pos_embedding=TRUE</code>. The layer is created as described
by Chollet, Kalinowski, and Allaire (2022, pp. 378)</p>
<p>Masking, normalization, and the creation of the input layer as well
as the output layer are done automatically.</p>
<p>After you have created a new classifier, you can begin training.</p>
<blockquote>
<p><strong>Note:</strong> In contrast to the text embedding models your
decision about the machine learning framework is more important since
the classifier can only be used with the framework you created and
trained the model.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="training-a-classifier">5.2 Training a Classifier<a class="anchor" aria-label="anchor" href="#training-a-classifier"></a>
</h3>
<p>To start the training of your classifier, you have to call the
<code>train</code> method. Similarly, for the creation of the
classifier, you must provide the text embedding to
<code>data_embeddings</code> and the categories/classes as target data
to <code>data_targets</code>. Please remember that
<code>data_targets</code> expects a <strong>named</strong> factor where
the names correspond to the IDs of the corresponding text embeddings.
Text embeddings and target data that cannot be matched are omitted from
training.</p>
<p>To train a classifier, it is necessary that you provide a path to
<code>dir_checkpoint</code>. This directory stores the best set of
weights during each training epoch. After training, these weights are
automatically used as final weights for the classifier.</p>
<p>For performance estimation, training splits the data into several
chunks based on cross-fold validation. The number of folds is set with
<code>data_n_test_samples</code>. In every case, one fold is not used
for training and serves as a <em>test</em> sample. The remaining data is
used to create a <em>training</em> and a <em>validation</em> sample. All
performance values saved in the trained classifier refer to the test
sample. This data has never been used during training and provides a
more realistic estimation of a classifier`s performance.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_targets</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">example_targets</span><span class="op">)</span><span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span></span>
<span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>   data_embeddings <span class="op">=</span> <span class="va">bert_embeddings</span>,</span>
<span>   data_targets <span class="op">=</span> <span class="va">example_targets</span>,</span>
<span>   data_n_test_samples<span class="op">=</span><span class="fl">5</span>,</span>
<span>   use_baseline<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   bsl_val_size<span class="op">=</span><span class="fl">0.33</span>,</span>
<span>   use_bsc<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   bsc_methods<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"dbsmote"</span><span class="op">)</span>,</span>
<span>   bsc_max_k<span class="op">=</span><span class="fl">10</span>,</span>
<span>   bsc_val_size<span class="op">=</span><span class="fl">0.25</span>,</span>
<span>   use_bpl<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   bpl_max_steps<span class="op">=</span><span class="fl">5</span>,</span>
<span>   bpl_epochs_per_step<span class="op">=</span><span class="fl">30</span>,</span>
<span>   bpl_dynamic_inc<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   bpl_balance<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>   bpl_max<span class="op">=</span><span class="fl">1.00</span>,</span>
<span>   bpl_anchor<span class="op">=</span><span class="fl">1.00</span>,</span>
<span>   bpl_min<span class="op">=</span><span class="fl">0.00</span>,</span>
<span>   bpl_weight_inc<span class="op">=</span><span class="fl">0.00</span>,</span>
<span>   bpl_weight_start<span class="op">=</span><span class="fl">1.00</span>,</span>
<span>   bpl_model_reset<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   epochs<span class="op">=</span><span class="fl">30</span>,</span>
<span>   batch_size<span class="op">=</span><span class="fl">8</span>,</span>
<span>   sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>   sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>   sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>   trace<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   view_metrics<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>   keras_trace<span class="op">=</span><span class="fl">0</span>,</span>
<span>   n_cores<span class="op">=</span><span class="fl">2</span>,</span>
<span>   dir_checkpoint<span class="op">=</span><span class="st">"training/classifier"</span><span class="op">)</span></span></code></pre></div>
<p>Since <em>aifedcuation</em> tries to address the special needs in
educational and social science, some special training steps are
integrated into this method.</p>
<ul>
<li>
<strong>Baseline:</strong> If you are interested in training your
classifier without applying any additional statistical techniques, you
should set <code>use_baseline = TRUE</code>. In this case, the
classifier is trained with the provided data as it is. Cases with
missing values in target data are omitted. Even if you would like to
apply further statistical adjustments, it makes sense to compute a
baseline model for comparing the effect of the modified training process
with unmodified training. By using <code>bsl_val_size</code> you can
determine how much data should be used as training data and how much
should be used as validation data.</li>
<li>
<strong>Balanced Synthetic Cases:</strong> In case of imbalanced
data, it is recommended to set <code>use_bsc=TRUE</code>. Before
training, a number of synthetic units is created via different
techniques. Currently you can request <em>Basic Synthetic Minority
Oversampling Technique</em>, <em>Density-Bases Synthetic Minority
Oversampling Technique</em>, and <em>Adaptive Synthetic Sampling
Approach for Imbalanced Learning</em>. The aim is to create new cases
that fill the gap to the majority class. Multi-class problems are
reduced to a two class problem (class under investigation vs. each
other) for generating these units. You can even request several
techniques at once. If the number of synthetic units and original
minority units exceeds the number of cases of the majority class, a
random sample is drawn. If the technique allows to set the number of
neighbors during generation, <code>k = bsc_max_k</code> is used.</li>
<li>
<strong>Balanced Pseudo-Labeling:</strong> This technique is
relevant if you have labeled target data and a large number of unlabeled
target data. With the different parameter starting with “bpl_”, you can
request different implementations of pseudo-labeling, for example based
on the work by Lee (2013) or by Cascante-Bonilla et al. (2020). To turn
on pseudo-labeling, you have to set <code>use_bpl=TRUE</code>.</li>
</ul>
<p>To request pseudo-labeling based on Cascante-Bonilla et al. (2020),
the following parameters have to be set:</p>
<ul>
<li>
<code>bpl_max_steps = 5</code> (splits the unlabeled data into five
chunks)</li>
<li>
<code>bpl_dynamic_inc = TRUE</code> (ensures that the number of used
chunks increases at every step)</li>
<li>
<code>bpl_model_reset = TRUE</code> (re-initializes the model for
every step)</li>
<li>
<code>bpl_epochs_per_step=30</code> (number of training epochs
within each step)</li>
<li>
<code>bpl_balance=FALSE</code> (ensures that the cases with the
highest certainty are added to training regardless of the absolute
frequencies of the classes)</li>
<li>
<code>bpl_weight_inc=0.00</code> and
<code>bpl_weight_start=1.00</code> (ensures that labeled and unlabeled
data have the same weight during training)</li>
<li>
<code>bpl_max=1.00</code>, <code>bpl_anchor=1.00</code>, and
<code>bpl_min=0.00</code> (ensures that all unlabeled data is considered
for training and that cases with the highest certainty are used for
training.)</li>
</ul>
<p>To request the original pseudo-labeling proposed by Lee (2013), you
have to set the following parameters:</p>
<ul>
<li>
<code>bpl_max_steps=30</code> (steps must be treated as epochs)</li>
<li>
<code>bpl_dynamic_inc=FALSE</code> (ensures that all pseudo-labeled
cases are used)</li>
<li>
<code>bpl_model_reset=FALSE</code> (the model is not allowed to be
re-initialized)</li>
<li>
<code>bpl_epochs_per_step=1</code> (steps are treated as epochs so
this must be one)</li>
<li>
<code>bpl_balance=FALSE</code> (ensures that all cases are added
regardless of the absolute frequencies of the classes)</li>
<li>
<code>bpl_weight_inc=0.02</code> and
<code>bpl_weight_start=0.00</code> (gives the pseudo labeled data an
increasing weight with every step)</li>
<li>
<code>bpl_max=1.00</code>, <code>bpl_anchor=1.00</code>, and
<code>bpl_min=0.00</code> (ensures that all pseudo labeled cases are
used for training. <code>bpl_anchor</code> does not affect the
calculations)</li>
</ul>
<p>Please note that while Lee (2013) suggests to recalculate the
pseudo-labels of the unlabeled data after every weight actualization, in
<em>aifeducation</em>, the pseudo-labels are recalculated after every
epoch.</p>
<p><code>bpl_max=1.00</code>, <code>bpl_anchor=1.00</code>, and
<code>bpl_min=0.00</code> are used to describe the certainty of a
prediction. 0 refers to random guessing while 1 refers to perfect
certainty. <code>bpl_anchor</code> is used as a reference value. The
distance to <code>bpl_anchor</code> is calculated for every case. Then,
they are sorted with an increasing distance from
<code>bpl_anchor</code>. The resulting order of cases is relevant if you
set <code>bpl_dynamic_inc=TRUE</code> or
<code>bpl_balance=TRUE</code>.</p>
<p>Figure 5 illustrates the training loop for the cases that all three
options are set to <code>TRUE</code>.</p>
<div class="float">
<img src="img_articles/classif_fig_05.png" style="width:100.0%" alt="Figure 5: Overview of the Steps to Perform a Classification"><div class="figcaption">Figure 5: Overview of the Steps to Perform a
Classification</div>
</div>
<p>The example above applies the algorithm proposed by Cascante-Bonilla
et al. (2020). After training the classifier on the labeled data, the
unlabeled data is introduced into the training. The classifier predicts
the potential labels of the unlabeled data and adds 20% of the cases
with the highest certainty for their pseudo-labels to the training. The
classifier is re-initialized and trained again. After training, the
classifier predicts the potential labels of <em>all</em> originally
unlabeled data and adds 40% of the pseudo-labeled data to the training
data. The model is again re-initialized and trained again until all
unlabeled data is used for training.</p>
<p>Since training a neural net is energy consuming <em>aifeducation</em>
allows you to estimate its ecological impact with help of the python
library <code>codecarbon</code>. Thus, <code>sustain_track</code> is set
to <code>TRUE</code> by default. If you use the sustainability tracker
you must provide the alpha-3 code for the country where your computer is
located (e.g., “CAN”=“Canada”, “Deu”=“Germany”). A list with the codes
can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For USA and Canada you can additionally specify a region
by setting <code>sustain_region</code>. Please refer to the
documentation of codecarbon for more information.</p>
<p>Finally, <code>trace</code>, <code>view_metrics</code>, and
<code>keras_trace</code> allow you to control how much information about
the training progress is printed to the console. Please note that
training the classifier can take some time.</p>
<p>Please note that after performance estimation, the final training of
the classifier makes use of all data available. That is, the test sample
is left empty.</p>
</div>
<div class="section level3">
<h3 id="evaluating-classifiers-performance">5.3 Evaluating Classifier’s Performance<a class="anchor" aria-label="anchor" href="#evaluating-classifiers-performance"></a>
</h3>
<p>After finishing training, you can evaluate the performance of the
classifier. For every fold, the classifier is applied to the test sample
and the results are compared to the true categories/classes. Since the
test sample is never part of the training, all performance measures
provide a more realistic idea of the classifier`s performance.</p>
<p>To support researchers in judging the quality of the predictions,
<em>aifeducation</em> utilizes several measures and concepts from
content analysis. These are</p>
<ul>
<li>Iota Concept of the Second Generation (Berding &amp; Pargmann
2022)</li>
<li>Krippendorff’s Alpha (Krippendorff 2019)</li>
<li>Percentage Agreement</li>
<li>Gwet’s AC1/AC2 (Gwet 2014)</li>
<li>Kendall’s coefficient of concordance W</li>
<li>Cohen’s Kappa unweighted</li>
<li>Cohen’s Kappa with equal weights</li>
<li>Cohen’s Kappa with squared weights</li>
<li>Fleiss’ Kappa for multiple raters without exact estimation</li>
</ul>
<p>You can access the concrete values by accessing the field
<code>reliability</code> which stores all relevant information. In this
list you will find the reliability values for every fold and for every
requested training configuration. In addition, the reliability of every
step within balanced pseudo-labeling is reported.</p>
<p>The central estimates for the reliability values can be found via
<code>reliability$test_metric_mean</code>. In our example this would
be:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">test_metric_mean</span></span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test_metric_mean</span></span>
<span><span class="co">#&gt;          iota_index  min_iota2 avg_iota2 max_iota2 min_alpha avg_alpha</span></span>
<span><span class="co">#&gt; Baseline  0.6320000 0.10294118 0.3877251 0.6725090     0.136     0.549</span></span>
<span><span class="co">#&gt; BSC       0.4346667 0.06895416 0.2676750 0.4663959     0.072     0.512</span></span>
<span><span class="co">#&gt; BPL       0.6293333 0.51019563 0.6401731 0.7701506     0.580     0.756</span></span>
<span><span class="co">#&gt; Final     0.6293333 0.51019563 0.6401731 0.7701506     0.580     0.756</span></span>
<span><span class="co">#&gt;          max_alpha static_iota_index dynamic_iota_index kalpha_nominal</span></span>
<span><span class="co">#&gt; Baseline     0.962         0.5455732          0.5281005    -0.04487101</span></span>
<span><span class="co">#&gt; BSC          0.952         0.3785559          0.3744595    -0.25488654</span></span>
<span><span class="co">#&gt; BPL          0.932         0.3846018          0.5242565     0.54678492</span></span>
<span><span class="co">#&gt; Final        0.932         0.3846018          0.5242565     0.54678492</span></span>
<span><span class="co">#&gt;          kalpha_ordinal   kendall     kappa2 kappa_fleiss kappa_light</span></span>
<span><span class="co">#&gt; Baseline    -0.04487101 0.5531797 0.10142922   0.10142922  0.10142922</span></span>
<span><span class="co">#&gt; BSC         -0.25488654 0.5199922 0.02869454   0.02869454  0.02869454</span></span>
<span><span class="co">#&gt; BPL          0.54678492 0.7827658 0.55104690   0.55104690  0.55104690</span></span>
<span><span class="co">#&gt; Final        0.54678492 0.7827658 0.55104690   0.55104690  0.55104690</span></span>
<span><span class="co">#&gt;          percentage_agreement  gwet_ac</span></span>
<span><span class="co">#&gt; Baseline            0.6866667 0.543828</span></span>
<span><span class="co">#&gt; BSC                 0.4920000 0.106742</span></span>
<span><span class="co">#&gt; BPL                 0.8146667 0.686684</span></span>
<span><span class="co">#&gt; Final               0.8146667 0.686684</span></span></code></pre></div>
<p>You now have a table with all relevant values. Of particular interest
are the values for alpha from the Iota Concept since they represent a
measure of reliability which is independent from the frequency
distribution of the classes/categories. The alpha values describe the
probability that a case of a specific class is recognized as that
specific class. As you can see, compared to the baseline model, applying
<em>Balanced Synthetic Cases increased</em> increases the minimal value
of alpha, reducing the risk to miss cases which belong to a rare class
(see row with “BSC”). On the contrary, the alpha values for the major
category decrease slightly, thus losing its unjustified bonus from a
high number of cases in the training set. This provides a more realistic
performance estimation of the classifier.</p>
<p>Furthermore, you can see that the application of pseudo-labeling
increases the alpha values for the minor class further, up to step
3.</p>
<p>Finally, you can plot a coding stream scheme showing how the cases of
different classes are labeled. Here we use the package
<em>iotarelr</em>.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/iotarelr/" class="external-link">iotarelr</a></span><span class="op">)</span></span>
<span><span class="fu">iotarelr</span><span class="fu">::</span><span class="fu"><a href="https://fberding.github.io/iotarelr/reference/plot_iota2_alluvial.html" class="external-link">plot_iota2_alluvial</a></span><span class="op">(</span><span class="va">test_classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">iota_object_end_free</span><span class="op">)</span></span></code></pre></div>
<div class="float">
<img src="img_articles/classif_fig_06.png" style="width:100.0%" alt="Figure 6: Coding Stream of the Classifier"><div class="figcaption">Figure 6: Coding Stream of the Classifier</div>
</div>
<p>Here you can see that a small number of negative reviews is treated
as a good review while a larger number of positive reviews is treated as
a bad review. Thus, the data for the major class (negative reviews) is
more reliable and valid as the the data for the minor class (positive
reviews).</p>
<p>Evaluating the performance of a classifier is a complex task and and
beyond the scope of this vignette. Instead, we would like to refer to
the cited literature of content analysis and machine learning if you
would like to dive deeper into this topic.</p>
</div>
<div class="section level3">
<h3 id="sustainability-1">5.4 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability-1"></a>
</h3>
<p>In the case the classifier was trained with an active sustainability
tracker you can receive information on sustainability by calling
<code>classifier$get_sustainability_data()</code>.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sustainability_data</span></span>
<span><span class="co">#&gt; $sustainability_tracked</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $date</span></span>
<span><span class="co">#&gt; [1] "Thu Oct  5 11:20:53 2023"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data</span></span>
<span><span class="co">#&gt; $sustainability_data$duration_sec</span></span>
<span><span class="co">#&gt; [1] 7286.503</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$co2eq_kg</span></span>
<span><span class="co">#&gt; [1] 0.05621506</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$cpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.08602103</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$gpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.05598303</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$ram_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.01180879</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$total_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.1538128</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical</span></span>
<span><span class="co">#&gt; $technical$tracker</span></span>
<span><span class="co">#&gt; [1] "codecarbon"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$py_package_version</span></span>
<span><span class="co">#&gt; [1] "2.3.1"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_count</span></span>
<span><span class="co">#&gt; [1] 12</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_model</span></span>
<span><span class="co">#&gt; [1] "12th Gen Intel(R) Core(TM) i5-12400F"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_count</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_model</span></span>
<span><span class="co">#&gt; [1] "1 x NVIDIA GeForce RTX 4070"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$ram_total_size</span></span>
<span><span class="co">#&gt; [1] 15.84258</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region</span></span>
<span><span class="co">#&gt; $region$country_name</span></span>
<span><span class="co">#&gt; [1] "Germany"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$country_iso_code</span></span>
<span><span class="co">#&gt; [1] "DEU"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$region</span></span>
<span><span class="co">#&gt; [1] NA</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="saving-and-loading-a-classifier">5.5 Saving and Loading a Classifier<a class="anchor" aria-label="anchor" href="#saving-and-loading-a-classifier"></a>
</h3>
<p>If you have created a classifier, saving and loading is very easy.
The process for saving a model is similar to the process for text
embedding models. You only have to pass the model and a directory path
to the function <code>save_ai_model</code>.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">classifier</span>,</span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers"</span>,</span>
<span>  dir_name<span class="op">=</span><span class="st">"movie_classifier"</span>,</span>
<span>  save_format <span class="op">=</span> <span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>In contrast to text embedding models you can specify the additional
argument <code>save_format</code>. In the case of pytorch models this
arguments allows you to choose between
<code>save_format = "safetensors"</code> and
<code>save_format = "pt"</code>. We recommend to chose
<code>save_format = "safetensors"</code> since this is a safer method to
save your models. In the case of tensorflow models this arguments allows
you to choose between <code>save_format = "keras"</code>,
<code>save_format = "tf"</code> and <code>save_format = "h5"</code>. We
recommend to chose <code>save_format = "keras"</code> since this is the
recommended format by keras. If you set
<code>save_format = "default"</code> .safetensors is used for pytorch
models and .keras is used for tensorflow models.</p>
<p>If you would like to load a model you can call the function
<code>load_ai_model</code>.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers/movie_classifier"</span><span class="op">)</span></span></code></pre></div>
<blockquote>
<p><strong>Note:</strong> Classifiers depend on the framework which was
used during creation. Thus, a classifier is always initalized with its
original framework. The argument <code>ml_framework</code> has no
effect.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="predicting-new-data">5.6 Predicting New Data<a class="anchor" aria-label="anchor" href="#predicting-new-data"></a>
</h3>
<p>If you would like to apply your classifier to new data, two steps are
necessary. You must first transform the raw text into a numerical
expression by using <em>exactly</em> the same text embedding model that
was used for training your classifier. In the case of our example
classifier we use our BERT model.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If our mode is not loaded</span></span>
<span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/bert_embedding"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a numerical representation of the text</span></span>
<span><span class="va">text_embeddings</span><span class="op">&lt;-</span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text <span class="op">=</span> <span class="va">textual_data</span><span class="op">$</span><span class="va">texts</span>,</span>
<span>  doc_id <span class="op">=</span> <span class="va">textual_data</span><span class="op">$</span><span class="va">doc_id</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">8</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To transform raw texts into a numeric representation Just pass the
raw texts and the IDs of every text to the method <code>embed</code> of
the loaded model. This is very easy if you used the package <a href="https://cran.r-project.org/package=readtext" class="external-link">readtext</a> to read
raw text from disk, since the object resulting from
<code>readtext</code> always stores the texts in the column “texts” and
the IDs in the column “doc_id”.</p>
<p>Depending on your machine, embedding raw texts may take some time. In
case you use a machine with a graphic device, it is possible that an
“out of memory” error occurs. In this case reduce the batch size. If the
error still occurs, restart the <em>R</em> session, switch to cpu-only
mode <em>directly</em> after loading the libraries with
<code><a href="../reference/set_config_cpu_only.html">aifeducation::set_config_cpu_only()</a></code> and request the
embedding again.</p>
<p>In the example above, the text embeddings are stored in
<code>text_embedding</code>. Since embedding texts may take some time,
it is a good idea to save the embeddings for future analysis (use the
<code>save</code> function of <em>R</em>). This allows you to load the
embedding without the need to apply the text embedding model on the same
raw texts again.</p>
<p>The resulting object can then be passed to the method
<code>predict</code> of our classifier and you will get the predictions
together with an estimate of certainty for each class/category.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If your classifier is not loaded</span></span>
<span><span class="va">classifier</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers/movie_review_classifier"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict the classes of new texts</span></span>
<span><span class="va">predicted_categories</span><span class="op">&lt;-</span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span></span>
<span>  newdata <span class="op">=</span> <span class="va">text_embeddings</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">8</span>,</span>
<span>  verbose<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p>After the classifier finishes the prediction, the estimated
categories/classes are stored as <code>predicted_categories</code>. This
object is a <code>data.frame</code> containing texts’ IDs in the rows
and the probabilities of the different categories/classes in the
columns. The last column with the name <code>expected_category</code>
represents the category which is assigned to a text due the highest
probability.</p>
<p>The estimates can be used in further analysis with common methods of
the educational and social sciences such as correlation analysis,
regression analysis, structural equation modeling, latent class analysis
or analysis of variance.</p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The
Long-Document Transformer. <a href="https://doi.org/10.48550/arXiv.2004.05150" class="external-link uri">https://doi.org/10.48550/arXiv.2004.05150</a></p>
<p>Berding, F., &amp; Pargmann, J. (2022). Iota Reliability Concept of
the Second Generation. Berlin: Logos. <a href="https://doi.org/10.30819/5581" class="external-link uri">https://doi.org/10.30819/5581</a></p>
<p>Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A.,
&amp; Rebmann, K. (2022). Performance and Configuration of Artificial
Intelligence in Educational Settings.: Introducing a New Reliability
Concept Based on Content Analysis. Frontiers in Education, 1–21. <a href="https://doi.org/10.3389/feduc.2022.818365" class="external-link uri">https://doi.org/10.3389/feduc.2022.818365</a></p>
<p>Campesato, O. (2021). Natural Language Processing Fundamentals for
Developers. Mercury Learning &amp; Information. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713</a></p>
<p>Cascante-Bonilla, P., Tan, F., Qi, Y. &amp; Ordonez, V. (2020).
Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised
Learning. <a href="https://doi.org/10.48550/arXiv.2001.06001" class="external-link uri">https://doi.org/10.48550/arXiv.2001.06001</a></p>
<p>Chollet, F., Kalinowski, T., &amp; Allaire, J. J. (2022). Deep
learning with R (Second edition). Manning Publications Co. <a href="https://learning.oreilly.com/library/view/-/9781633439849/?ar" class="external-link uri">https://learning.oreilly.com/library/view/-/9781633439849/?ar</a></p>
<p>Dai, Z., Lai, G., Yang, Y. &amp; Le, Q. V. (2020).
Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing. <a href="https://doi.org/10.48550/arXiv.2006.03236" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03236</a></p>
<p>Devlin, J., Chang, M.‑W., Lee, K., &amp; Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171–4186).
Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423" class="external-link uri">https://doi.org/10.18653/v1/N19-1423</a></p>
<p>Gwet, K. L. (2014). Handbook of inter-rater reliability: The
definitive guide to measuring the extent of agreement among raters
(Fourth edition). Gaithersburg: STATAXIS.</p>
<p>He, P., Liu, X., Gao, J. &amp; Chen, W. (2020). DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. <a href="https://doi.org/10.48550/arXiv.2006.03654" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03654</a></p>
<p>Krippendorff, K. (2019). Content Analysis: An Introduction to Its
Methodology (4th ed.). Los Angeles: SAGE.</p>
<p>Lane, H., Howard, C., &amp; Hapke, H. M. (2019). Natural language
processing in action: Understanding, analyzing, and generating text with
Python. Shelter Island: Manning.</p>
<p>Larusson, J. A., &amp; White, B. (Eds.). (2014). Learning Analytics:
From Research to Practice. New York: Springer. <a href="https://doi.org/10.1007/978-1-4614-3305-7" class="external-link uri">https://doi.org/10.1007/978-1-4614-3305-7</a></p>
<p>Lee, D.‑H. (2013). Pseudo-Label: The Simple and Efficient
Semi-Supervised Learning Method for Deep Neural Networks. CML 2013
Workshop: Challenges in Representation Learning.</p>
<p>Lee-Thorp, J., Ainslie, J., Eckstein, I. &amp; Ontanon, S. (2021).
FNet: Mixing Tokens with Fourier Transforms. <a href="https://doi.org/10.48550/arXiv.2105.03824" class="external-link uri">https://doi.org/10.48550/arXiv.2105.03824</a></p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). RoBERTa: A
Robustly Optimized BERT Pretraining Approach. <a href="https://doi.org/10.48550/arXiv.1907.11692" class="external-link uri">https://doi.org/10.48550/arXiv.1907.11692</a></p>
<p>Papilloud, C., &amp; Hinneburg, A. (2018). Qualitative Textanalyse
mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler.
Wiesbaden: Springer. <a href="https://doi.org/10.1007/978-3-658-21980-2" class="external-link uri">https://doi.org/10.1007/978-3-658-21980-2</a></p>
<p>Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., &amp; Dehak,
N. (2019). Hierarchical Transformers for Long Document Classification.
In 2019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU) (pp. 838–844). IEEE. <a href="https://doi.org/10.1109/ASRU46091.2019.9003958" class="external-link uri">https://doi.org/10.1109/ASRU46091.2019.9003958</a></p>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe:
Global Vectors for Word Representation. Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing. <a href="https://aclanthology.org/D14-1162.pdf" class="external-link uri">https://aclanthology.org/D14-1162.pdf</a></p>
<p>Schreier, M. (2012). Qualitative Content Analysis in Practice. Los
Angeles: SAGE.</p>
<p>Tunstall, L., Werra, L. von, Wolf, T., &amp; Géron, A. (2022).
Natural language processing with transformers: Building language
applications with hugging face (Revised edition). Heidelberg:
O’Reilly.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention Is All
You Need. <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link uri">https://doi.org/10.48550/arXiv.1706.03762</a></p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A.,
Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa,
H., . . . Dean, J. (2016). Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation. <a href="https://doi.org/10.48550/arXiv.1609.08144" class="external-link uri">https://doi.org/10.48550/arXiv.1609.08144</a></p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Berding Florian.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>

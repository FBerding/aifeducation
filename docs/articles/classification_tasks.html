<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>03 Using R syntax • aifeducation</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="03 Using R syntax">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/aifeducation.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>For users</h6></li>
    <li><a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a></li>
    <li><a class="dropdown-item" href="../articles/gui_aife_studio.html">02 Aifeducation Studio</a></li>
    <li><a class="dropdown-item" href="../articles/classification_tasks.html">03 Using the Package without Studio</a></li>
    <li><a class="dropdown-item" href="../articles/model_configuration.html">04 Model configuration</a></li>
    <li><a class="dropdown-item" href="../articles/sharing_and_publishing.html">05 Sharing and Using Trained AI/Models</a></li>
    <li><a class="dropdown-item" href="../articles/a01_layers_stacks.html">Appendix 01 Layers and Stacks</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>For developers</h6></li>
    <li><a class="dropdown-item" href="../articles/transformers.html">01 Transformers</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>03 Using R syntax</h1>
                        <h4 data-toc-skip class="author">Florian
Berding, Yuliia Tykhonova, Julia Pargmann, Andreas Slopinski, Elisabeth
Riebenbauer, Karin Rebmann</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/classification_tasks.Rmd" class="external-link"><code>vignettes/classification_tasks.Rmd</code></a></small>
      <div class="d-none name"><code>classification_tasks.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-and-overview">1 Introduction and Overview<a class="anchor" aria-label="anchor" href="#introduction-and-overview"></a>
</h2>
<div class="section level3">
<h3 id="preface">1.1 Preface<a class="anchor" aria-label="anchor" href="#preface"></a>
</h3>
<p>This vignette introduces the package <em>aifeducation</em> and its
usage with <em>R</em> syntax. For users who are unfamiliar with
<em>R</em> or those who do not have coding skills in relevant languages
(e.g., python), we recommend to start with the graphical user interface
<em>Aifeducation - Studio,</em> which is described in the vignette <a href="gui_aife_studio.html">02 Using the graphical user interface
Aifeducation - Studio</a>.</p>
<p>We assume that <em>aifeducation</em> is installed as described in
vignette <a href="https://fberding.github.io/aifeducation/articles/aifeducation.html">01
Get Started</a>. The introduction starts with a brief explanation of
basic concepts, which are necessary to work with this package.</p>
</div>
<div class="section level3">
<h3 id="basic-concepts">1.2 Basic Concepts<a class="anchor" aria-label="anchor" href="#basic-concepts"></a>
</h3>
<p>In the educational and social sciences, assigning scientific concepts
to an observation is an important task that allows researchers to
understand an observation, to generate new insights, and to derive
recommendations for research and practice.</p>
<p>In educational science, several areas deal with this kind of task.
For example, diagnosing students’ characteristics is an important aspect
of a teachers’ profession and necessary to understand and promote
learning. Another example is the use of learning analytics, where data
about students is used to provide learning environments adapted to their
individual needs. On another level, educational institutions such as
schools and universities can use this information for data-driven
performance decisions (Laurusson &amp; White 2014) as well as where and
how to improve it. In any case, a real-world observation is aligned with
scientific models to use scientific knowledge as a technology for
improved learning and instruction.</p>
<p>Supervised machine learning is one concept that allows a link between
real-world observations and existing scientific models and theories
(Berding et al. 2022). For educational science, this is a great
advantage because it allows researchers to use the existing knowledge
and insights to apply AI. The drawback of this approach is that the
training of AI requires both information about the real world
observations and information on the corresponding alignment with
scientific models and theories.</p>
<p>A valuable source of data in educational science are written texts,
since textual data can be found almost everywhere in the realm of
learning and teaching (Berding et al. 2022). For example, teachers often
require students to solve a task which they provide in a written form.
Students have to create a solution for the tasks which they often
document with a short written essay or a presentation. This data can be
used to analyze learning and teaching. Teachers’ written tasks for their
students may provide insights into the quality of instruction while
students’ solutions may provide insights into their learning outcomes
and prerequisites.</p>
<p>AI can be a helpful assistant in analyzing textual data since the
analysis of textual data is a challenging and time-consuming task for
humans.</p>
<blockquote>
<p>Please note that an introduction to content analysis, natural
language processing or machine learning is beyond the scope of this
vignette. If you would like to learn more, please refer to the cited
literature.</p>
</blockquote>
<p>Before we start, it is necessary to introduce a definition of our
understanding of some basic concepts, since applying AI to educational
contexts means to combine the knowledge of different scientific
disciplines using different, sometimes overlapping, concepts. Even
within a single research area, concepts are not unified. Figure 1
illustrates this package’s understanding.</p>
<div class="float">
<img src="classif_fig_01.png" style="width:100.0%" alt="Figure 1: Understanding of Central Concepts"><div class="figcaption">Figure 1: Understanding of Central
Concepts</div>
</div>
<p>Since <em>aifeducation</em> looks at the application of AI for
classification tasks from the perspective of the empirical method of
content analysis, there is some overlapping between the concepts of
content analysis and machine learning. In content analysis, a phenomenon
like performance or colors can be described as a scale/dimension which
is made up by several categories (e.g. Schreier 2012, pp. 59). In our
example, an exam’s performance (scale/dimension) could be “good”,
“average” or “poor”. In terms of colors (scale/dimension) categories
could be “blue”, “green”, etc. Machine learning literature uses other
words to describe this kind of data. In machine learning, “scale” and
“dimension” correspond to the term “label” while “categories” refer to
the term “classes” (Chollet, Kalinowski &amp; Allaire 2022, p. 114).</p>
<p>With these clarifications, classification means that a text is
assigned to the correct category of a scale or, respectively, that the
text is labeled with the correct class. As Figure 2 illustrates, two
kinds of data are necessary to train an AI to classify text in line with
supervised machine learning principles.</p>
<div class="float">
<img src="classif_fig_02.png" style="width:100.0%" alt="Figure 2: Basic Structure of Supervised Machine Learning"><div class="figcaption">Figure 2: Basic Structure of Supervised Machine
Learning</div>
</div>
<p>By providing AI with both the textual data as input data and the
corresponding information about the class as target data, AI can learn
which texts imply a specific class or category. In the above exam
example, AI can learn which texts imply a “good”, an “average” or a
“poor” judgment. After training, AI can be applied to new texts and
predict the most likely class of every new text. The generated class can
be used for further statistical analysis or to derive recommendations
about learning and teaching.</p>
<p>In use cases as described in this vignette, AI has to “understand”
natural language: „Natural language processing is an area of research in
computer science and artificial intelligence (AI) concerned with
processing natural languages such as English and Mandarin. This
processing generally involves translating natural language into data
(numbers) that a computer can use to learn about the world. (…)” (Lane ,
Howard &amp; Hapke 2019, p. 4)</p>
<p>Thus, the first step is to transform raw texts into a a form that is
usable for a computer, hence raw texts must be transformed into numbers.
In modern approaches, this is usually done through word embeddings.
Campesato (2021, p. 102) describes them as “the collective name for a
set of language modeling and feature learning techniques (…) where words
or phrases from the vocabulary are mapped to vectors of real numbers.”
The definition of a word vector is similar: „Word vectors represent the
semantic meaning of words as vectors in the context of the training
corpus.” (Lane, Howard &amp; Hapke 2019, p. 191). In the next step, the
words or text embeddings can be used as input data and the labels as
target data when training AI to classify a text.</p>
<p>In <em>aifeducation,</em> these steps are covered with three
different types of models, as shown in Figure 3.</p>
<div class="float">
<img src="classif_model_hierachy.png" style="width:100.0%" alt="Figure 3: Model Types in aifeducation"><div class="figcaption">Figure 3: Model Types in aifeducation</div>
</div>
<ul>
<li><p><strong>Base Models:</strong> The base models contain the
capacities to understand natural language. In general, these are
transformers such as BERT, RoBERTa, etc. A huge number of pre-trained
models can be found on <a href="https://huggingface.co/" class="external-link">Hugging
Face</a>.</p></li>
<li><p><strong>Text Embedding Models:</strong> The modes are built on
top of base models and store directions on how to use these base models
for converting raw texts into sequences of numbers. Please note that the
same base model can be used to create different text embedding
models.</p></li>
<li><p><strong>Classifiers:</strong> Classifiers are used on top of a
text embedding model. They are used to classify a text into
categories/classes based on the numeric representation provided by the
corresponding text embedding model. Please note that a text embedding
model can be used to create different classifiers (e.g. one classifier
for colors, one classifier to estimate the quality of a text,
etc.).</p></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="start-working">2 Start Working<a class="anchor" aria-label="anchor" href="#start-working"></a>
</h2>
<div class="section level3">
<h3 id="starting-a-new-session">2.1 Starting a New Session<a class="anchor" aria-label="anchor" href="#starting-a-new-session"></a>
</h3>
<p>Before you can work with <em>aifeducation,</em> you must set up a new
<em>R</em> session. First, you can load <code>aifeducation</code>.
Second, it is necessary that you set up python via ‘reticulate’ and
chose the environment where all necessary python libraries are
available. In case you installed python as suggested in vignette <a href="aifeducation.html">01 Get started</a> you may start a new session
like this:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/aifeducation/">aifeducation</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/prepare_session.html">prepare_session</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; Python is not initalized.</span></span>
<span><span class="co">#&gt; Try to use virtual environment 'aifeducation'.</span></span>
<span><span class="co">#&gt; Set virtual environment to 'aifeducation'.</span></span>
<span><span class="co">#&gt; Initializing python.</span></span>
<span><span class="co">#&gt; Detected OS: windows</span></span>
<span><span class="co">#&gt; Checking python packages. This can take a moment.</span></span>
<span><span class="co">#&gt; All necessary python packages are available.</span></span>
<span><span class="co">#&gt; python: 3.10</span></span>
<span><span class="co">#&gt; torch: 2.7.1+cu126</span></span>
<span><span class="co">#&gt; pyarrow: 20.0.0</span></span>
<span><span class="co">#&gt; transformers: 4.52.4</span></span>
<span><span class="co">#&gt; tokenizers: 0.21.1</span></span>
<span><span class="co">#&gt; pandas: 2.3.0</span></span>
<span><span class="co">#&gt; datasets: 3.6.0</span></span>
<span><span class="co">#&gt; codecarbon: 3.0.2</span></span>
<span><span class="co">#&gt; safetensors: 0.5.3</span></span>
<span><span class="co">#&gt; torcheval: 0.0.7</span></span>
<span><span class="co">#&gt; accelerate: 1.8.1</span></span>
<span><span class="co">#&gt; numpy: 2.2.6</span></span>
<span><span class="co">#&gt; GPU Acceleration: TRUE</span></span>
<span><span class="co">#&gt; Location for Temporary Files:C:\Users\User\AppData\Local\Temp\RtmpK6Cq3o/r_aifeducation</span></span></code></pre></div>
<blockquote>
<p><strong>Note:</strong> Please remember: Every time you start a new
session in <em>R,</em> you have to load the library
<code>aifeducation</code> and to configure python. We recommend to use
the function <code>prepare_session</code> because it performs all
necessary steps for setting up python correctly.</p>
</blockquote>
<p>Now you can start your work.</p>
</div>
<div class="section level3">
<h3 id="data-management">2.2 Data Management<a class="anchor" aria-label="anchor" href="#data-management"></a>
</h3>
<div class="section level4">
<h4 id="introducation">2.2.1 Introducation<a class="anchor" aria-label="anchor" href="#introducation"></a>
</h4>
<p>In the context of use cases for <em>aifeducation,</em> three
different types of data are necessary: raw texts, text embeddings, and
target data which represent the categories/classes of a text.</p>
<p>To deal with the first two types and to allow the use of large data
sets that may not fit into the memory of your machine, the packages
ships with two specialized objects.</p>
<p>The first is <code>LargeDataSetForText</code>. Objects of this class
are used to read raw texts from .txt, .pdf, and .xlsx files and store
them for further computations. The second is
<code>LargeDataSetForTextEmbeddings</code> which are used to store the
text embeddings of raw texts which are generated with
<code>TextEmbeddingModel</code>s. We will describe the transformation of
raw texts into text embeddings later.</p>
</div>
<div class="section level4">
<h4 id="raw-texts">2.2.2 Raw Texts<a class="anchor" aria-label="anchor" href="#raw-texts"></a>
</h4>
<p>The creation of a <code>LargeDataSetForText</code> is necessary if
you would like to create or train a base model or to generate text
embeddings. In case you would like to create such a data set for the
first time you have to call the method:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_texts</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/LargeDataSetForText.html">LargeDataSetForText</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Now you have an empty data set. To fill this object with raw texts
different methods are available depending on the file type you use for
storing raw texts.</p>
<p><strong>.txt files</strong></p>
<p>The first alternative is to store raw texts in .txt files. To use
these you have to structure your data in a specific way:</p>
<ul>
<li>Create a main folder for storing your data.</li>
<li>Store every raw text/document into a <em>single</em> .txt file into
its <em>own</em> folder within the main folder. In every folder there
should be only one file for a raw text/document.</li>
<li>Add an additional .txt file to the folder named
<code>bib_entry.txt</code>. This file contains bibliographic information
for the raw text.</li>
<li>Add an additional .txt file to the folder named
<code>license.txt</code> which contains a short statement for the
license of the text such as “CC BY”.</li>
<li>Add an additional .txt file to the folder named
<code>url_license.txt</code> which contains the url/link to the license’
text such as “<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link uri">https://creativecommons.org/licenses/by/4.0/</a>”.</li>
<li>Add an additional .txt file to the folder named
<code>text_license.txt</code> which contains the full license in raw
texts.</li>
<li>Add an additional .txt file to the folder named
<code>url_source.txt</code> which contains the url/link to the text file
in the internet.</li>
</ul>
<p>Applying these rules may result in a data structure as follows:</p>
<ul>
<li>Folder “main folder”
<ul>
<li>Folder Text A
<ul>
<li>text_a.txt</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
<li>Folder Text B
<ul>
<li>text_b.txt</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
<li>Folder Text C
<ul>
<li>text_C.txt</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Now you can call the method <code>add_from_files_txt</code> by
passing the path to the directory of the main folder to
<code>dir_path</code>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_texts</span><span class="op">$</span><span class="fu">add_from_files_txt</span><span class="op">(</span></span>
<span>  dir_path <span class="op">=</span> <span class="st">"main folder"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The data set will now read all the raw texts in the main folder and
will assign every text the corresponding bib entry and license. Please
note that adding a <code>bib_entry.txt</code>, <code>license.txt</code>,
<code>url_license.txt</code>, <code>text_license.txt</code>, and
<code>url_soruce.text</code> to every folder is optional. If there is no
such file in the corresponding folder, there will be an empty entry in
the data set. However, against the backdrop of the European AI Act, we
recommend to provide both the license and bibliographic information to
make the documentation of your models more straightforward. Furthermore,
some licenses such as those provided by Creative Commons require
statements about the creators, a copyright note, a URL or link to the
source material (if possible), the license of the material and a URL or
link to the license’s text on the internet or the license text itself.
Please check the licenses of the material you are using for the
requirements.</p>
<p><strong>.pdf files</strong></p>
<p>The second alternative is to use .pdf files as a source for raw
texts. Here, the necessary structure is similar to .txt files:</p>
<ul>
<li>Create a main folder for storing your data.</li>
<li>Store every raw text/document into a <em>single</em> .pdf file into
its <em>own</em> folder within the main folder. In every folder there
should be only one file for a raw text/document.</li>
<li>Add an additional .txt file to the folder named
<code>bib_entry.txt</code>. This file contains bibliographic information
for the raw text.</li>
<li>Add an additional .txt file to the folder named
<code>license.txt</code> which contains a short statement for the
license of the text such as “CC BY”.</li>
<li>Add an additional .txt file to the folder named
<code>url_license.txt</code> which contains the URL/link to the license
text such as “<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link uri">https://creativecommons.org/licenses/by/4.0/</a>”.</li>
<li>Add an additional .txt file to the folder named
<code>text_license.txt</code> which contains the full license in raw
texts.</li>
<li>Add an additional .txt file to the folder named
<code>url_source.txt</code> which contains the url/link to the text file
in the internet.</li>
</ul>
<p>Applying these rules may result in a data structure as follows:</p>
<ul>
<li>Folder “main folder”
<ul>
<li>Folder Text A
<ul>
<li>text_a.pdf</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
<li>Folder Text B
<ul>
<li>text_b.pdf</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
<li>Folder Text C
<ul>
<li>text_C.pdf</li>
<li>bib_entry.txt</li>
<li>license.txt</li>
<li>url_license.txt</li>
<li>text_license.txt</li>
<li>url_source.txt</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Please not that all files except the text file must be .txt, not
.pdf.</p>
<p>Now you can call the method <code>add_from_files_pdf</code> by
passing the path to the directory of the main folder to
<code>dir_path</code>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_texts</span><span class="op">$</span><span class="fu">add_from_files_pdf</span><span class="op">(</span></span>
<span>  dir_path <span class="op">=</span> <span class="st">"main folder"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>As stated above, <code>bib_entry.txt</code>,
<code>license.txt</code>, <code>url_license.txt</code>,
<code>text_license.txt</code>, and <code>url_soruce.text</code> are
optional.</p>
<p><strong>.xlsx files</strong></p>
<p>The third alternative is to store the raw texts into .xlsx files.
This alternative is useful if you have many small raw texts. For raw
texts that are very large such as books or papers we recommend to store
them as .txt or .pdf files.</p>
<p>In order to add raw texts from .xlsx files, the files need a special
structure:</p>
<ul>
<li>Create a main folder for storing all .xlsx files you would like to
read.</li>
<li>All .xlsx files must contain the names of the columns in the first
row and the names must be identical for each column across all .xslx
files you would like to read.</li>
<li>Every .xslx files must contain a column storing the text ID and must
contain a column storing the raw text. Every text must have a unique ID
across all .xlsx files.</li>
<li>Every .xslx file can contain an additional column for the bib
entry.</li>
<li>Every .xslx file can contain an additional column for the
license.</li>
<li>Every .xslx file can contain an additional column for the license’s
URL.</li>
<li>Every .xslx file can contain an additional column for the license
text.</li>
<li>Every .xslx file can contain an additional column for the source’s
URL.</li>
</ul>
<p>Your .xlsx file may look like</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
<col width="14%">
</colgroup>
<thead><tr class="header">
<th>id</th>
<th>text</th>
<th>bib</th>
<th>license</th>
<th>url_license</th>
<th>text_license</th>
<th>url_source</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>z3</td>
<td>This is an example.</td>
<td>Author (2019)</td>
<td>CC BY</td>
<td>Example URL</td>
<td>Text</td>
<td>Example URL</td>
</tr>
<tr class="even">
<td>a3</td>
<td>This is a second example.</td>
<td>Author (2022)</td>
<td>CC BY</td>
<td>Example URL</td>
<td>Text</td>
<td>Example URL</td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Now you can call the method <code>add_from_files_xlsx</code> by
passing the path to the directory of the main folder to
<code>dir_path</code>. Please do not forget to specify the column names
for ID, text as well as bibliographic and license information.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_texts</span><span class="op">$</span><span class="fu">add_from_files_xlsx</span><span class="op">(</span></span>
<span>  dir_path <span class="op">=</span> <span class="st">"main folder"</span>,</span>
<span>  id_column <span class="op">=</span> <span class="st">"id"</span>,</span>
<span>  text_column <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>  bib_entry_column <span class="op">=</span> <span class="st">"bib_entry"</span>,</span>
<span>  license_column <span class="op">=</span> <span class="st">"license"</span>,</span>
<span>  url_license_column <span class="op">=</span> <span class="st">"url_license"</span>,</span>
<span>  text_license_column <span class="op">=</span> <span class="st">"text_license"</span>,</span>
<span>  url_source_column <span class="op">=</span> <span class="st">"url_source"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p><strong>Saving and loading a data set</strong></p>
<p>Once you have create a <code>LargeDataSetForText</code> you can save
your data to disk by calling the function <code>save_to_disk</code>. In
our example the code would be:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_to_disk.html">save_to_disk</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">raw_texts</span>,</span>
<span>  dir_path <span class="op">=</span> <span class="st">"examples"</span>,</span>
<span>  folder_name <span class="op">=</span> <span class="st">"raw_texts"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The argument <code>object</code> requires the object you would like
to save. In our case this is <code>raw_texts</code>. With
<code>dir_path</code> you specific the location where to save the object
and with <code>folder_name</code> you define the name of the folder that
will be created within that directory. In this folder the data set is
saved.</p>
<p>To load an existing data set, you can call the function
<code>load_from_disk</code> with the directory path where you stored the
data. In our case this would be.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_text_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/raw_texts"</span><span class="op">)</span></span></code></pre></div>
<p>Now you can work with your data.</p>
</div>
<div class="section level4">
<h4 id="text-embeddings">2.2.3 Text Embeddings<a class="anchor" aria-label="anchor" href="#text-embeddings"></a>
</h4>
<p>The numerical representations of raw texts (called text embeddings)
are stored with objects of class
<code>LargeDataSetForTextEmbeddings</code>. These kinds of data sets are
generated by some models such as <code>TextEmbeddingModel</code>s. Thus,
you will never need to create such a data set manually.</p>
<p>However, you will need this kind of data set to train a classifier or
to predict the categories/classes of raw texts. Thus, it may be
advantageous to save already transformed data. You can save and load an
object of this class with the functions <code>save_to_disk</code> and
<code>load_from_disk</code>.</p>
<p>Let us assume that we have a
<code>LargeDataSetForTextEmbeddings</code> <em>text_embeddings</em>.
Saving this object may look like:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_to_disk.html">save_to_disk</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">text_embeddings</span>,</span>
<span>  dir_path <span class="op">=</span> <span class="st">"examples"</span>,</span>
<span>  folder_name <span class="op">=</span> <span class="st">"text_embeddings"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The data set will be saved at <code>examples/text_embeddings</code>.
Loading this data set may look like:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_text_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/text_embeddings"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="target-data">2.2.4 Target Data<a class="anchor" aria-label="anchor" href="#target-data"></a>
</h4>
<p>The last data type necessary for working with
<code>aifeducation</code> are the categories/classes of given raw texts.
For this kind of data we currently do not provide a special object. You
just need a <em>named</em> <code>factor</code> storing the
classes/categories for a dimension. It is also important that the names
equal the ID of the corresponding raw texts/text embeddings since
matching the classes/categories to texts is done with the help of these
names.</p>
<p>Saving and loading can be done with <em>R</em>’s functions
<code>save</code> and <code>load</code>.</p>
</div>
</div>
<div class="section level3">
<h3 id="example-data-for-this-vignette">2.3 Example Data for this Vignette<a class="anchor" aria-label="anchor" href="#example-data-for-this-vignette"></a>
</h3>
<p>To illustrate the steps in this vignette, we cannot use data from
educational settings since these data is generally protected by privacy
policies. Therefore, we use a subset of the Standford Movie Review
Dataset provided by Maas et al. (2011) which is part of the package. You
can access the data set with <code>imdb_movie_reviews</code>.</p>
<p>We now have a data set with three columns. The first column contains
the raw text, the second contains the rating of the movie (positive or
negative), and the third column the ID of the movie review. About 200
reviews imply a positive rating of a movie and about 100 imply a
negative rating.</p>
<p>For this tutorial, we modify this data set by setting about 50
positive and 25 negative reviews to <code>NA</code>, indicating that
these reviews are not labeled.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span> <span class="op">&lt;-</span> <span class="va">imdb_movie_reviews</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">76</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">201</span><span class="op">:</span><span class="fl">250</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; neg pos </span></span>
<span><span class="co">#&gt;  75 150</span></span></code></pre></div>
<p>We will now create a <code>LargeDataSetForText</code> from this
<code>data.frame</code>. Before we can do this we must ensure that the
<code>data.set</code> has all necessary columns:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "text"  "label" "id"</span></span></code></pre></div>
<p>Now we have to add two columns. For this tutorial we do not add any
bibliographic or license information although this is recommended in
practice.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span><span class="op">$</span><span class="va">bib_entry</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">license</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "text"      "label"     "id"        "bib_entry" "license"</span></span></code></pre></div>
<p>Now the <code>data.frame</code> is ready as input for our data set.
The “label” column will not be included in this data set.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_set_reviews_text</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/LargeDataSetForText.html">LargeDataSetForText</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_set_reviews_text</span><span class="op">$</span><span class="fu">add_from_data.frame</span><span class="op">(</span><span class="va">example_data</span><span class="op">)</span></span></code></pre></div>
<p>We save the categories/labels within a separate factor.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">review_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">review_labels</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">example_data</span><span class="op">$</span><span class="va">id</span></span></code></pre></div>
<p>We will now use this data to show you how to use the different
objects and functions in <em>aifeducation</em>.</p>
</div>
</div>
<div class="section level2">
<h2 id="base-models">3 Base Models<a class="anchor" aria-label="anchor" href="#base-models"></a>
</h2>
<div class="section level3">
<h3 id="overview">3.1 Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h3>
<p>Base models are the foundation of all further models in
<em>aifeducation</em>. At the moment, these are transformer models such
modernBERT (Warner et al. 2024), MPNet(Song et al. 2020), BERT (Devlin
et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version 2 (He et
al. 2020), Funnel-Transformer (Dai et al. 2020), and Longformer
(Beltagy, Peters &amp; Cohan 2020). In general, these models are trained
on a large corpus of general texts in the first step. In the next step,
the models are fine-tuned to domain-specific texts and/or fine-tuned for
specific tasks. Since the creation of base models requires a huge number
of texts resulting in high computational time, it is recommended to use
pre-trained models. These can be found on <a href="https://huggingface.co/" class="external-link">Hugging Face</a>. Sometimes, however, it
is more straightforward to create a new model to fit a specific purpose.
<em>aifeducation</em> supports the option to both create and
train/fine-tune base models.</p>
</div>
<div class="section level3">
<h3 id="creation-of-base-models">3.2 Creation of Base Models<a class="anchor" aria-label="anchor" href="#creation-of-base-models"></a>
</h3>
<p>Every transformer model is composed of two parts: 1) the tokenizer
which splits raw texts into smaller pieces to model a large number of
words with a limited, small number of tokens and 2) the neural network
that is used to model the capabilities for understanding natural
language.</p>
<p>At the beginning you can choose between the different supported
transformer architectures. Depending on the architecture, you have
different options determining the shape of your neural network. For this
vignette we use a BERT (Devlin et al. 2019) model which can be created
with the function <code>aife_transformer.make</code>.</p>
<blockquote>
<p>See p. 3 Transformer Maker <a href="transformers.html">01
Transformers for Developers</a> for details.</p>
</blockquote>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">base_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/aife_transformer.make.html">aife_transformer.make</a></span><span class="op">(</span><span class="st">"bert"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "BERT Model has been initialized."</span></span>
<span><span class="va">base_model</span><span class="op">$</span><span class="fu">create</span><span class="op">(</span></span>
<span>  model_dir <span class="op">=</span> <span class="st">"examples/my_own_transformer"</span>,</span>
<span>  text_dataset <span class="op">=</span> <span class="va">data_set_reviews_text</span>,</span>
<span>  vocab_size <span class="op">=</span> <span class="fl">30522</span>,</span>
<span>  vocab_do_lower_case <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  max_position_embeddings <span class="op">=</span> <span class="fl">512</span>,</span>
<span>  hidden_size <span class="op">=</span> <span class="fl">768</span>,</span>
<span>  num_hidden_layer <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  num_attention_heads <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  intermediate_size <span class="op">=</span> <span class="fl">3072</span>,</span>
<span>  hidden_act <span class="op">=</span> <span class="st">"gelu"</span>,</span>
<span>  hidden_dropout_prob <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  log_write_interval <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:13 2025 Start Sustainability Tracking</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:15 2025 Creating Tokenizer Draft</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:15 2025 Start Computing Vocabulary</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:15 2025 Start Computing Vocabulary - Done</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:15 2025 Saving Draft</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:16 2025 Creating Tokenizer</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:16 2025 Creating Tokenizer - Done</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:19 2025 Creating Transformer Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:21 2025 Saving BERT Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:21 2025 Saving Tokenizer Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:21 2025 Saving Sustainability Data</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:21 2025 Done</span></span></code></pre></div>
<p>For this function to work, you must provide a path to a directory
where your new transformer should be saved (<code>model_dir</code>).</p>
<p>Furthermore, you must provide raw texts to <code>text_dataset</code>.
This object should be of class <code>LargeDataSetForText</code> as
described in section 2.2.2. These texts are <strong>not</strong> used to
train the transformer but for calculating the vocabulary. In this
example we use the text from the movie reviews. Please note, that this
data set is to small for creating a new transformer. We use this here
only for a fast running illustration. For real use cases a larger data
set is necessary.</p>
<p>The maximum size of the vocabulary is determined by
<code>vocab_size</code>. Modern tokenizers such as <em>WordPiece</em>
(Wu et al. 2016) use algorithms that splits words into smaller elements,
allowing them to build a huge number of words with a small number of
elements. Thus, even with only small number of about 30,000 tokens, they
are able to represent a very large number of words.</p>
<p>The other parameters allow you to customize your BERT model. For
example, you could increase the number of hidden layers from 12 to 24 or
reduce the hidden size from 768 to 256, allowing you to build and to
test larger or smaller models.</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">04 Model
configuration</a> provides details on how to configure a base model.</p>
</blockquote>
<p>Please note that with <code>max_position_embeddings</code> you
determine how many tokens your transformer can process. If your text has
more tokens, these tokens are ignored. However, if you would like to
analyze long documents, please avoid to increase this number too
significantly because the computational time does not increase in a
linear way but quadratic (Beltagy, Peters &amp; Cohan 2020). For long
documents you can use another architecture of BERT (e.g. Longformer from
Beltagy, Peters &amp; Cohan 2020) or split a long document into several
chunks which are used sequentially for classification (e.g., Pappagari
et al. 2019). Using chunks is supported by <em>aifedcuation</em> for all
models.</p>
<p>Since creating a transformer model is energy consuming,
<em>aifeducation</em> allows you to estimate its ecological impact with
help of the python library <code>codecarbon</code>. Thus,
<code>sustain_track</code> is set to <code>TRUE</code> by default. If
you use the sustainability tracker you must provide the alpha-3 code for
the country where your computer is located (e.g., “CAN”=“Canada”,
“DEU”=“Germany”). A list with the codes can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">Wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For the USA and Canada you can additionally specify a
region by setting <code>sustain_region</code>. Please refer to the
documentation of <code>codecarbon</code> for more information.</p>
<p>After calling the function, you will find your new model in your
model directory.</p>
</div>
<div class="section level3">
<h3 id="trainfine-tune-a-base-model">3.3 Train/Fine-Tune a Base Model<a class="anchor" aria-label="anchor" href="#trainfine-tune-a-base-model"></a>
</h3>
<p>If you would like to train a new base model (see section 3.2) for the
first time or want to adapt a pre-trained model to a domain-specific
language or task, you can call the corresponding
<code>train</code>-method.</p>
<blockquote>
<p>See p. 3 Transformer Maker <a href="transformers.html">01
Transformers for Developers</a> for details.</p>
</blockquote>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">base_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/aife_transformer.make.html">aife_transformer.make</a></span><span class="op">(</span><span class="st">"bert"</span><span class="op">)</span></span>
<span><span class="va">base_model</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>  output_dir <span class="op">=</span> <span class="st">"examples/my_own_transformer_trained"</span>,</span>
<span>  model_dir_path <span class="op">=</span> <span class="st">"examples/my_own_transformer"</span>,</span>
<span>  text_dataset <span class="op">=</span> <span class="va">data_set_reviews_text</span>,</span>
<span>  p_mask <span class="op">=</span> <span class="fl">0.15</span>,</span>
<span>  whole_word <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  val_size <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  n_epoch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  chunk_size <span class="op">=</span> <span class="fl">250</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  log_write_interval <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:21 2025 Start Sustainability Tracking</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:23 2025 Loading Existing Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:23 2025 Creating Chunks of Sequences for Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:24 2025 459 Chunks Created</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:24 2025 Using Whole Word Masking</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:24 2025 Preparing Training of the Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:24 2025 Start Fine Tuning</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:31 2025 Saving BERT Model</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:32 2025 Saving Tokenizer</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:32 2025 Saving Sustainability Data</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:32 2025 Done</span></span></code></pre></div>
<p>Here it is important that you provide the path to the directory where
your new transformer is stored. Furthermore, it is important that you
provide <em>another</em> directory where your trained transformer should
be saved to avoid reading and writing collisions.</p>
<p>Now, the provided raw data is used to train your model. In case of a
Bert model, the learning objective is <em>Masked Language Modeling</em>.
Other models may use other learning objectives. Please refer to the
documentation for more details on every model.</p>
<p>First, you can set the length of token sequences with
<code>chunk_size</code> leading the tokenizer to split long texts into
several chunks with the given size. with <code>val_size</code>, you set
how many of these chunks should be used for the validation sample. With
<code>whole_word</code> you can choose between masking single tokens or
masking complete words (Please remember that modern tokenizers split
words into several tokens. Thus, tokens and words are not forced to
match each other directly). Finally, with <code>p_mask</code> you can
determine how many tokens should be masked.</p>
<p>Please remember to set the correct alpha-3 code for tracking the
ecological impact of training your model
(<code>sustain_iso_code</code>).</p>
<p>If you work on a machine and your graphic device only has small
memory capacity, please reduce the batch size significantly.</p>
<p>After the training finishes, you can find the transformer ready to
use in your <code>output_dir</code>. Now you are able to create a text
embedding model.</p>
</div>
</div>
<div class="section level2">
<h2 id="text-embedding-models">4 Text Embedding Models<a class="anchor" aria-label="anchor" href="#text-embedding-models"></a>
</h2>
<div class="section level3">
<h3 id="introduction">4.1 Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>The text embedding model is the interface to <em>R</em> in
<em>aifeducation</em>. In order to create a new model, you need a base
model that provides the ability to understand natural language. A text
embedding model is stored as an object of class
<code>TextEmbeddingModel</code>. This object contains all relevant
information for transforming raw texts into a numeric representation
that can be used for machine learning.</p>
<p>In <em>aifedcuation</em>, the transformation of raw texts into
numbers is a separate step from downstream tasks such as classification.
This is to reduce computational time on machines with low performance.
By separating text embedding from other tasks, the text embedding has to
be calculated only once and can be used for different tasks at the same
time. Another advantage is that the training of the downstream tasks
involves only the downstream tasks an not the parameters of the
embedding model, making training less time-consuming, thus decreasing
computational intensity. Finally, this approach allows the analysis of
long documents by applying the same algorithm to different parts of a
text.</p>
<p>The text embedding model provides a unified interface: After creating
the model with different methods, the handling of the model is always
the same.</p>
</div>
<div class="section level3">
<h3 id="create-a-text-embedding-model">4.2 Create a Text Embedding Model<a class="anchor" aria-label="anchor" href="#create-a-text-embedding-model"></a>
</h3>
<p>First you have to choose the base model that forms the foundation of
your new text embedding model. In order to illustrate its use we apply a
pre-trained model from <a href="https://huggingface.co/" class="external-link">Hugging
Face</a> called [BERT base model (uncased) ][<a href="https://huggingface.co/google-bert/bert-base-uncased" class="external-link uri">https://huggingface.co/google-bert/bert-base-uncased</a>]
published by Devlin et al. (2019). Download all files into a new folder.
Here we store the model in at ‘examples/bert_uncased’.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/TextEmbeddingModel.html">TextEmbeddingModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  model_label <span class="op">=</span> <span class="st">"Text Embedding via BERT"</span>,</span>
<span>  model_language <span class="op">=</span> <span class="st">"english"</span>,</span>
<span>  max_length <span class="op">=</span> <span class="fl">512</span>,</span>
<span>  chunks <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  overlap <span class="op">=</span> <span class="fl">30</span>,</span>
<span>  emb_layer_min <span class="op">=</span> <span class="st">"middle"</span>,</span>
<span>  emb_layer_max <span class="op">=</span> <span class="st">"2_3_layer"</span>,</span>
<span>  emb_pool_type <span class="op">=</span> <span class="st">"average"</span>,</span>
<span>  model_dir <span class="op">=</span> <span class="st">"examples/bert_uncased"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Next, you have to provide the directory where your base model is
stored. In this example this would be
<code>model_dir="my_own_transformer_trained</code>. Of course you can
use any other pre-trained model from Hugging Face which addresses your
needs.</p>
<p>Using a BERT model for text embedding is not a problem since your
text does not provide more tokens than the transformer can process. This
maximum value is set in the configuration of the transformer (see
section 3.2). If the text produces more tokens, the last tokens are
ignored. In some instances you might want to analyze long texts. In
these situations, reducing the text to the first tokens (e.g. only the
first 512 tokens) could result in a problematic loss of information. To
deal with these situations, you can configure a text embedding model in
<em>aifecuation</em> to split long texts into several chunks which are
processed by the base model. The maximum number of chunks is set with
<code>chunks</code>. In our example above, the text embedding model
would split a text consisting of 1024 tokens into two chunks with every
chunk consisting of 512 tokens. For every chunk, a text embedding is
calculated. As a result, you receive a sequence of embeddings. The first
embedding characterizes the first part of the text and the second
embedding characterizes the second part of the text (and so on). Thus,
our sample text embedding model is able to process texts with about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>*</mo><mn>512</mn><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">4*512=2048</annotation></semantics></math>
tokens. This approach is inspired by the work by Pappagari et
al. (2019).</p>
<p>Since transformers are able to account for the context, it may be
useful to interconnect every chunk to bring context into the
calculations. This can be done with <code>overlap</code> to determine
how many tokens of the end of a prior chunk should be added to the next.
In our example the last 30 tokens of the prior chunks are added at the
beginning of the following chunk. This can help to add the correct
context of the text sections into the analysis. Altogether, this model
can analyse a maximum of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>4</mn><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>*</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>512</mn><mo>−</mo><mn>30</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1958</mn></mrow><annotation encoding="application/x-tex">512+(4-1)*(512-30)=1958</annotation></semantics></math>
tokens of a text.</p>
<p>Finally, you have to decide from which hidden layer(s) the embeddings
should be drawn. With <code>emb_layer_min</code> and
<code>emb_layer_max</code> you can decide from which layers the average
value for every token should be calculated. Please note that the
calculation considers all layers between <code>emb_layer_min</code> and
<code>emb_layer_max</code>. In their initial work, Devlin et al. (2019)
used the hidden states of different layers for classification.</p>
<p>With <code>emb_pool_type,</code> you decide which tokens are used for
pooling within every layer. In the case of
<code>emb_pool_type="cls",</code> only the cls token is used. In the
case of <code>emb_pool_type="average"</code> all tokens within a layer
are averaged except padding tokens.</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">04 Model
configuration</a> provides details on how to configure a text embedding
model.</p>
</blockquote>
<p>After deciding about the configuration, you can use your model.</p>
<p>You can see the number of learnable parameters of the underlying base
model with</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">count_parameter</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 108891648</span></span></code></pre></div>
<p>Another important value is the number of features which you can
request by calling <code>get_n_features</code>.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">get_n_features</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 768</span></span></code></pre></div>
<p>This number describes the number of dimensions for a text embedding.
That is, the number of dimensions which is used to characterize the
content of every chunk of text. This value is important as it determines
the complexity a classifier of feature extractor has to deal with. Some
of classifier’s and feature extractor’s parameters depend on this value.
We elaborate this at the relevant point for the different models.</p>
</div>
<div class="section level3">
<h3 id="transforming-raw-texts-into-embedded-texts">4.3 Transforming Raw Texts into Embedded Texts<a class="anchor" aria-label="anchor" href="#transforming-raw-texts-into-embedded-texts"></a>
</h3>
<p>To transform raw text into a numeric representation, you only have to
use the <code>embed_large</code> method of your model. To do this, you
must provide a <code>LargeDataSetForText</code> to
<code>large_datas_set</code>. Relying on the sample data from section
2.3, we can use the movie reviews as raw texts.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">review_embeddings</span> <span class="op">&lt;-</span> <span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed_large</span><span class="op">(</span></span>
<span>  large_datas_set <span class="op">=</span> <span class="va">data_set_reviews_text</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:37 2025 Batch 1 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:38 2025 Batch 2 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:39 2025 Batch 3 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:40 2025 Batch 4 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:41 2025 Batch 5 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:42 2025 Batch 6 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:43 2025 Batch 7 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:44 2025 Batch 8 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:44 2025 Batch 9 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:45 2025 Batch 10 / 10 done</span></span></code></pre></div>
<p>The method <code>embed_large</code>creates an object of class
<code>LargeDataSetForTextEmbeddings</code>. This is just a data set
consisting of the embeddings of every text. The embeddings are an array,
of which the first dimension refers to specific texts, the second
dimension refers to chunks/sequences, and the third dimension refers to
the features.</p>
<p>With the embedded texts you now have the input to train a new
classifier or to apply a pre-trained classifier for predicting
categories/classes. In the next chapter we will show you how to use
these classifiers. But before we start, we will show you how to save and
load your model.</p>
</div>
<div class="section level3">
<h3 id="saving-and-loading-embedded-texts">4.4 Saving and Loading Embedded Texts<a class="anchor" aria-label="anchor" href="#saving-and-loading-embedded-texts"></a>
</h3>
<p>Since transforming raw texts into text embeddings is time and energy
consuming we recommend to save them to disk in order to use the
embeddings for further tasks and analysis.</p>
<p>To save the them just call the function <code>save_to_disk</code> as
shown below.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_to_disk.html">save_to_disk</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  dir_path <span class="op">=</span> <span class="st">"examples"</span>,</span>
<span>  folder_name <span class="op">=</span> <span class="st">"imdb_movie_reviews"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>To load the embeddings you can call the function
<code>load_from_disk</code>.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">review_embeddings</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/imdb_movie_reviews"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="saving-and-loading-text-embedding-models">4.5 Saving and Loading Text Embedding Models<a class="anchor" aria-label="anchor" href="#saving-and-loading-text-embedding-models"></a>
</h3>
<p>Saving a created text embedding model is very easy in
<em>aifeducation</em> by using the function <code>save_to_disk</code>.
This function provides a unique interface for all text embedding models.
For saving your work you can pass your model to <code>object</code> and
the directory where to save the model to <code>dir_path</code>. With
<code>folder_name</code> you can determine the name of the folder that
should be created in that directory to store the model.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_to_disk.html">save_to_disk</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">bert_modeling</span>,</span>
<span>  dir_path <span class="op">=</span> <span class="st">"examples"</span>,</span>
<span>  folder_name <span class="op">=</span> <span class="st">"bert_te_model"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>In this example the model is saved in a folder at the location
<code>examples/bert_te_model</code>. If you want to load your model you
can call <code>load_from_disk</code>.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/bert_te_model"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="sustainability">4.6 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability"></a>
</h3>
<p>In case the underlying model was trained with an active
sustainability tracker (section 3.2 and 3.3) you can receive a table
showing you the energy consumption, CO2 emissions, and hardware used
during training by calling the method
<code>get_sustainability_data()</code>. For our example this would be
<code>bert_modeling$get_sustainability_data()</code>.</p>
</div>
<div class="section level3">
<h3 id="training-history">4.7 Training History<a class="anchor" aria-label="anchor" href="#training-history"></a>
</h3>
<p>If you would like to see the training history of the underlying base
model you can call a special method.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">plot_training_history</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Please note that this plot is not available for this example since
the necessary data is not directly available for this model on hugging
face. If you traing a model with this package the training history is
always saved.</p>
</div>
</div>
<div class="section level2">
<h2 id="classifiers">5 Classifiers<a class="anchor" aria-label="anchor" href="#classifiers"></a>
</h2>
<div class="section level3">
<h3 id="overview-1">5.1 Overview<a class="anchor" aria-label="anchor" href="#overview-1"></a>
</h3>
<p>Classifiers are built on top of a <code>TextEmbeddingModel</code>.
They use the embedded texts produced by these models and predict
classes/categories. You can build your classifier with the help of two
components.</p>
<p>First, you choose a core model. It determines where different layers
are located and how the outputs of the different layers are combined
into the final output of the model.</p>
<div class="float">
<img src="core_arch_sequential.png" style="width:100.0%" alt="Figure 4: Basic Architecture of a Sequential Classifier"><div class="figcaption">Figure 4: Basic Architecture of a Sequential
Classifier</div>
</div>
<p>The sequential architecture provides models where the input is passed
to a specific number of layers step by step. All layers are grouped by
the kind of layer into stacks.</p>
<p>In contrast, the parallel architecture offers a model where an input
is passed to different types of layers separately. At the end the
outputs are combined to create the final output of the whole model.</p>
<div class="float">
<img src="core_arch_parallel.png" style="width:100.0%" alt="Figure 5: Basic Architecture of a Parallel Classifier"><div class="figcaption">Figure 5: Basic Architecture of a Parallel
Classifier</div>
</div>
<p>You can find the name of the used core model in the name of the
classifier. For example <code>TEClassifierSequential</code> uses a
<em>sequential</em> core model while <code>TEClassifierParallel</code>
uses a <em>parallel</em> core model.</p>
<p>In general, all layers within a core model allow further
customization allowing you to build a high number of different
models.</p>
<blockquote>
<p>A detailed description of all layers can be found in vignette <a href="https://fberding.github.io/aifeducation/articles/a01_layers_and_stack_layers.html">A01
Layers and Stack of Layers</a>:</p>
</blockquote>
<p>Second, you can choose how the core model is used for classification.
At the moment a probability and metric based classifier is possible.</p>
<ul>
<li>
<strong>Probability Classifiers:</strong> Probability classifiers
are used to predict a probability distribution for different
classes/categories. This is the standard case most common in
literature.</li>
<li>
<strong>Prototype Based Classifiers:</strong> Prototype based
classifiers are a kind of metric based classifiers. Here the classifiers
do not predict a probability distribution. Instead it calculates a
prototype for every class/category and measures the distance between a
case and all prototypes. The class/category of the prototype with the
smallest distance to the case is assigned to that case. In contrast to
the probability classifiers these models can handle classes/categories
that were not part of the training. For more details please refer to
section 6.1.</li>
</ul>
<blockquote>
<p>Please note that creating, training, and predicting works for all
types of classifiers as described in the sections below.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="create-a-classifier">5.2 Create a Classifier<a class="anchor" aria-label="anchor" href="#create-a-classifier"></a>
</h3>
<p>To show you how to create a classifier we use a classifier of class
<code>TEClassifierSequential</code> as an example. With the sample data
from section 2.3 and the text embeddings from section 4.3, the creation
of a new classifier may look like:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/TEClassifierSequential.html">TEClassifierSequential</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  label <span class="op">=</span> <span class="st">"Classifier for Estimating a Postive or Negative Rating of Movie Reviews"</span>,</span>
<span>  text_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  feature_extractor <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  target_levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"neg"</span>, <span class="st">"pos"</span><span class="op">)</span>,</span>
<span>  skip_connection_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  cls_pooling_features<span class="op">=</span><span class="fl">50</span>,</span>
<span>  cls_pooling_type<span class="op">=</span><span class="st">"min_max"</span>,</span>
<span>  feat_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  feat_size<span class="op">=</span><span class="fl">256</span>,</span>
<span>  feat_bias<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  feat_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span>  feat_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  feat_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  ng_conv_n_layers<span class="op">=</span><span class="fl">0</span>,</span>
<span>  ng_conv_ks_min<span class="op">=</span><span class="fl">2</span>,</span>
<span>  ng_conv_ks_max<span class="op">=</span><span class="fl">4</span>,</span>
<span>  ng_conv_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  ng_conv_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  ng_conv_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  ng_conv_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  dense_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  dense_n_layers<span class="op">=</span><span class="fl">1</span>,</span>
<span>  dense_dropout<span class="op">=</span><span class="fl">0.5</span>,</span>
<span>  dense_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  dense_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  dense_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  dense_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  rec_act_fct<span class="op">=</span><span class="st">"tanh"</span>,</span>
<span>  rec_n_layers<span class="op">=</span><span class="fl">2</span>,</span>
<span>  rec_type<span class="op">=</span><span class="st">"gru"</span>,</span>
<span>  rec_bidirectional<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  rec_dropout<span class="op">=</span><span class="fl">0.2</span>,</span>
<span>  rec_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  rec_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  rec_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  rec_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  tf_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  tf_dense_dim<span class="op">=</span><span class="fl">512</span>,</span>
<span>  tf_n_layers<span class="op">=</span><span class="fl">0</span>,</span>
<span>  tf_dropout_rate_1<span class="op">=</span><span class="fl">0.2</span>,</span>
<span>  tf_dropout_rate_2<span class="op">=</span><span class="fl">0.5</span>,</span>
<span>  tf_attention_type<span class="op">=</span><span class="st">"multihead"</span>,</span>
<span>  tf_embedding_type<span class="op">=</span><span class="st">"absolute"</span>,</span>
<span>  tf_num_heads<span class="op">=</span><span class="fl">1</span>,</span>
<span>  tf_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  tf_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  tf_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  tf_residual_type<span class="op">=</span><span class="st">"residual_gate"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Similarly to the text embedding model, you should provide a label
(<code>label</code>) for your new classifier. With
<code>text_embeddings</code> you have to provide a
<code>LargeDataSetForTextEmbeddings</code>. The data set is created with
a <code>TextEmbeddingModel</code> as described in section 4. We here
continue our example and use the embedding produced by our BERT
model.</p>
<p><code>target_levels</code> take the categories/classes you classifier
should predict. This can be numbers or even words.</p>
<blockquote>
<p>In case you would like to use ordinal data, it is very important that
you provide the classes/categories in the correct order. That is,
classes/categories representing a “higher” level must be stated after
categories/classes with a lower level. If you provide the wrong order,
the performance indices are not valid. In case of nominal data the order
does not matter.</p>
</blockquote>
<p>With <code>feature_extractor</code> you can add a feature extractor
that tries to reduce the number of features of your text embedding
before passing the embeddings to the classifier. You can read more on
this in Section 6.2.</p>
<p>With the help of the other parameters you can define the complexity
and abilities of your model. A description of the different models can
be found in <a href="https://fberding.github.io/aifeducation/articles/a01_layers_and_stack_layers.html">A01
Layers and Stack of Layers</a>.</p>
<p>Please note that you have to choose the parameter
<code>feat_size</code> depending on the number of features of the
underlying text embedding model. You can request this number by calling
the method <code>get_n_features</code> of the used text embedding model.
In our example this would be:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">get_n_features</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 768</span></span></code></pre></div>
<p>The number for <code>feat_size</code> should be equal or lesser as
this value since this layer tries to compress the text embedding to a
lower number of dimensions. While this reduces the number of parameters
for all following layers and decreases the time to train and use the
model it can cost information. You can experiment with this value to
find a good balance between speed and performance.</p>
<p>In addition, the parameter <code>cls_pooling_features</code> should
be equal or less the number you used for <code>feat_size</code>. With
<code>cls_pooling_features</code> you determine how many of the
resulting features sould be used for classification. Thus, this value
acts as a filter.</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">04 Model
configuration</a> provides details on how to configure a classifier.</p>
</blockquote>
<p>In our example we use only two recurrent layers
(<code>rec_n_layers=2</code>) and one dense layer
(<code>dense_n_layers=1</code>). All other layers are omitted from the
model by setting the number of layers to zero
(<code>conv_n_layers=0</code>,<code>tf_n_layers=0</code>).</p>
<p>As pooling method we use minimum and maximum
(<code>cls_pooling_type="min_max"</code>). That is, the 15 highest and
the 15 lowest features are used for calculating the classes/labels. The
number of values can be determined with
<code>intermediate_features=30</code>.</p>
<p>After you have created a new classifier, you can begin training.</p>
<p>You can see the number of learnable parameters of your model
with:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">count_parameter</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1049450</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="training-a-classifier">5.3 Training a Classifier<a class="anchor" aria-label="anchor" href="#training-a-classifier"></a>
</h3>
<p>To start the training of your classifier, you have to call the
<code>train</code> method. Similarly, for the creation of the
classifier, you must provide the text embedding to
<code>data_embeddings</code> and the categories/classes as target data
to <code>data_targets</code>. Please remember that
<code>data_targets</code> expects a <strong>named</strong> factor where
the names correspond to the IDs of the corresponding text embeddings.
Text embeddings and target data that cannot be matched are omitted from
training.</p>
<p>For performance estimation, training splits the data into several
chunks based on cross-fold validation. The number of folds is set with
<code>data_folds</code>. In every case, one fold is not used for
training and serves as a <em>test</em> sample. The remaining data is
used to create a <em>training</em> and a <em>validation</em> sample. The
percentage of cases within each fold used as a validation sample is
determined with <code>data_val_size</code>. This sample is used to
determine the state of the model that generalizes best. All performance
values saved in the trained classifier refer to the test sample. This
data has never been used during training and provides a more realistic
estimation of a classifier’s performance.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>  data_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  data_targets <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  data_folds <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  data_val_size <span class="op">=</span> <span class="fl">0.25</span>,</span>
<span>  loss_balance_class_weights <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  loss_balance_sequence_length <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  loss_cls_fct_name<span class="op">=</span><span class="st">"FocalLoss"</span>,</span>
<span>  use_sc <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  sc_method <span class="op">=</span> <span class="st">"knnor"</span>,</span>
<span>  sc_min_k <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  sc_max_k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  use_pl <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  pl_max_steps <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  pl_max <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_anchor <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_min <span class="op">=</span> <span class="fl">0.00</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">150</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  ml_trace <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  log_write_interval <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  n_cores <span class="op">=</span> <span class="fu"><a href="../reference/auto_n_cores.html">auto_n_cores</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  lr_rate<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span>  lr_warm_up_ratio<span class="op">=</span><span class="fl">0.02</span>,</span>
<span>  optimizer<span class="op">=</span><span class="st">"adamw"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:46 2025 Total Cases: 300 Unique Cases: 300 Labeled Cases: 225</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:46 2025 Start</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:48 2025 | Iteration 1 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:29:48 2025 | Iteration 1 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:06 2025 | Iteration 2 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:06 2025 | Iteration 2 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:23 2025 | Iteration 3 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:23 2025 | Iteration 3 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:40 2025 | Iteration 4 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:40 2025 | Iteration 4 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:58 2025 | Iteration 5 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:30:58 2025 | Iteration 5 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:16 2025 | Iteration 6 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:16 2025 | Iteration 6 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:33 2025 | Iteration 7 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:33 2025 | Iteration 7 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:50 2025 | Iteration 8 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:31:50 2025 | Iteration 8 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:07 2025 | Iteration 9 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:07 2025 | Iteration 9 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:25 2025 | Iteration 10 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:25 2025 | Iteration 10 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:42 2025 | Final training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:32:42 2025 | Final training | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:00 2025 Training Complete</span></span></code></pre></div>
<p>You can further modify the training process with different arguments.
With <code>loss_balance_class_weights=TRUE</code> the absolute
frequencies of the classes/categories are adjusted according to the
‘Inverse Class Frequency’ method. This option should be activated if you
have to deal with imbalanced data.</p>
<p>With <code>loss_balance_sequence_length=TRUE</code> you can increase
performance if you have to deal with texts that differ in their lengths
and have an imbalanced frequency. If this option is enabled, the loss is
adjusted to the absolute frequencies of length of your texts according
to the ‘Inverse Class Frequency’ method.</p>
<p><code>epochs</code> determines the maximal number of epochs. During
training, the model with the best balanced accuracy is saved and
used.</p>
<p><code>batch_size</code>sets the number of cases that should be
processed simultaneously. Please adjust this value to your machine’s
capacities. Please note that the batch size can have an impact on the
classifier’s performance.</p>
<p>Since <em>aifedcuation</em> tries to address the special needs in
educational and social science, some special training steps are
integrated into this method.</p>
<ul>
<li><p><strong>Synthetic Cases:</strong> In case of imbalanced data, it
is recommended to set <code>use_sc=TRUE</code>. Before training, a
number of synthetic units is created via different techniques. Currently
you can request the <em>K-Nearest Neighbor OveRsampling Approach
(KNNOR)</em> developed by Islam et al. (2022). The aim is to create new
cases that fill the gap to the majority class. Multi-class problems are
reduced to a two class problem (class under investigation vs. each
other) for generating these units. If the technique allows to set the
number of neighbors during generation, you can configure the data
generation with <code>sc_min_k</code> and <code>sc_max_k</code>. The
synthetic cases for every class are generated for all <em>k</em> between
<code>sc_min_k</code> and <code>sc_max_k</code>. Every <em>k</em>
contributes proportionally to the synthetic cases.</p></li>
<li><p><strong>Pseudo-Labeling:</strong> This technique is relevant if
you have labeled target data and a large number of unlabeled target
data. With the different parameters starting with “pl_”, you can
configure the process of pseudo-labeling. Implementation of
pseudo-labeling is based on Cascante-Bonilla et al. (2020). To apply
pseudo-labeling, you have to set <code>use_pl=TRUE</code>.
<code>pl_max=1.00</code>, <code>pl_anchor=1.00</code>, and
<code>pl_min=0.00</code> are used to describe the certainty of a
prediction. 0 refers to random guessing while 1 refers to perfect
certainty. <code>pl_anchor</code> is used as a reference value. The
distance to <code>pl_anchor</code> is calculated for every case. Then,
they are sorted with an increasing distance from <code>pl_anchor</code>.
The proportion of added pseudo-labeled data into training increases with
every step. The maximum number of steps is determined with
<code>pl_max_steps</code>.</p></li>
</ul>
<p>Figure 6 illustrates the training loop for the cases that all options
are set to <code>TRUE</code>.</p>
<div class="float">
<img src="classif_fig_05.png" style="width:100.0%" alt="Figure 6: Overview of the Steps to Perform a Classification"><div class="figcaption">Figure 6: Overview of the Steps to Perform a
Classification</div>
</div>
<p>The example above applies the generation of synthetic cases and the
algorithm proposed by Cascante-Bonilla et al. (2020). For every fold,
the training starts with generating synthetic cases to fill the gap
between the classes and the majority class. After this, an initial
training of the classifiers starts. The trained classifier is used to
predict pseudo-labels for the unlabeled part of the data and adds 20% of
the cases with the highest certainty for their pseudo-labels to the
training data set. Now new synthetic cases are generated based on both
the labeled data and the newly added pseudo-labeled data. The classifier
is re-initialized and trained again. After training, the classifier
predicts the potential labels of <em>all</em> originally unlabeled data
and adds 40% of the pseudo-labeled data to the training data with the
highest certainty. Again, new synthetic cases are generated on both the
labeled and added pseudo-labeled data. The model is again re-initialized
and trained again until the maximum number of steps for pseudo labeling
(<code>pl_max_steps</code>) is reached. After this, the logarithm is
restated for the next fold until the number of folds
(<code>data_folds</code>) is reached. All of these steps are only used
to estimate the performance of the classifier to evaluate for the
classifier’s unknown data.</p>
<p>The last phase of the training begins after the last fold. In the
final training, the data set is split only into a training and
validation set without a test set to provide the maximum amount of data
for the best performance in final training.</p>
<p>In case options like the generation of synthetic cases
(<code>use_sc</code>) or pseudo-labeling (<code>use_pl</code>) are
disabled, the training process is shorter.</p>
<p>Since training a neural net is energy consuming,
<em>aifeducation</em> allows you to estimate its ecological impact with
the help of the python library <code>codecarbon</code>. Thus,
<code>sustain_track</code> is set to <code>TRUE</code> by default. If
you use the sustainability tracker you must provide the alpha-3 code for
the country where your computer is located (e.g., “CAN”=“Canada”,
“DEU”=“Germany”). A list with the codes can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">Wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For the USA and Canada, you can additionally specify a
region by setting <code>sustain_region</code>. Please refer to the
documentation of codecarbon for more information.</p>
<p>Finally, <code>trace</code>, and <code>ml_trace</code> allow you to
control how much information about the training progress is printed to
the console. Please note that training the classifier can take some
time.</p>
<p>Please note that after performance estimation, the final training of
the classifier makes use of all data available. That is, the test sample
is left empty.</p>
<p>In order to visualize the progress of training you can request a plot
which shows how important performance measures develop over epochs.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">plot_training_history</span><span class="op">(</span></span>
<span>  final_training<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  pl_step<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  measure<span class="op">=</span><span class="st">"loss"</span>,</span>
<span>  y_min<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  y_max<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  add_min_max<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  text_size<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="classification_tasks_files/figure-html/unnamed-chunk-37-1.png" alt="Figure 7: Training History of a Classifier" width="700"><p class="caption">
Figure 7: Training History of a Classifier
</p>
</div>
<p>For this method it is important to decide which performance measure
you would like to plot. For classifiers loss (<code>"loss"</code>),
accuracy (<code>"accuracy"</code>), and balanced
accuracy(<code>"balanced_accuracy"</code>) are possible. If you would
like to see only the development of the last training phase set
<code>final_training=FALSE</code>. If this parameter is
<code>TRUE</code> the plot uses the data generated during the
performance estimation phase.</p>
</div>
<div class="section level3">
<h3 id="evaluating-classifiers-performance">5.4 Evaluating Classifier’s Performance<a class="anchor" aria-label="anchor" href="#evaluating-classifiers-performance"></a>
</h3>
<p>After finishing training, you can evaluate the performance of the
classifier. For every fold, the classifier is applied to the test sample
and the results are compared to the true categories/classes. Since the
test sample is never part of the training, all performance measures
provide a more realistic idea of the classifier’s performance.</p>
<p>To support researchers in judging the quality of the predictions,
<em>aifeducation</em> utilizes several measures and concepts from
content analysis. These are</p>
<ul>
<li>Iota Concept of the Second Generation (Berding &amp; Pargmann
2022).</li>
<li>Krippendorff’s Alpha (Krippendorff 2019).</li>
<li>Percentage Agreement.</li>
<li>Gwet’s AC1/AC2 (Gwet 2014).</li>
<li>Kendall’s coefficient of concordance W.</li>
<li>Cohen’s Kappa unweighted (Cohen 1960).</li>
<li>Cohen’s Kappa with equal weights (Cohen 1968).</li>
<li>Cohen’s Kappa with squared weights (Cohen 1968).</li>
<li>Fleiss’ Kappa for multiple raters without exact estimation (Fleiss
1971).</li>
</ul>
<p>You can access the concrete values by accessing the field
<code>reliability,</code> which stores all relevant information. In this
list you will find the reliability values for every fold. In addition,
the reliability of every step within pseudo-labeling is reported.</p>
<p>The central estimates for the reliability values can be found via
<code>reliability$test_metric_mean</code>. In our example this would
be:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">test_metric_mean</span></span>
<span><span class="co">#&gt;           iota_index            min_iota2            avg_iota2 </span></span>
<span><span class="co">#&gt;            0.5784585            0.5061328            0.6170804 </span></span>
<span><span class="co">#&gt;            max_iota2            min_alpha            avg_alpha </span></span>
<span><span class="co">#&gt;            0.7280280            0.6213095            0.7543452 </span></span>
<span><span class="co">#&gt;            max_alpha    static_iota_index   dynamic_iota_index </span></span>
<span><span class="co">#&gt;            0.8873810            0.2767836            0.4880141 </span></span>
<span><span class="co">#&gt;       kalpha_nominal       kalpha_ordinal              kendall </span></span>
<span><span class="co">#&gt;            0.5171082            0.5171082            0.7634111 </span></span>
<span><span class="co">#&gt;   c_kappa_unweighted       c_kappa_linear      c_kappa_squared </span></span>
<span><span class="co">#&gt;            0.5127365            0.5127365            0.5127365 </span></span>
<span><span class="co">#&gt;         kappa_fleiss percentage_agreement    balanced_accuracy </span></span>
<span><span class="co">#&gt;            0.5061392            0.7869565            0.7543452 </span></span>
<span><span class="co">#&gt;     gwet_ac1_nominal      gwet_ac2_linear   gwet_ac2_quadratic </span></span>
<span><span class="co">#&gt;            0.6218700            0.6218700            0.6218700 </span></span>
<span><span class="co">#&gt;        avg_precision           avg_recall               avg_f1 </span></span>
<span><span class="co">#&gt;            0.7780917            0.7543452            0.7530696</span></span></code></pre></div>
<p>Of particular interest are the values for alpha from the Iota
Concept, since they represent a measure of reliability which is
independent from the frequency distribution of the classes/categories.
The alpha values describe the probability that a case of a specific
class is recognized as that specific class. As you can see, compared to
the baseline model, applying <em>Balanced Synthetic Cases increased</em>
increases the minimal value of alpha, reducing the risk to miss cases
which belong to a rare class (see row with “BSC”). On the contrary, the
alpha values for the major category decrease slightly, thus losing its
unjustified bonus from a high number of cases in the training set. This
provides a more realistic performance estimation of the classifier.</p>
<p>An addition, standard measures from machine learning are reported.
These are</p>
<ul>
<li>Precision</li>
<li>Recall</li>
<li>F1-Score</li>
</ul>
<p>You can access these values as follows:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">standard_measures_mean</span></span>
<span><span class="co">#&gt;     precision    recall        f1</span></span>
<span><span class="co">#&gt; neg 0.7170851 0.6553571 0.6654309</span></span>
<span><span class="co">#&gt; pos 0.8390982 0.8533333 0.8407083</span></span></code></pre></div>
<p>Finally, you can plot a coding stream scheme showing how the cases of
different classes are labeled.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">plot_coding_stream</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="classification_tasks_files/figure-html/unnamed-chunk-41-1.png" alt="Figure 8: Coding Stream of the Classifier" width="700"><p class="caption">
Figure 8: Coding Stream of the Classifier
</p>
</div>
<p>Here you can see that a small number of negative reviews is treated
as a good review, while a larger number of positive reviews is treated
as a bad review. Thus, the data for the major class (negative reviews)
is more reliable and valid as the the data for the minor class (positive
reviews).</p>
<p>Evaluating the performance of a classifier is a complex task and and
beyond the scope of this vignette. Instead, we would like to refer to
the cited literature of content analysis and machine learning if you
would like to dive deeper into this topic.</p>
</div>
<div class="section level3">
<h3 id="sustainability-1">5.5 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability-1"></a>
</h3>
<p>In case the classifier was trained with an active sustainability
tracker, you can receive information on sustainability by calling
<code>classifier$get_sustainability_data()</code>.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">get_sustainability_data</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; $sustainability_tracked</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $date</span></span>
<span><span class="co">#&gt; [1] "Fri Jul 18 17:33:00 2025"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data</span></span>
<span><span class="co">#&gt; $sustainability_data$duration_sec</span></span>
<span><span class="co">#&gt; [1] 192.3049</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$co2eq_kg</span></span>
<span><span class="co">#&gt; [1] 0.001603161</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$cpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.002269852</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$gpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.001404458</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$ram_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.0005340138</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$total_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.004208323</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical</span></span>
<span><span class="co">#&gt; $technical$tracker</span></span>
<span><span class="co">#&gt; [1] "codecarbon"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$py_package_version</span></span>
<span><span class="co">#&gt; [1] "3.0.2"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_count</span></span>
<span><span class="co">#&gt; [1] 12</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_model</span></span>
<span><span class="co">#&gt; [1] "12th Gen Intel(R) Core(TM) i5-12400F"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_count</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_model</span></span>
<span><span class="co">#&gt; [1] "1 x NVIDIA GeForce RTX 4070"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$ram_total_size</span></span>
<span><span class="co">#&gt; [1] 15.84258</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region</span></span>
<span><span class="co">#&gt; $region$country_name</span></span>
<span><span class="co">#&gt; [1] "Germany"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$country_iso_code</span></span>
<span><span class="co">#&gt; [1] "DEU"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$region</span></span>
<span><span class="co">#&gt; [1] NA</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="saving-and-loading-a-classifier">5.6 Saving and Loading a Classifier<a class="anchor" aria-label="anchor" href="#saving-and-loading-a-classifier"></a>
</h3>
<p>Saving and loading follows the same pattern as for the other objects
in <em>aifeducation</em>. You can save the classifier by calling
<code>save_to_disk</code>. In our example this may be:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_to_disk.html">save_to_disk</a></span><span class="op">(</span></span>
<span>  object <span class="op">=</span> <span class="va">classifier</span>,</span>
<span>  dir_path <span class="op">=</span> <span class="st">"examples"</span>,</span>
<span>  folder_name <span class="op">=</span> <span class="st">"cls_imdb_movie_reviews"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The classifier is saved to
<code>examples/cls_imdb_movie_reviews</code>. To load the model call
<code>load_from_disk</code>.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/cls_imdb_movie_reviews"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="predicting-new-data">5.7 Predicting New Data<a class="anchor" aria-label="anchor" href="#predicting-new-data"></a>
</h3>
<p>If you would like to apply your classifier to new data, two steps are
necessary. You must first transform the raw text into a numerical
expression by using <em>exactly</em> the same text embedding model that
was used to train your classifier (see section 4). In the case of our
example classifier, we use our BERT model.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If our mode is not loaded</span></span>
<span><span class="va">bert_modeling</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/bert_te_model"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a numerical representation of the text</span></span>
<span><span class="va">review_embeddings</span> <span class="op">&lt;-</span> <span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed_large</span><span class="op">(</span></span>
<span>  large_datas_set <span class="op">=</span> <span class="va">data_set_reviews_text</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:03 2025 Batch 1 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:04 2025 Batch 2 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:05 2025 Batch 3 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:06 2025 Batch 4 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:07 2025 Batch 5 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:07 2025 Batch 6 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:08 2025 Batch 7 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:09 2025 Batch 8 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:10 2025 Batch 9 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:11 2025 Batch 10 / 10 done</span></span></code></pre></div>
<p>To transform raw texts into a numeric representation just pass the
raw texts to the method <code>embed_large</code> of the loaded model.
The raw texts should be an object of class
<code>LargeDataSetForText</code>. To create such a data set, please
refer to section 2.</p>
<p>In the example above, the text embeddings are stored in
<code>review_embeddings</code>. Since embedding texts may take some
time, it is a good idea to save the embeddings for future analysis (see
section 2 for more details). This allows you to load the embeddings
without the need to apply the text embedding model on the same raw texts
again.</p>
<p>The resulting object can then be passed to the method
<code>predict</code> of our classifier and you will get the predictions
together with an estimate of certainty for each class/category.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If your classifier is not loaded</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_from_disk.html">load_from_disk</a></span><span class="op">(</span><span class="st">"examples/cls_imdb_movie_reviews"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict the classes of new texts</span></span>
<span><span class="va">predicted_categories</span> <span class="op">&lt;-</span> <span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span></span>
<span>  newdata <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">8</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Show predicted categories</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">predicted_categories</span><span class="op">)</span></span></code></pre></div>
<p>After the classifier finishes the prediction, the estimated
categories/classes are stored as <code>predicted_categories</code>. This
object is a <code>data.frame</code> containing texts’ IDs in the rows
and the probabilities of the different categories/classes in the
columns. The last column with the name <code>expected_category</code>
represents the category which is assigned to a text due the highest
probability.</p>
<p>The estimates can be used in further analysis with common methods of
the educational and social sciences such as correlation analysis,
regression analysis, structural equation modeling, latent class analysis
or analysis of variance.</p>
<p>Now you are ready to to use <em>aifeducation</em>. In section 6 we
describe further models for classification tasks and for improving model
performance.</p>
</div>
</div>
<div class="section level2">
<h2 id="extensions">6 Extensions<a class="anchor" aria-label="anchor" href="#extensions"></a>
</h2>
<div class="section level3">
<h3 id="classifiers-protonet">6.1 Classifiers: ProtoNet<a class="anchor" aria-label="anchor" href="#classifiers-protonet"></a>
</h3>
<div class="section level4">
<h4 id="introduction-1">6.1.1 Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"></a>
</h4>
<p>The classifier introduced in section 5 is a regular classifier which
comes with the traditional challenges of deep learning, such as the need
for a large number of training data, expensive hardware requirements,
and only a limited possibility to interpret the model’s parameters
(Jadon &amp; Garg 2020, pp.13-14). Since in the educational and social
sciences data is a bottle neck, a classifier that can work with only
small data sets would be preferable. These types of models are discussed
in the literature with terms such as “meta-learning” (Zou 2023) or
“few-shot learning” (Jadon &amp; Garg 2020). The basic idea behind these
approaches is that the model learns to use a supporting data set to
predict the output for a query data set (e.g., Zou 2023, pp. 2-3).
However, the model is not explicitly trained for the query data set.</p>
<p>One type of models within this area are Prototypical Networks
(ProtoNet) which were initially proposed by Snell, Swersky, and Zemel
(2017). This type of network was developed to create classifiers that
are able to generalize to new classes that the model did not see during
training, using only the information of a few examples of each class
provided to the network (support data set). To achieve this goal, the
networks learn to create a prototype for every class in the support data
set with help of the examples for every class. Then, the network
compares the new data with these prototypes and assigns the class of the
nearest prototype to the new data. Since the network calculates the
distance of every new case to every prototype, it belongs to the
metric-based meta-learning approaches (Zhou 2023, pp. 48).</p>
<p>Since ProtoNet is a simple, easy to understand approach and provides
good performance, several extensions have been suggested.
<em>aifeducation</em> replaces the original loss function with the loss
function suggested by Zhang et al. (2019) and adds the learnable metric
described by Oreshkin, Rodriguez, and Lacoste (2019) to increase
performance.</p>
</div>
<div class="section level4">
<h4 id="configuration-traning-and-application-without-sample-data">6.1.2 Configuration, Traning, and Application without Sample
Data<a class="anchor" aria-label="anchor" href="#configuration-traning-and-application-without-sample-data"></a>
</h4>
<p>The application of a classifier based on ProtoNet is similar to the
regular classifiers. The only difference is <code>embedding_dim</code>.
A ProtoNet classifier uses a network to project the similarity and
differences between the single cases and all prototypes into a
n-dimensional space. Similar cases are located near each other while
different cases are located further away. The number of dimensions of
this space is determined by <code>embedding_dim</code>. In case
<code>embedding_dim</code> is set to 1,2 or 3 the position of every case
and the prototypes can be easily visualized. For this example we use the
same data as in section 5. Let us first create and configure the new
classifier.</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_prototype</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/TEClassifierSequentialPrototype.html">TEClassifierSequentialPrototype</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  label <span class="op">=</span> <span class="st">"ProtoNet classifier for Estimating a Postive or Negative Rating of Movie Reviews"</span>,</span>
<span>  text_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  feature_extractor <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  target_levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"neg"</span>, <span class="st">"pos"</span><span class="op">)</span>,</span>
<span>  skip_connection_type <span class="op">=</span> <span class="st">"residual_gate"</span>,</span>
<span>  cls_pooling_features <span class="op">=</span> <span class="fl">50</span>,</span>
<span>  cls_pooling_type <span class="op">=</span> <span class="st">"min_max"</span>,</span>
<span>  metric_type <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>  feat_act_fct <span class="op">=</span> <span class="st">"elu"</span>,</span>
<span>  feat_size <span class="op">=</span> <span class="fl">256</span>,</span>
<span>  feat_bias <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  feat_dropout <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span>  feat_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  feat_normalization_type <span class="op">=</span> <span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_act_fct <span class="op">=</span> <span class="st">"elu"</span>,</span>
<span>  ng_conv_n_layers <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  ng_conv_ks_min <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  ng_conv_ks_max <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  ng_conv_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  ng_conv_dropout <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  ng_conv_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  ng_conv_normalization_type <span class="op">=</span> <span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_residual_type <span class="op">=</span> <span class="st">"residual_gate"</span>,</span>
<span>  dense_act_fct <span class="op">=</span> <span class="st">"elu"</span>,</span>
<span>  dense_n_layers <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  dense_dropout <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  dense_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  dense_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  dense_normalization_type <span class="op">=</span> <span class="st">"layer_norm"</span>,</span>
<span>  dense_residual_type <span class="op">=</span> <span class="st">"residual_gate"</span>,</span>
<span>  rec_act_fct <span class="op">=</span> <span class="st">"tanh"</span>,</span>
<span>  rec_n_layers <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  rec_type <span class="op">=</span> <span class="st">"gru"</span>,</span>
<span>  rec_bidirectional <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  rec_dropout <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  rec_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  rec_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  rec_normalization_type <span class="op">=</span> <span class="st">"layer_norm"</span>,</span>
<span>  rec_residual_type <span class="op">=</span> <span class="st">"residual_gate"</span>,</span>
<span>  tf_act_fct <span class="op">=</span> <span class="st">"elu"</span>,</span>
<span>  tf_dense_dim <span class="op">=</span> <span class="fl">512</span>,</span>
<span>  tf_n_layers <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  tf_dropout_rate_1 <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  tf_dropout_rate_2 <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  tf_attention_type <span class="op">=</span> <span class="st">"multihead"</span>,</span>
<span>  tf_embedding_type <span class="op">=</span> <span class="st">"absolute"</span>,</span>
<span>  tf_num_heads <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  tf_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  tf_parametrizations <span class="op">=</span> <span class="st">"None"</span>,</span>
<span>  tf_normalization_type <span class="op">=</span> <span class="st">"layer_norm"</span>,</span>
<span>  tf_residual_type <span class="op">=</span> <span class="st">"residual_gate"</span>,</span>
<span>  embedding_dim <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Now we can plot how the untrained classifiers embeds the different
cases and the prototypes. To create the corresponding plot you can call
the method <code>plot_embeddings</code>. The argument
<code>embeddings_q</code> takes the embeddings of the different cases as
the input of the classifier. In case you have the true classes for all
or some of the cases, you can add them to the plot by using the argument
<code>classes_q</code>. The resulting plot is shown in the following
Figure.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_untrained</span> <span class="op">&lt;-</span> <span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">plot_embeddings</span><span class="op">(</span></span>
<span>  embeddings_q <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  classes_q <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  inc_margin <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">plot_untrained</span></span></code></pre></div>
<div class="figure">
<img src="classification_tasks_files/figure-html/unnamed-chunk-49-1.png" alt="Figure `r running_num_figures`: Embeddings of an untrained classifier of type 'ProtoNet'" width="700"><p class="caption">
Figure <code>r running_num_figures</code>: Embeddings of an untrained
classifier of type ‘ProtoNet’
</p>
</div>
<p>The large triangles represent the prototypes for every class while
the dots refer to the labeled cases in the data set. For these, the
color represents their true class. For unlabeled cases, a square is
used. Here, the color indicates the class of the estimates. As you can
see, all cases are located very similarly and there seems to be no clear
structure. Let us see how this changes when we train the model.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>  data_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  data_targets <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  data_folds <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  data_val_size <span class="op">=</span> <span class="fl">0.25</span>,</span>
<span>  loss_pt_fct_name <span class="op">=</span> <span class="st">"MultiWayContrastiveLoss"</span>,</span>
<span>  use_sc <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  sc_method <span class="op">=</span> <span class="st">"knnor"</span>,</span>
<span>  sc_min_k <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  sc_max_k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  use_pl <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  pl_max_steps <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  pl_max <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_anchor <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_min <span class="op">=</span> <span class="fl">0.00</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">150</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">35</span>,</span>
<span>  Ns <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  Nq <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  loss_alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  loss_margin <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>  sampling_separate <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  sampling_shuffle <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  ml_trace <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  log_write_interval <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  n_cores <span class="op">=</span> <span class="fu"><a href="../reference/auto_n_cores.html">auto_n_cores</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  lr_rate <span class="op">=</span> <span class="fl">1e-3</span>,</span>
<span>  lr_warm_up_ratio <span class="op">=</span> <span class="fl">0.02</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="st">"adamw"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:11 2025 Total Cases: 300 Unique Cases: 300 Labeled Cases: 225</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:11 2025 Start</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:13 2025 | Iteration 1 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:13 2025 | Iteration 1 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:32 2025 | Iteration 2 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:32 2025 | Iteration 2 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:46 2025 | Iteration 3 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:33:46 2025 | Iteration 3 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:10 2025 | Iteration 4 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:10 2025 | Iteration 4 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:23 2025 | Iteration 5 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:23 2025 | Iteration 5 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:37 2025 | Iteration 6 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:37 2025 | Iteration 6 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:58 2025 | Iteration 7 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:34:58 2025 | Iteration 7 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:25 2025 | Iteration 8 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:25 2025 | Iteration 8 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:38 2025 | Iteration 9 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:38 2025 | Iteration 9 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:58 2025 | Iteration 10 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:35:58 2025 | Iteration 10 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:19 2025 | Final training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:19 2025 | Final training | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:37 2025 Training Complete</span></span></code></pre></div>
<p>While there are no arguments for requesting a balance of the class
weights (<code>loss_balance_class_weights</code>) or balancing the
sequence length (<code>loss_balance_sequence_length</code>), four new
arguments are available. With <code>Ns</code> you determine how many
examples of <em>every</em> class should be used during training within
the support sample. These examples are used to calculate the prototypes
for every class. With <code>Nq</code> you determine how many examples of
<em>every</em> class should be part of the query sample. During training
the network tries to predict the correct classes of the examples.</p>
<p>The arguments <code>loss_alpha</code> and <code>loss_margin</code>
refer to the configuration of the loss function describes by Zhang et
al. (2019). <code>loss_margin</code> refers to the minimal distance all
examples of the query sample should have to all prototypes that do no
represent their class. <code>loss_alpha</code> determines if the loss
should pay more attention to minimize the distance between the examples
to their corresponding prototype or if it should pay more attention to
maximize the distance to the prototypes that do not represent their
class. If you set <code>loss_alpha=1,</code> the loss tries to minimize
the distance of the examples to their corresponding prototype. If you
set <code>loss_alpha=0,</code> loss pays tries to maximize the distance
of all examples to all prototypes that do not reflect their class.</p>
<p>The next two important arguments refer to the sampling strategies
during training. With <code>sampling_separate=TRUE</code>, cases for
sample and query a drawn from the same pool of cases. Thus, a specific
case can be a sample case in one epoch and a query case in another
epoch. However, it is ensured that a specific cases never occurs as a
sample <strong>and</strong> a query during the same training step. In
addition, it is ensured that every case exists only once during a
training step. If you set <code>sampling_separate=FALSE</code>, the
training data set is split into one data pool for sample and one data
pool for query. Thus, a case can only be a sample case
<strong>or</strong> query case. With <code>shuffle</code> you can
request that for every training step a random sample is chosen from the
training data set, resulting in different combinations of sample and
query cases. For the training we highly recommend to set
<code>shuffle=TRUE</code>, since this will result in better performing
classifiers.</p>
<blockquote>
<p>During training the model generates prototpyes based on all available
data and classes of the training data set. These classes and prototypes
are used for the case that no sample data is provided.</p>
</blockquote>
<p>After training we can request a visualization of the data again. We
first omit all unlabeled cases by setting
<code>inc_unlabeled=FALSE</code> in order to get an impression of the
quality of training.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_trained_1</span> <span class="op">&lt;-</span> <span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">plot_embeddings</span><span class="op">(</span></span>
<span>  embeddings_q <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  classes_q <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  inc_unlabeled <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">plot_trained_1</span></span></code></pre></div>
<div class="figure">
<img src="classification_tasks_files/figure-html/unnamed-chunk-51-1.png" alt="Figure `r running_num_figures`: Embeddings of a trained classifier of type 'ProtoNet' without unlabeled cases" width="700"><p class="caption">
Figure <code>r running_num_figures</code>: Embeddings of a trained
classifier of type ‘ProtoNet’ without unlabeled cases
</p>
</div>
<p>As shown in the figure, all cases are now sorted. Cases of the class
“neg” are located close to the prototype for “neg”, while cases of the
class “pos” are located near the prototype for “pos”. The black cycle
around the prototpyes represent the margin used during training
(<code>loss_margin</code>). Since we use the same data as during
training, this result has to be expected. Only a small number of cases
is located near the wrong prototype. This can be seen if a red dot is
close to the prototype for “pos” and a green dot is close to the red
prototype for “neg”.</p>
<p>Let us now add the unlabeled cases to the plot by setting
<code>inc_unlabeled=TRUE</code>.</p>
<p>As the following figure shows, the model estimates the class of these
cases according to their distance to the two prototypes. Cases that are
close to the prototype for “pos” are assigned to “pos”, while cases near
the prototype for “neg” are assigned to “neg”.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_trained_2</span> <span class="op">&lt;-</span> <span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">plot_embeddings</span><span class="op">(</span></span>
<span>  embeddings_q <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  classes_q <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  inc_unlabeled <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">plot_trained_2</span></span></code></pre></div>
<div class="figure">
<img src="classification_tasks_files/figure-html/unnamed-chunk-54-1.png" alt="Figure `r running_num_figures`: Embedding of a trained classifier of type 'ProtoNet' including unlabeled cases" width="700"><p class="caption">
Figure <code>r running_num_figures</code>: Embedding of a trained
classifier of type ‘ProtoNet’ including unlabeled cases
</p>
</div>
<p>Finally, let us report the reliability of this classifier.</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_prototype</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">test_metric_mean</span></span>
<span><span class="co">#&gt;           iota_index            min_iota2            avg_iota2 </span></span>
<span><span class="co">#&gt;            0.6616601            0.6100541            0.6946330 </span></span>
<span><span class="co">#&gt;            max_iota2            min_alpha            avg_alpha </span></span>
<span><span class="co">#&gt;            0.7792118            0.7015476            0.8069643 </span></span>
<span><span class="co">#&gt;            max_alpha    static_iota_index   dynamic_iota_index </span></span>
<span><span class="co">#&gt;            0.9123810            0.3691780            0.5736048 </span></span>
<span><span class="co">#&gt;       kalpha_nominal       kalpha_ordinal              kendall </span></span>
<span><span class="co">#&gt;            0.6239497            0.6239497            0.8155116 </span></span>
<span><span class="co">#&gt;   c_kappa_unweighted       c_kappa_linear      c_kappa_squared </span></span>
<span><span class="co">#&gt;            0.6219786            0.6219786            0.6219786 </span></span>
<span><span class="co">#&gt;         kappa_fleiss percentage_agreement    balanced_accuracy </span></span>
<span><span class="co">#&gt;            0.6153840            0.8308300            0.8069643 </span></span>
<span><span class="co">#&gt;     gwet_ac1_nominal      gwet_ac2_linear   gwet_ac2_quadratic </span></span>
<span><span class="co">#&gt;            0.6955060            0.6955060            0.6955060 </span></span>
<span><span class="co">#&gt;        avg_precision           avg_recall               avg_f1 </span></span>
<span><span class="co">#&gt;            0.8259615            0.8069643            0.8076920</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="application-with-sample-data">6.1.3 Application with Sample Data<a class="anchor" aria-label="anchor" href="#application-with-sample-data"></a>
</h4>
<p>Up to this point we did not provide sample data. Thus, the model used
the classes and prototypes available during training. This is
<strong>not</strong> the regular case for this kind of models. In
general, a query and a sample data is given to the classifier. We
describe how this works in the following sections.</p>
<p>To illustrate this process, we modify the data from section 2.3. We
label some of the positive reviews as “very positive” and some of the
negative reviews as “very negative”. Thus, we increase the number of
classes/categories from 2 to 4.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span> <span class="op">&lt;-</span> <span class="va">imdb_movie_reviews</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">15</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"very negative"</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">76</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">201</span><span class="op">:</span><span class="fl">250</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">251</span><span class="op">:</span><span class="fl">260</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"very positive"</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span>,useNA<span class="op">=</span><span class="st">"ifany"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           neg           pos very negative very positive          &lt;NA&gt; </span></span>
<span><span class="co">#&gt;            60           140            15            10            75</span></span></code></pre></div>
<p>Our aim is now to predict the 75 cases with no labels with the help
of our trained model although the model was trained only for two and
different classes. Thus, we first have to split the data. The cases with
classes/categories form the <em>sample set</em> and the cases without
any classes/categories form the <em>query set</em>.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sample_set_raw</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">example_data</span>,<span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html" class="external-link">is.na</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sample_classes</span><span class="op">=</span><span class="va">sample_set_raw</span><span class="op">$</span><span class="va">label</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">sample_classes</span>,useNA<span class="op">=</span><span class="st">"ifany"</span><span class="op">)</span></span>
<span><span class="co">#&gt; sample_classes</span></span>
<span><span class="co">#&gt;           neg           pos very negative very positive </span></span>
<span><span class="co">#&gt;            60           140            15            10</span></span>
<span></span>
<span><span class="va">sample_texts</span><span class="op">=</span><span class="va"><a href="../reference/LargeDataSetForText.html">LargeDataSetForText</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">sample_texts</span><span class="op">$</span><span class="fu">add_from_data.frame</span><span class="op">(</span><span class="va">sample_set_raw</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">query_set_raw</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">example_data</span>,<span class="fu"><a href="https://rdrr.io/r/base/NA.html" class="external-link">is.na</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">query_set_raw</span><span class="op">$</span><span class="va">label</span>,useNA<span class="op">=</span><span class="st">"ifany"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           neg           pos very negative very positive          &lt;NA&gt; </span></span>
<span><span class="co">#&gt;             0             0             0             0            75</span></span>
<span></span>
<span><span class="va">query_texts</span><span class="op">=</span><span class="va"><a href="../reference/LargeDataSetForText.html">LargeDataSetForText</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">query_texts</span><span class="op">$</span><span class="fu">add_from_data.frame</span><span class="op">(</span><span class="va">query_set_raw</span><span class="op">)</span></span></code></pre></div>
<p>Now we must create text embeddings for the query and the sample data
set. We have to use the same embedding model as we used during training
the classifier.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sample_embeddings</span> <span class="op">&lt;-</span> <span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed_large</span><span class="op">(</span></span>
<span>  large_datas_set <span class="op">=</span> <span class="va">sample_texts</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:40 2025 Batch 1 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:41 2025 Batch 2 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:41 2025 Batch 3 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:42 2025 Batch 4 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:43 2025 Batch 5 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:44 2025 Batch 6 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:45 2025 Batch 7 / 8 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:45 2025 Batch 8 / 8 done</span></span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">query_embeddings</span> <span class="op">&lt;-</span> <span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed_large</span><span class="op">(</span></span>
<span>  large_datas_set <span class="op">=</span> <span class="va">query_texts</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:46 2025 Batch 1 / 3 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:47 2025 Batch 2 / 3 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:48 2025 Batch 3 / 3 done</span></span></code></pre></div>
<p>Now we are ready to use our classifier. First we predict the classes
of the query sample.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_predictions</span><span class="op">=</span><span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">predict_with_samples</span><span class="op">(</span></span>
<span>  newdata <span class="op">=</span> <span class="va">query_embeddings</span>,</span>
<span>  embeddings_s <span class="op">=</span> <span class="va">sample_embeddings</span>,</span>
<span>  classes_s <span class="op">=</span> <span class="va">sample_classes</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">new_predictions</span><span class="op">)</span></span>
<span><span class="co">#&gt;            neg       pos very negative very positive expected_category</span></span>
<span><span class="co">#&gt; 8016 0.2540384 0.2466857     0.2532511     0.2460248               neg</span></span>
<span><span class="co">#&gt; 9118 0.2562909 0.2438422     0.2566554     0.2432115     very negative</span></span>
<span><span class="co">#&gt; 6376 0.2540419 0.2467364     0.2531370     0.2460847               neg</span></span>
<span><span class="co">#&gt; 5766 0.2565490 0.2438955     0.2562973     0.2432582               neg</span></span>
<span><span class="co">#&gt; 5942 0.2561368 0.2437437     0.2569975     0.2431220     very negative</span></span>
<span><span class="co">#&gt; 478  0.2562481 0.2437577     0.2568662     0.2431280     very negative</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">new_predictions</span><span class="op">$</span><span class="va">expected_category</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           neg           pos very negative very positive </span></span>
<span><span class="co">#&gt;            17            27             8            23</span></span></code></pre></div>
<p>Although we trained our classifier for two classes we can now use it
to predict other classes. Next, we plot the results.</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_with_samples</span><span class="op">&lt;-</span><span class="va">classifier_prototype</span><span class="op">$</span><span class="fu">plot_embeddings</span><span class="op">(</span></span>
<span>  embeddings_q <span class="op">=</span> <span class="va">query_embeddings</span>,</span>
<span>  embeddings_s <span class="op">=</span> <span class="va">sample_embeddings</span>,</span>
<span>  classes_s <span class="op">=</span> <span class="va">sample_classes</span>,</span>
<span>  inc_unlabeled <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  inc_margin <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">plot_with_samples</span></span></code></pre></div>
<div class="figure">
<img src="classification_tasks_files/figure-html/unnamed-chunk-63-1.png" alt="Figure `r running_num_figures`: Embeddings of a trained classifier of type 'ProtoNet' trained for two classes and applied for a sample set with four classes." width="700"><p class="caption">
Figure <code>r running_num_figures</code>: Embeddings of a trained
classifier of type ‘ProtoNet’ trained for two classes and applied for a
sample set with four classes.
</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="feature-extractors">6.2 Feature Extractors<a class="anchor" aria-label="anchor" href="#feature-extractors"></a>
</h3>
<p>Another option to increase a model’s performance and/or to increase
computational speed is to apply a feature extractor. For example, the
work by Ganesan et al. (2021) indicates that a reduction of the hidden
size can increase a model’s accuracy. In <em>aifeducation,</em> a
feature extractor is a model that tries to reduce the number of features
of given text embeddings before feeding the embeddings as input to a
classifier.</p>
<p>The feature extractors implemented in <em>aifeducation</em> are
auto-encoders that support sequential data and sequences of different
length. The basic architecture of all extractors is shown in the
following figure.</p>
<div class="float">
<img src="classif_feature_extractor.png" style="width:100.0%" alt="Figure 13: Basic architecture of feature extractors"><div class="figcaption">Figure 13: Basic architecture of feature
extractors</div>
</div>
<p>The learning objective of the feature extractors is first to compress
information by reducing the number of features to the number of features
of the latent space (Frochte 2019, p.281). In the figure above, this
would mean to reduce the number of features from 8 to 4 and to store as
much information as possible from the 8 dimensions in only 4 dimensions.
In the next step, the extractor tries to reconstruct the original
information from the compressed information of the latent space (Frochte
2019, pp.280-281). The information is extended from 4 dimensions to 8.
After training, the hidden representation of the latent space is used as
a compression of the original input.</p>
<p>You can create a feature extractor as follows. In this example we use
the text embeddings from section 4.</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">feature_extractor</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/TEFeatureExtractor.html">TEFeatureExtractor</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">feature_extractor</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="st">"feature_extractor_bert_movie_reviews"</span>,</span>
<span>  label <span class="op">=</span> <span class="st">"Feature extractor for Text Embeddings via BERT"</span>,</span>
<span>  text_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  features <span class="op">=</span> <span class="fl">576</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"dense"</span>,</span>
<span>  noise_factor <span class="op">=</span> <span class="fl">0.2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Similarly to the other models, you can use <code>label</code> for the
model’s label. The argument <code>text_embeddings</code> takes on object
of class <code>EmbeddedText</code> or
<code>LargeDataSetForTextEmbeddings</code>. With this object you connect
your feature extractor with a specific TextEmbeddingModel. That is, the
feature extractor works only with embeddings from exactly the same
<code>TextEmbeddingModel</code>.</p>
<p><code>features</code> determines the number of features for the
compressed representation. The lower the number, the higher the
requested compression. This value corresponds to the features of the
latent space in the figure above.</p>
<p>You should use this value depending on the number of features of the
underlying text embedding model. You can request this value by calling
the method <code>get_n_features</code> like this:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">get_n_features</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 768</span></span></code></pre></div>
<p>In our example we reduce the number of dimensions by 75 %.</p>
<p>With <code>method</code> you determine the type of layer the feature
extractor should use. If set <code>method="lstm"</code>, all layers of
the model are long short-term memory layers. If set
<code>method="dense"</code> all layers are standard dense layers.</p>
<p>Independently from your choice, all models try to generate the latent
space such that the co-variance of the features to be zero. Thus, all
features represent unique information. In addition, all methods except
<code>"lstm"</code> use an orthogonal parameterization to prevent
over-fitting and apply parameter sharing. The opposite layers use the
same parameters. For more details please refer to Ranjan (2019).</p>
<p>With <code>noise_factor</code> you can add some noise during training
making the feature extractor perform a denoising auto-encoder, which can
provide more robust generalizations.</p>
<p>Training the extractor is identical to the other models in
<em>aifeducation</em>. Please note that the text embeddings provided to
<code>data_embeddings</code> must be generated with the same
TextEmbeddingModels as the embeddings provided during the configuration
of your model.</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">feature_extractor</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>  data_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  data_val_size <span class="op">=</span> <span class="fl">0.25</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">500</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  ml_trace <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="st">"adamw"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 17:36:49 2025 Start</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:27 2025 Training finished</span></span></code></pre></div>
<blockquote>
<p>In this example we use the same text embeddings as we use to traing
the classifier. It can be beneficial if you use a larger sample of text
for training a <code>TEFeatureExtractor</code> to improve performance
and/or to allow a broad application of the feature extractor.</p>
</blockquote>
<p>You can plot the training history of the model with:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">feature_extractor</span><span class="op">$</span><span class="fu">plot_training_history</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<img src="classification_tasks_files/figure-html/unnamed-chunk-68-1.png" alt="Figure `r running_num_figures`: Training History of a Feature Extractor." width="700"><p class="caption">
Figure <code>r running_num_figures</code>: Training History of a Feature
Extractor.
</p>
</div>
<p>After you have trained your feature extractor, you can use it for
every classifier. Just pass the feature extractor to
<code>feature_extractor</code> during configuration of the classifier.
Please note that you now have to set the values for
<code>feat_size</code> and <code>cls_pooling_features</code> depending
on the number of features of the feature extractor and <em>not</em>
depending on the number of features of the text embedding model since
the aim of the feature extractor is reduced this number.
<code>feat_size</code> should be equal or less the number for features
of the feature extractor and <code>cls_pooling_features</code> should be
equal or less the value for <code>feat_size</code>. For the classifier
described in section 5 this would look like:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_with_fe</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/TEClassifierSequential.html">TEClassifierSequential</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">classifier_with_fe</span><span class="op">$</span><span class="fu">configure</span><span class="op">(</span></span>
<span>  label <span class="op">=</span> <span class="st">"Classifier for Estimating a Postive or Negative Rating of Movie Reviews"</span>,</span>
<span>  text_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  feature_extractor <span class="op">=</span> <span class="va">feature_extractor</span>,</span>
<span>  target_levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"neg"</span>, <span class="st">"pos"</span><span class="op">)</span>,</span>
<span>  skip_connection_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  cls_pooling_features<span class="op">=</span><span class="fl">50</span>,</span>
<span>  cls_pooling_type<span class="op">=</span><span class="st">"min_max"</span>,</span>
<span>  feat_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  feat_size<span class="op">=</span><span class="fl">256</span>,</span>
<span>  feat_bias<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  feat_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span>  feat_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  feat_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  ng_conv_n_layers<span class="op">=</span><span class="fl">0</span>,</span>
<span>  ng_conv_ks_min<span class="op">=</span><span class="fl">2</span>,</span>
<span>  ng_conv_ks_max<span class="op">=</span><span class="fl">4</span>,</span>
<span>  ng_conv_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  ng_conv_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  ng_conv_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  ng_conv_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  ng_conv_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  dense_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  dense_n_layers<span class="op">=</span><span class="fl">1</span>,</span>
<span>  dense_dropout<span class="op">=</span><span class="fl">0.5</span>,</span>
<span>  dense_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  dense_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  dense_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  dense_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  rec_act_fct<span class="op">=</span><span class="st">"tanh"</span>,</span>
<span>  rec_n_layers<span class="op">=</span><span class="fl">2</span>,</span>
<span>  rec_type<span class="op">=</span><span class="st">"gru"</span>,</span>
<span>  rec_bidirectional<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  rec_dropout<span class="op">=</span><span class="fl">0.2</span>,</span>
<span>  rec_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  rec_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  rec_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  rec_residual_type<span class="op">=</span><span class="st">"residual_gate"</span>,</span>
<span>  tf_act_fct<span class="op">=</span><span class="st">"elu"</span>,</span>
<span>  tf_dense_dim<span class="op">=</span><span class="fl">512</span>,</span>
<span>  tf_n_layers<span class="op">=</span><span class="fl">0</span>,</span>
<span>  tf_dropout_rate_1<span class="op">=</span><span class="fl">0.2</span>,</span>
<span>  tf_dropout_rate_2<span class="op">=</span><span class="fl">0.5</span>,</span>
<span>  tf_attention_type<span class="op">=</span><span class="st">"multihead"</span>,</span>
<span>  tf_embedding_type<span class="op">=</span><span class="st">"absolute"</span>,</span>
<span>  tf_num_heads<span class="op">=</span><span class="fl">1</span>,</span>
<span>  tf_bias<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  tf_parametrizations<span class="op">=</span><span class="st">"None"</span>,</span>
<span>  tf_normalization_type<span class="op">=</span><span class="st">"layer_norm"</span>,</span>
<span>  tf_residual_type<span class="op">=</span><span class="st">"residual_gate"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_with_fe</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>  data_embeddings <span class="op">=</span> <span class="va">review_embeddings</span>,</span>
<span>  data_targets <span class="op">=</span> <span class="va">review_labels</span>,</span>
<span>  data_folds <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  data_val_size <span class="op">=</span> <span class="fl">0.25</span>,</span>
<span>  loss_balance_class_weights <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  loss_balance_sequence_length <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  loss_cls_fct_name<span class="op">=</span><span class="st">"FocalLoss"</span>,</span>
<span>  use_sc <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  sc_method <span class="op">=</span> <span class="st">"knnor"</span>,</span>
<span>  sc_min_k <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  sc_max_k <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  use_pl <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  pl_max_steps <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  pl_max <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_anchor <span class="op">=</span> <span class="fl">1.00</span>,</span>
<span>  pl_min <span class="op">=</span> <span class="fl">0.00</span>,</span>
<span>  sustain_track <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code <span class="op">=</span> <span class="st">"DEU"</span>,</span>
<span>  sustain_region <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  sustain_interval <span class="op">=</span> <span class="fl">15</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">150</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  ml_trace <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  log_dir <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  log_write_interval <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  n_cores <span class="op">=</span> <span class="fu"><a href="../reference/auto_n_cores.html">auto_n_cores</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  lr_rate<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span>  lr_warm_up_ratio<span class="op">=</span><span class="fl">0.02</span>,</span>
<span>  optimizer<span class="op">=</span><span class="st">"adamw"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:28 2025 Batch 1 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:28 2025 Batch 2 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:28 2025 Batch 3 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:29 2025 Batch 4 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:29 2025 Batch 5 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:29 2025 Batch 6 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:29 2025 Batch 7 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:30 2025 Batch 8 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:30 2025 Batch 9 / 10 done </span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:30 2025 Batch 10 / 10 done</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:30 2025 Total Cases: 300 Unique Cases: 300 Labeled Cases: 225</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:31 2025 Start</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:32 2025 | Iteration 1 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:32 2025 | Iteration 1 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:49 2025 | Iteration 2 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:49:49 2025 | Iteration 2 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:07 2025 | Iteration 3 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:07 2025 | Iteration 3 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:24 2025 | Iteration 4 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:24 2025 | Iteration 4 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:41 2025 | Iteration 5 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:41 2025 | Iteration 5 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:57 2025 | Iteration 6 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:50:57 2025 | Iteration 6 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:15 2025 | Iteration 7 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:15 2025 | Iteration 7 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:32 2025 | Iteration 8 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:32 2025 | Iteration 8 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:49 2025 | Iteration 9 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:51:49 2025 | Iteration 9 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:52:06 2025 | Iteration 10 from 10</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:52:06 2025 | Iteration 10 from 10 | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:52:23 2025 | Final training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:52:23 2025 | Final training | Training</span></span>
<span><span class="co">#&gt; Fri Jul 18 18:52:40 2025 Training Complete</span></span></code></pre></div>
<p>That is all. Now you can use and train the classifier in the same way
as you did without a feature extractor. Even saving and loading is done
automatically. For example, let us explore the performance of the
classifier.</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier_with_fe</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">test_metric_mean</span></span>
<span><span class="co">#&gt;           iota_index            min_iota2            avg_iota2 </span></span>
<span><span class="co">#&gt;            0.4486166            0.4197280            0.5312641 </span></span>
<span><span class="co">#&gt;            max_iota2            min_alpha            avg_alpha </span></span>
<span><span class="co">#&gt;            0.6428001            0.5992857            0.6886905 </span></span>
<span><span class="co">#&gt;            max_alpha    static_iota_index   dynamic_iota_index </span></span>
<span><span class="co">#&gt;            0.7780952            0.1019799            0.3825965 </span></span>
<span><span class="co">#&gt;       kalpha_nominal       kalpha_ordinal              kendall </span></span>
<span><span class="co">#&gt;            0.3777857            0.3777857            0.6879698 </span></span>
<span><span class="co">#&gt;   c_kappa_unweighted       c_kappa_linear      c_kappa_squared </span></span>
<span><span class="co">#&gt;            0.3699955            0.3699955            0.3699955 </span></span>
<span><span class="co">#&gt;         kappa_fleiss percentage_agreement    balanced_accuracy </span></span>
<span><span class="co">#&gt;            0.3636101            0.7152174            0.6886905 </span></span>
<span><span class="co">#&gt;     gwet_ac1_nominal      gwet_ac2_linear   gwet_ac2_quadratic </span></span>
<span><span class="co">#&gt;            0.4806008            0.4806008            0.4806008 </span></span>
<span><span class="co">#&gt;        avg_precision           avg_recall               avg_f1 </span></span>
<span><span class="co">#&gt;            0.6885651            0.6886905            0.6818050</span></span></code></pre></div>
<p>If you would like to save and load a <code>TEFeatureExtractor</code>
independently from a classifier you can use the function pair
<code>save_to_disk</code> and <code>load_from_disk</code> as with the
other objects of this package. This is useful if you would like to use
the <code>TEFeatureExtractor</code> at a later time point.</p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The
Long-Document Transformer. <a href="https://doi.org/10.48550/arXiv.2004.05150" class="external-link uri">https://doi.org/10.48550/arXiv.2004.05150</a></p>
<p>Berding, F., &amp; Pargmann, J. (2022). Iota Reliability Concept of
the Second Generation. Berlin: Logos. <a href="https://doi.org/10.30819/5581" class="external-link uri">https://doi.org/10.30819/5581</a></p>
<p>Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A.,
&amp; Rebmann, K. (2022). Performance and Configuration of Artificial
Intelligence in Educational Settings.: Introducing a New Reliability
Concept Based on Content Analysis. Frontiers in Education, 1–21. <a href="https://doi.org/10.3389/feduc.2022.818365" class="external-link uri">https://doi.org/10.3389/feduc.2022.818365</a></p>
<p>Campesato, O. (2021). Natural Language Processing Fundamentals for
Developers. Mercury Learning &amp; Information. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713</a></p>
<p>Cascante-Bonilla, P., Tan, F., Qi, Y. &amp; Ordonez, V. (2020).
Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised
Learning. <a href="https://doi.org/10.48550/arXiv.2001.06001" class="external-link uri">https://doi.org/10.48550/arXiv.2001.06001</a></p>
<p>Chollet, F., Kalinowski, T., &amp; Allaire, J. J. (2022). Deep
learning with R (Second edition). Manning Publications Co. <a href="https://learning.oreilly.com/library/view/-/9781633439849/?ar" class="external-link uri">https://learning.oreilly.com/library/view/-/9781633439849/?ar</a></p>
<p>Dai, Z., Lai, G., Yang, Y. &amp; Le, Q. V. (2020).
Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing. <a href="https://doi.org/10.48550/arXiv.2006.03236" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03236</a></p>
<p>Devlin, J., Chang, M.‑W., Lee, K., &amp; Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171–4186).
Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423" class="external-link uri">https://doi.org/10.18653/v1/N19-1423</a></p>
<p>Frochte, J. (2019). Maschinelles Lernen: Grundlagen und Algorithmen
in Python (2., aktualisierte Auflage). Hanser.</p>
<p>Ganesh, P., Chen, Y., Lou, X., Khan, M. A., Yang, Y., Sajjad, H.,
Nakov, P., Chen, D., &amp; Winslett, M. (2021). Compressing Large-Scale
Transformer-Based Models: A Case Study on BERT. Transactions of the
Association for Computational Linguistics, 9, 1061–1080. <a href="https://doi.org/10.1162/tacl_a_00413" class="external-link uri">https://doi.org/10.1162/tacl_a_00413</a></p>
<p>Gwet, K. L. (2014). Handbook of inter-rater reliability: The
definitive guide to measuring the extent of agreement among raters
(Fourth edition). Gaithersburg: STATAXIS.</p>
<p>He, P., Liu, X., Gao, J. &amp; Chen, W. (2020). DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. <a href="https://doi.org/10.48550/arXiv.2006.03654" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03654</a></p>
<p>Islam, A., Belhaouari, S. B., Rehman, A. U. &amp; Bensmail, H.
(2022). KNNOR: An oversampling technique for imbalanced datasets.
Applied Soft Computing, 115, 108288. <a href="https://doi.org/10.1016/j.asoc.2021.108288" class="external-link uri">https://doi.org/10.1016/j.asoc.2021.108288</a></p>
<p>Jadon, S., &amp; Garg, A. (2020). Hands-On One-shot Learning with
Python: Learn to Implement Fast and Accurate Deep Learning Models with
Fewer Training Samples Using Pytorch. Packt Publishing Limited. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6175328" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6175328</a></p>
<p>Krippendorff, K. (2019). Content Analysis: An Introduction to Its
Methodology (4th ed.). Los Angeles: SAGE.</p>
<p>Lane, H., Howard, C., &amp; Hapke, H. M. (2019). Natural language
processing in action: Understanding, analyzing, and generating text with
Python. Shelter Island: Manning.</p>
<p>Larusson, J. A., &amp; White, B. (Eds.). (2014). Learning Analytics:
From Research to Practice. New York: Springer. <a href="https://doi.org/10.1007/978-1-4614-3305-7" class="external-link uri">https://doi.org/10.1007/978-1-4614-3305-7</a></p>
<p>Lee, D.‑H. (2013). Pseudo-Label: The Simple and Efficient
Semi-Supervised Learning Method for Deep Neural Networks. CML 2013
Workshop: Challenges in Representation Learning.</p>
<p>Lee-Thorp, J., Ainslie, J., Eckstein, I. &amp; Ontanon, S. (2021).
FNet: Mixing Tokens with Fourier Transforms. <a href="https://doi.org/10.48550/arXiv.2105.03824" class="external-link uri">https://doi.org/10.48550/arXiv.2105.03824</a></p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). RoBERTa: A
Robustly Optimized BERT Pretraining Approach. <a href="https://doi.org/10.48550/arXiv.1907.11692" class="external-link uri">https://doi.org/10.48550/arXiv.1907.11692</a></p>
<p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp;
Potts, C. (2011). Learning Word Vectors for Sentiment Analysis. In D.
Lin, Y. Matsumoto, &amp; R. Mihalcea (Eds.), Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (pp. 142–150). Association for Computational
Linguistics. <a href="https://aclanthology.org/P11-1015" class="external-link uri">https://aclanthology.org/P11-1015</a></p>
<p>Oreshkin, B. N., Rodriguez, P., &amp; Lacoste, A. (2018). TADAM: Task
dependent adaptive metric for improved few-shot learning. Advance online
publication. <a href="https://doi.org/10.48550/arXiv.1805.10123" class="external-link uri">https://doi.org/10.48550/arXiv.1805.10123</a></p>
<p>Papilloud, C., &amp; Hinneburg, A. (2018). Qualitative Textanalyse
mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler.
Wiesbaden: Springer. <a href="https://doi.org/10.1007/978-3-658-21980-2" class="external-link uri">https://doi.org/10.1007/978-3-658-21980-2</a></p>
<p>Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., &amp; Dehak,
N. (2019). Hierarchical Transformers for Long Document Classification.
In 2019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU) (pp. 838–844). IEEE. <a href="https://doi.org/10.1109/ASRU46091.2019.9003958" class="external-link uri">https://doi.org/10.1109/ASRU46091.2019.9003958</a></p>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe:
Global Vectors for Word Representation. Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing. <a href="https://aclanthology.org/D14-1162.pdf" class="external-link uri">https://aclanthology.org/D14-1162.pdf</a></p>
<p>Ranjan, &amp; Chitta. (2019). Build the right Autoencoder — Tune and
Optimize using PCA principles.: Part I. <a href="https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b" class="external-link uri">https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b</a></p>
<p>Schreier, M. (2012). Qualitative Content Analysis in Practice. Los
Angeles: SAGE.</p>
<p>Snell, J., Swersky, K., &amp; Zemel, R. S. (2017). Prototypical
Networks for Few-shot Learning. <a href="https://doi.org/10.48550/arXiv.1703.05175" class="external-link uri">https://doi.org/10.48550/arXiv.1703.05175</a></p>
<p>Song, K., Tan, X., Qin, T., Lu, J. &amp; Liu, T.‑Y. (2020). MPNet:
Masked and Permuted Pre-training for Language Understanding. <a href="https://doi.org/10.48550/arXiv.2004.09297" class="external-link uri">https://doi.org/10.48550/arXiv.2004.09297</a></p>
<p>Tunstall, L., Werra, L. von, Wolf, T., &amp; Géron, A. (2022).
Natural language processing with transformers: Building language
applications with hugging face (Revised edition). Heidelberg:
O’Reilly.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention Is All
You Need. <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link uri">https://doi.org/10.48550/arXiv.1706.03762</a></p>
<p>Warner, B., Chaffin, A., Clavié, B., Weller, O., Hallström, O.,
Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T.,
Cooper, N., Adams, G., Howard, J. &amp; Poli, I. (2024). Smarter,
Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory
Efficient, and Long Context Finetuning and Inference. <a href="https://doi.org/10.48550/arXiv.2412.13663" class="external-link uri">https://doi.org/10.48550/arXiv.2412.13663</a></p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A.,
Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa,
H., . . . Dean, J. (2016). Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation. <a href="https://doi.org/10.48550/arXiv.1609.08144" class="external-link uri">https://doi.org/10.48550/arXiv.1609.08144</a></p>
<p>Zhang, X., Nie, J., Zong, L., Yu, H., &amp; Liang, W. (2019). One
Shot Learning with Margin. In Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang,
&amp; S.-J. Huang (Eds.), Lecture Notes in Computer Science. Advances in
Knowledge Discovery and Data Mining (Vol. 11440, pp. 305–317). Springer
International Publishing. <a href="https://doi.org/10.1007/978-3-030-16145-3_24" class="external-link uri">https://doi.org/10.1007/978-3-030-16145-3_24</a></p>
<p>ou, L. (2023). Meta-Learning: Theory, Algorithms and Applications.
Elsevier Science &amp; Technology. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=7134465" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=7134465</a></p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Berding Florian, Tykhonova Yuliia.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>

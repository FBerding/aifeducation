<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="aifeducation">
<title>03 Using R syntax • aifeducation</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="03 Using R syntax">
<meta property="og:description" content="aifeducation">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">aifeducation</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.4</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/aifeducation.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/aifeducation.html">01 Get started</a>
    <a class="dropdown-item" href="../articles/gui_aife_studio.html">02a Aifeducation Studio</a>
    <a class="dropdown-item" href="../articles/classification_tasks.html">02b Classification tasks</a>
    <a class="dropdown-item" href="../articles/sharing_and_publishing.html">03 Sharing and Using Trained AI/Models</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/cran/aifeducation/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>03 Using R syntax</h1>
                        <h4 data-toc-skip class="author">Florian
Berding, Julia Pargmann, Andreas Slopinski, Elisabeth Riebenbauer, Karin
Rebmann</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/cran/aifeducation/blob/HEAD/vignettes/classification_tasks.Rmd" class="external-link"><code>vignettes/classification_tasks.Rmd</code></a></small>
      <div class="d-none name"><code>classification_tasks.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-and-overview">1 Introduction and Overview<a class="anchor" aria-label="anchor" href="#introduction-and-overview"></a>
</h2>
<div class="section level3">
<h3 id="preface">1.1 Preface<a class="anchor" aria-label="anchor" href="#preface"></a>
</h3>
<p>This vignette introduces the package <em>aifeducation</em> and its
usage with <em>R</em> syntax. For users who are unfamiliar with
<em>R</em> or those who do not have coding skills in relevant languages
(e.g., python) we recommend to start with the graphical user interface
<em>Aifeducation - Studio</em> which is described in the vignette <a href="gui_aife_studio.html">02 Using the graphical user interface
Aifeducation - Studio</a>.</p>
<p>We assume that <em>aifeducation</em> is installed as described in
vignette <a href="https://fberding.github.io/aifeducation/articles/aifeducation.html" class="external-link">01
Get Started</a>. The introduction starts with a brief explanation of
basic concepts, which are necessary to work with this package.</p>
</div>
<div class="section level3">
<h3 id="basic-concepts">1.2 Basic Concepts<a class="anchor" aria-label="anchor" href="#basic-concepts"></a>
</h3>
<p>In the educational and social sciences, assigning an observation to
scientific concepts is an important task that allows researchers to
understand an observation, to generate new insights, and to derive
recommendations for research and practice.</p>
<p>In educational science, several areas deal with this kind of task.
For example, diagnosing students’ characteristics is an important aspect
of a teachers’ profession and necessary to understand and promote
learning. Another example is the use of learning analytics, where data
about students is used to provide learning environments adapted to their
individual needs. On another level, educational institutions such as
schools and universities can use this information for data-driven
performance decisions (Laurusson &amp; White 2014) as well as where and
how to improve it. In any case, a real-world observation is aligned with
scientific models to use scientific knowledge as a technology for
improved learning and instruction.</p>
<p>Supervised machine learning is one concept that allows a link between
real-world observations and existing scientific models and theories
(Berding et al. 2022). For the educational sciences, this is a great
advantage because it allows researchers to use the existing knowledge
and insights to apply AI. The drawback of this approach is that the
training of AI requires both information about the real world
observations and information on the corresponding alignment with
scientific models and theories.</p>
<p>A valuable source of data in educational science are written texts,
since textual data can be found almost everywhere in the realm of
learning and teaching (Berding et al. 2022). For example, teachers often
require students to solve a task which they provide in a written form.
Students have to create a solution for the tasks which they often
document with a short written essay or a presentation. This data can be
used to analyze learning and teaching. Teachers’ written tasks for their
students may provide insights into the quality of instruction while
students’ solutions may provide insights into their learning outcomes
and prerequisites.</p>
<p>AI can be a helpful assistant in analyzing textual data since the
analysis of textual data is a challenging and time-consuming task for
humans.</p>
<blockquote>
<p>Please note that an introduction to content analysis, natural
language processing or machine learning is beyond the scope of this
vignette. If you would like to learn more, please refer to the cited
literature.</p>
</blockquote>
<p>Before we start, it is necessary to introduce a definition of our
understanding of some basic concepts, since applying AI to educational
contexts means to combine the knowledge of different scientific
disciplines using different, sometimes overlapping concepts. Even within
a single research area, concepts are not unified. Figure 1 illustrates
this package’s understanding.</p>
<div class="float">
<img src="img_articles/classif_fig_01.png" style="width:100.0%" alt="Figure 1: Understanding of Central Concepts"><div class="figcaption">Figure 1: Understanding of Central
Concepts</div>
</div>
<p>Since <em>aifeducation</em> looks at the application of AI for
classification tasks from the perspective of the empirical method of
content analysis, there is some overlapping between the concepts of
content analysis and machine learning. In content analysis, a phenomenon
like performance or colors can be described as a scale/dimension which
is made up by several categories (e.g. Schreier 2012, pp. 59). In our
example, an exam’s performance (scale/dimension) could be “good”,
“average” or “poor”. In terms of colors (scale/dimension) categories
could be “blue”, “green”, etc. Machine learning literature uses other
words to describe this kind of data. In machine learning, “scale” and
“dimension” correspond to the term “label” while “categories” refer to
the term “classes” (Chollet, Kalinowski &amp; Allaire 2022, p. 114).</p>
<p>With these clarifications, classification means that a text is
assigned to the correct category of a scale or, respectively, that the
text is labeled with the correct class. As Figure 2 illustrates, two
kinds of data are necessary to train an AI to classify text in line with
supervised machine learning principles.</p>
<div class="float">
<img src="img_articles/classif_fig_02.png" style="width:100.0%" alt="Figure 2: Basic Structure of Supervised Machine Learning"><div class="figcaption">Figure 2: Basic Structure of Supervised Machine
Learning</div>
</div>
<p>By providing AI with both the textual data as input data and the
corresponding information about the class as target data, AI can learn
which texts imply a specific class or category. In the above exam
example, AI can learn which texts imply a “good”, an “average” or a
“poor” judgment. After training, AI can be applied to new texts and
predict the most likely class of every new text. The generated class can
be used for further statistical analysis or to derive recommendations
about learning and teaching.</p>
<p>In use cases as described in this vignette, AI has to “understand”
natural language: „Natural language processing is an area of research in
computer science and artificial intelligence (AI) concerned with
processing natural languages such as English and Mandarin. This
processing generally involves translating natural language into data
(numbers) that a computer can use to learn about the world. (…)” (Lane ,
Howard &amp; Hapke 2019, p. 4)</p>
<p>Thus, the first step is to transform raw texts into a a form that is
usable for a computer, hence raw texts must be transformed into numbers.
In modern approaches, this is usually done through word embeddings.
Campesato (2021, p. 102) describes them as “the collective name for a
set of language modeling and feature learning techniques (…) where words
or phrases from the vocabulary are mapped to vectors of real numbers.”
The definition of a word vector is similar: „Word vectors represent the
semantic meaning of words as vectors in the context of the training
corpus.” (Lane, Howard &amp; Hapke 2019, p. 191). In the next step, the
words or text embeddings can be used as input data and the labels as
target data for training AI to classify a text.</p>
<p>In <em>aifeducation,</em> these steps are covered with three
different types of models, as shown in Figure 3.</p>
<div class="float">
<img src="img_articles/classif_model_hierachy.png" style="width:100.0%" alt="Figure 3: Modells Types in aifeducation"><div class="figcaption">Figure 3: Modells Types in aifeducation</div>
</div>
<ul>
<li><p><strong>Base Models:</strong> The base models are the models
which contain the capacities to understand natural language. In general,
these are transformers such as BERT, RoBERTa, etc. A huge number of
pre-trained models can be found on <a href="https://huggingface.co/" class="external-link">Huggingface</a>.</p></li>
<li><p><strong>Text Embedding Models:</strong> The modes are built on
top of base models and store directions on how to use these base models
for converting raw texts into sequences of numbers. Please note that the
same base model can be used to create different text embedding
models.</p></li>
<li><p><strong>Classifiers:</strong> Classifiers are used on top of a
text embedding model. They are used to classify a text into
categories/classes based on the numeric representation provided by the
corresponding text embedding model. Please note that a text embedding
model can be used to create different classifiers (e.g. one classifier
for colors, one classifier to estimate the quality of a text,
etc.).</p></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="start-working">2 Start Working<a class="anchor" aria-label="anchor" href="#start-working"></a>
</h2>
<div class="section level3">
<h3 id="starting-a-new-session">2.1 Starting a New Session<a class="anchor" aria-label="anchor" href="#starting-a-new-session"></a>
</h3>
<p>Before you can work with <em>aifeducation</em> you must set up a new
<em>R</em> session. First, it is necessary that you load the library.
Second, you must set up python via reticulate. In case you installed
python as suggested in vignette <a href="aifeducation.html">01 Get
started</a> you may start a new session like this:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/use_python.html" class="external-link">use_condaenv</a></span><span class="op">(</span>condaenv <span class="op">=</span> <span class="st">"aifeducation"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/aifeducation/" class="external-link">aifeducation</a></span><span class="op">)</span></span></code></pre></div>
<p>Next you have to choose the machine learning framework you would like
to use. You can set the framework for the complete session with</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#For tensorflow</span></span>
<span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">set_global_ml_backend</span><span class="op">(</span><span class="st">"tensorflow"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/set_transformers_logger.html">set_transformers_logger</a></span><span class="op">(</span><span class="st">"ERROR"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#For PyTorch</span></span>
<span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">set_global_ml_backend</span><span class="op">(</span><span class="st">"pytorch"</span><span class="op">)</span></span></code></pre></div>
<p>Setting the global machine learning framework is only for
convenience. You can change the framework at any time during a session
by calling this method again or by setting the argument ‘ml_framework’
of methods and functions manually.</p>
<p>In the case that you would like to use <em>tensorflow</em> now is a
good time to configure that backend, since some configurations can only
be done <strong>before</strong> <em>tensorflow</em> is used the first
time.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#if you would like to use only cpus</span></span>
<span><span class="fu"><a href="../reference/set_config_cpu_only.html">set_config_cpu_only</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#if you have a graphic device with low memory</span></span>
<span><span class="fu"><a href="../reference/set_config_gpu_low_memory.html">set_config_gpu_low_memory</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#if you would like to reduce the tensorflow output to errors</span></span>
<span><span class="fu"><a href="../reference/set_config_os_environ_logger.html">set_config_os_environ_logger</a></span><span class="op">(</span>level <span class="op">=</span> <span class="st">"ERROR"</span><span class="op">)</span></span></code></pre></div>
<blockquote>
<p><strong>Note:</strong> Please remember: Every time you start a new
session in <em>R</em> you have to to set the correct conda environment,
to load the library <em>aifeducation</em>, and to chose your machine
learning framework.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="reading-texts-into-r">2.2 Reading Texts into <em>R</em><a class="anchor" aria-label="anchor" href="#reading-texts-into-r"></a>
</h3>
<p>For most applications of <em>aifeducation</em> it’s necessary to read
the text you would like to use into <em>R</em>. For this task, several
packages are available on CRAN. Our experience has been good with the
package <a href="https://cran.r-project.org/package=readtext" class="external-link">readtext</a> since it
allows you to process different kind of sources for textual data. Please
refer to readtext’s documentation for more details. If you have not
installed this package on your machine, you can request it by</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"readtext"</span><span class="op">)</span></span></code></pre></div>
<p>For example, if you have stored your texts in an excel sheet with two
columns (<em>texts</em> for the texts and <em>id</em> for the texts’ id)
you can read the data by</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#for excel files</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"text_data.xlsx"</span>,</span>
<span>  text_field <span class="op">=</span> <span class="st">"texts"</span>,</span>
<span>  docid_field <span class="op">=</span> <span class="st">"id"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Here it is crucial that you pass the file path to <code>file</code>
and the name of the column for the texts to <code>text_field</code> and
the name of the column for the id to <code>docid_field</code>.</p>
<p>In other cases you may have stored each text in a separate file
(e.g., .txt or .pdf). For these cases you can pass the directory of the
files and read the data. In the following example the files are stored
in the directory “data”.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#read all files with the extension .txt in the directory data</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"data/*.txt"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co">#read all files with the extension .pdf in the directory data</span></span>
<span><span class="va">textual_data</span><span class="op">&lt;-</span><span class="fu">readtext</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/readtext/man/readtext.html" class="external-link">readtext</a></span><span class="op">(</span></span>
<span>  file<span class="op">=</span><span class="st">"data/*.pdf"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>If you read texts for several files you do not need to specify the
arguments <code>docid_field</code> and <code>text_field</code>. The id
of the texts is automatically set to the file names.</p>
<p>After the text is read we recommend to do some text cleaning.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#remove multiple spaces and new lines</span></span>
<span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span><span class="op">=</span><span class="fu">stringr</span><span class="fu">::</span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_replace.html" class="external-link">str_replace_all</a></span><span class="op">(</span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span>,pattern <span class="op">=</span> <span class="st">"[:space:]{1,}"</span>,replacement <span class="op">=</span> <span class="st">" "</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#remove hyphenation</span></span>
<span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span><span class="op">=</span><span class="fu">stringr</span><span class="fu">::</span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_replace.html" class="external-link">str_replace_all</a></span><span class="op">(</span><span class="va">textual_data</span><span class="op">$</span><span class="va">text</span>,pattern <span class="op">=</span> <span class="st">"-(?=[:space:])"</span>,replacement <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span></code></pre></div>
<p>Please refer to the documentation of the function readtext within the
readtext library for more information.</p>
</div>
<div class="section level3">
<h3 id="example-data-for-this-vignette">2.3 Example Data for this Vignette<a class="anchor" aria-label="anchor" href="#example-data-for-this-vignette"></a>
</h3>
<p>To illustrate the steps in this vignette, we cannot use data from
educational settings since these data is generally protected by privacy
policies. Therefore, we use a subset of the Standford Movie Review
Dataset provided by Maas et al. (2011) which is part of the package. You
can access the data set with <code>imdb_movie_reviews</code>.</p>
<p>We now have a data set with three columns. The first contains
contains the raw text, the second contains the rating of the movie
(positive or negative), and the third column the ID of the movie review.
About 200 reviews imply a positive rating of a movie and about 100 imply
a negative rating.</p>
<p>For this tutorial, we modify this data set by setting about 100
positive reviews to <code>NA</code>, indicating that these reviews are
not labeled.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_data</span><span class="op">=</span><span class="va">imdb_movie_reviews</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">201</span><span class="op">:</span><span class="fl">300</span><span class="op">)</span><span class="op">]</span><span class="op">=</span><span class="cn">NA</span></span>
<span><span class="va">example_targets</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; neg pos </span></span>
<span><span class="co">#&gt; 100 100</span></span></code></pre></div>
<p>We will now use this data to show you how to use the different
objects and functions in <em>aifeducation</em>.</p>
</div>
</div>
<div class="section level2">
<h2 id="base-models">3 Base Models<a class="anchor" aria-label="anchor" href="#base-models"></a>
</h2>
<div class="section level3">
<h3 id="overview">3.1 Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h3>
<p>Base models are the foundation of all further models in
<em>aifeducation</em>. At the moment, these are transformer models such
as BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version
2 (He et al. 2020), Funnel-Transformer (Dai et al. 2020), and Longformer
(Beltagy, Peters &amp; Cohan 2020). In general, these models are trained
on a large corpus of general texts in the first step. In the next step,
the models are fine-tuned to domain-specific texts and/or fine-tuned for
specific tasks. Since the creation of base models requires a huge number
of texts resulting in high computational time, it is recommended to use
pre-trained models. These can be found on <a href="https://huggingface.co/" class="external-link">Huggingface</a>. Sometimes, however, it
is more straightforward to create a new model to fit a specific purpose.
Aifeducation Studio supports the opportunity to both create and
train/fine-tune base models.</p>
</div>
<div class="section level3">
<h3 id="creation-of-base-models">3.2 Creation of Base Models<a class="anchor" aria-label="anchor" href="#creation-of-base-models"></a>
</h3>
<p>Every transformer model is composed of two parts: 1) the tokenizer
which splits raw texts into smaller pieces to model a large number of
words with a limited, small number of tokens and 2) the neural network
that is used to model the capabilities for understanding natural
language.</p>
<p>At the beginning you can choose between the different supported
transformer architectures. Depending on the architecture, you have
different options determining the shape of your neural network. For this
vignette we use a BERT (Devlin et al. 2019) model which can be created
with the function <code>create_bert_model</code>.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/create_bert_model.html">create_bert_model</a></span><span class="op">(</span></span>
<span>    ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>    model_dir <span class="op">=</span> <span class="st">"my_own_transformer"</span>,</span>
<span>    vocab_raw_texts<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>    vocab_size<span class="op">=</span><span class="fl">30522</span>,</span>
<span>    vocab_do_lower_case<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>    max_position_embeddings<span class="op">=</span><span class="fl">512</span>,</span>
<span>    hidden_size<span class="op">=</span><span class="fl">768</span>,</span>
<span>    num_hidden_layer<span class="op">=</span><span class="fl">12</span>,</span>
<span>    num_attention_heads<span class="op">=</span><span class="fl">12</span>,</span>
<span>    intermediate_size<span class="op">=</span><span class="fl">3072</span>,</span>
<span>    hidden_act<span class="op">=</span><span class="st">"gelu"</span>,</span>
<span>    hidden_dropout_prob<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>    sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>    sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>    sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>    sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>    trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>First, the function receives the machine learning framework you chose
at the start of the session. However, you can change this by setting
<code>ml_framework="tensorflow"</code> or by
<code>ml_framework="pytorch"</code>.</p>
<p>For this function to work, you must provide a path to a directory
where your new transformer should be saved (<code>model_dir</code>).
Furthermore, you must provide raw texts. These texts are
<strong>not</strong> used for training the transformer but for training
the vocabulary. The maximum size of the vocabulary is determined by
<code>vocab_size</code>. Modern tokenizers such as <em>WordPiece</em>
(Wu et al. 2016) use algorithms that splits tokens into smaller
elements, allowing them to build a huge number of words with a small
number of elements. Thus, even with only small number of about 30,000
tokens, they are able to represent a very large number of words.</p>
<p>The other parameters allow you to customize your BERT model. For
example, you could increase the number of hidden layers from 12 to 24 or
reduce the hidden size from 768 to 256, allowing you to build and to
test larger or smaller models.</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">Optimal model
configuration</a> provides details on how to configurate a base
model.</p>
</blockquote>
<p>Please note that with <code>max_position_embeddings</code> you
determine how many tokens your transformer can process. If your text has
more tokens, these tokens are ignored. However, if you would like to
analyze long documents, please avoid to increase this number too
significantly because the computational time does not increase in a
linear way but quadratic (Beltagy, Peters &amp; Cohan 2020). For long
documents you can use another architecture of BERT (e.g. Longformer from
Beltagy, Peters &amp; Cohan 2020) or split a long document into several
chunks which are used sequentially for classification (e.g., Pappagari
et al. 2019). Using chunks is supported by <em>aifedcuation</em> for all
models.</p>
<p>Since creating a transformer model is energy consuming
<em>aifeducation</em> allows you to estimate its ecological impact with
help of the python library <code>codecarbon</code>. Thus,
<code>sustain_track</code> is set to <code>TRUE</code> by default. If
you use the sustainability tracker you must provide the alpha-3 code for
the country where your computer is located (e.g., “CAN”=“Canada”,
“Deu”=“Germany”). A list with the codes can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For USA and Canada you can additionally specify a region
by setting <code>sustain_region</code>. Please refer to the
documentation of codecarbon for more information.</p>
<p>After calling the function, you will find your new model in your
model directory.</p>
</div>
<div class="section level3">
<h3 id="traintune-a-base-model">3.3 Train/Tune a Base Model<a class="anchor" aria-label="anchor" href="#traintune-a-base-model"></a>
</h3>
<p>If you would like to train a new base model (see section 3.2) for the
first time or want to adapt a pre-trained model to a domain-specific
language or task, you have to can call the corresponding training
function. In case of a BERT model this is
<code><a href="../reference/train_tune_bert_model.html">train_tune_bert_model()</a></code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/train_tune_bert_model.html">train_tune_bert_model</a></span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"my_own_transformer_trained"</span>,</span>
<span>  model_dir_path <span class="op">=</span> <span class="st">"my_own_transformer"</span>,</span>
<span>  raw_texts <span class="op">=</span> <span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  p_mask<span class="op">=</span><span class="fl">0.15</span>,</span>
<span>  whole_word<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  val_size<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  n_epoch<span class="op">=</span><span class="fl">1</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">12</span>,</span>
<span>  chunk_size<span class="op">=</span><span class="fl">250</span>,</span>
<span>  n_workers<span class="op">=</span><span class="fl">1</span>,</span>
<span>  multi_process<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>  sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>Here it is important that you provide the path to the directory where
your new transformer is stored. Furthermore, it is important that you
provide <em>another</em> directory where your trained transformer should
be saved to avoid reading and writing collisions.</p>
<p>Now, the provided raw data is used to train your model. In the case
of a BERT model the learning objective is <em>Masked Language
Modeling</em>. Other models may use other learning objectives. Please
refer to the documentation for more detials on every model.</p>
<p>First, you can set the length of token sequences with
<code>chunk_size</code>. With <code>whole_word</code> you can choose
between masking single tokens or masking complete words (Please remember
that modern tokenizers split words into several tokens. Thus, tokens and
words are not forced to match each other directly). With
<code>p_mask</code> you can determine how many tokens should be masked.
Finally, with <code>val_size</code>, you set how many chunks of tokens
should be used for the validation sample.</p>
<p>Please remember to set the correct alpha-3 code for tracking the
ecological impact of training your model
(<code>sustain_iso_code</code>).</p>
<p>If you work on a machine and your graphic device only has small
memory, please reduce the batch size significantly. We also recommend to
change the usage of memory with <code><a href="../reference/set_config_gpu_low_memory.html">set_config_gpu_low_memory()</a></code>
at the beginning of the session if you use <em>tensorflow</em> as
framework.</p>
<p>After the training finishes, you can find the transformer ready to
use in your output_directory. Now you are able to create a text
embedding model.</p>
<p>Again you can change the machine learning framework by setting
<code>ml_framework="tensorflow"</code> or by
<code>ml_framework="pytorch"</code>. If you do not change this argument
the framework you chose at the beginning is used.</p>
</div>
</div>
<div class="section level2">
<h2 id="text-embedding-models">4 Text Embedding Models<a class="anchor" aria-label="anchor" href="#text-embedding-models"></a>
</h2>
<div class="section level3">
<h3 id="introduction">4.1 Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>The text embedding model is the interface to <em>R</em> in
<em>aifeducation</em>. In order to create a new model, you need a base
model that provides the ability to understand natural language. A text
embedding model is stored as an object of class
<code>TextEmbeddingModel</code>. This object contains all relevant
information for transforming raw texts into a numeric representation
that can be used for machine learning.</p>
<p>In <em>aifedcuation</em>, the transformation of raw texts into
numbers is a separate step from downstream tasks such as classification.
This is to reduce computational time on machines with low performance.
By separating text embedding from other tasks, the text embedding has to
be calculated only once and can be used for different tasks at the same
time. Another advantage is that the training of the downstream tasks
involves only the downstream tasks an not the parameters of the
embedding model, making training less time-consuming, thus decreasing
computational intensity. Finally, this approach allows the analysis of
long documents by applying the same algorithm to different parts.</p>
<p>The text embedding model provides a unified interface: After creating
the model with different methods, the handling of the model is always
the same.</p>
</div>
<div class="section level3">
<h3 id="create-a-text-embedding-model">4.2 Create a Text Embedding Model<a class="anchor" aria-label="anchor" href="#create-a-text-embedding-model"></a>
</h3>
<p>First you have to choose the base model that should form the
foundation of your new text embedding model.Since we use a BERT model in
our example, we have to set <code>method = "bert"</code>.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TextEmbeddingModel.html">TextEmbeddingModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  model_name<span class="op">=</span><span class="st">"bert_embedding"</span>,</span>
<span>  model_label<span class="op">=</span><span class="st">"Text Embedding via BERT"</span>,</span>
<span>  model_version<span class="op">=</span><span class="st">"0.0.1"</span>,</span>
<span>  model_language<span class="op">=</span><span class="st">"english"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"bert"</span>,</span>
<span>  max_length <span class="op">=</span> <span class="fl">512</span>,</span>
<span>  chunks<span class="op">=</span><span class="fl">4</span>,</span>
<span>  overlap<span class="op">=</span><span class="fl">30</span>,</span>
<span>  emb_layer_min<span class="op">=</span><span class="st">"middle"</span>,</span>
<span>  emb_layer_max<span class="op">=</span><span class="st">"2_3_layer"</span>,</span>
<span>  emb_pool_type<span class="op">=</span><span class="st">"average"</span>,</span>
<span>  model_dir<span class="op">=</span><span class="st">"my_own_transformer_trained"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Next, you have to provide the directory where your base model is
stored. In this example this would be
<code>model_dir="my_own_transformer_trained</code>. Of course you can
use any other pre-trained model from Huggingface which addresses your
needs.</p>
<p>Using a BERT model for text embedding is not a problem since your
text does not provide more tokens than the transformer can process. This
maximal value is set in the configuration of the transformer (see
section 3.2). If the text produces more tokens the last tokens are
ignored. In some instances you might want to analyze long texts. In
these situations, reducing the text to the first tokens (e.g. only the
first 512 tokens) could result in a problematic loss of information. To
deal with these situations you can configure a text embedding model in
<em>aifecuation</em> to split long texts into several chunks which are
processed by the base model. The maximal number of chunks is set with
<code>chunks</code>. In our example above, the text embedding model
would split a text consisting of 1024 tokens into two chunks with every
chunk consisting of 512 tokens. For every chunk a text embedding is
calculated. As a result, you receive a sequence of embeddings. The first
embeddings characterizes the first part of the text and the second
embedding characterizes the second part of the text (and so on). Thus,
our example text embedding model is able to process texts with about
4*512=2048 tokens. This approach is inspired by the work by Pappagari et
al. (2019).</p>
<p>Since transformers are able to account for the context, it may be
useful to interconnect every chunk to bring context into the
calculations. This can be done with <code>overlap</code> to determine
how many tokens of the end of a prior chunk should be added to the next.
In our example the last 30 tokens of the prior chunks are added at the
beginning of the following chunk. This can help to add the correct
context of the text sections into the analysis. Altogether, this example
model can analyse a maximum of 512+(4-1)*(512-30)=1958 tokens of a
text.</p>
<p>Finally, you have to decide from which hidden layer or layers the
embeddings should be drawn. With <code>emb_layer_min</code> and
<code>emb_layer_max</code> you can decide over which layers the average
value for every token should be calculated. Please note that the
calculation considers all layers between <code>emb_layer_min</code> and
<code>emb_layer_max</code>. In their initial work, Devlin et al. (2019)
used the hidden states of different layers for classification.</p>
<p>With <code>emb_pool_type</code> you decide which tokens are used for
pooling within every layer. In the case of
<code>emb_pool_type="cls"</code> only the cls token is used. In the case
of <code>emb_pool_type="average"</code> all tokens within a layer are
averaged except padding tokens.</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">Optimal model
configuration</a> provides details on how to configurate a text
embedding model.</p>
</blockquote>
<p>After deciding about the configuration, you can use your model.</p>
<blockquote>
<p><strong>Note:</strong> With version 0.3.1 of aifeducation every
transformer can be used with both machine learning frameworks. Even the
pre-trained weights can be used across backends. However, in the future
models my be implemented that are available only for a specific
framework.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="transforming-raw-texts-into-embedded-texts">4.3 Transforming Raw Texts into Embedded Texts<a class="anchor" aria-label="anchor" href="#transforming-raw-texts-into-embedded-texts"></a>
</h3>
<p>To transform raw text into a numeric representation you only have to
use the <code>embed</code> method of your model. To do this, you must
provide the raw texts to <code>raw_text</code>. In addition, it is
necessary that you provide a character vector containing the ID of every
text. The IDs must be unique.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_embeddings</span><span class="op">&lt;-</span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">text</span>,</span>
<span>  doc_id<span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span>, </span>
<span>  trace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>The method <code>embed</code>creates an object of class
<code>EmbeddedText</code>. This is just a array consisting the embedding
of every text. The first dimension of the array refers to specific
texts, the second dimension refers to chunks/sequences, and the third
dimension refers to the features.</p>
<p>With the embedded texts you now have the input to train a new
classifier or to apply a pre-trained classifier for predicting
categories/classes. In the next chapter we will show you how to use
these classifiers. But before we start, we will show you how to save and
load your model.</p>
</div>
<div class="section level3">
<h3 id="saving-and-loading-text-embedding-models">4.4 Saving and Loading Text Embedding Models<a class="anchor" aria-label="anchor" href="#saving-and-loading-text-embedding-models"></a>
</h3>
<p>Saving a created text embedding model is very easy in
<em>aifeducation</em> by using the function <code>save_ai_model</code>.
This function provides a unique interface for all text embedding models.
For saving your work you can pass your model to <code>model</code> and
the directory where to save the model to <code>model_dir</code>. Please
do only pass the path of a directory and not the path of a file to this
function. Internally the function creates a new folder in the directory
where all files belonging to a model are stored.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">bert_modeling</span>, </span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models"</span>,</span>
<span>  dir_name<span class="op">=</span><span class="st">"model_transformer_bert"</span>,</span>
<span>  save_format<span class="op">=</span><span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>As you can see the text embedding model is saved within a directory
named “text_embedding_models”. Within this directory the function
creates a unique folder for every model if it does not exist. The name
of this folder is specified with <code>dir_name</code>.</p>
<p>If you set <code>dir_name=NULL</code> and
<code>append_ID=FALSE</code> the the name of the folder is created by
using the models’ name. If you change the argument
<code>append_ID</code> to <code>append_ID=TRUE</code> and set
<code>dir_name=NULL</code> the unique ID of the model is added to the
directory. The ID is added automatically to ensure that every model has
a unique name. This is important if you would like to share your work
with other persons.</p>
<blockquote>
<p>Since the files are stored with a special structure please do
<strong>not</strong> change the files manually.</p>
</blockquote>
<p>If you want to load your model, just call the function
<code>load_ai_model</code> and you can continue using your model.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/model_transformer_bert"</span>,</span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>With <code>ml_framework</code> you can decide which framework the
model should use. If you set <code>ml_framework="auto"</code> the models
will be initialized with the same framework during saving the model.
Please note that at the moment all implemented text embedding models can
be used with both frameworks. However, this may change in the
future.</p>
</div>
<div class="section level3">
<h3 id="sustainability">4.5 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability"></a>
</h3>
<p>In the case the underlying model was trained with an active
sustainability tracker (section 3.2 and 3.3) you can receive a table
showing you the energy consumption, CO2 emissions, and hardware used
during training by calling the method
<code>get_sustainability_data()</code>. For our example this would be
<code>bert_modeling$get_sustainability_data()</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="classifiers">5 Classifiers<a class="anchor" aria-label="anchor" href="#classifiers"></a>
</h2>
<div class="section level3">
<h3 id="create-a-classifier">5.1 Create a Classifier<a class="anchor" aria-label="anchor" href="#create-a-classifier"></a>
</h3>
<p>Classifiers are built on top of a text embedding model. You can
create a new classifier by calling
<code>TEClassifierRegular$new()</code>. The <code>TE</code> in the
object class refers to the idea that the classifiers uses text
embeddings instead of raw texts.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_targets</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">example_targets</span><span class="op">)</span><span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span></span>
<span></span>
<span><span class="va">classifier</span><span class="op">&lt;-</span><span class="va"><a href="../reference/TEClassifierRegular.html">TEClassifierRegular</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  ml_framework<span class="op">=</span><span class="va">aifeducation_config</span><span class="op">$</span><span class="fu">get_framework</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  name<span class="op">=</span><span class="st">"movie_review_classifier"</span>,</span>
<span>  label<span class="op">=</span><span class="st">"Classifier for Estimating a Postive or Negative Rating of Movie Reviews"</span>,</span>
<span>  text_embeddings<span class="op">=</span><span class="va">bert_embeddings</span>,</span>
<span>  targets<span class="op">=</span><span class="va">example_targets</span>,</span>
<span>  use_fe<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>  fe_features<span class="op">=</span><span class="fl">128</span>,</span>
<span>  fe_method<span class="op">=</span><span class="st">"lstm"</span>,</span>
<span>  fe_noise_factor<span class="op">=</span><span class="fl">0.2</span>,</span>
<span>  hidden<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">40</span>,<span class="fl">20</span><span class="op">)</span>,</span>
<span>  rec<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">40</span>,<span class="fl">40</span><span class="op">)</span>,</span>
<span>  rec_type<span class="op">=</span><span class="st">"gru"</span>,</span>
<span>  self_attention_heads<span class="op">=</span><span class="fl">1</span>,</span>
<span>  intermediate_size<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>  attention_type<span class="op">=</span><span class="st">"fourier"</span>,</span>
<span>  add_pos_embedding<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>  rec_dropout<span class="op">=</span><span class="fl">0.3</span>,</span>
<span>  repeat_encoder<span class="op">=</span><span class="fl">0</span>,</span>
<span>  dense_dropout<span class="op">=</span><span class="fl">0.3</span>,</span>
<span>  recurrent_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>  encoder_dropout<span class="op">=</span><span class="fl">0.3</span>,</span>
<span>  optimizer<span class="op">=</span><span class="st">"adam"</span><span class="op">)</span></span></code></pre></div>
<p>Similar to the text embedding model you should provide a name
(<code>name</code>) and a label (<code>label</code>) for your new
classifier. With <code>text_embeddings</code> you have to provide an
embedded text. The embedded texts is created with a text embedding model
as described in section 4. We here continue our example and use the
embedding produced by our BERT model.</p>
<p><code>targets</code> takes the target data for the supervised
learning. Please do not omit cases which have no category/class since
they can be used with a special training technique we will show you
later. It is very important that you provide the target data as factors.
Otherwise an error will occur. It is also important that you name your
elements of the factor. That is, the entries of the factor mus have
names that correspond to the IDs of the corresponding texts. Without
these names the method cannot match the input data (text embeddings) to
the target data.</p>
<p>With <code>use_fe</code> you can request a feature extractor. The aim
of the feature extractor is to reduce the number of dimensions of the
text embeddings and saving as much information as possible. Reducing the
number of dimensions can speed up training and inference and at the same
time increase the performance of your classifier. For more details
please refer to vignette <a href="model_configuration.html">Optimal
model configuration</a>. With <code>fe_features</code> you can decide to
which number of dimensions the text embeddings should be compressed.
<code>fe_method</code> allows you to select a model for the reduction
(“lstm” or “dense”). Sine all feature extractors in
<em>aifeducation</em> are based on denoising autoencoder you can decide
how many noise should be added during training the extractor
(<code>fe_noise_factor</code>).</p>
<p>With the other parameters you decide about the structure of your
classifier. Figure 4 illustrates this.</p>
<div class="float">
<img src="img_articles/classif_fig_04.png" style="width:100.0%" alt="Figure 4: Overview of Possible Structure of a Classifier"><div class="figcaption">Figure 4: Overview of Possible Structure of a
Classifier</div>
</div>
<p><code>hidden</code> takes a vector of integers, determining the
number of layers and the number of neurons. In our example, there two
dense layers. The first with 40 and the second with 20 neurons.
<code>rec</code> also takes a vector of integers determining the number
and size of the recurrent layers. In this example, we use two layer with
40 neurons each. With <code>rec_type</code> you can choose between two
types of recurrent layers. <code>rec_type="gru"</code> implements a
Gated Recurrent Unit (GRU) network and <code>rec_type="lstm"</code>
implements a Long Short-Term Memory layer.</p>
<p>Since the classifiers in <em>aifeducation</em> use a standardized
scheme for their creation, dense layers are used after the gru layers.
If you want to omit gru layers or dense layers, set the corresponding
argument to <code>NULL</code>.</p>
<p>If you use a text embedding model that processes more than one chunk
we would like to recommend to use recurrent layers since they are able
to use the sequential structure of your data. In all other cases you can
rely on dense layers only.</p>
<p>If you use text embeddings with more than one chunk, it is a good
idea to try self-attention layering in order to take the context of all
chunks into account. To add self-attention you have two choices:</p>
<ul>
<li><p>You can use the attention mechanism used in classic transformer
models as multihead attention (Vaswani et al. 2017). For this variant
you have to set <code>attention_type="multihead"</code>,
<code>repeat_encoder</code> to a value of at least 1, and
<code>self_attention_heads</code> to a value of at least 1.</p></li>
<li><p>Furthermore you can use the attention mechanism described in
Lee-Thorp et al. (2021) of the FNet model which allows much fast
computations at low accuracy costs. To use this kind of attention you
have to set <code>attention_type="fourier</code> and
<code>repeat_encoder</code> to a value of at least 1.</p></li>
</ul>
<p>With <code>repeat_encoder</code> you can chose how many times an
encoder layer should be added. The encoder is implemented as described
by Chollet, Kalinowski, and Allaire (2022, pp. 373) for both variants of
attention. In our example we have only 300 cases altogether and only 4
chunks. Thus, we do not use any encoder layer.</p>
<p>You can further extend the abilities of your network by adding
positional embeddings. Positional embeddings take care for the order of
your chunks. Thus, adding such a layer may increase performance if the
order of information is important. You can add this layer by setting
<code>add_pos_embedding=TRUE</code>. The layer is created as described
by Chollet, Kalinowski, and Allaire (2022, pp. 378).</p>
<blockquote>
<p>The vignette <a href="model_configuration.html">Optimal model
configuration</a> provides details on how to configurate a
classifier.</p>
</blockquote>
<p>Masking, normalization, and the creation of the input layer as well
as the output layer are done automatically.</p>
<p>After you have created a new classifier, you can begin training.</p>
<blockquote>
<p><strong>Note:</strong> In contrast to the text embedding models your
decision about the machine learning framework is more important since
the classifier can only be used with the framework you created and
trained the model.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="training-a-classifier">5.2 Training a Classifier<a class="anchor" aria-label="anchor" href="#training-a-classifier"></a>
</h3>
<p>To start the training of your classifier, you have to call the
<code>train</code> method. Similarly, for the creation of the
classifier, you must provide the text embedding to
<code>data_embeddings</code> and the categories/classes as target data
to <code>data_targets</code>. Please remember that
<code>data_targets</code> expects a <strong>named</strong> factor where
the names correspond to the IDs of the corresponding text embeddings.
Text embeddings and target data that cannot be matched are omitted from
training.</p>
<p>To train a classifier, it is necessary that you provide a path to
<code>dir_checkpoint</code>. This directory stores the best set of
weights during each training epoch. After training, these weights are
automatically used as final weights for the classifier.</p>
<p>For performance estimation, training splits the data into several
chunks based on cross-fold validation. The number of folds is set with
<code>data_folds</code>. In every case, one fold is not used for
training and serves as a <em>test</em> sample. The remaining data is
used to create a <em>training</em> and a <em>validation</em> sample. The
percentage of cases within each fold used as a validation sample is
determined with <code>data_val_size</code>. This sample is used to
determine the state of the model that generalizes best. All performance
values saved in the trained classifier refer to the test sample. This
data has never been used during training and provides a more realistic
estimation of a classifier`s performance.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_targets</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">example_data</span><span class="op">$</span><span class="va">label</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">example_targets</span><span class="op">)</span><span class="op">=</span><span class="va">example_data</span><span class="op">$</span><span class="va">id</span></span>
<span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span></span>
<span>   data_embeddings <span class="op">=</span> <span class="va">bert_embeddings</span>,</span>
<span>   data_targets <span class="op">=</span> <span class="va">example_targets</span>,</span>
<span>   data_folds<span class="op">=</span><span class="fl">5</span>,</span>
<span>   data_val_size<span class="op">=</span><span class="fl">0.25</span>,</span>
<span>   fe_epochs<span class="op">=</span><span class="fl">1000</span>,</span>
<span>   fe_val_size<span class="op">=</span><span class="fl">0.25</span>,</span>
<span>   balance_class_weights<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   balance_sequence_length<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   use_sc<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   sc_method<span class="op">=</span><span class="st">"dbsmote"</span>,</span>
<span>   sc_min_k<span class="op">=</span><span class="fl">1</span>,</span>
<span>   sc_max_k<span class="op">=</span><span class="fl">10</span>,</span>
<span>   use_pl<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   pl_max_steps<span class="op">=</span><span class="fl">5</span>,</span>
<span>   pl_max<span class="op">=</span><span class="fl">1.00</span>,</span>
<span>   pl_anchor<span class="op">=</span><span class="fl">1.00</span>,</span>
<span>   pl_min<span class="op">=</span><span class="fl">0.00</span>,</span>
<span>   sustain_track<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   sustain_iso_code<span class="op">=</span><span class="st">"DEU"</span>,</span>
<span>   sustain_region<span class="op">=</span><span class="cn">NULL</span>,</span>
<span>   sustain_interval<span class="op">=</span><span class="fl">15</span>,</span>
<span>   epochs<span class="op">=</span><span class="fl">60</span>,</span>
<span>   batch_size<span class="op">=</span><span class="fl">32</span>,</span>
<span>   dir_checkpoint<span class="op">=</span><span class="st">"training/classifier"</span>,</span>
<span>   trace<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>   keras_trace<span class="op">=</span><span class="fl">2</span>,</span>
<span>   pytorch_trace<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>You can further modify the training process with different arguments.
With <code>balance_class_weights=TRUE</code> the absolute frequencies of
the classes/categories is adjusted according to the ‘Inverse Class
Frequency’ method. This option should be activated if you have to deal
with imbalanced data.</p>
<p>With <code>balance_sequence_length=TRUE</code> you can increase
performance if you have to deal with texts that differ in their lengths
and have an imbalanced frequency. If this option is enabled, the loss is
adjusted to the absolute frequencies of length of your texts according
to the ‘Inverse Class Frequency’ method.</p>
<p><code>epochs</code> determines the maximal number of epochs. During
training, the model with the best balanced accuracy is saved and
used.</p>
<p><code>batch_size</code>sets the number of cases that should be
processed simultaneously. Please adjust this value to your machine’s
capacities. Please note that the batch size can have an impact on the
classifier’s performance.</p>
<p>Since <em>aifedcuation</em> tries to address the special needs in
educational and social science, some special training steps are
integrated into this method.</p>
<ul>
<li><p><strong>Synthetic Cases:</strong> In case of imbalanced data, it
is recommended to set <code>use_sc=TRUE</code>. Before training, a
number of synthetic units is created via different techniques. Currently
you can request <em>Basic Synthetic Minority Oversampling
Technique</em>, <em>Density-Bases Synthetic Minority Oversampling
Technique</em>, and <em>Adaptive Synthetic Sampling Approach for
Imbalanced Learning</em>. The aim is to create new cases that fill the
gap to the majority class. Multi-class problems are reduced to a two
class problem (class under investigation vs. each other) for generating
these units. If the technique allows to set the number of neighbors
during generation, you can configure the data generation with
<code>sc_min_k</code> and <code>sc_max_k</code>. The synthetic cases for
every class a generated for all <em>k</em> between <code>sc_min_k</code>
and <code>sc_max_k</code>. Every <em>k</em> contributes proportional to
the synthetic cases. To apply the addition of synthetic data, you have
to set <code>use_sc=TRUE</code></p></li>
<li><p><strong>Pseudo-Labeling:</strong> This technique is relevant if
you have labeled target data and a large number of unlabeled target
data. With the different parameter starting with “pl_”, you can
configure the process of pseudo-labeling. Implementation of
pseudo-labeling is based on Cascante-Bonilla et al. (2020). To apply
pseudo-labeling, you have to set <code>use_pl=TRUE</code>.
<code>pl_max=1.00</code>, <code>pl_anchor=1.00</code>, and
<code>pl_min=0.00</code> are used to describe the certainty of a
prediction. 0 refers to random guessing while 1 refers to perfect
certainty. <code>pl_anchor</code> is used as a reference value. The
distance to <code>pl_anchor</code> is calculated for every case. Then,
they are sorted with an increasing distance from <code>pl_anchor</code>.
The proportion of added pseudo-labeled data into training increases with
every step. The maximum number of steps is determined with
<code>pl_max_steps</code>.</p></li>
</ul>
<p>Figure 5 illustrates the training loop for the cases that all options
are set to <code>TRUE</code>.</p>
<div class="float">
<img src="img_articles/classif_fig_05.png" style="width:100.0%" alt="Figure 5: Overview of the Steps to Perform a Classification"><div class="figcaption">Figure 5: Overview of the Steps to Perform a
Classification</div>
</div>
<p>The example above applies the generation of synthetic cases and the
algorithm proposed by Cascante-Bonilla et al. (2020). For every fold the
training starts with generating synthetic cases to fill the gab between
the classes and the majority class. After this an initial training of
the classifiers starts. The trained classifier is used to predict
pseudo-labels for the unlabeled part of the data and adds 20% of the
cases with the highest certainty for their pseudo-labels to the training
data set. Now new synthetic cases are generated based on both the
labeled data and the new added pseudo-labeled data. The classifier is
re-initialized and trained again. After training, the classifier
predicts the potential labels of <em>all</em> originally unlabeled data
and adds 40% of the pseudo-labeled data to the training data with the
highest certainty. Again new synthetic cases are generated on both the
labeled and added pseudo-labeled data. The model is again re-initialized
and trained again until the maximum number of steps for pseudo labeling
(<code>pl_max_steps</code>) is reached. After this, the logarithm is
restated for the next fold until the number of folds
(<code>data_folds</code>) is reached. All of these steps are used only
to estimate the performance of the classifier to evaluate for the
classifier unknown data.</p>
<p>The last phase of the training begins after the last fold. In the
final training the data set is split only into a train and validation
set without a test set to provide the maximal amount of data for the
best performance in final training.</p>
<p>In the case options like the generation of synthetic cases
(<code>use_sc</code>) or pseudo-labeling (<code>use_pl</code>) are
disabled the training process is shorter.</p>
<p>In the case your classifier uses a feature extractor you can
configure the training process with <code>fe_epochs</code> and
<code>fe_val_size</code>. The first argument determines the maximal
number of training steps while the last determines the number of text
embeddings used as a validation sample. The model with the lowest loss
on the validation sample is applied for compressing the text
embeddings.</p>
<p>Since training a neural net is energy consuming <em>aifeducation</em>
allows you to estimate its ecological impact with help of the python
library <code>codecarbon</code>. Thus, <code>sustain_track</code> is set
to <code>TRUE</code> by default. If you use the sustainability tracker
you must provide the alpha-3 code for the country where your computer is
located (e.g., “CAN”=“Canada”, “Deu”=“Germany”). A list with the codes
can be found on <a href="https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3" class="external-link">wikipedia</a>.
The reason is that different countries use different sources and
techniques for generating their energy resulting in a specific impact on
CO2 emissions. For USA and Canada you can additionally specify a region
by setting <code>sustain_region</code>. Please refer to the
documentation of codecarbon for more information.</p>
<p>Finally, <code>trace</code>, <code>pytorch_trace</code>, and
<code>keras_trace</code> allow you to control how much information about
the training progress is printed to the console. Please note that
training the classifier can take some time.</p>
<p>Please note that after performance estimation, the final training of
the classifier makes use of all data available. That is, the test sample
is left empty.</p>
</div>
<div class="section level3">
<h3 id="evaluating-classifiers-performance">5.3 Evaluating Classifier’s Performance<a class="anchor" aria-label="anchor" href="#evaluating-classifiers-performance"></a>
</h3>
<p>After finishing training, you can evaluate the performance of the
classifier. For every fold, the classifier is applied to the test sample
and the results are compared to the true categories/classes. Since the
test sample is never part of the training, all performance measures
provide a more realistic idea of the classifier`s performance.</p>
<p>To support researchers in judging the quality of the predictions,
<em>aifeducation</em> utilizes several measures and concepts from
content analysis. These are</p>
<ul>
<li>Iota Concept of the Second Generation (Berding &amp; Pargmann
2022)</li>
<li>Krippendorff’s Alpha (Krippendorff 2019)</li>
<li>Percentage Agreement</li>
<li>Gwet’s AC1/AC2 (Gwet 2014)</li>
<li>Kendall’s coefficient of concordance W</li>
<li>Cohen’s Kappa unweighted</li>
<li>Cohen’s Kappa with equal weights</li>
<li>Cohen’s Kappa with squared weights</li>
<li>Fleiss’ Kappa for multiple raters without exact estimation</li>
</ul>
<p>You can access the concrete values by accessing the field
<code>reliability</code> which stores all relevant information. In this
list you will find the reliability values for every fold. In addition,
the reliability of every step within pseudo-labeling is reported.</p>
<p>The central estimates for the reliability values can be found via
<code>reliability$test_metric_mean</code>. In our example this would
be:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">test_metric_mean</span></span>
<span><span class="co">#&gt;              iota_index               min_iota2               avg_iota2 </span></span>
<span><span class="co">#&gt;               0.4900000               0.5367404               0.5726142 </span></span>
<span><span class="co">#&gt;               max_iota2               min_alpha               avg_alpha </span></span>
<span><span class="co">#&gt;               0.6084880               0.6300000               0.7250000 </span></span>
<span><span class="co">#&gt;               max_alpha       static_iota_index      dynamic_iota_index </span></span>
<span><span class="co">#&gt;               0.8200000               0.1179700               0.4152288 </span></span>
<span><span class="co">#&gt;          kalpha_nominal          kalpha_ordinal                 kendall </span></span>
<span><span class="co">#&gt;               0.4433395               0.4433395               0.7283896 </span></span>
<span><span class="co">#&gt;       kappa2_unweighted   kappa2_equal_weighted kappa2_squared_weighted </span></span>
<span><span class="co">#&gt;               0.4500000               0.4500000               0.4500000 </span></span>
<span><span class="co">#&gt;            kappa_fleiss    percentage_agreement       balanced_accuracy </span></span>
<span><span class="co">#&gt;               0.4362932               0.7250000               0.7250000 </span></span>
<span><span class="co">#&gt;                 gwet_ac           avg_precision              avg_recall </span></span>
<span><span class="co">#&gt;               0.4622700               0.7323169               0.7250000 </span></span>
<span><span class="co">#&gt;                  avg_f1 </span></span>
<span><span class="co">#&gt;               0.7181466</span></span></code></pre></div>
<p>Of particular interest are the values for alpha from the Iota Concept
since they represent a measure of reliability which is independent from
the frequency distribution of the classes/categories. The alpha values
describe the probability that a case of a specific class is recognized
as that specific class. As you can see, compared to the baseline model,
applying <em>Balanced Synthetic Cases increased</em> increases the
minimal value of alpha, reducing the risk to miss cases which belong to
a rare class (see row with “BSC”). On the contrary, the alpha values for
the major category decrease slightly, thus losing its unjustified bonus
from a high number of cases in the training set. This provides a more
realistic performance estimation of the classifier.</p>
<p>An addition, standard measures from machine learning are reported.
These are</p>
<ul>
<li>Precision</li>
<li>Recall</li>
<li>F1-Score</li>
</ul>
<p>You can acces these values as follows:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">standard_measures_mean</span></span>
<span><span class="co">#&gt;     precision recall        f1</span></span>
<span><span class="co">#&gt; neg 0.7345009   0.69 0.6995648</span></span>
<span><span class="co">#&gt; pos 0.7301329   0.76 0.7367284</span></span></code></pre></div>
<p>Finally, you can plot a coding stream scheme showing how the cases of
different classes are labeled. Here we use the package
<em>iotarelr</em>.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fberding.github.io/iotarelr/" class="external-link">iotarelr</a></span><span class="op">)</span></span>
<span><span class="fu">iotarelr</span><span class="fu">::</span><span class="fu"><a href="https://fberding.github.io/iotarelr/reference/plot_iota2_alluvial.html" class="external-link">plot_iota2_alluvial</a></span><span class="op">(</span><span class="va">classifier</span><span class="op">$</span><span class="va">reliability</span><span class="op">$</span><span class="va">iota_object_end_free</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="classification_tasks_files/figure-html/unnamed-chunk-21-1.png" alt="Figure 6: Coding Stream of the Classifier" width="700"><p class="caption">
Figure 6: Coding Stream of the Classifier
</p>
</div>
<p>Here you can see that a small number of negative reviews is treated
as a good review while a larger number of positive reviews is treated as
a bad review. Thus, the data for the major class (negative reviews) is
more reliable and valid as the the data for the minor class (positive
reviews).</p>
<p>Evaluating the performance of a classifier is a complex task and and
beyond the scope of this vignette. Instead, we would like to refer to
the cited literature of content analysis and machine learning if you
would like to dive deeper into this topic.</p>
</div>
<div class="section level3">
<h3 id="sustainability-1">5.4 Sustainability<a class="anchor" aria-label="anchor" href="#sustainability-1"></a>
</h3>
<p>In the case the classifier was trained with an active sustainability
tracker you can receive information on sustainability by calling
<code>classifier$get_sustainability_data()</code>.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">get_sustainability_data</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; $sustainability_tracked</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $date</span></span>
<span><span class="co">#&gt; [1] "Wed May 15 20:16:21 2024"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data</span></span>
<span><span class="co">#&gt; $sustainability_data$duration_sec</span></span>
<span><span class="co">#&gt; [1] 1415.552</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$co2eq_kg</span></span>
<span><span class="co">#&gt; [1] 0.002187042</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$cpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.004089835</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$gpu_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$ram_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.001894242</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sustainability_data$total_energy_kwh</span></span>
<span><span class="co">#&gt; [1] 0.005984076</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical</span></span>
<span><span class="co">#&gt; $technical$tracker</span></span>
<span><span class="co">#&gt; [1] "codecarbon"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$py_package_version</span></span>
<span><span class="co">#&gt; [1] "2.3.4"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_count</span></span>
<span><span class="co">#&gt; [1] 8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$cpu_model</span></span>
<span><span class="co">#&gt; [1] "11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_count</span></span>
<span><span class="co">#&gt; [1] NA</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$gpu_model</span></span>
<span><span class="co">#&gt; [1] NA</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $technical$ram_total_size</span></span>
<span><span class="co">#&gt; [1] 15.73279</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region</span></span>
<span><span class="co">#&gt; $region$country_name</span></span>
<span><span class="co">#&gt; [1] "Germany"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$country_iso_code</span></span>
<span><span class="co">#&gt; [1] "DEU"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $region$region</span></span>
<span><span class="co">#&gt; [1] NA</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="saving-and-loading-a-classifier">5.5 Saving and Loading a Classifier<a class="anchor" aria-label="anchor" href="#saving-and-loading-a-classifier"></a>
</h3>
<p>If you have created a classifier, saving and loading is very easy.
The process for saving a model is similar to the process for text
embedding models. You only have to pass the model and a directory path
to the function <code>save_ai_model</code>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/save_ai_model.html">save_ai_model</a></span><span class="op">(</span></span>
<span>  model<span class="op">=</span><span class="va">classifier</span>,</span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers"</span>,</span>
<span>  dir_name<span class="op">=</span><span class="st">"movie_classifier"</span>,</span>
<span>  save_format <span class="op">=</span> <span class="st">"default"</span>,</span>
<span>  append_ID<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>In contrast to text embedding models you can specify the additional
argument <code>save_format</code>. In the case of pytorch models this
arguments allows you to choose between
<code>save_format = "safetensors"</code> and
<code>save_format = "pt"</code>. We recommend to chose
<code>save_format = "safetensors"</code> since this is a safer method to
save your models. In the case of tensorflow models this argument allows
you to choose between <code>save_format = "keras"</code>,
<code>save_format = "tf"</code> and <code>save_format = "h5"</code>. We
recommend to chose <code>save_format = "keras"</code> since this is the
recommended format by keras. If you set
<code>save_format = "default"</code> .safetensors is used for pytorch
models and .keras is used for tensorflow models.</p>
<p>If you would like to load a model you can call the function
<code>load_ai_model</code>.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers/movie_classifier"</span><span class="op">)</span></span></code></pre></div>
<blockquote>
<p><strong>Note:</strong> Classifiers depend on the framework which was
used during creation. Thus, a classifier is always initalized with its
original framework. The argument <code>ml_framework</code> has no
effect.</p>
</blockquote>
</div>
<div class="section level3">
<h3 id="predicting-new-data">5.6 Predicting New Data<a class="anchor" aria-label="anchor" href="#predicting-new-data"></a>
</h3>
<p>If you would like to apply your classifier to new data, two steps are
necessary. You must first transform the raw text into a numerical
expression by using <em>exactly</em> the same text embedding model that
was used for training your classifier (see section 4). In the case of
our example classifier we use our BERT model.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If our mode is not loaded</span></span>
<span><span class="va">bert_modeling</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"text_embedding_models/bert_embedding"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a numerical representation of the text</span></span>
<span><span class="va">text_embeddings</span><span class="op">&lt;-</span><span class="va">bert_modeling</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span></span>
<span>  raw_text <span class="op">=</span> <span class="va">textual_data</span><span class="op">$</span><span class="va">texts</span>,</span>
<span>  doc_id <span class="op">=</span> <span class="va">textual_data</span><span class="op">$</span><span class="va">doc_id</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">8</span>,</span>
<span>  trace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To transform raw texts into a numeric representation just pass the
raw texts and the IDs of every text to the method <code>embed</code> of
the loaded model. This is very easy if you used the package <a href="https://cran.r-project.org/package=readtext" class="external-link">readtext</a> to read
raw text from disk, since the object resulting from
<code>readtext</code> always stores the texts in the column “texts” and
the IDs in the column “doc_id”.</p>
<p>Depending on your machine, embedding raw texts may take some time. In
case you use a machine with a graphic device, it is possible that an
“out of memory” error occurs. In this case reduce the batch size. If the
error still occurs, restart the <em>R</em> session, switch to cpu-only
mode <em>directly</em> after loading the libraries with
<code><a href="../reference/set_config_cpu_only.html">aifeducation::set_config_cpu_only()</a></code> and request the
embedding again.</p>
<p>In the example above, the text embeddings are stored in
<code>text_embedding</code>. Since embedding texts may take some time,
it is a good idea to save the embeddings for future analysis (use the
<code>save</code> function of <em>R</em>). This allows you to load the
embeddings without the need to apply the text embedding model on the
same raw texts again.</p>
<p>The resulting object can then be passed to the method
<code>predict</code> of our classifier and you will get the predictions
together with an estimate of certainty for each class/category.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># If your classifier is not loaded</span></span>
<span><span class="va">classifier</span><span class="op">&lt;-</span><span class="fu"><a href="../reference/load_ai_model.html">load_ai_model</a></span><span class="op">(</span></span>
<span>  model_dir<span class="op">=</span><span class="st">"classifiers/movie_review_classifier"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict the classes of new texts</span></span>
<span><span class="va">predicted_categories</span><span class="op">&lt;-</span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span></span>
<span>  newdata <span class="op">=</span> <span class="va">text_embeddings</span>,</span>
<span>  batch_size<span class="op">=</span><span class="fl">8</span>,</span>
<span>  verbose<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p>After the classifier finishes the prediction, the estimated
categories/classes are stored as <code>predicted_categories</code>. This
object is a <code>data.frame</code> containing texts’ IDs in the rows
and the probabilities of the different categories/classes in the
columns. The last column with the name <code>expected_category</code>
represents the category which is assigned to a text due the highest
probability.</p>
<p>The estimates can be used in further analysis with common methods of
the educational and social sciences such as correlation analysis,
regression analysis, structural equation modeling, latent class analysis
or analysis of variance.</p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The
Long-Document Transformer. <a href="https://doi.org/10.48550/arXiv.2004.05150" class="external-link uri">https://doi.org/10.48550/arXiv.2004.05150</a></p>
<p>Berding, F., &amp; Pargmann, J. (2022). Iota Reliability Concept of
the Second Generation. Berlin: Logos. <a href="https://doi.org/10.30819/5581" class="external-link uri">https://doi.org/10.30819/5581</a></p>
<p>Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A.,
&amp; Rebmann, K. (2022). Performance and Configuration of Artificial
Intelligence in Educational Settings.: Introducing a New Reliability
Concept Based on Content Analysis. Frontiers in Education, 1–21. <a href="https://doi.org/10.3389/feduc.2022.818365" class="external-link uri">https://doi.org/10.3389/feduc.2022.818365</a></p>
<p>Campesato, O. (2021). Natural Language Processing Fundamentals for
Developers. Mercury Learning &amp; Information. <a href="https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713" class="external-link uri">https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713</a></p>
<p>Cascante-Bonilla, P., Tan, F., Qi, Y. &amp; Ordonez, V. (2020).
Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised
Learning. <a href="https://doi.org/10.48550/arXiv.2001.06001" class="external-link uri">https://doi.org/10.48550/arXiv.2001.06001</a></p>
<p>Chollet, F., Kalinowski, T., &amp; Allaire, J. J. (2022). Deep
learning with R (Second edition). Manning Publications Co. <a href="https://learning.oreilly.com/library/view/-/9781633439849/?ar" class="external-link uri">https://learning.oreilly.com/library/view/-/9781633439849/?ar</a></p>
<p>Dai, Z., Lai, G., Yang, Y. &amp; Le, Q. V. (2020).
Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing. <a href="https://doi.org/10.48550/arXiv.2006.03236" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03236</a></p>
<p>Devlin, J., Chang, M.‑W., Lee, K., &amp; Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171–4186).
Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423" class="external-link uri">https://doi.org/10.18653/v1/N19-1423</a></p>
<p>Gwet, K. L. (2014). Handbook of inter-rater reliability: The
definitive guide to measuring the extent of agreement among raters
(Fourth edition). Gaithersburg: STATAXIS.</p>
<p>He, P., Liu, X., Gao, J. &amp; Chen, W. (2020). DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. <a href="https://doi.org/10.48550/arXiv.2006.03654" class="external-link uri">https://doi.org/10.48550/arXiv.2006.03654</a></p>
<p>Krippendorff, K. (2019). Content Analysis: An Introduction to Its
Methodology (4th ed.). Los Angeles: SAGE.</p>
<p>Lane, H., Howard, C., &amp; Hapke, H. M. (2019). Natural language
processing in action: Understanding, analyzing, and generating text with
Python. Shelter Island: Manning.</p>
<p>Larusson, J. A., &amp; White, B. (Eds.). (2014). Learning Analytics:
From Research to Practice. New York: Springer. <a href="https://doi.org/10.1007/978-1-4614-3305-7" class="external-link uri">https://doi.org/10.1007/978-1-4614-3305-7</a></p>
<p>Lee, D.‑H. (2013). Pseudo-Label: The Simple and Efficient
Semi-Supervised Learning Method for Deep Neural Networks. CML 2013
Workshop: Challenges in Representation Learning.</p>
<p>Lee-Thorp, J., Ainslie, J., Eckstein, I. &amp; Ontanon, S. (2021).
FNet: Mixing Tokens with Fourier Transforms. <a href="https://doi.org/10.48550/arXiv.2105.03824" class="external-link uri">https://doi.org/10.48550/arXiv.2105.03824</a></p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). RoBERTa: A
Robustly Optimized BERT Pretraining Approach. <a href="https://doi.org/10.48550/arXiv.1907.11692" class="external-link uri">https://doi.org/10.48550/arXiv.1907.11692</a></p>
<p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp;
Potts, C. (2011). Learning Word Vectors for Sentiment Analysis. In D.
Lin, Y. Matsumoto, &amp; R. Mihalcea (Eds.), Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (pp. 142–150). Association for Computational
Linguistics. <a href="https://aclanthology.org/P11-1015" class="external-link uri">https://aclanthology.org/P11-1015</a></p>
<p>Papilloud, C., &amp; Hinneburg, A. (2018). Qualitative Textanalyse
mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler.
Wiesbaden: Springer. <a href="https://doi.org/10.1007/978-3-658-21980-2" class="external-link uri">https://doi.org/10.1007/978-3-658-21980-2</a></p>
<p>Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., &amp; Dehak,
N. (2019). Hierarchical Transformers for Long Document Classification.
In 2019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU) (pp. 838–844). IEEE. <a href="https://doi.org/10.1109/ASRU46091.2019.9003958" class="external-link uri">https://doi.org/10.1109/ASRU46091.2019.9003958</a></p>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe:
Global Vectors for Word Representation. Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing. <a href="https://aclanthology.org/D14-1162.pdf" class="external-link uri">https://aclanthology.org/D14-1162.pdf</a></p>
<p>Schreier, M. (2012). Qualitative Content Analysis in Practice. Los
Angeles: SAGE.</p>
<p>Tunstall, L., Werra, L. von, Wolf, T., &amp; Géron, A. (2022).
Natural language processing with transformers: Building language
applications with hugging face (Revised edition). Heidelberg:
O’Reilly.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention Is All
You Need. <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link uri">https://doi.org/10.48550/arXiv.1706.03762</a></p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A.,
Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa,
H., . . . Dean, J. (2016). Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation. <a href="https://doi.org/10.48550/arXiv.1609.08144" class="external-link uri">https://doi.org/10.48550/arXiv.1609.08144</a></p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Berding Florian.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>

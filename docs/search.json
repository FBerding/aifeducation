[{"path":[]},{"path":"/articles/aifeducation.html","id":"introduction","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Introduction","title":"01 Get started","text":"Several packages allow users use machine learning directly R nnet single layer neural nets, rpart decision trees, ranger random forests. Furthermore, mlr3verse series packages exists managing different algorithms unified interface. packages can used ‘normal’ computer provide easy installation. terms natural language processing, approaches currently limited. State---art approaches rely neural nets multiple layers consist huge number parameters making computationally demanding. specialized libraries keras tensorflow, graphical processing units (gpu) can help speed computations significantly. However, many specialized libraries machine learning written python. Fortunately, interface python provided via R package reticulate. R package Artificial Intelligence Education (aifeducation) aims provide educators, educational researchers, social researchers convincing interface state---art models natural language processing tries address special needs challenges educational social sciences. package currently supports application Artificial Intelligence (AI) tasks text embedding, classification, question answering. Since state---art approaches natural language processing rely large models compared classical statistical methods (e.g., latent class analysis, structural equation modeling) based largely python, additional installation steps necessary. like train develop models AIs, compatible graphic device necessary. Even low performing graphic device can speed computations significantly. prefer using pre-trained models however, necessary. case ‘normal’ office computer without graphic device sufficient cases.","code":""},{"path":"/articles/aifeducation.html","id":"step-1---install-the-r-package","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 1 - Install the R Package","title":"01 Get started","text":"order use package, first need install . can done : command, necessary R packages installed machine.","code":"# install.packages(\"devtools\") devtools::install_github(\"FBerding/aifeducation\",                          dependencies = TRUE)"},{"path":"/articles/aifeducation.html","id":"step-2---install-python","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 2 - Install Python","title":"01 Get started","text":"Since natural language processing neural nets based models computationally intensive, keras tensorflow used within package together specialized python libraries. install , need install python machine first. may take time. can check everything working using function reticulate::py_available(). return TRUE.","code":"reticulate::install_python() reticulate::py_available(initialize = TRUE)"},{"path":"/articles/aifeducation.html","id":"step-3---install-miniconda","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 3 - Install Miniconda","title":"01 Get started","text":"next step install miniconda since aifeducation uses conda environments managing different modules.","code":"reticulate::install_miniconda()"},{"path":"/articles/aifeducation.html","id":"step-4---install-support-for-graphic-devices","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 4 - Install Support for Graphic Devices","title":"01 Get started","text":"like use graphic card computations need install software. list links downloads can found : https://www.tensorflow.org/install/pip#linux general need NVIDIA GPU Drivers CUDA Toolkit cuDNN SDK","code":""},{"path":"/articles/aifeducation.html","id":"step-5---install-specialized-python-libraries","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 5 - Install Specialized Python Libraries","title":"01 Get started","text":"everything working, can now install remaining python libraries. convenience, aifeducation comes auxiliary function install_py_modules() . function installs following python modules: transformers, tokenizers, datasets, torch, keras, tensorflow dependencies environment “aifeducation”. like use aifeducation packages within environments, please ensure python modules available. check_aif_py_modules() can check, modules successfully installed. Now everything ready use package. start new R session, please note call reticulate::use_condaenv(condaenv = \"aifeducation\") make python modules available work.","code":"install_py_modules(envname=\"aifeducation\") aifeducation::check_aif_py_modules(print=TRUE)"},{"path":"/articles/aifeducation.html","id":"configuration","dir":"Articles","previous_headings":"","what":"2) Configuration","title":"01 Get started","text":"general, educators educational researchers neither access high performance computing computers performing graphic device work. Thus, additional configuration can done get computations working machine. use computer graphic device, can disable graphic device support tensorflow function set_config_cpu_only(). Now machine uses cpus computations. machine graphic card limited memory, recommended change configuration memory usage set_config_gpu_low_memory() enables machine compute ‘large’ models limited resources. ‘small’ models, option relevant since decreases computational speed. Finally, cases might want disable tensorflow print information console. can change behavior function set_config_tf_logger(). can choose five levels “FATAL”, “ERROR”, “WARN”, “INFO”, “DEBUG”, setting minimal level logging.","code":"aifeducation::set_config_cpu_only() aifeducation::set_config_gpu_low_memory() aifeducation::set_config_tf_logger()"},{"path":"/articles/classification_tasks.html","id":"introduction-and-overview","dir":"Articles","previous_headings":"","what":"1 Introduction and Overview","title":"02 Classification Tasks","text":"educational social sciences, assignment observation scientific concepts important task allows researchers understand observation, generate new insights, derive recommendations research practice. educational science, several areas deal kind task. example, diagnosing students’ characteristics important aspect teachers’ profession necessary understand promote learning. Another example use learning analytics, data students used provide learning environments adapted individual needs. another level, educational institutions schools universities can use information data-driven performance decisions (Laurusson & White 2014) well improve . case, real-world observation aligned scientific models use scientific knowledge technology improved learning instruction. Supervised machine learning one concept allows link real-world observations existing scientific models theories (Berding et al. 2022). educational sciences great advantage allows researchers use existing knowledge insights applications AI. drawback approach training AI requires information real world observations information corresponding alignment scientific models theories. valuable source data educational science written texts, since textual data can found almost everywhere realm learning teaching (Berding et al. 2022). example, teachers often require students solve task provide written form. Students create solution tasks often document short-written essay presentation. data can used analyze learning teaching. Teachers’ written tasks students may provide insights quality instruction students’ solutions may provide insights learning outcomes prerequisites. AI can helpful assistant analyzing textual data since analysis textual data challenging time-consuming task humans. vignette, like show create AI can help tasks using package aifedcuation. Please note introduction content analysis, natural language processing machine learning beyond scope vignette. like learn , please refer cited literature. start necessary introduce definition understanding basic concepts since applying AI educational contexts means combine knowledge different scientific disciplines using different, sometimes overlapping concepts. Even within research area, concepts unified. Figure 1 illustrates package’s understanding. Since aifeducation looks application AI classification tasks perspective empirical method content analysis, overlapping concepts content analysis machine learning. content analysis, phenomenon like performance colors can described scale/dimension made several categories (e.g. Schreier 2012 pp. 59). example, exam’s performance (scale/dimension) “good”, “average” “poor”. terms colors (scale/dimension) categories “blue”, “green”, etc. Machine learning literature uses words describe kind data. machine learning, “scale” “dimension” correspond term “label” “categories” refer term “classes” (Chollet, Kalinowski & Allaire 2022, p. 114). clarifications, classification means text assigned correct category scale text labeled correct class. Figure 2 illustrates, two kinds data necessary train AI classify text line supervised machine learning principles. providing AI textual data input data corresponding information class target data, AI can learn texts imply specific class category. exam example, AI can learn texts imply “good”, “average” “poor” judgment. training, AI can applied new texts predict likely class every new text. generated class can used statistical analysis derive recommendations learning teaching. achieve support artificial intelligence, several steps necessary. Figure 3 provides overview integrating functions objects aifeducation. first step transform raw texts form computers can use. , raw texts must transformed numbers. modern approaches, usually done word embeddings. Campesato (2021, p. 102) describes “collective name set language modeling feature learning techniques (…) words phrases vocabulary mapped vectors real numbers.” definition word vector similar: „Word vectors represent semantic meaning words vectors context training corpus.” (Lane, Howard & Hapke 2019, p. 191) Campesato (2021, pp. 112) clusters approaches creating word embeddings three groups, reflecting ability provide context-sensitive numerical representations. Approaches group one account context. Typical methods rely bag--words assumptions. Thus, normally able provide word embedding single words. Group two consists approaches word2vec, GloVe (Pennington, Socher & Manning 2014) fastText, able provide one embedding word regardless context. Thus, account one context. last group consists approaches BERT (Devlin et al. 2019), able produce multiple word embeddings depending context words. different groups, aifedcuation implements several methods. Topic Modeling: Topic modeling approach uses frequencies tokens within text. frequencies tokens models observable variables one latent topic (Campesato 2021, p. 113). estimation topic model often based Latent Dirichlet Analysis (LDA) describes text distribution topics. topics described distribution words/tokens (Campesato 2021, p. 114). relationship texts, words, topics can used create text embedding computing relative amount every topic text based every token text. GlobalVectorClusters: GlobalVectors newer approach utilizes co-occurrence words/tokens compute GlobalVectors (Campesato 2021, p. 110). vectors generated way tokens/words similar meaning located close (Pennington, Socher & Manning 2014). order create text embedding word embeddings, aifeducation groups tokens clusters based vectors. Thus, tokens similar meaning members cluster. text embedding, tokens text counted every cluster frequencies every cluster text used numerical representation text. Transformers: Transformers current state---art approach many natural language tasks (Tunstall, von Werra & Wolf 2022, p. xv). help self-attention mechanism (Vaswani et al. 2017), able produce context-sensitive word embeddings (Chollet, Kalinowski & Allaire, 2022 pp.366). approaches managed used unified interface provided object TextEmbeddingModel. object can easily convert raw texts numerical representation, can use different classification tasks time. makes possible reduce computational time. created text embedding stored object class EmbeddedText. object additionally contains information text embedding model created object. best case can apply existing text embedding model using transformer Huggingface using model colleagues. , aifeducation provides several functions allowing create models. Depending approach like use, different steps necessary. case Topic Modeling GlobalVectorClusters, must first create draft vocabulary two functions bow_pp_create_vocab_draft() bow_pp_create_basic_text_rep(). calling functions, determine central properties resulting model. case transformers, first configure train vocabulary create_xxx_model() next step can train model train_tune_xxx_model(). Every step explained next chapters. Please note xxx stands different architectures transformers supported aifedcuation. object class TextEmbeddingModel can create input data supervised machine learning process. Additionally, need target data must named factor containing classes/categories text. kinds data, able create new object class TextEmbeddingClassifierNeuralNet classifier. train classifier several options cover detail chapter 3. training classifier can share researchers apply new texts. Please note application new texts requires text transformed numbers exactly text embedding model passing text classifier. Please note: pass raw texts classifier, embedded texts work! next chapters, guide complete process, starting creation text embedding models. Please note creation new text embedding model necessary rely existing model rely pre-trained transformer.","code":""},{"path":"/articles/classification_tasks.html","id":"starting-a-new-session","dir":"Articles","previous_headings":"","what":"2 Starting a New Session","title":"02 Classification Tasks","text":"can work aifeducation must set new R session. First, necessary load library. Second, must set python via reticulate. case installed python suggested vignette 01 Get started may start new session like : Now good time configure tensorflow, since configurations can done tensorflow used first time. Now everything ready start preparation tasks.","code":"library(aifeducation)  #if you use a machine with windows reticulate::use_condaenv(condaenv = \"aifeducation\")  #if you use a machine with mac or linux reticulate::use_virtualenv(virtualenv= \"aifedcuation\") #if you would like to use only cpus set_config_cpu_only()  #if you have a graphic device with low memory set_config_gpu_low_memory()  #if you would like to reduce the tensorflow output to errors set_config_os_environ_logger(level = \"ERROR\")"},{"path":[]},{"path":"/articles/classification_tasks.html","id":"example-data-for-this-vignette","dir":"Articles","previous_headings":"3 Preparation Tasks","what":"3.1 Example Data for this Vignette","title":"02 Classification Tasks","text":"illustrate steps vignette, use data educational settings since data generally protected privacy policies. Therefore, use data set data_corpus_moviereviews package quanteda.textmodels illustrate usage package. quanteda.textmodels automatically installed install aifeducation. now data set three columns. first contains ID movie review, second contains rating movie (positive negative), third column contains raw texts. can see, data balanced. 1,000 reviews imply positive rating movie 1,000 imply negative rating. tutorial, modify data set setting half negative positive reviews NA, indicating reviews labeled. Furthermore, bring imbalance setting 250 positive reviews NA. now use data show use different objects functions aifeducation.","code":"example_data<-data.frame(   id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,   label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment) example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)  table(example_data$label) #>  #>  neg  pos  #> 1000 1000 example_data$label[c(1:500,1001:1500)]=NA summary(example_data$label) #>  neg  pos NA's  #>  500  500 1000 example_data$label[1501:1750]=NA summary(example_data$label) #>  neg  pos NA's  #>  500  250 1250"},{"path":"/articles/classification_tasks.html","id":"topic-modeling-and-globalvectorclusters","dir":"Articles","previous_headings":"3 Preparation Tasks","what":"3.2 Topic Modeling and GlobalVectorClusters","title":"02 Classification Tasks","text":"like create new text embedding model Topic Modeling GlobalVectorClusters, first create draft vocabulary. can calling function bow_pp_create_vocab_draft(). main input function vector texts. function’s aims create list tokens texts, reduce tokens tokens carry semantic meaning, provide lemma every token. Since Topic Modeling depends bag--word approach, reason pre-process step reduce tokens tokens really carry semantic meaning. general, tokens words either nouns, verbs adjectives (Papilloud & Hinneburg 2018, p. 32). example data, application function : can see, additional parameter: path_language_model. must insert path udpipe pre-trained language model since function uses udpipe package part--speech tagging lemmataziation. collection pre-trained models 65 languages can found [https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131]. Just download relevant model machine provide path model. parameter upos can select tokens selected. example, tokens represent noun, adjective verb remain analysis. list possible tags can found : [https://universaldependencies.org/u/pos/index.html]. Please forget provide label udpipe model use please also provide language analyzing. information important since transferred text embedding model. researchers/users need information decide model help work. next step, can use draft vocabulary create basic text representation function bow_pp_create_basic_text_rep(). function takes raw texts draft vocabulary main input. function aims remove tokens referring stopwords, clean data (e.g., removing punctuation, numbers), lower case tokens requested, remove tokens specific minimal frequency, remove tokens occur many documents create document-feature-matrix (dfm), create feature-co-occurrence-matrix (fcm). Applied example, call function look like : data takes raw texts vocab_draft takes draft vocabulary created first step. main goal create document-feature-matrix(dfm) feature-co- occurrence-matrix (fcm). dfm matrix reports texts rows number tokens columns. matrix later used create text embedding model based topic modeling. dfm reduced tokens correspond part--speech tags vocabulary draft. Punctuation, symbols, numbers etc. removed matrix set corresponding parameter TRUE. set use_lemmata = TRUE can reduce dimensionality matrix using lemmas instead tokens (Papilloud & Hinneburg 2018, p.33). set to_lower = TRUE tokens transformed lower case. end get matrix tries represent semantic meaning text smallest possible number tokens. applies fcm. , tokens/features reduced way. However, features reduced, token’s co-occurrence calculated. aim window used shifted across text, counting tokens left right token investigation. size window can determined window. weights can provide weights counting. example, tokens far away token investigation count less tokens closer token investigation. fcm later used create text embedding model based GlobalVectorClusters. may notice, dfm counts words text. Thus, position text within sentence matter. lower-case tokens use lemmas, syntactic information lost advantage dfm lower dimensionality losing little semantic meaning. contrast, fcm matrix describes often different tokens occur together. Thus, fcm recovers part position words sentence text. Now, everything ready create new text embedding model based Topic Modeling GlobalVectorClusters. show create new model, look preparation new transformer.","code":"vocab_draft<-bow_pp_create_vocab_draft(   path_language_model=\"language_model/english-gum-ud-2.5-191206.udpipe\",   data=example_data$text,   upos=c(\"NOUN\", \"ADJ\",\"VERB\"),   label_language_model=\"english-gum-ud-2.5-191206\",   language=\"english\",   trace=TRUE) basic_text_rep<-bow_pp_create_basic_text_rep(   data = example_data$text,   vocab_draft = vocab_draft,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = FALSE,   split_tags = FALSE,   language_stopwords=\"eng\",   use_lemmata = FALSE,   to_lower=FALSE,   min_termfreq = NULL,   min_docfreq= NULL,   max_docfreq=NULL,   window = 5,   weights = 1 / (1:5),   trace=TRUE)"},{"path":"/articles/classification_tasks.html","id":"creating-a-new-transformer","dir":"Articles","previous_headings":"3 Preparation Tasks","what":"3.3 Creating a New Transformer","title":"02 Classification Tasks","text":"general, recommended use pre-trained model since creation new transformer requires large data set texts computationally intensive. vignette illustrate process BERT model. However, many transformers, process . creation new transformer requires least two steps. First, must decide architecture transformer. includes creation set vocabulary. aifedcuation can calling function create_bert_model(). example look like : function work, must provide path directory new transformer saved. Furthermore, must provide raw texts. texts used training transformer training vocabulary. maximum size vocabulary determined vocab_size. Please provide size 50,000 60,000 since kind vocabulary works differently approaches described section 2.2. Modern tokenizers WordPiece (Wu et al. 2016) use algorithms splits tokens smaller elements, allowing build huge number words small number elements. Thus, even small number 30,000 tokens, able represent large number words. consequence, kinds vocabularies many times smaller vocabularies build section 2.2. parameters allow customize BERT model. example, increase number hidden layers 12 24 reduce hidden size 768 256, allowing build test larger smaller transformers. Please note max_position_embeddings determine many tokens transformer can process. text tokens tokenization, tokens ignored. However, like analyze long documents, please avoid increase number significantly computational time increase linear way quadratic (Beltagy, Peters & Cohan 2020). long documents can use another architecture BERT (e.g. Longformer Beltagy, Peters & Cohan 2020) split long document several chunks used sequentially classification (e.g., Pappagari et al. 2019). Using chunks supported aifedcuation. calling function, find new model model directory. next step train model calling train_tune_bert_model(). important provide path directory new transformer stored. Furthermore, important provide another directory trained transformer saved avoid reading writing collisions. Now, provided raw data used train model using Masked Language Modeling. First, can set length token sequences chunk_size. whole_word can choose masking single tokens masking complete words (Please remember modern tokenizers split words several tokens. Thus, tokens words forced match directly). p_mask can determine many tokens masked. Finally, val_size, set many chunks used validation sample. work machine graphic device small memory, please reduce batch size significantly. also recommend change usage memory set_config_gpu_low_memory(). training finishes, can find transformer ready use output_directory. Now able create text embedding model.","code":"basic_text_rep<-bow_pp_create_basic_text_rep( create_bert_model(     model_dir = \"my_own_transformer\",     vocab_raw_texts=example_data$text,     vocab_size=30522,     vocab_do_lower_case=FALSE,     max_position_embeddings=512,     hidden_size=768,     num_hidden_layer=12,     num_attention_heads=12,     intermediate_size=3072,     hidden_act=\"gelu\",     hidden_dropout_prob=0.1,     trace=TRUE) train_tune_bert_model(   output_dir = \"my_own_transformer_trained\",   bert_model_dir_path = \"my_own_transformer\",   raw_texts = example_data$text,   aug_vocab_by=0,   p_mask=0.15,   whole_word=TRUE,   val_size=0.1,   n_epoch=1,   batch_size=12,   chunk_size=250,   n_workers=1,   multi_process=FALSE,   trace=TRUE)"},{"path":[]},{"path":"/articles/classification_tasks.html","id":"introduction","dir":"Articles","previous_headings":"4 Text Embedding","what":"4.1 Introduction","title":"02 Classification Tasks","text":"aifedcuation, text embedding model stored object class TextEmbeddingModel. object contains relevant information transforming raw texts numeric representation can used machine learning. aifedcuation, transformation raw texts numbers separate step downstream tasks classification. reduce computational time machines low performance. separating text embedding tasks, text embedding calculated can used different tasks time. Another advantage training downstream tasks involves downstream tasks parameters embedding model, making training less time-consuming, thus decreasing computational intensity. Finally, approach allows analysis long documents applying algorithm different parts. text embedding model provides unified interface: creating model different methods, handling model always . following show use object. start Topic Modeling.","code":""},{"path":[]},{"path":"/articles/classification_tasks.html","id":"topic-modeling","dir":"Articles","previous_headings":"4 Text Embedding > 4.2 Creating Text Embedding Models","what":"4.2.1 Topic Modeling","title":"02 Classification Tasks","text":"creating new text embedding model based Topic Modeling, need basic text representation generated function bow_pp_create_basic_text_rep() (see section 2.2). Now can create new instance text embedding model calling TextEmbeddingModel$new(). First provide name new model (model_name). unique short name without spaces. model_label can provide label model freedom. important provide version model case want create improved version future. model_language provide users information language model designed. important plan share model wider community. method determine approach used model. like use Topic Modeling, set method = \"lda\". number topics set via bow_n_dim. example like create topic model twelve topics. number topics also determines dimensionality text embedding. Consequently, every text characterized twelve topics. Please forget pass basic text representation bow_basic_text_rep. model estimated, stored topic_modeling example.","code":"topic_modeling<-TextEmbeddingModel$new(   model_name=\"topic_model_embedding\",   model_label=\"Text Embedding via Topic Modeling\",   model_version=\"0.0.1\",   model_language=\"english\",   method=\"lda\",   bow_basic_text_rep=basic_text_rep,   bow_n_dim=12,   bow_max_iter=500,   bow_cr_criterion=1e-8,   trace=TRUE )"},{"path":"/articles/classification_tasks.html","id":"globalvectorclusters","dir":"Articles","previous_headings":"4 Text Embedding > 4.2 Creating Text Embedding Models","what":"4.2.2 GlobalVectorClusters","title":"02 Classification Tasks","text":"creation text embedding model based GlobalVectorClusters similar model based Topic Modeling. two differences. First, request model based GlobalVectorCluster setting method=\"glove_cluster\". Second, determine dimensionality global vectors bow_n_dim number clusters bow_n_cluster. creating new text embedding model, global vector token calculated based feature-co-occurrence-matrix (fcm) provide basic_text_rep. token, vector calculated length bow_n_dim. Since vectors word embeddings text embeddings, additional step necessary create text embeddings. aifedcuation word embeddings used group words clusters. number clusters set bow_n_cluster. Now, text embedding produced counting tokens every cluster every text. final model stored global_vector_clusters_modeling.","code":"global_vector_clusters_modeling<-TextEmbeddingModel$new(   model_name=\"global_vector_clusters_embedding\",   model_label=\"Text Embedding via Clusters of GlobalVectors\",   model_version=\"0.0.1\",   model_language=\"english\",   method=\"glove_cluster\",   bow_basic_text_rep=basic_text_rep,   bow_n_dim=96,   bow_n_cluster=384,   bow_max_iter=500,   bow_max_iter_cluster=500,   bow_cr_criterion=1e-8,   trace=TRUE )"},{"path":"/articles/classification_tasks.html","id":"transformers","dir":"Articles","previous_headings":"4 Text Embedding > 4.2 Creating Text Embedding Models","what":"4.2.3 Transformers","title":"02 Classification Tasks","text":"Using transformer creating text embedding model similar two approaches. request model based transformer must set method accordingly. Since use BERT model example, set method = \"bert\". Next, provide directory model stored. example bert_model_dir_path=\"my_own_transformer_trained. course can use pre-trained model Huggingface addresses needs. Using BERT model text embedding problem since text provide tokens transformer can process. maximal value set configuration transformer (see section 2.3). text produces tokens last tokens ignored. instances might want analyze long texts. situations, reducing text first tokens (e.g. first 512 tokens) result problematic loss information. deal situations can configure text embedding model aifecuation split long texts several chunks processed transformer. maximal number chunks set chunks. example , text embedding model split text consisting 1024 tokens two chunks every chunk consisting 512 tokens. every chunk text embedding calculated. result, receive sequence embeddings. first embeddings characterizes first part text second embedding characterizes second part text (). Thus, example text embedding model able process texts 4*512=2048 tokens. approach inspired work Pappagari et al. (2019). Since transformers able account context, may useful interconnect every chunk bring context calculations. can done overlap determine many tokens end prior chunk added next. Finally, decide hidden layer layers embeddings drawn (aggregation=\"last\"). initial work, Devlin et al. (2019) used hidden states different layers classification. deciding configuration, can use model.","code":"bert_modeling<-TextEmbeddingModel$new(   model_name=\"bert_embedding\",   model_label=\"Text Embedding via BERT\",   model_version=\"0.0.1\",   model_language=\"english\",   method = \"bert\",   max_length = 512,   chunks=4,   overlap=30,   aggregation=\"last\",   model_dir=\"my_own_transformer_trained\"   )"},{"path":"/articles/classification_tasks.html","id":"transforming-raw-texts-into-embedded-texts","dir":"Articles","previous_headings":"4 Text Embedding","what":"4.3 Transforming Raw Texts into Embedded Texts","title":"02 Classification Tasks","text":"Although mechanics within text embedding model different, usage always . transform raw text numeric representation use embed method model. , must provide raw texts raw_text. addition, necessary provide character vector containing ID every text. IDs must unique. method embedcreates object class EmbeddedText. just data.frame consisting embedding every text. Depending method, data.frame different meaning: Topic Modeling: Regarding topic modeling, rows represent texts columns represent percentage every topic within text. GlobalVectorClusters: , rows represent texts columns represent absolute frequencies tokens belonging semantic cluster. Transformer - Bert: BERT, rows represent texts columns represents contextualized text embedding BERT’s understanding relevant text chunk. Please note case BERT models, embeddings every chunks interlinked. embedded texts now input train new classifier apply pre-trained classifier predicting categories/classes. next chapter show use classifiers. start, show save load model.","code":"topic_embeddings<-topic_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)  cluster_embeddings<-global_vector_clusters_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)  bert_embeddings<-bert_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)"},{"path":"/articles/classification_tasks.html","id":"saving-and-loading-text-embedding-models","dir":"Articles","previous_headings":"4 Text Embedding","what":"4.4 Saving and Loading Text Embedding Models","title":"02 Classification Tasks","text":"Saving created text embedding model easy aifeducation using function save_ai_model. function provides unique interface text embedding models. saving work can pass model model directory save model model_dir. Please pass path directory path file function. Internally function creates new folder directory files belonging model stored. can see three text embedding models saved within directory named “text_embedding_models”. Within directory function creates unique folder every model. name folder created using models’ names. Since files stored special structure please change files manually. want load model, just call function load_ai_model can continue using model. Please note add name model directory path. example stored three models directory “text_embedding_models”. model saved within folder. folder’s name created automatically help name model. Thus, loading model must specify model want load adding model’s name directory path shown . point may wonder ID model’s name although enter ID model’s creation. ID added automatically ensure every model unique name. important like share work persons. Now can use text embedding model.","code":"save_ai_model(   model=topic_modeling,    model_dir=\"text_embedding_models\")  save_ai_model(   model=global_vector_clusters_modeling,    model_dir=\"text_embedding_models\")  save_ai_model(   model=bert_modeling,    model_dir=\"text_embedding_models\") topic_modeling<-load_ai_model(   model_dir=\"text_embedding_models/topic_model_embedding_ID_DfO25E1Guuaqw7tM\")  global_vector_clusters_modeling<-load_ai_model(   model_dir=\"text_embedding_models/global_vector_clusters_embedding_ID_5Tu8HFHegIuoW14l\")  bert_modeling<-load_ai_model(   model_dir=\"text_embedding_models/bert_embedding_ID_CmyAQKtts5RdlLaS\")"},{"path":[]},{"path":"/articles/classification_tasks.html","id":"creating-a-new-classifier","dir":"Articles","previous_headings":"5 Using AI for Classification","what":"5.1 Creating a New Classifier","title":"02 Classification Tasks","text":"aifedcuation, classifiers based neural nets stored objects class TextEmbeddingClassifierNeuralNet. can create new classifier calling TextEmbeddingClassifierNeuralNet$new(). Similar text embedding model provide name (name) label (label) new classifier. text_embeddings provide embedded text. like recommend use embedding like use training. continue example use embedding produced BERT model. targets takes target data supervised learning. Please omit cases category/class since can used special training technique show later. important provide target data factors. Otherwise error occur. also important name factor. , entries factor mus names correspond IDs corresponding texts. Without names method match input data (text embeddings) target data. parameters decide structure classifier. Figure 4 illustrates . hidden takes vector integers, determining number layers number neurons. example, dense layers. rec also takes vector integers determining number size Gated Recurrent Unit (gru). example, use one layer 256 neurons. Since classifiers aifeducation use standardized scheme creation, dense layers used gru layers. want omit gru layers dense layers, set corresponding argument NULL. use text embedding model processes one chunk like recommend use recurrent layers since able use sequential structure data. cases can rely dense layers . use text embeddings one chunk, good idea try self-attention layering order take context chunks account. add self-attention layer must provide integer greater 0 self_attention_heads. add self-attention layer transformer encoder described Chollet, Kalinowski, Allaire (2022, pp. 373). Masking, normalization, creation input layer well output layer done automatically. created new classifier, can begin training.","code":"example_targets<-as.factor(example_data$label) names(example_targets)=example_data$id  classifier<-TextEmbeddingClassifierNeuralNet$new(   name=\"movie_review_classifier\",   label=\"Classifier for Estimating a Postive or Negative Rating of Movie Reviews\",   text_embeddings=bert_embeddings,   targets=example_targets,   hidden=NULL,   rec=c(256),   self_attention_heads = 2,   dropout=0.4,   recurrent_dropout=0.4,   l2_regularizer=0.01,   optimizer=\"adam\",   act_fct=\"gelu\",   rec_act_fct=\"tanh\")"},{"path":"/articles/classification_tasks.html","id":"training-a-classifier","dir":"Articles","previous_headings":"5 Using AI for Classification","what":"5.2 Training a Classifier","title":"02 Classification Tasks","text":"start training classifier, call train method. Similarly, creation classifier, must provide text embedding data_embeddings categories/classes target data data_targets. Please remember data_targets expects named factor names correspond IDs corresponding text embeddings. Text embeddings target data matched omitted training. train classifier, necessary provide path dir_checkpoint. directory stores best set weights training epoch. training, weights automatically used final weights classifier. performance estimation, training splits data several chunks based cross-fold validation. number folds set data_n_test_samples. every case, one fold used training serves test sample. remaining data used create training validation sample. performance values saved trained classifier refer test sample. data never used training provides realistic estimation classifier`s performance. Since aifedcuation tries address special needs educational social science, special training steps integrated method. Baseline: interested training classifier without applying additional statistical techniques, set use_baseline = TRUE. case, classifier trained provided data . Cases missing values target data omitted. Even like apply statistical adjustments, makes sense compute baseline model comparing effect modified training process unmodified training. using bsl_val_size can determine much data used training data much used validation data. Balanced Synthetic Cases: case imbalanced data, recommended set use_bsc=TRUE. training, number synthetic units created via different techniques. Currently can request Basic Synthetic Minority Oversampling Technique, Density-Bases Synthetic Minority Oversampling Technique, Adaptive Synthetic Sampling Approach Imbalanced Learning. aim create new cases fill gap majority class. Multi-class problems reduced two class problem (class investigation vs. ) generating units. can even request several techniques . number synthetic units original minority units exceeds number cases majority class, random sample drawn. technique allows set number neighbors generation, k = bsc_max_k used. Balanced Pseudo-Labeling: technique relevant labeled target data large number unlabeled target data. different parameter starting “bpl_”, can request different implementations pseudo-labeling, example based work Lee (2013) Cascante-Bonilla et al. (2020). turn pseudo-labeling, set use_bpl=TRUE. request pseudo-labeling based Cascante-Bonilla et al. (2020), following parameters set: bpl_max_steps = 5 (splits unlabeled data five chunks) bpl_dynamic_inc = TRUE (ensures number used chunks increases every step) bpl_model_reset = TRUE (re-initializes model every step) bpl_epochs_per_step=30 (number training epochs within step) bpl_balance=FALSE (ensures cases highest certainty added training regardless absolute frequencies classes) bpl_weight_inc=0.00 bpl_weight_start=1.00 (ensures labeled unlabeled data weight training) bpl_max=1.00, bpl_anchor=1.00, bpl_min=0.00 (ensures unlabeled data considered training cases highest certainty used training.) request original pseudo-labeling proposed Lee (2013), set following parameters: bpl_max_steps=30 (steps must treated epochs) bpl_dynamic_inc=FALSE (ensures pseudo-labeled cases used) bpl_model_reset=FALSE (model allowed re-initialized) bpl_epochs_per_step=1 (steps treated epochs must one) bpl_balance=FALSE (ensures cases added regardless absolute frequencies classes) bpl_weight_inc=0.02 bpl_weight_start=0.00 (gives pseudo labeled data increasing weight every step) bpl_max=1.00, bpl_anchor=1.00, bpl_min=0.00 (ensures pseudo labeled cases used training. bpl_anchor affect calculations) Please note Lee (2013) suggests recalculate pseudo-labels unlabeled data every weight actualization, aifeducation, pseudo-labels recalculated every epoch. bpl_max=1.00, bpl_anchor=1.00, bpl_min=0.00 used describe certainty prediction. 0 refers random guessing 1 refers perfect certainty. bpl_anchor used reference value. distance bpl_anchor calculated every case. , sorted increasing distance bpl_anchor. resulting order cases relevant set bpl_dynamic_inc=TRUE bpl_balance=TRUE. Figure 5 illustrates training loop cases three options set TRUE. example applies algorithm proposed Cascante-Bonilla et al. (2020). training classifier labeled data, unlabeled data introduced training. classifier predicts potential labels unlabeled data adds 20% cases highest certainty pseudo-labels training. classifier re-initialized trained . training, classifier predicts potential labels originally unlabeled data adds 40% pseudo-labeled data training data. model re-initialized trained unlabeled data used training. Finally, trace, view_metrics, keras_trace allow control much information training progress printed console. Please note training classifier can take time. Please note performance estimation, final training classifier makes use data available. , test sample left empty.","code":"example_targets<-as.factor(example_data$label) names(example_targets)=example_data$id  classifier$train(    data_embeddings = bert_embeddings,    data_targets = example_targets,    data_n_test_samples=5,    use_baseline=TRUE,    bsl_val_size=0.33,    use_bsc=TRUE,    bsc_methods=c(\"dbsmote\"),    bsc_max_k=10,    bsc_val_size=0.25,    use_bpl=TRUE,    bpl_max_steps=5,    bpl_epochs_per_step=30,    bpl_dynamic_inc=TRUE,    bpl_balance=FALSE,    bpl_max=1.00,    bpl_anchor=1.00,    bpl_min=0.00,    bpl_weight_inc=0.00,    bpl_weight_start=1.00,    bpl_model_reset=TRUE,    epochs=30,    batch_size=8,    trace=TRUE,    view_metrics=FALSE,    keras_trace=0,    n_cores=2,    dir_checkpoint=\"training/classifier\")"},{"path":"/articles/classification_tasks.html","id":"evaluating-classifiers-performance","dir":"Articles","previous_headings":"5 Using AI for Classification","what":"5.3 Evaluating Classifier’s Performance","title":"02 Classification Tasks","text":"finishing training, can evaluate performance classifier. every fold, classifier applied test sample results compared true categories/classes. Since test sample never part training, performance measures provide realistic idea classifier`s performance. support researchers judging quality predictions, aifeducation utilizes several measures concepts content analysis. Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa equal weights Fleiss’ Kappa multiple raters exact estimation Light’s Kappa multiple raters can access concrete values accessing field reliability stores relevant information. list find reliability values every fold every requested training configuration. addition, reliability every step within balanced pseudo-labeling reported. central estimates reliability values can found via reliability$test_metric_mean. example : now table relevant values. particular interest values alpha Iota Concept since represent measure reliability independent frequency distribution classes/categories. alpha values describe probability case specific class recognized specific class. can see, compared baseline model, applying Balanced Synthetic Cases increased increases minimal value alpha, reducing risk miss cases belong rare class (see row “BSC”). contrary, alpha values major category decrease slightly, thus losing unjustified bonus high number cases training set. provides realistic performance estimation classifier. Furthermore, can see application pseudo-labeling increases alpha values minor class , step 3. Finally, can plot coding stream scheme showing cases different classes labeled. use package iotarelr. Figure 6: Coding Stream Classifier can see small number negative reviews treated good review larger number positive reviews treated bad review. Thus, data major class (negative reviews) reliable valid data minor class (positive reviews). Evaluating performance classifier complex task beyond scope vignette. Instead, like refer cited literature content analysis machine learning like dive deeper topic.","code":"classifier$reliability$test_metric_mean #>          iota_index min_iota2 avg_iota2 max_iota2 min_alpha avg_alpha max_alpha #> Baseline  0.5546667 0.4359583 0.5742172 0.7124761     0.544     0.714     0.884 #> BSC       0.5493333 0.4857597 0.5966513 0.7075429     0.630     0.742     0.854 #> BPL       0.6213333 0.5620768 0.6560003 0.7499238     0.710     0.790     0.870 #> Final     0.6213333 0.5620768 0.6560003 0.7499238     0.710     0.790     0.870 #>          static_iota_index dynamic_iota_index kalpha_nominal kalpha_ordinal #> Baseline         0.2700657          0.4633180      0.4380976      0.4380976 #> BSC              0.1967275          0.4601609      0.4803104      0.4803104 #> BPL              0.2073401          0.5187046      0.5762644      0.5762644 #> Final            0.2073401          0.5187046      0.5762644      0.5762644 #>            kendall    kappa2 kappa_fleiss kappa_light percentage_agreement #> Baseline 0.7313550 0.4464147    0.4464147   0.4464147            0.7666667 #> BSC      0.7476550 0.4851209    0.4851209   0.4851209            0.7720000 #> BPL      0.7900515 0.5764172    0.5764172   0.5764172            0.8106667 #> Final    0.7900515 0.5764172    0.5764172   0.5764172            0.8106667 #>           gwet_ac #> Baseline 0.598012 #> BSC      0.591286 #> BPL      0.657888 #> Final    0.657888 library(iotarelr) #> Loading required package: ggplot2 #> Loading required package: ggalluvial iotarelr::plot_iota2_alluvial(test_classifier$reliability$iota_object_end_free)"},{"path":"/articles/classification_tasks.html","id":"saving-and-loading-a-classifier","dir":"Articles","previous_headings":"5 Using AI for Classification","what":"5.4 Saving and Loading a Classifier","title":"02 Classification Tasks","text":"created classifier, saving loading easy. process saving model similar process text embedding models. pass model directory path function save_ai_model. contrast text embedding models can specify additional argument save_format allowing choose save_format = \"tf\" save_format = \"h5\". recommend chose save_format = \"tf\" since recommended format tensorflow. like load model can call function load_ai_model. Similar text embedding models ID added name creation classifier ensuring unique name model. Thus, specify correct folder loading classifier.","code":"save_ai_model(   model=classifier,   model_dir=\"classifiers\",   save_format = \"tf\") classifier<-load_ai_model(   model_dir=\"classifiers/movie_review_classifier_ID_oWsaNEB7b09A1pPB\")"},{"path":"/articles/classification_tasks.html","id":"predicting-new-data","dir":"Articles","previous_headings":"5 Using AI for Classification","what":"5.5 Predicting New Data","title":"02 Classification Tasks","text":"like apply classifier new data, two steps necessary. must first transform raw text numerical expression using exactly text embedding model used training classifier. resulting object can passed method predict get predictions together estimate certainty class/category.","code":""},{"path":"/articles/classification_tasks.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"02 Classification Tasks","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Berlin: Logos. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1–21. https://doi.org/10.3389/feduc.2022.818365 Cascante-Bonilla, P., Tan, F., Qi, Y. & Ordonez, V. (2020). Curriculum Labeling: Revisiting Pseudo-Labeling Semi-Supervised Learning. https://doi.org/10.48550/arXiv.2001.06001 Campesato, O. (2021). Natural Language Processing Fundamentals Developers. Mercury Learning & Information. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713 Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning R (Second edition). Manning Publications Co. https://learning.oreilly.com/library/view/-/9781633439849/?ar Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). Gaithersburg: STATAXIS. Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th ed.). Los Angeles: SAGE. Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing action: Understanding, analyzing, generating text Python. Shelter Island: Manning. Larusson, J. ., & White, B. (Eds.). (2014). Learning Analytics: Research Practice. New York: Springer. https://doi.org/10.1007/978-1-4614-3305-7 Lee, D.‑H. (2013). Pseudo-Label : Simple Efficient Semi-Supervised Learning Method Deep Neural Networks. CML 2013 Workshop : Challenges RepresentationLearning. https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks Papilloud, C., & Hinneburg, . (2018). Qualitative Textanalyse mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler. Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-21980-2 Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N. (2019). Hierarchical Transformers Long Document Classification. 2019 IEEE Automatic Speech Recognition Understanding Workshop (ASRU) (pp. 838–844). IEEE. https://doi.org/10.1109/ASRU46091.2019.9003958 Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural Language Processing. https://aclanthology.org/D14-1162.pdf Schreier, M. (2012). Qualitative Content Analysis Practice. Los Angeles: SAGE. Tunstall, L., Werra, L. von, Wolf, T., & Géron, . (2022). Natural language processing transformers: Building language applications hugging face (Revised edition). Heidelberg: O’Reilly. Vaswani, ., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, . N., Kaiser, L., & Polosukhin, . (2017). Attention Need. https://doi.org/10.48550/arXiv.1706.03762 Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, ., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., . . . Dean, J. (2016). Google’s Neural Machine Translation System: Bridging Gap Human Machine Translation. https://doi.org/10.48550/arXiv.1609.08144","code":""},{"path":"/articles/sharing_and_publishing.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1 Introduction","title":"03 Sharing and Using Trained AI/Models","text":"educational social sciences, common practice share research instruments questionnaires tests. example, Open Test Archive provides researchers practitioners access large number open access instruments. aifeducation assumes AI-based classifiers shareable, similarly research instruments, empower educational social science researchers support application AI educational purposes. Thus, aifeducation aims make sharing process easy possible. aim, every object generated aifeducation can prepared publication basic steps. vignette, like show make AI ready publication use models persons. Now start guide preparing text embedding models.","code":""},{"path":[]},{"path":"/articles/sharing_and_publishing.html","id":"adding-model-descriptions","dir":"Articles","previous_headings":"2 Text Embedding Models","what":"2.1 Adding Model Descriptions","title":"03 Sharing and Using Trained AI/Models","text":"Every object class TextEmbeddingModel comes several methods allowing provide important information potential users model. First, every model needs clear description developed, modified can used. can add description via method set_model_description. method allows provide description English native language model make distribution model easier. can write description HTML allows add links sources publications, add tables highlight important aspects model. like recommend write least English description allow wider community recognize work. Furthermore, description include: kind data used create model. much data used create model. steps performed method used. kinds tasks materials model can used. abstract_eng abstract_native can provide summary description. important like share work repository. keywords_eng keywords_native can set vector keywords help find work search engines. like recommend least provide information English. can access model’s description using method get_model_description Besides description work necessary provide information people involved creating model. can done method set_publication_info. First decide type information like add. two choices: “developer”, “modifier”, set type. type=\"developer\" stores information people involved process developing model. use transformer model Hugging Face, people description model entered developers. cases can use type providing description developed model. cases might wish modify existing model. might case use transformer model adapt model specific domain task. case rely work people modify work. cases can describe modifications setting type=modifier. every type person can add relevant individuals via authors. Please use R’s function personList() . citationyou can provide free text cite work different persons. url can provide link relevant sites model. can access information using get_publication_info. Finally, must provide license using model. can done set_software_license get_software_license. Please note, cases license model must “GPL-3” since software used create model licensed “GPL-3”. Thus, derivative work must also licensed “GPL-3”. documentation work part software. can set licenses Creative Common (CC) Free Documentation License (FDL). can set license documentation using method ‘set_documentation_license’. Now able share work. Please remember save now fully described object described following section 2.2.","code":"example_model$set_model_description(   eng=NULL,   native=NULL,   abstract_eng=NULL,   abstract_native=NULL,   keywords_eng=NULL,   keywords_native=NULL) example_model$get_model_description() example_model$set_publication_info(   type,   authors,   citation,   url=NULL) example_model$get_publication_info() example_model$set_software_license(\"GPL-3\") example_model$set_documentation_license(\"CC BY-SA\")"},{"path":"/articles/sharing_and_publishing.html","id":"saving-and-loading","dir":"Articles","previous_headings":"2 Text Embedding Models","what":"2.2 Saving and Loading","title":"03 Sharing and Using Trained AI/Models","text":"Saving created text embedding model easy using function save_ai_model. function provides unique interface text embedding models. saving work can pass model model directory save model model_dir. Please pass path directory path file function. Internally function creates new folder directory files belonging model stored. can see three text embedding models saved within directory named “text_embedding_models”. Within directory function creates unique folder every model. name folder created using models’ names. Since files stored special structure please edit files manually. want load model, just call function load_ai_model can continue using model. Please note add name model directory path. example stored three models directory “text_embedding_models”. model saved within folder. folder’s name created automatically help name model. Thus, loading model must specify model want load adding model’s name directory path shown . point may wonder ID model’s name although enter ID model’s creation. ID added automatically ensure every model unique name. important like share work persons. Now ready share work. Just provide files within model folder. bert model example folder \"text_embedding_models/bert_embedding_ID_CmyAQKtts5RdlLaS\".","code":"save_ai_model(   model=topic_modeling,    model_dir=\"text_embedding_models\")  save_ai_model(   model=global_vector_clusters_modeling,    model_dir=\"text_embedding_models\")  save_ai_model(   model=bert_modeling,    model_dir=\"text_embedding_models\") topic_modeling<-load_ai_model(   model_dir=\"text_embedding_models/topic_model_embedding_ID_DfO25E1Guuaqw7tM\")  global_vector_clusters_modeling<-load_ai_model(   model_dir=\"text_embedding_models/global_vector_clusters_embedding_ID_5Tu8HFHegIuoW14l\")  bert_modeling<-load_ai_model(   model_dir=\"text_embedding_models/bert_embedding_ID_CmyAQKtts5RdlLaS\")"},{"path":[]},{"path":"/articles/sharing_and_publishing.html","id":"adding-model-descriptions-1","dir":"Articles","previous_headings":"3 Classifiers","what":"3.1 Adding Model Descriptions","title":"03 Sharing and Using Trained AI/Models","text":"Adding model description classifier similar TextEmbeddingModels. methods set_model_description get_model_description can provide detailed description (parameter eng native) classifier English native language classifier. abstract_eng abstract_native can provide corresponding abstract descriptions keywords_eng keywords_native take vector corresponding keywords. case classifier, description include: short reference theoretical models guided development. clear detailed description every single category/class. short statement classifier can used. description kind quantity data used training. Information potential bias data. possible, information inter-coder-reliability coding process providing data. possible, provide link corresponding text embedding model. , can provide description HTML include tables (e.g. reporting reliability initial coding process) links sources publications. Please report performance values classifier description. can accessed directly via example_classifier$reliability$test_metric_mean. methods set_publication_info get_publication_info can provide bibliographic information classifier. contrast TextEmbeddingModels different types author groups. Finally, can manage license using classifier via set_software_license get_software_license. Similar TextEmbeddingModels classifier licensed via “GPL-3” since software used creating classifier applies license. documentation can choose license since documentation part software. setting receivinf license can call methods ‘set_documentation_license’ ‘get_documentation_license’ Now ready sharing classifier. Please remember save changes described following section 3.2.","code":"example_classifier$set_model_description( eng=\"This classifier targets the realization of the need for competence from    the self-determination theory of motivation by Deci and Ryan in lesson plans    and materials. It describes a learner’s need to perceive themselves as capable.    In this classifier, the need for competence can take on the values 0 to 2.    A value of 0 indicates that the learners have no space in the lesson plan to    perceive their own learning progress and that there is no possibility for    self-comparison. At level 1, competence growth is made visible implicitly,    e.g. by demonstrating the ability to carry out complex exercises or peer    control. At level 2, the increase in competence is made explicit by giving    each learner insights into their progress towards the competence goal. For    example, a comparison between the target vs. actual development towards the    learning objectives of the lesson can be made, or the learners receive    explicit feedback on their competence growth from the teacher. Self-assessment    is also possible. The classifier was trained using 790 lesson plans, 298    materials and up to 1,400 textbook tasks. Two people who received coding    training were involved in the coding and the inter-coder reliability for the    need for competence increased from a dynamic iota value of 0.615 to 0.646 over    two rounds of training. The Krippendorffs alpha value, on the other hand,    decreased from 0.516 to 0.484. The classifier is suitable for use in all    settings in which lesson plans and materials are to be reviewed with regard    to their implementation of the need for competence.\", native=\"Dieser Classifier bewertet Unterrichtsentwürfe und Lernmaterial danach,    ob sie das Bedürfnis nach Kompetenzerleben aus der Selbstbestimmungstheorie    der Motivation nach Deci und Ryan unterstützen. Das Kompetenzerleben stellt    das Bedürfnis dar, sich als wirksam zu erleben. Der Classifer unterteilt es    in drei Stufen, wobei 0 bedeutet, dass die Lernenden im Unterrichtsentwurf    bzw. Material keinen Raum haben, ihren eigenen Lernfortschritt wahrzunehmen    und auch keine Möglichkeit zum Selbstvergleich besteht. Bei einer Ausprägung    von 1 wird der Kompetenzzuwachs implizit, also z.B. durch die Durchführung    komplexer Übungen oder einer Peer-Kontrolle ermöglicht. Auf Stufe 2 wird der    Kompetenzzuwachs explizit aufgezeigt, indem jede:r Lernende einen objektiven    Einblick erhält. So kann hier bspw. ein Soll-Ist-Vergleich mit den Lernzielen    der Stunde erfolgen oder die Lernenden erhalten dezidiertes Feedback zu ihrem    Kompetenzzuwachs durch die Lehrkraft. Auch eine Selbstbewertung ist möglich.   Der Classifier wurde anhand von 790 Unterrichtsentwürfen, 298 Materialien und    bis zu 1400 Schulbuchaufgaben traniert. Es waren an der Kodierung zwei Personen    beteiligt, die eine Kodierschulung erhalten haben und die    Inter-Coder-Reliabilität für das Kompetenzerleben würde über zwei    Trainingsrunden von einem dynamischen Iota-Wert von 0,615 auf 0,646 gesteigert.    Der Krippendorffs Alpha-Wert sank hingegen von 0,516 auf 0,484. Er eignet sich    zum Einsatz in allen Settings, in denen Unterrichtsentwürfe und Lernmaterial    hinsichtlich ihrer Umsetzung des Kompetenzerlebens überprüft werden sollen.\", abstract_eng=\"This classifier targets the realization of the need for    competence from Deci and Ryan’s self-determination theory of motivation in l   esson plans and materials. It describes a learner’s need to perceive themselves    as capable. The variable need for competence is assessed by a scale of 0-2.    The classifier was developed using 790 lesson plans, 298 materials and up to    1,400 textbook tasks. A coding training was conducted and the inter-coder    reliabilities of different measures    (i.e. Krippendorff’s Alpha and Dynamic Iota Index) of the individual categories    were calculated at different points in time.\", abstract_native=\"Dieser Classifier bewertet Unterrichtsentwürfe und    Lernmaterial danach, ob sie das Bedürfnis nach Kompetenzerleben aus der    Selbstbestimmungstheorie der Motivation nach Deci & Ryan unterstützen. Das    Kompetenzerleben stellt das Bedürfnis dar, sich als wirksam zu erleben. Der    Classifer unterteilt es in drei Stufen und wurde anhand von 790    Unterrichtsentwürfen, 298 Materialien und bis zu 1400 Schulbuchaufgaben    entwickelt. Es wurden stets Kodierschulungen durchgeführt und die    Inter-Coder-Reliabilitäten der einzelnen Kategorien zu verschiedenen    Zeitpunkten berechnet.\", keywords_eng=c(\"Self-determination theory\", \"motivation\", \"lesson planning\", \"business didactics\"), keywords_native=c(\"Selbstbestimmungstheorie\", \"Motivation\", \"Unterrichtsplanung\", \"Wirtschaftsdidaktik\") example_classifier$set_publication_info(   authors,   citation,   url=NULL) example_classifier$set_software_license(\"GPL-3\") example_classifier$set_documentation_license(\"CC BY-SA\")"},{"path":"/articles/sharing_and_publishing.html","id":"saving-and-loading-1","dir":"Articles","previous_headings":"3 Classifiers","what":"3.2 Saving and Loading","title":"03 Sharing and Using Trained AI/Models","text":"created classifier, saving loading easy due functions save_ai_model load_ai_model. process saving model similar process text embedding models. pass model directory path function save_ai_model. contrast text embedding models can specify additional argument save_format allowing choose save_format = \"tf\" save_format = \"h5\". recommend chose save_format = \"tf\" since recommended format tensorflow. like load model can call function load_ai_model. Similar text embedding models ID added name creation classifier ensuring unique name model. Thus, specify correct folder loading classifier. like share classifier persons provide files within folder \"classifiers/movie_review_classifier_ID_oWsaNEB7b09A1pPB\". Since files stored specific structure change edit files manually. Please note need TextEmbeddingModel used training order predict new data classifier. can request name, label, configuration model example_classifier$get_text_embedding_model()$model. Thus, like share classifier, ensure also share corresponding text embedding model. like apply classifier new data, two steps necessary. First, must transform raw text numerical expression using exactly text embedding model used train classifier. resulting object can passed method predict receive predictions together estimate certainty class/category. information can found vignette 02 classification tasks.","code":"save_ai_model(   model=classifier,   model_dir=\"classifiers\",   save_format = \"tf\") classifier<-load_ai_model(   model_dir=\"classifiers/movie_review_classifier_ID_oWsaNEB7b09A1pPB\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Berding Florian. Author, maintainer. Pargmann Julia. Contributor. Riebenbauer Elisabeth. Contributor. Rebmann Karin. Contributor. Slopinski Andreas. Contributor.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Florian Berding, Julia Pargmann, Elisabeth Riebenbauer,   Karin Rebmann, Andreas Slopinski (2023). AI Education (aifeducation).   R package educators researchers educational social sciences.   URL=https://fberding.github.io/aifeducation/index.html","code":"@Manual{,   title = {AI for Education (aifeducation). A R package for educators and       reserachers of the educational and social sciences.},   author = {Florian Berding and Julia Pargmann and Elisabeth Riebenbauer and Karin Rebmann and Andreas Slopinski},   year = {2023},   url = {https://fberding.github.io/aifeducation/index.html}, }"},{"path":"/index.html","id":"aifeducation","dir":"","previous_headings":"","what":"Artificial Intelligence for Education","title":"Artificial Intelligence for Education","text":"R package Artificial Intelligence Education (aifeducation) designed special needs educators, educational researchers, social researchers. target audience package educators researchers like develop models well persons like use models created researchers/educators. package supports application Artificial Intelligence (AI) Natural Language Processing tasks text embedding, classification, question answering special conditions educational social sciences. : digital data availability: educational social science, data often available handwritten form. example, schools universities, students pupils often solve tasks creating handwritten documents. Thus, educators researchers first transform analogue data digital form, involving human actions. makes data generation financially expensive time-consuming, leading small data sets. high privacy policy standards: Furthermore, educational social science, data often refers humans /actions. kinds data protected privacy policies many countries, limiting access usage data, also results small data sets. long research tradition: Educational social sciences long research tradition generating insights social phenomena well learning teaching. insights incorporated applications AI (e.g., Luan et al. 2020; Wong et al. 2019). makes supervised machine learning important technology since provides link educational social theories models one hand machine learning hand (Berding et al. 2022). However, kind machine learning requires humans generate valid data set training process, leading small data sets. complex constructs: Compared classification tasks , instance, AI differentiate ‘good’ ‘bad’ movie review, constructs educational social sciences complex. example, research instruments motivational psychology require infer personal motifs written essays (e.g., Gruber & Kreuzpointner 2013). reliable valid interpretation kind information requires well qualified human raters, making data generation expensive. also limits size data set. imbalanced data: Finally, data educational social sciences often occurs imbalanced pattern several empirical studies show (Bloemen 2011; Stütz et al. 2022). Imbalanced means categories characteristics data set high absolute frequencies compared categories characteristics. Imbalance AI training guides algorithms focus prioritize categories characteristics high absolute frequencies, increasing risk miss categories/characteristics low frequencies (Haixiang et al. 2017). can lead AI prefer special groups people/materials, imply false recommendations conclusions, miss rare categories characteristics. Currently, package focuses classification tasks can either used diagnose characteristics learners written materials estimate properties learning teaching materials. future, tasks implemented.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Artificial Intelligence for Education","text":"can install development version aifeducation GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"FBerding/aifeducation\",                           dependencies = TRUE)"},{"path":[]},{"path":"/index.html","id":"transforming-texts-into-numbers","dir":"","previous_headings":"Classification Tasks","what":"Transforming Texts into Numbers","title":"Artificial Intelligence for Education","text":"Classification tasks require transformation raw texts representation numbers. step, aifeducation supports newer approaches BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), Longformer (Beltagy, Peters & Cohan 2020) older approaches GlobalVectors (Pennington, Socher & Manning 2014) Latent Dirichlet Allocation/Topic Modeling classification tasks. aifeducation supports usage pre-trained transformer models provided Hugging Face creation new transformers allowing educators researchers develop specialized domain-specific models. package supports analysis long texts. Depending method, long texts transformed vectors , long, split several chunks result sequence vectors.","code":""},{"path":"/index.html","id":"training-ai-under-challenging-conditions","dir":"","previous_headings":"Classification Tasks","what":"Training AI under Challenging Conditions","title":"Artificial Intelligence for Education","text":"second step within classification task, aifeducation integrates important statistical mathematical methods dealing main challenges educational social sciences applying AI. order deal problem imbalanced data sets, package integrates Synthetic Minority Oversampling Technique learning process. Currently Basic Synthetic Minority Oversampling Technique (Chawla et al. 2002), Density-Bases Synthetic Minority Oversampling Technique (Bunkhumpornpat, Sinapiromsaran & Lursinsap 2012), Adaptive Synthetic Sampling Approach Imbalanced Learning (Hem Garcia & Li 2008) implemented via R package smotefamiliy. order address problem small data sets, training loops AI integrate pseudo-labeling (e.g., Lee 2013). Pseudo-labeling technique can used supervised learning. , educators researchers rate part data set train AI part. remainder data processed humans. Instead, AI uses part data learn . Thus, educators researchers provide additional data AI’s learning process without coding . offers possibility add data training process reduce labor costs.","code":""},{"path":"/index.html","id":"evaluating-performance","dir":"","previous_headings":"Classification Tasks","what":"Evaluating Performance","title":"Artificial Intelligence for Education","text":"Classification tasks machine learning comparable empirical method content analysis social science. method looks back long research tradition ongoing discussion evaluate reliability validity generated data. order provide link research tradition provide educators well educational social researchers performance measures familiar , every AI trained package evaluated following measures concepts: Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa equal weights Fleiss’ Kappa multiple raters exact estimation Light’s Kappa multiple raters","code":""},{"path":"/index.html","id":"sharing-trained-ai","dir":"","previous_headings":"","what":"Sharing Trained AI","title":"Artificial Intelligence for Education","text":"Since package based keras, tensorflow, transformer libraries, every trained AI can shared educators researchers. package supports easy use pre-trained AI within R also provides possibility export trained AI environments. Using pre-trained AI classification requires classifier corresponding text embedding model. Just load R start predictions. Vignette 02 classification tasks describes save load objects. vignette 03 Sharing Using Trained AI/Models can find detailed guide document share models.","code":""},{"path":"/index.html","id":"tutorial-and-guides","dir":"","previous_headings":"","what":"Tutorial and Guides","title":"Artificial Intelligence for Education","text":"guide install configure package can found via Get started. short introduction package examples classification tasks can found vignette 02 classification tasks.","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Artificial Intelligence for Education","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Berlin: Logos. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1-21. https://doi.org/10.3389/feduc.2022.818365 Bloemen, . (2011). Lernaufgaben Schulbüchern der Wirtschaftslehre: Analyse, Konstruktion und Evaluation von Lernaufgaben für die Lernfelder industrieller Geschäftsprozesse. Hampp. Bunkhumpornpat, C., Sinapiromsaran, K., & Lursinsap, C. (2012). DBSMOTE: Density-Based Synthetic Minority -sampling Technique. Applied Intelligence, 36(3), 664–684. https://doi.org/10.1007/s10489-011-0287-y Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority -sampling Technique. Journal Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Gruber, N., & Kreuzpointner, L. (2013). Measuring reliability picture story exercises like TAT. PloS One, 8(11), e79450. https://doi.org/10.1371/journal.pone.0079450 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). STATAXIS. Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., & Bing, G. (2017). Learning class-imbalanced data: Review methods applications. Expert Systems Applications, 73, 220–239. https://doi.org/10.1016/j.eswa.2016.12.035 , H., Bai, Y., Garcia, E. ., & Li, S. (2008). ADASYN: Adaptive synthetic sampling approach imbalanced learning. 2008 IEEE International Joint Conference Neural Networks (IEEE World Congress Computational Intelligence) (pp. 1322–1328). IEEE. https://doi.org/10.1109/IJCNN.2008.4633969 Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th Ed.). SAGE. Lee, D.‑H. (2013). Pseudo-Label: Simple Efficient Semi-Supervised Learning Method Deep Neural Networks. CML 2013 Workshop: Challenges RepresentationLearning. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 Luan, H., Geczy, P., Lai, H., Gobert, J., Yang, S. J. H., Ogata, H., Baltes, J., Guerra, R., Li, P., & Tsai, C.‑C. (2020). Challenges Future Directions Big Data Artificial Intelligence Education. Frontiers Psychology, 11, 1–11. https://doi.org/10.3389/fpsyg.2020.580820 Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural Language Processing. https://aclanthology.org/D14-1162.pdf Stütz, S., Berding, F., Reincke, S., & Scheper, L. (2022). Characteristics learning tasks accounting textbooks: AI assisted analysis. Empirical Research Vocational Education Training, 14(1). https://doi.org/10.1186/s40461-022-00138-2 Wong, J., Baars, M., Koning, B. B. de, van der Zee, T., Davis, D., Khalil, M., Houben, G.‑J., & Paas, F. (2019). Educational Theories Learning Analytics: Data Knowledge. D. Ifenthaler, D.-K. Mah, & J. Y.-K. Yau (Eds.), Utilizing Learning Analytics Support Study Success (pp. 3–25). Springer. https://doi.org/10.1007/978-3-319-64792-0_1","code":""},{"path":"/reference/array_to_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Array to matrix — array_to_matrix","title":"Array to matrix — array_to_matrix","text":"Function transforming array matrix.","code":""},{"path":"/reference/array_to_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Array to matrix — array_to_matrix","text":"","code":"array_to_matrix(text_embedding)"},{"path":"/reference/array_to_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Array to matrix — array_to_matrix","text":"text_embedding array containing text embedding. array created via object class TextEmbeddingModel.","code":""},{"path":"/reference/array_to_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Array to matrix — array_to_matrix","text":"Returns matrix contains cases rows columns represent features sequences. sequences concatenated.","code":""},{"path":[]},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"function prepares raw texts use TextEmbeddingModel.","code":""},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"","code":"bow_pp_create_basic_text_rep(   data,   vocab_draft,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = FALSE,   split_tags = FALSE,   language_stopwords = \"de\",   use_lemmata = FALSE,   to_lower = FALSE,   min_termfreq = NULL,   min_docfreq = NULL,   max_docfreq = NULL,   window = 5,   weights = 1/(1:5),   trace = TRUE )"},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"data vector containing raw texts. vocab_draft Object created bow_pp_create_vocab_draft. remove_punct bool TRUE punctuation removed. remove_symbols bool TRUE symbols removed. remove_numbers bool TRUE numbers removed. remove_url bool TRUE urls removed. remove_separators bool TRUE separators removed. split_hyphens bool TRUE hyphens split several tokens. split_tags bool TRUE tags split. language_stopwords string Abbreviation language stopwords removed. use_lemmata bool TRUE lemmas instead original tokens used. to_lower bool TRUE tokens lemmas used lower cases. min_termfreq int Minimum frequency token part vocabulary. min_docfreq int Minimum appearance token documents part vocabulary. max_docfreq int Maximum appearance token documents part vocabulary. window int size window creating feature-co-occurance matrix. weights vector weights corresponding window. vector length must equal window size. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"Returns list class basic_text_rep following components. dfm: Document-Feature-Matrix. Rows correspond documents. Columns represent number tokens document. fcm: Feature-Co-Occurance-Matrix. information: list containing information used vocabulary. : n_sentence:  Number sentences n_document_segments:  Number document segments/raw texts n_token_init:  Number initial tokens n_token_final:  Number final tokens n_lemmata:  Number lemmas configuration: list containing information vocabulary created lower cases vocabulary uses original tokens lemmas. language_model: list containing information applied language model. : model:  udpipe language model label:  label udpipe language model upos:  applied universal part--speech tags language:  language vocab:  data.frame original vocabulary","code":""},{"path":[]},{"path":"/reference/bow_pp_create_vocab_draft.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","title":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","text":"Function creating first draft vocabulary function creates list tokens refer specific universal part--speech tags (UPOS) provides corresponding lemmas.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","text":"","code":"bow_pp_create_vocab_draft(   path_language_model,   data,   upos = c(\"NOUN\", \"ADJ\", \"VERB\"),   label_language_model = NULL,   language = NULL,   chunk_size = 100,   trace = TRUE )"},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","text":"path_language_model string Path udpipe language model used tagging lemmatization. data vector containing raw texts. upos vector containing universal part--speech tags used build vocabulary. label_language_model string Label udpipe language model used. language string Name language (e.g., English, German) chunk_size int Number raw texts processed . trace bool TRUE information progress printed console.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","text":"list following components. vocabdata.frame containing tokens, lemmas, tokens lower case, lemmas lower case. language_model ud_language_modeludpipe language model used tagging. label_language_modelLabel udpipe language model. languageLanguage raw texts. uposUsed univerisal part--speech tags. n_sentenceint Estimated number sentences raw texts. n_tokenint Estimated number tokens raw texts. n_document_segmentsint Estimated number document segments/raw texts.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a first draft of a vocabulary\r\nThis function creates a list of tokens which refer to specific\r\nuniversal part-of-speech tags (UPOS) and provides the corresponding lemmas. — bow_pp_create_vocab_draft","text":"list possible tags can found : https://universaldependencies.org/u/pos/index.html. huge number models can found : https://ufal.mff.cuni.cz/udpipe/2/models.","code":""},{"path":[]},{"path":"/reference/check_aif_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if all necessary python modules are available — check_aif_py_modules","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function checks  python modules necessary package aifeducation work available.","code":""},{"path":"/reference/check_aif_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"","code":"check_aif_py_modules(trace = TRUE)"},{"path":"/reference/check_aif_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"trace bool TRUE list modules availability printed console.","code":""},{"path":"/reference/check_aif_py_modules.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function prints table relevant packages shows modules available unavailable. relevant modules available, functions returns TRUE. cases returns FALSE","code":""},{"path":[]},{"path":"/reference/check_embedding_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Check of compatible text embedding models — check_embedding_models","title":"Check of compatible text embedding models — check_embedding_models","text":"function checks different objects based text embedding model. necessary ensure classifiers used data generated compatible embedding models.","code":""},{"path":"/reference/check_embedding_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check of compatible text embedding models — check_embedding_models","text":"","code":"check_embedding_models(object_list, same_class = FALSE)"},{"path":"/reference/check_embedding_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check of compatible text embedding models — check_embedding_models","text":"object_list list object class EmbeddedText TextEmbeddingClassifierNeuralNet. same_class bool TRUE object must class.","code":""},{"path":"/reference/check_embedding_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check of compatible text embedding models — check_embedding_models","text":"Returns TRUE objects refer text embedding model. FALSE cases.","code":""},{"path":[]},{"path":"/reference/combine_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine embedded texts — combine_embeddings","title":"Combine embedded texts — combine_embeddings","text":"Function combining embedded texts model","code":""},{"path":"/reference/combine_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine embedded texts — combine_embeddings","text":"","code":"combine_embeddings(embeddings_list)"},{"path":"/reference/combine_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine embedded texts — combine_embeddings","text":"embeddings_list list objects class EmbeddedText.","code":""},{"path":"/reference/combine_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine embedded texts — combine_embeddings","text":"Returns object class EmbeddedText contains unique cases input objects.","code":""},{"path":[]},{"path":"/reference/create_bert_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a new transformer based on BERT — create_bert_model","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"function creates transformer configuration based BERT base architecture vocabulary based WordPiece using python libraries 'transformers' 'tokenizers'.","code":""},{"path":"/reference/create_bert_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"","code":"create_bert_model(   model_dir,   vocab_raw_texts = NULL,   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   trace = TRUE )"},{"path":"/reference/create_bert_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"model_dir string Path directory model saved. vocab_raw_texts vector containing raw texts creating vocabulary. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximal position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string name activation function. hidden_dropout_prob double Ratio dropout trace bool TRUE information progress printed console.","code":""},{"path":"/reference/create_bert_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"function return object. Instead configuration vocabulary new model saved disk.","code":""},{"path":"/reference/create_bert_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"train model, pass directory model function train_tune_bert_model.","code":""},{"path":"/reference/create_bert_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171--4186). Association Computational Linguistics. doi:10.18653/v1/N19-1423 Hugging Face documentation https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM","code":""},{"path":[]},{"path":"/reference/create_iota2_mean_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an iota2 object — create_iota2_mean_object","title":"Create an iota2 object — create_iota2_mean_object","text":"Function creates object class iotarelr_iota2 can used package iotarelr. function internal use .","code":""},{"path":"/reference/create_iota2_mean_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an iota2 object — create_iota2_mean_object","text":"","code":"create_iota2_mean_object(   iota2_list,   free_aem = FALSE,   call = \"aifeducation::te_classifier_neuralnet\",   original_cat_labels )"},{"path":"/reference/create_iota2_mean_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an iota2 object — create_iota2_mean_object","text":"iota2_list list objects class iotarelr_iota2. free_aem bool TRUE iota2 objects estimated without forcing assumption weak superiority. call string characterizing source estimation. , function within object estimated. original_cat_labels vector containing original labels category.","code":""},{"path":"/reference/create_iota2_mean_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an iota2 object — create_iota2_mean_object","text":"Returns object class iotarelr_iota2 mean iota2 object.","code":""},{"path":[]},{"path":"/reference/create_longformer_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a new transformer based on Longformer — create_longformer_model","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"function creates transformer configuration based Longformer base architecture vocabulary based Byte-Pair Encoding (BPE) tokenizer using python libraries 'transformers' 'tokenizers'.","code":""},{"path":"/reference/create_longformer_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"","code":"create_longformer_model(   model_dir,   vocab_raw_texts = NULL,   vocab_size = 30522,   add_prefix_space = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   attention_window = 512,   trace = TRUE )"},{"path":"/reference/create_longformer_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"model_dir string Path directory model saved. vocab_raw_texts vector containing raw texts creating vocabulary. vocab_size int Size vocabulary. add_prefix_space bool TRUE additional space insert leading words. max_position_embeddings int Number maximal position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string name activation function. hidden_dropout_prob double Ratio dropout attention_probs_dropout_prob double Ratio dropout attention probabilities. attention_window int Size window around token attention mechanism every layer. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/create_longformer_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"function return object. Instead configuration vocabulary new model saved disk.","code":""},{"path":"/reference/create_longformer_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"train model, pass directory model function train_tune_longformer_model.","code":""},{"path":"/reference/create_longformer_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for creating a new transformer based on Longformer — create_longformer_model","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. doi:10.48550/arXiv.2004.05150 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig","code":""},{"path":[]},{"path":"/reference/create_roberta_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a new transformer based on RoBERTa — create_roberta_model","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"function creates transformer configuration based RoBERTa base architecture vocabulary based Byte-Pair Encoding (BPE) tokenizer using python libraries 'transformers' 'tokenizers'.","code":""},{"path":"/reference/create_roberta_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"","code":"create_roberta_model(   model_dir,   vocab_raw_texts = NULL,   vocab_size = 30522,   add_prefix_space = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   trace = TRUE )"},{"path":"/reference/create_roberta_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"model_dir string Path directory model saved. vocab_raw_texts vector containing raw texts creating vocabulary. vocab_size int Size vocabulary. add_prefix_space bool TRUE additional space insert leading words. max_position_embeddings int Number maximal position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string name activation function. hidden_dropout_prob double Ratio dropout trace bool TRUE information progress printed console.","code":""},{"path":"/reference/create_roberta_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"function return object. Instead configuration vocabulary new model saved disk.","code":""},{"path":"/reference/create_roberta_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"train model, pass directory model function train_tune_roberta_model.","code":""},{"path":"/reference/create_roberta_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for creating a new transformer based on RoBERTa — create_roberta_model","text":"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. doi:10.48550/arXiv.1907.11692 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaConfig","code":""},{"path":[]},{"path":"/reference/create_synthetic_units.html","id":null,"dir":"Reference","previous_headings":"","what":"Create synthetic units — create_synthetic_units","title":"Create synthetic units — create_synthetic_units","text":"Function creating synthetic cases order balance data training TextEmbeddingClassifierNeuralNet. auxiliary function use get_synthetic_cases allow parallel computations.","code":""},{"path":"/reference/create_synthetic_units.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create synthetic units — create_synthetic_units","text":"","code":"create_synthetic_units(embedding, target, k, max_k, method, cat, cat_freq)"},{"path":"/reference/create_synthetic_units.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create synthetic units — create_synthetic_units","text":"embedding Named data.frame containing text embeddings. cases object taken [EmbeddedText]EmbeddedText$embeddings. target Named factor containing labels/categories corresponding cases. k int number nearest neighbors sampling process. max_k int maximum number nearest neighbors sampling process. method vector containing strings requested methods generating new cases. Currently \"smote\",\"dbsmote\", \"adas\" package smotefamily available. cat string category new cases created. cat_freq Object class \"table\" containing absolute frequencies every category/label.","code":""},{"path":"/reference/create_synthetic_units.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create synthetic units — create_synthetic_units","text":"Returns list contains text embeddings new synthetic cases named data.frame labels named factor.","code":""},{"path":[]},{"path":"/reference/EmbeddedText.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedded text — EmbeddedText","title":"Embedded text — EmbeddedText","text":"Object class R6 stores text embeddings generated object class TextEmbeddingModel via method embed().","code":""},{"path":[]},{"path":"/reference/EmbeddedText.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Embedded text — EmbeddedText","text":"embeddings ('data.frame()') data.frame containing text embeddings chunks. Documents rows. Embedding dimensions columns.","code":""},{"path":[]},{"path":"/reference/EmbeddedText.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Embedded text — EmbeddedText","text":"EmbeddedText$new() EmbeddedText$get_model_info() EmbeddedText$get_model_label() EmbeddedText$clone()","code":""},{"path":"/reference/EmbeddedText.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Embedded text — EmbeddedText","text":"Creates new object representing text embeddings.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$new(   model_name = NA,   model_label = NA,   model_date = NA,   model_method = NA,   model_version = NA,   model_language = NA,   param_seq_length = NA,   param_chunks = NULL,   param_overlap = NULL,   param_aggregation = NULL,   embeddings )"},{"path":"/reference/EmbeddedText.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"model_name string Name model generates embedding. model_label string Label model generates embedding. model_date string Date embedding generating model created. model_method string Method underlying embedding model. model_version string Version model generated embedding. model_language string Language model generated embedding. param_seq_length int Maximum number tokens processes generating model chunk. param_chunks int Maximum number chunks supported generating model. param_overlap int Number tokens added beginning sequence next chunk model. param_aggregation string Aggregation method hidden states. embeddings data.frame containing text embeddings.","code":""},{"path":"/reference/EmbeddedText.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Embedded text — EmbeddedText","text":"Method retrieving information model generated embedding.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_model_info()"},{"path":"/reference/EmbeddedText.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"list contain saved information underlying text embedding model.","code":""},{"path":"/reference/EmbeddedText.html","id":"method-get-model-label-","dir":"Reference","previous_headings":"","what":"Method get_model_label()","title":"Embedded text — EmbeddedText","text":"Method retrieving label model generated embedding.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_model_label()"},{"path":"/reference/EmbeddedText.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"string Lable corresponding text embedding model","code":""},{"path":"/reference/EmbeddedText.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Embedded text — EmbeddedText","text":"objects class cloneable method.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$clone(deep = FALSE)"},{"path":"/reference/EmbeddedText.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"deep Whether make deep clone.","code":""},{"path":"/reference/generate_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate ID suffix for objects — generate_id","title":"Generate ID suffix for objects — generate_id","text":"Function generating ID suffix objects class TextEmbeddingModel TextEmbeddingClassifierNeuralNet.","code":""},{"path":"/reference/generate_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate ID suffix for objects — generate_id","text":"","code":"generate_id(length = 16)"},{"path":"/reference/generate_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate ID suffix for objects — generate_id","text":"length int determining length id suffix.","code":""},{"path":"/reference/generate_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate ID suffix for objects — generate_id","text":"Returns string requested length","code":""},{"path":[]},{"path":"/reference/get_coder_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate reliability measures based on content analysis — get_coder_metrics","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"function calculates different reliability measures based empirical research method content analysis.","code":""},{"path":"/reference/get_coder_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"","code":"get_coder_metrics(true_values, predicted_values)"},{"path":"/reference/get_coder_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"true_values factor containing true labels/categories. predicted_values factor containing predicted labels/categories.","code":""},{"path":"/reference/get_coder_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"Returns vector following reliability measures: #' iota_index: Iota Index Iota Reliability Concept Version 2. min_iota2: Minimal Iota Iota Reliability Concept Version 2. avg_iota2: Average Iota Iota Reliability Concept Version 2. max_iota2: Maximum Iota Iota Reliability Concept Version 2. min_alpha: Minmal Alpha Reliability Iota Reliability Concept Version 2. avg_alpha: Average Alpha Reliability Iota Reliability Concept Version 2. max_alpha: Maximum Alpha Reliability Iota Reliability Concept Version 2. static_iota_index: Static Iota Index Iota Reliability Concept Version 2. dynamic_iota_index: Dynamic Iota Index Iota Reliability Concept Version 2. kalpha_nominal: Krippendorff's Alpha nominal variables. kalpha_ordinal: Krippendorff's Alpha ordinal variables. kendall: Kendall's coefficient concordance W. kappa2: Cohen's Kappa equal weights. kappa_fleiss: Fleiss' Kappa multiple raters exact estimation. kappa_light: Light's Kappa multiple raters. percentage_agreement: Percentage Agreement. gwet_ac: Gwet's AC1/AC2 agreement coefficient.","code":""},{"path":[]},{"path":"/reference/get_folds.html","id":null,"dir":"Reference","previous_headings":"","what":"Create cross-validation samples — get_folds","title":"Create cross-validation samples — get_folds","text":"Function creates cross-validation samples ensures relative frequency every category/label within fold equals relative frequency category/label within initial data.","code":""},{"path":"/reference/get_folds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create cross-validation samples — get_folds","text":"","code":"get_folds(target, k_folds)"},{"path":"/reference/get_folds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create cross-validation samples — get_folds","text":"target Named factor containing relevant labels/categories. Missing cases declared NA. k_folds int number folds.","code":""},{"path":"/reference/get_folds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create cross-validation samples — get_folds","text":"Return list following components: val_sample: vector strings containing names cases validation sample. train_sample: vector strings containing names cases train sample. n_folds: int Number realized folds. unlabeled_cases: vector strings containing names unlabeled cases.","code":""},{"path":"/reference/get_folds.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create cross-validation samples — get_folds","text":"parameter target allows cases missing categories/labels. declared NA. cases ignored creating different folds. names saved within component unlabeled_cases. cases can used Pseudo Labeling. function checks absolute frequencies every category/label. absolute frequency sufficient ensure least four cases every fold, number folds adjusted. cases, warning printed console. least four cases per fold necessary ensure training TextEmbeddingClassifierNeuralNet works well options turned .","code":""},{"path":[]},{"path":"/reference/get_n_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the number of chunks/sequences for each case — get_n_chunks","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"Function calculating number chunks/sequences every case","code":""},{"path":"/reference/get_n_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"","code":"get_n_chunks(text_embeddings, features, times)"},{"path":"/reference/get_n_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"text_embeddings data.frame containing text embeddings. features int Number features within sequence. times int Number sequences","code":""},{"path":"/reference/get_n_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"Namedvector integers representing number chunks/sequences every case.","code":""},{"path":[]},{"path":"/reference/get_stratified_train_test_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a stratified random sample — get_stratified_train_test_split","title":"Create a stratified random sample — get_stratified_train_test_split","text":"function creates stratified random sample.difference get_train_test_split function require text embeddings split text embeddings train validation sample.","code":""},{"path":"/reference/get_stratified_train_test_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a stratified random sample — get_stratified_train_test_split","text":"","code":"get_stratified_train_test_split(targets, val_size = 0.25)"},{"path":"/reference/get_stratified_train_test_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a stratified random sample — get_stratified_train_test_split","text":"targets Named vector containing labels/categories case. val_size double Value 0 1 indicating many cases label/category part validation sample.","code":""},{"path":"/reference/get_stratified_train_test_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a stratified random sample — get_stratified_train_test_split","text":"list contains names cases belonging train sample validation sample.","code":""},{"path":[]},{"path":"/reference/get_synthetic_cases.html","id":null,"dir":"Reference","previous_headings":"","what":"Create synthetic cases for balancing training data — get_synthetic_cases","title":"Create synthetic cases for balancing training data — get_synthetic_cases","text":"function creates synthetic cases balancing training object class TextEmbeddingClassifierNeuralNet.","code":""},{"path":"/reference/get_synthetic_cases.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create synthetic cases for balancing training data — get_synthetic_cases","text":"","code":"get_synthetic_cases(   embedding,   times,   features,   target,   method = c(\"smote\"),   max_k = 6 )"},{"path":"/reference/get_synthetic_cases.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create synthetic cases for balancing training data — get_synthetic_cases","text":"embedding Named data.frame containing text embeddings. cases, object taken [EmbeddedText]EmbeddedText$embeddings. times int number sequences/times. features int number features within sequence. target Named factor containing labels corresponding embeddings. method vector containing strings requested methods generating new cases. Currently \"smote\",\"dbsmote\", \"adas\" package smotefamily available. max_k int maximum number nearest neighbors sampling process.","code":""},{"path":"/reference/get_synthetic_cases.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create synthetic cases for balancing training data — get_synthetic_cases","text":"list following components. syntetic_embeddings: Named data.frame containing text embeddings synthetic cases. syntetic_targetsNamed factor containing labels corresponding synthetic cases. n_syntetic_unitstable showing number synthetic cases every label/category.","code":""},{"path":[]},{"path":"/reference/get_train_test_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for splitting data into a train and validation sample — get_train_test_split","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"function creates train validation sample based stratified random sampling. relative frequencies category train validation sample equal relative frequencies initial data (proportional stratified sampling).","code":""},{"path":"/reference/get_train_test_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"","code":"get_train_test_split(embedding, target, val_size)"},{"path":"/reference/get_train_test_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"embedding Object class EmbeddedText. target Named factor containing labels every case. val_size double Ratio 0 1 indicating relative frequency cases used validation sample.","code":""},{"path":"/reference/get_train_test_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"Returns list following components. target_train: Named factor containing labels training sample. embeddings_train: Object class EmbeddedText containing text embeddings training sample target_test: Named factor containing labels validation sample. embeddings_test: Object class EmbeddedText containing text embeddings validation sample","code":""},{"path":[]},{"path":"/reference/install_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Installing necessary python modules to an environment — install_py_modules","title":"Installing necessary python modules to an environment — install_py_modules","text":"Function installing necessary python modules","code":""},{"path":"/reference/install_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Installing necessary python modules to an environment — install_py_modules","text":"","code":"install_py_modules(envname = \"aifeducation\")"},{"path":"/reference/install_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Installing necessary python modules to an environment — install_py_modules","text":"envname string Name environment packages installed.","code":""},{"path":[]},{"path":"/reference/load_ai_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Loading models created with aifeducation — load_ai_model","title":"Loading models created with aifeducation — load_ai_model","text":"Function loading models created aifeducation.","code":""},{"path":"/reference/load_ai_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loading models created with aifeducation — load_ai_model","text":"","code":"load_ai_model(model_dir)"},{"path":"/reference/load_ai_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loading models created with aifeducation — load_ai_model","text":"model_dir Path directory model stored.","code":""},{"path":[]},{"path":"/reference/matrix_to_array_c.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape matrix to array — matrix_to_array_c","title":"Reshape matrix to array — matrix_to_array_c","text":"Function written C++ reshaping matrix containing sequential data array use keras.","code":""},{"path":"/reference/matrix_to_array_c.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape matrix to array — matrix_to_array_c","text":"","code":"matrix_to_array_c(matrix, times, features)"},{"path":"/reference/matrix_to_array_c.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape matrix to array — matrix_to_array_c","text":"matrix matrix containing sequential data. times uword Number sequences. features uword Number features within sequence.","code":""},{"path":"/reference/matrix_to_array_c.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape matrix to array — matrix_to_array_c","text":"Returns array. first dimension corresponds cases, second times, third features.","code":""},{"path":[]},{"path":"/reference/QAExtractModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Question-Answer-Models of Type Extraction — QAExtractModel","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"R6 class stores information modeling question-answer-model. kind model extracts answer given text.","code":""},{"path":"/reference/QAExtractModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"model ('transformers.TFAutoModelForQuestionAnswering') Object class transformers.TFAutoModelForQuestionAnswering transformers python library. Stores qa model. tokenizer ('transformers.AutoTokenizer') Object class transformers.AutoTokenizer transformers python library. Stores tokenizer. qa_pipline ('transformers.QuestionAnsweringPipeline') Object class transformers.QuestionAnsweringPipeline transformers python library.","code":""},{"path":[]},{"path":"/reference/QAExtractModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"QAExtractModel$new() QAExtractModel$save_model() QAExtractModel$load_model() QAExtractModel$answer_question() QAExtractModel$clone()","code":""},{"path":"/reference/QAExtractModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"Method creating new question-answer-model based pretrained model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$new(   model_name,   model_version,   model_language,   model_license,   model_dir_path )"},{"path":"/reference/QAExtractModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"model_name Character Name new model. model_version Character Version model. model_language Character Language model supports. model_license Character License model. model_dir_path string Path directory model stored.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-save-model-","dir":"Reference","previous_headings":"","what":"Method save_model()","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"Method saving question-answer-model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$save_model(model_dir_path)"},{"path":"/reference/QAExtractModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"model_dir_path string Path directory model tokenizer stored.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-load-model-","dir":"Reference","previous_headings":"","what":"Method load_model()","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"Method loading question-answer-model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$load_model(model_dir_path)"},{"path":"/reference/QAExtractModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"model_dir_path string Path directory model tokenizer saved.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-answer-question-","dir":"Reference","previous_headings":"","what":"Method answer_question()","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"Method extracting answers given text.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$answer_question(   question,   knowledge_base,   n_answers = 1,   doc_stride = 128,   max_answer_len = 15,   max_seq_len = 384,   max_question_len = 64,   handle_impossible_answer = FALSE,   align_to_words = TRUE )"},{"path":"/reference/QAExtractModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"question string Question answered. knowledge_base list raw texts search answer. n_answers int Number possible answers generated texts. doc_stride int case knowledge base long model process , text divided several overlapping chunks. parameter determines size overlap. max_answer_len int Maximum length tokens possible answers. answers shorter considered answer. max_seq_len int Maximum length question knowledge base tokenization. context may divided several overlapping chunks. max_question_len int maximum length question tokenization. Longer sequences truncated. handle_impossible_answer bool TRUE impossible answers accepted. align_to_words bool true TRUE algorithm tries align answer real words increases quality results space separated languages.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"objects class cloneable method.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$clone(deep = FALSE)"},{"path":"/reference/QAExtractModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question-Answer-Models of Type Extraction — QAExtractModel","text":"deep Whether make deep clone.","code":""},{"path":"/reference/save_ai_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Saving models created with aifeducation — save_ai_model","title":"Saving models created with aifeducation — save_ai_model","text":"Function saving models created aifeducation.","code":""},{"path":"/reference/save_ai_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saving models created with aifeducation — save_ai_model","text":"","code":"save_ai_model(model, model_dir, save_format = \"tf\")"},{"path":"/reference/save_ai_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saving models created with aifeducation — save_ai_model","text":"model Object class TextEmbeddingClassifierNeuralNet TextEmbeddingModel saved. model_dir Path directory model stored. save_format Format saving model. \"tf\" SavedModel \"h5\" HDF5. relevant model class TextEmbeddingClassifierNeuralNet. recommended use \"tf\".","code":""},{"path":[]},{"path":"/reference/set_config_cpu_only.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting cpu only for tensorflow — set_config_cpu_only","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"functions configurates tensorflow use cpus.","code":""},{"path":"/reference/set_config_cpu_only.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"","code":"set_config_cpu_only()"},{"path":"/reference/set_config_cpu_only.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"function return anything. used side effects.","code":""},{"path":"/reference/set_config_cpu_only.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"os$environ$setdefault(\"CUDA_VISIBLE_DEVICES\",\"-1\")","code":""},{"path":[]},{"path":"/reference/set_config_gpu_low_memory.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting gpus' memory usage — set_config_gpu_low_memory","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function changes memory usage gpus allow computations machines small memory. function, computations large models may possible speed computation decreases. #'@return function return anything. used side effects.","code":""},{"path":"/reference/set_config_gpu_low_memory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"","code":"set_config_gpu_low_memory()"},{"path":"/reference/set_config_gpu_low_memory.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function sets TF_GPU_ALLOCATOR \"cuda_malloc_async\" sets memory growth TRUE.","code":""},{"path":[]},{"path":"/reference/set_config_os_environ_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information in tensor flow. — set_config_os_environ_logger","title":"Sets the level for logging information in tensor flow. — set_config_os_environ_logger","text":"function changes level logging information tensorflow via os environment. function must called importing tensorflow.","code":""},{"path":"/reference/set_config_os_environ_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information in tensor flow. — set_config_os_environ_logger","text":"","code":"set_config_os_environ_logger(level = \"ERROR\")"},{"path":"/reference/set_config_os_environ_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information in tensor flow. — set_config_os_environ_logger","text":"level string Minimal level printed console. Five levels available: INFO, WARNING, ERROR NONE.","code":""},{"path":"/reference/set_config_os_environ_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information in tensor flow. — set_config_os_environ_logger","text":"function return anything. used side effects.","code":""},{"path":[]},{"path":"/reference/set_config_tf_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information in tensor flow. — set_config_tf_logger","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"function changes level logging information tensorflow.","code":""},{"path":"/reference/set_config_tf_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"","code":"set_config_tf_logger(level = \"ERROR\")"},{"path":"/reference/set_config_tf_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"level string Minimal level printed console. Five levels available: FATAL, ERROR, WARN, INFO, DEBUG.","code":""},{"path":"/reference/set_config_tf_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"function return anything. used side effects.","code":""},{"path":[]},{"path":"/reference/split_labeled_unlabeled.html","id":null,"dir":"Reference","previous_headings":"","what":"Split data into labeled and unlabeled data — split_labeled_unlabeled","title":"Split data into labeled and unlabeled data — split_labeled_unlabeled","text":"functions splits data labeled unlabeled data.","code":""},{"path":"/reference/split_labeled_unlabeled.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split data into labeled and unlabeled data — split_labeled_unlabeled","text":"","code":"split_labeled_unlabeled(embedding, target)"},{"path":"/reference/split_labeled_unlabeled.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split data into labeled and unlabeled data — split_labeled_unlabeled","text":"embedding Object class EmbeddedText. target Named factor containing cases labels missing labels.","code":""},{"path":"/reference/split_labeled_unlabeled.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split data into labeled and unlabeled data — split_labeled_unlabeled","text":"Returns list following components embeddings_labeled: Object class EmbeddedText containing cases labels. embeddings_unlabeled: Object class EmbeddedText containing cases labels. targets_labeled: Named factor containing labels relevant cases.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":null,"dir":"Reference","previous_headings":"","what":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Abstract class neural nets keras tensorflow","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"model ('tensorflow_model()') Field storing tensorflow model loading. model_config ('list()') List storing information configuration model. information used predict new data. model_config$n_req: Number recurrent layers. model_config$n_hidden: Number dense layers. model_config$target_levels: Levels target variable. change manually. model_config$input_variables: Order name input variables. change manually. model_config$init_config: List storing parameters passed method new(). last_training ('list()') List storing history results last training. information overwritten new training started. last_training$learning_time: Duration training process. config$history: History last training. config$data: Object class table storing initial frequencies passed data. config$data_pb:l Matrix storing number additional cases (test training) added balanced pseudo-labeling. rows refer folds final training. columns refer steps pseudo-labeling. config$data_bsc_test: Matrix storing number cases category used testing phase balanced synthetic units. Please note frequencies include original synthetic cases. case number original synthetic cases exceeds limit majority classes, frequency represents number cases created cluster analysis. config$date: Time last training finished. config$config: List storing kind estimation requested last training. config$config$use_bsc:  TRUE  balanced synthetic cases requested. FALSE . config$config$use_baseline: TRUE baseline estimation requested. FALSE . config$config$use_bpl: TRUE  balanced, pseudo-labeling cases requested. FALSE . reliability ('list()') List storing central reliability measures last training. reliability$test_metric: Array containing reliability measures validation data every fold, method, step (case pseudo-labeling). reliability$test_metric_mean: Array containing reliability measures validation data every method step (case pseudo-labeling). values represent mean values every fold. reliability$raw_iota_objects: List containing iota_object generated package [iotarelr]iotarelr every fold start end last training. reliability$raw_iota_objects$iota_objects_start: List objects class iotarelr_iota2 containing estimated iota reliability second generation baseline model every fold. estimation baseline model requested, list set NULL. reliability$raw_iota_objects$iota_objects_end: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold. Depending requested training method values refer baseline model, trained model basis balanced synthetic cases, balanced pseudo labeling combination balanced synthetic cases pseudo labeling. reliability$raw_iota_objects$iota_objects_start_free: List objects class iotarelr_iota2 containing estimated iota reliability second generation baseline model every fold. estimation baseline model requested, list set NULL.Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$raw_iota_objects$iota_objects_end_free: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold. Depending requested training method, values refer baseline model, trained model basis balanced synthetic cases, balanced pseudo-labeling combination balanced synthetic cases pseudo-labeling. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$iota_object_start: Object class iotarelr_iota2 mean individual objects every fold. estimation baseline model requested, list set NULL. reliability$iota_object_start_free: Object class iotarelr_iota2 mean individual objects every fold. estimation baseline model requested, list set NULL. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$iota_object_end: Object class iotarelr_iota2 mean individual objects every fold. Depending requested training method, object refers baseline model, trained model basis balanced synthetic cases, balanced pseudo-labeling combination balanced synthetic cases pseudo-labeling. reliability$iota_object_end_free: Object class iotarelr_iota2 mean individual objects every fold. Depending requested training method, object refers baseline model, trained model basis balanced synthetic cases, balanced pseudo-labeling combination balanced synthetic cases pseudo-labeling. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"TextEmbeddingClassifierNeuralNet$new() TextEmbeddingClassifierNeuralNet$train() TextEmbeddingClassifierNeuralNet$predict() TextEmbeddingClassifierNeuralNet$get_model_info() TextEmbeddingClassifierNeuralNet$get_text_embedding_model() TextEmbeddingClassifierNeuralNet$set_publication_info() TextEmbeddingClassifierNeuralNet$get_publication_info() TextEmbeddingClassifierNeuralNet$set_software_license() TextEmbeddingClassifierNeuralNet$get_software_license() TextEmbeddingClassifierNeuralNet$set_documentation_license() TextEmbeddingClassifierNeuralNet$get_documentation_license() TextEmbeddingClassifierNeuralNet$set_model_description() TextEmbeddingClassifierNeuralNet$get_model_description() TextEmbeddingClassifierNeuralNet$save_model() TextEmbeddingClassifierNeuralNet$load_model() TextEmbeddingClassifierNeuralNet$get_package_versions() TextEmbeddingClassifierNeuralNet$clone()","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Creating new instance class.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$new(   name = NULL,   label = NULL,   text_embeddings = NULL,   targets = NULL,   hidden = c(128),   rec = c(128),   self_attention_heads = 0,   dropout = 0.4,   recurrent_dropout = 0.4,   l2_regularizer = 0.01,   optimizer = \"adam\",   act_fct = \"gelu\",   rec_act_fct = \"tanh\" )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"name Character Name new classifier. Please refer common name conventions. Free text can used parameter label. label Character Label new classifier. can use free text. text_embeddings object classTextEmbeddingModel. targets factor containing target values classifier. hidden vector containing number neurons dense layer. length vector determines number dense layers. want dense layer, set parameter NULL. rec vector containing number neurons recurrent layer. length vector determines number dense layers. want dense layer, set parameter NULL. self_attention_heads integer determining number attention heads self-attention layer. value greater 0, self-attention layer added recurrent dense layers together normalization recurrent layer. set 0, none layers added. dropout double ranging 0 lower 1, determining dropout recurrent layer. recurrent_dropout double ranging 0 lower 1, determining recurrent dropout recurrent layer. l2_regularizer double determining l2 regularizer every dense layer. optimizer Object class keras.optimizers. act_fct character naming activation function dense layers. rec_act_fct character naming activation function recurrent layers.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method training neural net keras tensorflow.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$train(   data_embeddings,   data_targets,   data_n_test_samples = 5,   use_baseline = TRUE,   bsl_val_size = 0.25,   use_bsc = TRUE,   bsc_methods = c(\"dbsmote\"),   bsc_max_k = 10,   bsc_val_size = 0.25,   bsc_add_all = FALSE,   use_bpl = TRUE,   bpl_max_steps = 3,   bpl_epochs_per_step = 1,   bpl_dynamic_inc = FALSE,   bpl_balance = TRUE,   bpl_max = 1,   bpl_anchor = 1,   bpl_min = 0,   bpl_weight_inc = 0.02,   bpl_weight_start = 0,   bpl_model_reset = FALSE,   epochs = 40,   batch_size = 32,   dir_checkpoint,   trace = TRUE,   keras_trace = 2,   n_cores = 2 )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"data_embeddings Object class TextEmbeddingModel. data_targets Factor containing labels cases stored data_embeddings. Factor must named use names used data_embeddings. data_n_test_samples int determining number cross-fold samples. use_baseline bool TRUE calculation baseline model requested. option relevant use_bsc=TRUE use_pbl=TRUE. FALSE, baseline model calculated. bsl_val_size double 0 1, indicating proportion cases class used validation sample estimation baseline model. remaining cases part training data. use_bsc bool TRUE estimation integrate balanced synthetic cases. FALSE . bsc_methods vector containing methods generating synthetic cases via smotefamily. Multiple methods can passed. Currently bsc_methods=c(\"adas\"), bsc_methods=c(\"smote\") bsc_methods=c(\"dbsmote\") possible. bsc_max_k int determining maximal number k used creating synthetic units. bsc_val_size double 0 1, indicating proportion cases class used validation sample estimation synthetic cases. bsc_add_all bool FALSE synthetic cases necessary fill gab class major class added data. TRUE generated synthetic cases added data. use_bpl bool TRUE estimation integrate balanced pseudo-labeling. FALSE . bpl_max_steps int determining maximum number steps pseudo-labeling. bpl_epochs_per_step int Number training epochs within every step. bpl_dynamic_inc bool TRUE, specific percentage cases included step. percentage determined \\(step/bpl_max_steps\\). FALSE, cases used. bpl_balance bool TRUE, number cases every category/class pseudo-labeled data used training. , number cases determined minor class/category. bpl_max double 0 1, setting maximal level confidence considering case pseudo-labeling. bpl_anchor double 0 1 indicating reference point sorting new cases every label. See notes details. bpl_min double 0 1, setting minimal level confidence considering case pseudo-labeling. bpl_weight_inc double value much sample weights increased cases pseudo-labels every step. bpl_weight_start dobule Starting value weights unlabeled cases. bpl_model_reset bool TRUE, model re-initialized every step. epochs int Number training epochs. batch_size int Size batches. dir_checkpoint string Path directory checkpoint training saved. directory exist, created. trace bool TRUE, information estimation phase printed console. keras_trace int keras_trace=0 cat information training process keras console. keras_trace=1 cat progress bar. keras_trace=2 prints one line information every epoch. n_cores int Number cores used creating synthetic units.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"bsc_max_k: values 2 bsc_max_k successively used. number bsc_max_k high, value reduced number allows calculating synthetic units. bpl_anchor: help value, new cases sorted. aim, distance anchor calculated cases arranged ascending order.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method predicting new data trained neural net.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$predict(newdata, batch_size = 32, verbose = 1)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"newdata Object class TextEmbeddingModel data.frame predictions made. batch_size int Size batches. verbose int verbose=0 cat information training process keras console. verbose=1 prints progress bar. verbose=2 prints one line information every epoch.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Returns data.frame containing predictions probabilities different labels case.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method requesting model information","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_model_info()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"list relevant model information","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-text-embedding-model-","dir":"Reference","previous_headings":"","what":"Method get_text_embedding_model()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method requesting text embedding model information","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_text_embedding_model()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"list relevant model information text embedding model underlying classifier","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method setting publication information classifier","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_publication_info(   authors,   citation,   url = NULL )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"authors List authors. citation Free text citation. url URL corresponding homepage.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method requesting bibliographic information classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_publication_info()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"list saved bibliographic information.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-software-license-","dir":"Reference","previous_headings":"","what":"Method set_software_license()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method setting license classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_software_license(license = \"GPL-3\")"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-software-license-","dir":"Reference","previous_headings":"","what":"Method get_software_license()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method getting license classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_software_license()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-documentation-license-","dir":"Reference","previous_headings":"","what":"Method set_documentation_license()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method setting license classifier's documentation.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_documentation_license(   license = \"CC BY-SA\" )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-documentation-license-","dir":"Reference","previous_headings":"","what":"Method get_documentation_license()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method getting license classifier's documentation.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_documentation_license()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method setting description classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_model_description(   eng = NULL,   native = NULL,   abstract_eng = NULL,   abstract_native = NULL,   keywords_eng = NULL,   keywords_native = NULL )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"eng string text describing training learner, theoretical empirical background, different output labels English. native string text describing training learner, theoretical empirical background, different output labels native language classifier. abstract_eng string text providing summary description English. abstract_native string text providing summary description native language classifier. keywords_eng vector keyword English. keywords_native vector keyword native language classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method requesting model description.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_model_description()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"list description classifier English native language.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-save-model-","dir":"Reference","previous_headings":"","what":"Method save_model()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method saving model tensorflow SavedModel format h5 format.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$save_model(dir_path, save_format = \"tf\")"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"dir_path string() Path directory model saved. save_format Format saving model. \"tf\" SavedModel \"h5\" HDF5.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-load-model-","dir":"Reference","previous_headings":"","what":"Method load_model()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method importing model tensorflow SavedModel format h5 format.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$load_model(dir_path)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"dir_path string() Path directory model saved.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-package-versions-","dir":"Reference","previous_headings":"","what":"Method get_package_versions()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Method requesting summary R python packages' versions used creating classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_package_versions()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"Returns list containing versions relevant R python packages.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"objects class cloneable method.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$clone(deep = FALSE)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TextEmbeddingClassifierNeuralNet","text":"deep Whether make deep clone.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Text embedding model — TextEmbeddingModel","title":"Text embedding model — TextEmbeddingModel","text":"R6 class stores text embedding model can used tokenize, encode, decode, embed raw texts. object provides unique interface different text processing methods.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text embedding model — TextEmbeddingModel","text":"basic_components ('list()') List storing information can apply methods. basic_components$method: Method underlying text embedding model. basic_components$max_length: Maximum number tokens sequence model processes. general, shorter sequences padded longer sequences divided chunks /truncated. transformer_components ('list()') List storing information apply BERT models. transformer_components$model: object class transformers.TFBertModel using transformers library. transformer_components$tokenizer: object class transformers.BertTokenizerFast using transformers library. transformer_components$aggregation: Aggregation method hidden states. transformer_components$chunks: Maximal number chunks processed model. transformer_components$overlap: Number tokens added beginning sequence next chunk. bow_components ('list()') List storing information apply bow_models. bow_components$model: data.frame describing relationship tokens corresponding text embeddings. bow_components$vocab: data.frame saving tokens, lemmas corresponding integer index. bow_components$configuration: List configuration parameters model. bow_components$configuration$to_lower TRUE tokens transformed lower case. bow_components$configuration$use_lemmata TRUE corresponding lemma used instead token. bow_components$configuration$bow_n_dim Number dimensions GlobalVectors Topic Modeling. bow_components$configuration$bow_n_cluster Number clusters grouping tokens based global vectors. apply method lda. bow_components$configuration$bow_max_iter Maximum number iterations calculate global vectors topics. bow_components$configuration$bow_max_iter_cluster Maximum number iterations creating clusters. Applies method glove. bow_components$configuration$bow_cr_criterion Convergence criterion calculating global vectors topics. bow_components$configuration$bow_learning_rate  Initial learning rate estimating global vectors. bow_components$aggregation: currently apply methods. bow_components$chunks: currently apply methods. bow_components$overlap: currently apply methods.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text embedding model — TextEmbeddingModel","text":"TextEmbeddingModel$new() TextEmbeddingModel$load_model() TextEmbeddingModel$save_model() TextEmbeddingModel$encode() TextEmbeddingModel$decode() TextEmbeddingModel$embed() TextEmbeddingModel$set_publication_info() TextEmbeddingModel$get_publication_info() TextEmbeddingModel$set_software_license() TextEmbeddingModel$get_software_license() TextEmbeddingModel$set_model_description() TextEmbeddingModel$get_model_description() TextEmbeddingModel$set_documentation_license() TextEmbeddingModel$get_documentation_license() TextEmbeddingModel$get_model_info() TextEmbeddingModel$clone()","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Text embedding model — TextEmbeddingModel","text":"Method creating new text embedding model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$new(   model_name = NULL,   model_label = NULL,   model_version = NULL,   model_language = NULL,   method = NULL,   max_length = 0,   chunks = 1,   overlap = 0,   aggregation = \"last\",   model_dir,   bow_basic_text_rep,   bow_n_dim = 10,   bow_n_cluster = 100,   bow_max_iter = 500,   bow_max_iter_cluster = 500,   bow_cr_criterion = 1e-08,   bow_learning_rate = 1e-08,   trace = FALSE )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"model_name string containing name new model. model_label string containing label/title new model. model_version string version model. model_language string containing language model represents (e.g., English). method string determining kind embedding model. Currently three types supported. method=\"bert\" Bidirectional Encoder Representations Transformers (BERT), method=\"glove\" GlobalVector Clusters, method=\"lda\" topic modeling. See details information. max_length int determining maximum length token sequences uses BERT models. relevant methods. chunks int Maximum number chunks. relevant BERT models. overlap int determining number tokens added beginning next chunk. relevant BERT models. aggregation string method aggregating text embeddings created BERT models. See details information. model_dir string path directory BERT model stored. bow_basic_text_rep object class basic_text_rep created via function bow_pp_create_basic_text_rep. relevant method=\"glove\" method=\"lda\". bow_n_dim int Number dimensions GlobalVector number topics LDA. bow_n_cluster int Number clusters created basis GlobalVectors. Parameter relevant method=\"lda\" method=\"bert\" bow_max_iter int Maximum number iterations fitting GlobalVectors Topic Models. bow_max_iter_cluster int Maximum number iterations fitting cluster method=\"glove\". bow_cr_criterion double convergence criterion GlobalVectors. bow_learning_rate double initial learning rate GlobalVectors. trace bool TRUE prints information progress. FALSE .","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text embedding model — TextEmbeddingModel","text":"method: case method=\"bert\", pretrained BERT model must supplied via model_dir. method=\"glove\" method=\"lda\" new model created based data provided via bow_basic_text_rep. original algorithm GlobalVectors provides word embeddings, text embeddings. achieve text embeddings words clustered based word embeddings kmeans. aggregation: creating text embedding BERT model, several options possible: last: aggregation=\"last\" uses hidden states last layer. second_to_last: aggregation=\"second_to_last\" uses hidden states second last layer. fourth_to_last: aggregation=\"fourth_to_last\" uses hidden states fourth last layer. : aggregation=\"\" uses mean hidden states hidden layers. last_four: aggregation=\"last_four\" uses mean hidden states last four layers.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-load-model-","dir":"Reference","previous_headings":"","what":"Method load_model()","title":"Text embedding model — TextEmbeddingModel","text":"Method loading transformers model R.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$load_model(model_dir)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"model_dir string containing path relevant model directory.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-save-model-","dir":"Reference","previous_headings":"","what":"Method save_model()","title":"Text embedding model — TextEmbeddingModel","text":"Method saving transformer model disk.Relevant transformer models.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$save_model(model_dir)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"model_dir string containing path relevant model directory.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-encode-","dir":"Reference","previous_headings":"","what":"Method encode()","title":"Text embedding model — TextEmbeddingModel","text":"Method encoding words raw texts integers.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$encode(   raw_text,   token_encodings_only = FALSE,   trace = FALSE )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"raw_text vector containing raw texts. token_encodings_only bool TRUE, token encodings returned. FALSE, complete encoding returned important BERT models. trace bool TRUE, information progress printed. FALSE requested.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list containing integer sequences raw texts special tokens.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-decode-","dir":"Reference","previous_headings":"","what":"Method decode()","title":"Text embedding model — TextEmbeddingModel","text":"Method decoding sequence integers tokens","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$decode(int_seqence)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"int_seqence list containing integer sequences transformed tokens single integer sequence vector","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list token sequences","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-embed-","dir":"Reference","previous_headings":"","what":"Method embed()","title":"Text embedding model — TextEmbeddingModel","text":"Method creating text embeddings raw texts case using GPU running memory reduce batch size restart R switch use cpu via set_config_cpu_only.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$embed(   raw_text = NULL,   doc_id = NULL,   batch_size = 8,   trace = FALSE )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"raw_text vector containing raw texts. doc_id vector containing corresponding IDs every text. batch_size int determining maximal size every batch. trace bool TRUE, information progression printed console.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Method returns R6 object class EmbeddedText. object contains embeddings data.frame information model creating embeddings.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting bibliographic information model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_publication_info(type, authors, citation, url = NULL)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"type string Type information changed/added. type=\"developer\", type=\"modifier\" possible. authors List people. citation string Citation free text. url string Corresponding URL applicable.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method getting bibliographic information model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_publication_info()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list bibliographic information.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-software-license-","dir":"Reference","previous_headings":"","what":"Method set_software_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting license model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_software_license(license = \"GPL-3\")"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-software-license-","dir":"Reference","previous_headings":"","what":"Method get_software_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting license model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_software_license()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"string License model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting description model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_model_description(   eng = NULL,   native = NULL,   abstract_eng = NULL,   abstract_native = NULL,   keywords_eng = NULL,   keywords_native = NULL )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"eng string text describing training classifier, theoretical empirical background, different output labels English. native string text describing training classifier, theoretical empirical background, different output labels native language model. abstract_eng string text providing summary description English. abstract_native string text providing summary description native language classifier. keywords_eng vector keywords English. keywords_native vector keywords native language classifier.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting model description.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_description()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list description model English native language.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-documentation-license-","dir":"Reference","previous_headings":"","what":"Method set_documentation_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting license models' documentation.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_documentation_license(license = \"CC BY-SA\")"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-documentation-license-","dir":"Reference","previous_headings":"","what":"Method get_documentation_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method getting license models' documentation.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_documentation_license()"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting model information","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_info()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list relevant model information","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text embedding model — TextEmbeddingModel","text":"objects class cloneable method.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$clone(deep = FALSE)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"deep Whether make deep clone.","code":""},{"path":"/reference/train_tune_bert_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for training and fine-tuning a BERT model — train_tune_bert_model","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"function can used train fine-tune transformer based BERT architecture help python libraries 'transformers', 'datasets', 'tokenizers'.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"","code":"train_tune_bert_model(   output_dir,   bert_model_dir_path,   raw_texts,   aug_vocab_by = 100,   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   learning_rate = 0.003,   n_workers = 1,   multi_process = FALSE,   trace = TRUE )"},{"path":"/reference/train_tune_bert_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"output_dir string Path directory final model saved. directory exist, created. bert_model_dir_path string Path directory original model stored. raw_texts vector containing raw texts training. aug_vocab_by int Number entries extending current vocabulary. See notes details p_mask double Ratio determining number words/tokens masking. whole_word bool TRUE whole word masking applied. FALSE token masking used. val_size double Ratio determining amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. learning_rate double Learning rate adam optimizer. n_workers int Number workers. multi_process bool TRUE multiple processes activated. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"function return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"aug_vocab_by > 0 raw text used training WordPiece tokenizer. end process, additional entries added vocabulary part original vocabulary. experimental state. Pre-Trained models can fine-tuned function available https://huggingface.co/. New models can created via function create_bert_model. Training model makes use dynamic masking contrast original paper static masking applied.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for training and fine-tuning a BERT model — train_tune_bert_model","text":"Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171--4186). Association Computational Linguistics. doi:10.18653/v1/N19-1423 Hugging Face documentation https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM","code":""},{"path":[]},{"path":"/reference/train_tune_longformer_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"function can used train fine-tune transformer based BERT architecture help python libraries 'transformers', 'datasets', 'tokenizers'.","code":""},{"path":"/reference/train_tune_longformer_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"","code":"train_tune_longformer_model(   output_dir,   model_dir_path,   raw_texts,   p_mask = 0.15,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   learning_rate = 0.03,   n_workers = 1,   multi_process = FALSE,   trace = TRUE )"},{"path":"/reference/train_tune_longformer_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. raw_texts vector containing raw texts training. p_mask double Ratio determining number words/tokens masking. val_size double Ratio determining amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. learning_rate bool Learning rate adam optimizer. n_workers int Number workers. multi_process bool TRUE multiple processes activated. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/train_tune_longformer_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"function return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"/reference/train_tune_longformer_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"Pre-Trained models can fine-tuned function available https://huggingface.co/. New models can created via function create_roberta_model. Training model makes use dynamic masking.","code":""},{"path":"/reference/train_tune_longformer_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for training and fine-tuning a Longformer model — train_tune_longformer_model","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. doi:10.48550/arXiv.2004.05150 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig","code":""},{"path":[]},{"path":"/reference/train_tune_roberta_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"function can used train fine-tune transformer based BERT architecture help python libraries 'transformers', 'datasets', 'tokenizers'.","code":""},{"path":"/reference/train_tune_roberta_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"","code":"train_tune_roberta_model(   output_dir,   model_dir_path,   raw_texts,   p_mask = 0.15,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   learning_rate = 0.03,   n_workers = 1,   multi_process = FALSE,   trace = TRUE )"},{"path":"/reference/train_tune_roberta_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. raw_texts vector containing raw texts training. p_mask double Ratio determining number words/tokens masking. val_size double Ratio determining amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. learning_rate bool Learning rate adam optimizer. n_workers int Number workers. multi_process bool TRUE multiple processes activated. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/train_tune_roberta_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"function return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"/reference/train_tune_roberta_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"Pre-Trained models can fine-tuned function available https://huggingface.co/. New models can created via function create_roberta_model. Training model makes use dynamic masking.","code":""},{"path":"/reference/train_tune_roberta_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function for training and fine-tuning a RoBERTa model — train_tune_roberta_model","text":"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. doi:10.48550/arXiv.1907.11692 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaConfig","code":""},{"path":[]}]

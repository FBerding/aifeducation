[{"path":"/articles/aifeducation.html","id":"installation-and-technical-requirements","dir":"Articles","previous_headings":"","what":"1) Installation and Technical Requirements","title":"01 Get started","text":"Several packages allow users use machine learning directly R nnet single layer neural nets, rpart decision tress, ranger random forest. Furthermore, mlr3verse series packages exist managing different algorithms unified interface. packages can used ‘normal’ computer provide easy installation. terms natural language processing approaches currently limited. State--art approaches rely neural nets multiple layers consist huge number parameters making computationally demanding. specialized libraries keras tensorflow graphical processing units (gpu) can help speed computations significantly. However, many specialized libraries machine learning written python. interface pyhton fourtunely provided via R package reticulate. R package Artificial Intelligence Education (aifeducation) aims provide educators, educational researchers, social researchers convince interface state--art models natural language processing tries address special needs challenges educational social science. package currently supports application Artificial Intelligence (AI) tasks text embedding, classification, question answering. Since state--art approaches natural language processing rely large models compared classical statistical methods (e.g., latent class analysis, structural equation modeling) based largely python additional installation steps necessary. like train develop models AIs compatible graphic device necessary. Even low performing graphic device can speed computations significantly. like use pre-trained models necessary. case ‘normal’ office computer without graphic device sufficient cases. order use package must first install . can done : command necessary R packages installed machine. Since natural language processing neural nets based models computational intensive keras tensorflow used within package together specialized python libraries. install must first install python machine. may take time. can check everything working using function reticulate::py_available(). return TRUE. everything working can now install remaining python libraries. convenience aifeducation comes helper function install_py_modules() . function installs following python modules: os, transformers, tokenizers, datasets, torch, keras, tensorflow dependencies environment “aifeducation”. like use aifeducation packages environments please ensure python modules available. check_aif_py_modules() can check modules successfully installed. Now everything ready use package. start new R session please note call reticulate::use_condaenv(condaenv = \"aifeducation\") machine windows reticulate::use_virtualenv(condaenv = \"aifeducation\") mac linux make python modules available work.","code":"# install.packages(\"devtools\") devtools::install_github(\"FBerding/aifeducation\") reticulate::install_python() reticulate::py_available(initialize = TRUE) install_py_modules(envname=\"aifeducation\") aifeducation::check_aif_py_modules(print=TRUE)"},{"path":"/articles/aifeducation.html","id":"configuration","dir":"Articles","previous_headings":"","what":"2) Configuration","title":"01 Get started","text":"general, educators educational researchers neither access high performance computing computers performing graphic device work. Thus, additional configuration can done make computations machine working. use computer graphic device can disable graphic device support tensorflow function set_config_cpu_only(). Now machine uses cpus computations. machine graphic card limited memory recommended change configuration memory usage set_config_gpu_low_memory() enables machine compute even ‘large’ models limited resources. ‘small’ models option relevant since decreases computational speed. Finally, cases like disable tensorflow print information console. can change behavior function set_config_tf_logger(). can choose five levels “FATAL”, “ERROR”, “WARN”, “INFO”, “DEBUG” setting minimal level logging.","code":"aifeducation::set_config_cpu_only() aifeducation::set_config_gpu_low_memory()"},{"path":"/articles/classification_tasks.html","id":"introduction-and-overview","dir":"Articles","previous_headings":"","what":"1 Introduction and Overview","title":"02 Classification Tasks","text":"educational social science assigning observation scientific concepts important task allowing understand observation, generate new insights, derive recommendations research practice. educational science several areas deal kind task. example, diagnosing students characteristics important aspect teachers’ profession understanding promoting learning. Another example learning analytics data students used provide learning environments adapted individual needs. level institutions schools universities can use information data driven decision performance (Laurusson & White 2014) improve . case real world observations aligned scientific models order use scientific knowledge technology improving learning instruction. Supervised machine learning one concept allowing link real world observations one hand existing scientific models theories hand (Berding et al. 2022). educational sciences great advantage allows use existing knowledge insights applications AI. drawback approach training AI requires information real world observation one hand information corresponding alignment scientific models theories hand. valuable source data educational science texts since textual data can found everywhere learning teaching (Berding et al. 2022). example, teachers often demand students solve task provide written form. Students create solution tasks often document short written essay presentation. data can used analyzing learning teaching. Teachers’ written tasks students may provide insights quality instruction. Students’ solutions may provide insights learning outcomes prerequisites. AI can helpful assistant analyzing textual data since analysis textual data challenging time consuming task humans conduct content analysis. vignette like show create AI can help tasks using package aifedcuation. Please note introduction content analysis, natural language processing machine learning behind scope vignette. like go details please refer cited literature. start introduce definition understanding basic concepts since applying AI educational contexts means combine knowledge different scientific disciplines using different, sometimes overlapping concepts. Even within research area concepts unified used. Figure 1 illustrates package’s understanding. Since aifeducation looks application AI classification tasks perspective empirical method content analysis overlapping concepts content analysis machine learning. content analysis phenomenon like performance colors can described scale/dimension made several categories (e.g. Schreier 2012 pp. 59). example exam’s performance (scale/dimension) “good”, “average” “poor”. terms colors (scale/dimension) categories “blue”, “green” etc. Machine learning literature uses words describe kind data. machine learning “scale” “dimension” corresponds term “label” “categories” refer term “classes” (Chollet, Kalinowski & Allaire 2022, p. 114). clarifications classification means text assigned correct category scale text labeled correct class. train AI classify text accordingly based supervised machine learning two kind data necessary Figure 2 illustrates. providing AI textual data input data corresponding information class target data AI can learn texts imply specific class category. example exams AI can learn texts imply “good”, “average” “poor” judgment. training AI can applied new texts predicts likely class every new text. generated class can used statistical analysis deriving recommendations learning teaching. achieve support artificial intelligence several steps necessary. Figure 3 provides overview integrating functions objects aifeducation. first step transform raw texts form computers can use. , raw texts must transformed numbers. modern approaches done using word embeddings. Campesato (2021, p. 102) describes “collective name set language modeling feature learning techniques (…) words phrases vocabulary mapped vectors real numbers.” Similar definition word vector: „Word vectors represent semantic meaning words vectors context training corpus.“ (Lane, Howard & Hapke 2019, p. 191) Campesato (2021, pp. 112) clusters approaches creating word embeddings three groups reflecting ability provide context sensitive numerical representations. Approaches group one account context. Typical methods rely bag--words assumptions. Thus, normally able provide word embedding single words. Group two consists approaches word2vec, GloVe (Pennington, Socher % Manning 2014) fastText able provide one embedding word regardless context. Thus, account one context. last group consists approaches BERT (Devlin et al. 2019), able produce multiple word embeddings depending context words. different groups aifedcuation implements several methods. Topic Modeling: Topic Modeling approach uses frequencies tokens within text. frequencies tokens models observable variables one latent topics (Campesato 2021, p. 113). estimation topic model often based Latent Dirichlet Analysis (LDA) describes text distribution topics. topics described distribution words/tokens (Campesato 2021, p. 114). relationship texts, words, topics can used create text embedding computing relative amount every topic text basis every token text. GlobalVectorClusters: GlobalVectors newer approach utilizes co-occurrence words/tokens compute GlobalVectors (Campesato 2021, p. 110). vectors generated way tokens/words similar meaning located near (Pennington, Socher & Manning 2014). order create text embedding word embeddings, aifeducation groups tokens clusters based vectors. Thus, tokens similar meaning members cluster. text embedding tokens text counted every cluster frequencies every cluster text used numerical representation text. Transformers: Transformers current state--art approach many natural language tasks (Tunstall, von Werra & Wolf 2022, p. xv). help self attention mechanism (Vaswani et al. 2017) able produce context sensitive word embeddings (Chollet, Kalinowski & Allaire, 2022 pp.366). aifeducation architecture BERT implemented foundation classification tasks. approaches managed used unified interface provided object TextEmbeddingModel. object can easily convert raw texts numerical representation can use different classification tasks time. makes possible reduce computational time. created text embedding stored object class EmbeddedText. object additionally contains information text embedding model created object. best case can apply existing text embedding model using transformer Huggingface using model colleagues. aifeducation provides several functions allowing create models. Depending approach like use different steps necessary. case Topic Modeling GlobalVectorClusters must first create draft vocabulary two functions bow_pp_create_vocab_draft() bow_pp_create_basic_text_rep(). calling functions determine central properties resulting model. case transformers first configure train vocabulary create_xxx_model() next step can train model train_tune_xxx_model(). Every step explained next chapters. Please note xxx stands different architectures transformers supported aifedcuation. object class TextEmbeddingModel can create input data supervised machine learning. Additionally need target data must named factor containing classes/categories text. kind data able create new object class TextEmbeddingClassifierNeuralNet classifier. training classifier several options cover detail chapter 3. training classifier can share researchers apply new texts. Please note application new texts requires text transformed numbers exactly text embedding model passing text classifier. , please pass raw texts embedded texts classifier. next chapters guide complete process. Starting creation text embedding models.","code":""},{"path":[]},{"path":"/articles/classification_tasks.html","id":"example-data-for-this-vignette","dir":"Articles","previous_headings":"2 Preparation Tasks","what":"2.1 Example Data for this Vignette","title":"02 Classification Tasks","text":"illustrate steps vignette use data educational settings since data general protected privacy policies. Therefore, use data set data_corpus_moviereviews package quanteda.textmodels illustrate usage package. quanteda.textmodels automatically installed install aifeducation. now data set three columns. first contains id movie review, second contains movie rated positive negative, third column contains raw texts. can see data balanced. 1000 reviews imply positive rating movie 1000 imply negative rating. tutorial modify data set setting half negative positive reviews NA indicating reviews labeled. Furthermore, bring imbalance setting 250 positive reviews NA. now use data show use different objects functions aifeducation.","code":"example_data<-data.frame(   id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,   label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment) example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)  table(example_data$label) #>  #>  neg  pos  #> 1000 1000 example_data$label[c(1:500,1001:1500)]=NA summary(example_data$label) #>  neg  pos NA's  #>  500  500 1000 example_data$label[1501:1750]=NA summary(example_data$label) #>  neg  pos NA's  #>  500  250 1250"},{"path":"/articles/classification_tasks.html","id":"topic-modeling-and-globalvectorclusters","dir":"Articles","previous_headings":"2 Preparation Tasks","what":"2.2 Topic Modeling and GlobalVectorClusters","title":"02 Classification Tasks","text":"like create new text embedding model Topic Modeling GlobalVectorClusters first create draft vocabulary. can calling function bow_pp_create_vocab_draft(). main input function vector texts. function’s aims create list tokens texts, reduce tokens tokens carry semantic meaning, provide lemma every token. Since Topic Modeling depends bag--word approach reason preprocess step reduce tokens tokens really carry semantic meaning. general tokens words either nouns, verbs adjectives (Papilloud & Hinneburg 2018, p. 32). example data application function : can see additional parameter path_language_model. must insert path udpipe pre-trained language model since function uses udpipe package part--speech tagging lemmataziation. collection pre-trained models 65 languages can found [https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131]. Just download relevant model machine provide path model. parameter upos can select tokens selected. example tokens represent noun, adjective verb remain analysis. list possible tags can found : [https://universaldependencies.org/u/pos/index.html]. Please forget provide label udpipe model use please also provide language analyzing. information important since transferred text embedding model. researchers/users need information order estimate model help work. next step can use draft vocabulary create basic text representation function bow_pp_create_basic_text_rep(). function takes raw texts draft vocabulary main input. function aims remove tokens referring stopwords, clean data (e.g., removing punctuation, numbers), lower case tokens requested, remove tokens specific minimal frequency, remove tokens occur many documents create document-feature-matrix (dfm), create feature co-occurrence matrix (fcm). Applied example call function look like : data takes raw texts vocab_draft takes draft vocabulary created first step. main goal create document-feature-matrix(dfm) feature co-occurrence matrix (fcm). dfm matrix reports texts rows number tokens columns. matrix later used create text embedding model based Topic Modeling. dfm reduced tokens correspond part--speech tags vocabulary draft. Punctuation, symbols, numbers etc. removed matrix set corresponding parameter TRUE. set use_lemmata = TRUE can reduce dimensionality matrix using lemmata instead tokens (Papilloud & Hinneburg 2018, p.33). set to_lower = TRUE tokens transformed lower case. end get matrix tries represent semantic meaning text minimum tokens possible. applies fcm. tokens/features reduced way. However, features reduced tokens co-occurrence calculated. aim window used shifted across text counting tokens left right token investigation. size window can determined window. weights can provide weights counting. example tokens far away token investigation count less tokens near token investigation. fcm later used creating text embedding model based GlobalVectorClusters. may notices dfm counts words text. Thus, position text within sentence matter. lower case tokens use lemmata syntactic information lost advantage dfm lower dimensionality losing semantic meaning. contrast, fcm matrix describes often different tokens occur together. Thus, fcm recovers part position words sentence text. Now everything ready create new text embedding model based Topic Modeling GlobalVectorClusters. show create new model look preparation new transformer.","code":"vocab_draft<-bow_pp_create_vocab_draft(   path_language_model=\"language_model/english-gum-ud-2.5-191206.udpipe\",   data=example_data$text,   upos=c(\"NOUN\", \"ADJ\",\"VERB\"),   label_language_model=\"english-gum-ud-2.5-191206\",   language=\"english\",   trace=TRUE) basic_text_rep<-bow_pp_create_basic_text_rep(   data = example_data$text,   vocab_draft = vocab_draft,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = FALSE,   split_tags = FALSE,   language_stopwords=\"eng\",   use_lemmata = FALSE,   to_lower=FALSE,   min_termfreq = NULL,   min_docfreq= NULL,   max_docfreq=NULL,   window = 5,   weights = 1 / (1:5),   trace=TRUE)"},{"path":"/articles/classification_tasks.html","id":"creating-a-new-transformer","dir":"Articles","previous_headings":"2 Preparation Tasks","what":"2.3 Creating a New Transformer","title":"02 Classification Tasks","text":"general recommended use pre-trained model since creation new transformers requires large data set texts computational intensive. vignette illustrated process BERT model. However, many transformers process . creation new transformer requires least two steps. First must decide architecture transformer. includes create vocabulary. aifedcuation can calling function create_bert_model(). example look like : function work must provide path directory new transformer saved. Furthermore, must provide raw texts. texts used training transformer training vocabulary. maximum size vocabulary determined vocab_size. Please provide size 50000 60000 since kind vocabulary works different approachs described section 2.2. Modern tokenizer WordPiece (Wu et al. 2016) use algorithms splits tokens smaller elements allowing build huge number words small number elements. Thus, even small number 30000 tokens able represent large number words. consequence, kind vocabulary many times smaller vocabularies built section 2.2. parameters allow customize BERT model. example, increase number hidden layers 12 24 reduce hidden size 768 256 allowing built test larger smaller transformers. Please note max_position_embeddings determine many tokens transformer can process. text tokens tokenaization tokens ignored. However, like analyze long documents please avoid increase number significantly computational times increase linear quadratic (Beltagy, Peters & Cohan 2020). long documents can use another architecture BERT (e.g. Longformer Beltagy, Peters & Cohan 2020) split long document several chunks seuential used classification (e.g., Pappagari et al. 2019). Using chunks supported aifedcuation. calling function find new model model directory. next step train model calling train_tune_bert_model(). , important provide path directory new transformer stored. Furthermore, important provide another directory trained transformer saved avoid reading writing collisions. Now provides raw data used training model using Masked Language Modeling. First, can set length token sequences chunk_size. whole_word can chose masking single tokens masking complete words (Please remember modern tokenizers split words several tokens. Thus, tokens word forced match directly). p_mask can determine many tokens masked. Finally, val_size set many chunks used validation sample. work machine graphic device small memory please reduce batch size significantly. also recommend change usage memory set_config_gpu_low_memory(). training finishes can find transformer ready use output_directory. Now able create text embedding model.","code":"basic_text_rep<-bow_pp_create_basic_text_rep( create_bert_model(     model_dir = \"my_own_transformer\",     vocab_raw_texts=example_data$text,     vocab_size=30522,     vocab_do_lower_case=FALSE,     max_position_embeddings=512,     hidden_size=768,     num_hidden_layer=12,     num_attention_heads=12,     intermediate_size=3072,     hidden_act=\"gelu\",     hidden_dropout_prob=0.1,     trace=TRUE) train_tune_bert_model(   output_dir = \"my_own_transformer_trained\",   bert_model_dir_path = \"my_own_transformer\",   raw_texts = example_data$text,   aug_vocab_by=0,   p_mask=0.15,   whole_word=TRUE,   val_size=0.1,   n_epoch=1,   batch_size=12,   chunk_size=250,   n_workers=1,   multi_process=FALSE,   trace=TRUE)"},{"path":[]},{"path":"/articles/classification_tasks.html","id":"introduction","dir":"Articles","previous_headings":"3 Text Embedding","what":"3.1 Introduction","title":"02 Classification Tasks","text":"aifedcuation text embedding model stored object class TextEmbeddingModel. object contains relevant information transforming raw texts numeric representation can used machine learning. aifedcuation transformation raw texts numbers separate step downstream task classification. reason reduce computational time machines low performance. separating text embedding tasks text embedding calculated can used different tasks time. Another advantage training downstream tasks involves downstream tasks parameters embedding model making training less time consuming decreases computational insensitivity. Finally, approach allows analysis long documents applying algorithm different parts long text. text embedding model provides unified interface. , creating model different methods handling model always . following show use object. start Topic Modeling.","code":""},{"path":[]},{"path":"/articles/classification_tasks.html","id":"topic-modeling","dir":"Articles","previous_headings":"3 Text Embedding > 3.2 Creating Text Embedding Models","what":"3.2.1 Topic Modeling","title":"02 Classification Tasks","text":"creating new text embedding model based Topic Modeling need basic text representation generated function bow_pp_create_basic_text_rep() (see section 2.2). Now can create new instance text embedding model calling TextEmbeddingModel$new(). First provide name new model (model_name). unique short name without spaces. model_label can provide label model freedom. important provide version model case create improved version future. model_language provide users information language model designed. important plan share model wider community. method determine approach used model. like use Topic Modeling set method = \"lda\". number topics set via bow_n_dim. example like create topic model 12 topics. number topics also determines dimensionality text embedding. , every text characterized 12 topics. Please forget pass basic text representation bow_basic_text_rep. model estimated stored topic_modeling example.","code":"topic_modeling<-TextEmbeddingModel$new(   model_name=\"topic_model_embedding\",   model_label=\"Text Embedding via Topic Modeling\",   model_version=\"0.0.1\",   model_language=\"english\",   method=\"lda\",   bow_basic_text_rep=basic_text_rep,   bow_n_dim=12,   bow_max_iter=500,   bow_cr_criterion=1e-8,   trace=TRUE )"},{"path":"/articles/classification_tasks.html","id":"globealvectorclusters","dir":"Articles","previous_headings":"3 Text Embedding > 3.2 Creating Text Embedding Models","what":"3.2.2 GlobealVectorClusters","title":"02 Classification Tasks","text":"creation text embedding model based GlobalVectorClusters similar model based Topic Modeling. two differences. First, request model based GlobalVectorCluster setting method=\"glove_cluster\". Second, determine dimensionalty global vectors bow_n_dim number clusters bow_n_cluster. creating new text embedding model global vector token calculated based feature-co-occurrence matrix (fcm) provide basic_text_rep. , token vector calculated length bow_n_dim. Since vectors word embeddings text embeddings additional step necessary create text embedding. aifedcuation word embeddings used group word clusters. number cluster set bow_n_cluster. Now, text embedding results counting tokens every cluster every text. final model stored global_vector_clusters_modeling.","code":"global_vector_clusters_modeling<-TextEmbeddingModel$new(   model_name=\"global_vector_clusters_embedding\",   model_label=\"Text Embedding via Clusters of GlobalVectors\",   model_version=\"0.0.1\",   model_language=\"english\",   method=\"glove_cluster\",   bow_basic_text_rep=basic_text_rep,   bow_n_dim=96,   bow_n_cluster=384,   bow_max_iter=500,   bow_max_iter_cluster=500,   bow_cr_criterion=1e-8,   trace=TRUE )"},{"path":"/articles/classification_tasks.html","id":"transformers","dir":"Articles","previous_headings":"3 Text Embedding > 3.2 Creating Text Embedding Models","what":"3.2.3 Transformers","title":"02 Classification Tasks","text":"Using transformer creating text embedding model similar approaches. request model based transformer must set method accordingly. Since use BERT model example set method = \"bert\". Next, provide directory model stored. example bert_model_dir_path=\"my_own_transformer_trained. course cane use pre-trained model Huggingface addresses needs. Using BERT model text embedding problem since text provide tokens transformer can process. maximal value set config transformer (see section 2.3). text produces tokens last tokens ignored. cases may want analyze long texts. situations reducing text first tokens (e.g. first 512 tokens) result problematic lose information. deal situations can config text embedding model aifecuation split long texts several chunks processed transformer. maximal number chunks set chunks. example , text embedding model split text consisting 1024 tokens 2 chunk every chunk consisting 512 tokens. every chunk text embedding calculated. results receive sequence embeddings. first embeddings characterizes first part text second embedding characterizes second part text (). Thus, example text embedding model able process texts 4*512=2048 tokens. approach inspired work Pappagari et al. (2019). Since transformers able account context may useful connect every chunk order bring context calculations. can done overlap determining many tokens end prior chunk added next chunk. Finally, decide like embedding classification token [CLS] text embedding mean token embeddings text embedding (use_cls_token). can decide hidden layer layers embeddings drawn (aggregation=\"last\"). initial work Devlin et al. (2019) used hidden states different layers classification. deciding configuration can use model.","code":"bert_modeling<-TextEmbeddingModel$new(   model_name=\"bert_embedding\",   model_label=\"Text Embedding via BERT\",   model_version=\"0.0.1\",   model_language=\"english\",   method = \"bert\",   max_length = 512,   chunks=4,   overlap=30,   aggregation=\"last\",   use_cls_token=TRUE,   model_dir=\"my_own_transformer_trained\"   )"},{"path":"/articles/classification_tasks.html","id":"transforming-raw-texts-into-embedded-texts","dir":"Articles","previous_headings":"3 Text Embedding","what":"3.3 Transforming Raw Texts into Embedded Texts","title":"02 Classification Tasks","text":"Although mechanics within text embedding model different usage always . transform raw text numeric representation use embed method model. Therefore, must provide raw texts raw_text. addition, necessary provide character vector containing id every text. ids must unique. method embedcreates object class EmbeddedText. just data.frame consisting embedding every text. Depending method data.frame different meaning: - Topic Modeling: case Topic Modeling rows represent texts columns represent percentage every topic within text. - GlobalVectorClusters: case rows represent texts columns represent absolute frequencies tokens belonging semantic cluster. - Transformer - Bert: case BERT rows represent texts columns represents contextualized text embedding. , Bert’s understanding relevant text chunk. Please case Bert models embeddings every chunks concatenated. embedded texts now input train new classifier apply pre-trained classifier predicting categories/classes. next chapter show use classifiers. start show save load model.","code":"topic_embeddings<-topic_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)  cluster_embeddings<-global_vector_clusters_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)  bert_embeddings<-bert_modeling$embed(   raw_text=example_data$text,   doc_id=example_data$id,    trace = TRUE)"},{"path":"/articles/classification_tasks.html","id":"saving-and-loading-text-embedding-models","dir":"Articles","previous_headings":"3 Text Embedding","what":"3.4 Saving and Loading Text Embedding Models","title":"02 Classification Tasks","text":"Saving created text embedding model easy. However, saving loading process different models based Topic Modeling GlobalVectorClusters hand models based transformers hand. models using Topic Modeling GlobalVectorClusters can call save() write model disk. want load model just call load() can continue using model. text embedding model based transformer saving loading requires steps. case text embedding models serves interface R. original model saved model directory. Thus, save interface model directory. Loading model requires two steps. First, load interface. Now text embedding model available R.. Next must re-initialize transformer calling corresponding method load_model model. Now can use text embedding model.","code":"save(topic_modeling,       file=\"models/embedding_model_topic.RData\") save(global_vector_clusters_modeling,       file=\"models/embedding_model_gvc.RData\") load(file=\"models/embedding_model_topic.RData\") load(file=\"models/embedding_model_gvc.RData\") save(bert_modeling,      file=\"my_own_transformer_trained/r_interface.RData\") load(file=\"my_own_transformer_trained/r_interface.RData\") bert_modeling$load_model(model_dir=\"my_own_transformer_trained\")"},{"path":[]},{"path":"/articles/classification_tasks.html","id":"creating-a-new-classifier","dir":"Articles","previous_headings":"4 Using AI for Classification","what":"4.1 Creating a New Classifier","title":"02 Classification Tasks","text":"aifedcuation classifiers based neural nets stored objects class TextEmbeddingClassifierNeuralNet. can create new classifier calling TextEmbeddingClassifierNeuralNet$new(). Similar text embedding model provide name (name) label (label) new classifier. text_embeddings provide embedded text. like recommend use embedding like use training. continue example use embedding produced BERT model. targets takes target data supervised learning. Please omit cases category/class since can used special training technique show later. important provide target data factor. Otherwise error occur. also important name factor. , entries factor mus names correspond ids corresponding texts. Without names method match text embeddings input data target data. config represents list decide structure classifier hidden takes vector integers determining number layers number neurons. example two layers. first 128 second 64 neurons. gru also takes vector integers determining number size Gated Recurrent Unit. example use two layers 128 . Since classifiers aifeducation use standardized scheme creation, dense layers used gru layers. want omit gru layers dense layers set corresponding argument NULL. use text embedding model processes 1 chunk like recommend use gru layers since able use sequential structure data. cases can rely dense layers . Masking, normalization, creation input layer well output layer done automatically. created new classifier can begin training.","code":"example_targets<-as.factor(example_data$label) names(example_targets)=example_data$id  classifier<-TextEmbeddingClassifierNeuralNet$new(   name=\"movie_review_classifier\",   label=\"Classifier for Estimating a Postive or Negative Rating of Movie Reviews\",   text_embeddings=bert_embeddings,   targets=example_targets,   config=list(     hidden=c(128,64),     gru=c(128,128),     dropout=0.2,     recurrent_dropout=0.4,     l2_regularizer=0.001,     optimizer=\"adam\",     act_fct=\"gelu\",     act_fct_last=\"softmax\",     err_fct=\"categorical_crossentropy\") )"},{"path":"/articles/classification_tasks.html","id":"training-a-classifier","dir":"Articles","previous_headings":"4 Using AI for Classification","what":"4.2 Training a Classifier","title":"02 Classification Tasks","text":"starting training classifier call train method. Similar creation classifier must provide text embedding data_embeddings categories/classes target data data_targets. Please remember data_targets expects named factor names corresponds ids corresponding text embeddings. Text embeddings target data matched ommited training. training classifier necessary provide path dir_checkpoint. directory stores best set weights training step. training weights automatically used final weights classifier. performance estimation training splits data several chunky based cross fold validation. number folds set data_n_valid_samples. every case one fold used training serves test sample. remaining data used creating training validation sample. performance values saved trained classifier refer test sample. , data never used training provides realistic estimation classifier`s performance. Since aifedcuation tries address special needs educational social science special training steps integrated method. Baseline: interested training classifier without applying additional statistical techniques set use_baseline = TRUE. case classifier trained provided data . Cases missing values target data omitted. Even like apply statistical adjustments makes sense compute baseline model comparing effect modified training process unmodified training. using bsl_val_size can determine many data used training data many data used validation data. Balanced Synthetic Cases: case imbalanced data recommended set use_bsc=TRUE. Now, training number synthetic units created via different techniques. Currently can request Basic Synthetic Minority Oversampling Technique, Density-Bases Synthetic Minority Oversampling Technique, Adaptive Synthetic Sampling Approach Imbalanced Learning. aim create new cases fill gap majority class. Multiclass problems reduced two class problem (class investigation vs. ) generating units. can even request several techniques . number synthetic units original minority units exceed number cases majority class random sample drawn. technique allows set number neighbors generation k = bsc_max_k used. Balanced Pseudo Labeling: technique relevant labeled target data large number unlabeled target data. option activates implementation pseudo labeling. , classifier trained cases labeled data available. training classifier predicts classes/categories unlabeled cases. percentage cases determined bpl_inc_ratio selected added training test sample according bpl_valid_size. classifier trained extended data training predicts classes/categories remaining unlabeled data. process iterated long pseudo-labeled data contains cases every class/category. process stops number maximum steps reached (bpl_max_steps) unlabeled data used training. Please note number cases added within step determined classes/categories category lowest absolute frequency weighted bpl_inc_ratio ensure balance new cases. cases available cases sorted distance bpl_anchor. value describing certainty pseudo labels. 0 equals random guessing, 1 equals perfect certainty. recommended include cases high perfect certainty improve quality classifier. Thus, recommend include small percentage (bpl_inc_ratio) every step training next step. technique requires use_baseline = TRUE use_bsc=TRUE. set true classifier trained basis balanced synthetic cases used. Figure 4 illustrates training loop cases three options set TRUE. Figure 4: Overview Training Loop Options Activiated Finally, trace, view_metrics, keras_trace allow control many information training progress printed console. Please note training classifier can take time. Please note performance estimation final training classifier makes use data available. , test sample left empty.","code":"example_targets<-as.factor(example_data$label) names(example_targets)=example_data$id  classifier$train(    data_embeddings = bert_embeddings,    data_targets = example_targets,    data_n_valid_samples=5,    use_baseline=TRUE,    bsl_val_size=0.25,    use_bsc=TRUE,    bsc_methods=c(\"dbsmote\"),    bsc_max_k=10,    use_bpl=TRUE,    bpl_max_steps=10,    bpl_inc_ratio=0.25,    bpl_anchor=0.75,    bpl_valid_size=0.33,    opt_model_reset=TRUE,    epochs=100,    batch_size=32,    dir_checkpoint=\"tmp/checkpoints_classifier\",    trace=TRUE,    view_metrics=FALSE,    keras_trace=2,    n_cores=2)"},{"path":"/articles/classification_tasks.html","id":"evaluating-classifiers-performance","dir":"Articles","previous_headings":"4 Using AI for Classification","what":"4.3 Evaluating Classifier’s Performance","title":"02 Classification Tasks","text":"finishing training can evaluate performance classifier. every fold classifier applied test sample results compared true categories/class. Since test sample never part training performance measures provide realistic idea classifier`s performance. support researchers judging quality predictions aifeducation utilizes several measures concepts content analysis. Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa equal weights Fleiss’ Kappa multiple raters exact estimation Light’s Kappa multiple raters can access concrete values accessing field reliability stores relevant information. list find reliability values every fold every requested training configuration. addition, reliability every step within balanced pseudo labeling reported. central estimates reliabilites can found via reliability$val_metric_mean. example : know table relevant values. Evaluating performance classifier complex task behind scope vignette. , like refer cited literature content analysis machine learning.","code":"classifier$reliability$val_metric_mean"},{"path":"/articles/classification_tasks.html","id":"saving-and-loading-a-classifier","dir":"Articles","previous_headings":"4 Using AI for Classification","what":"4.4 Saving and Loading a Classifier","title":"02 Classification Tasks","text":"created classifier saving loading easy due R package bundle. can just use save() load(). example ","code":"save(classifier,      file=\"classifiers/movie_review.RData\")  load(file=\"classifiers/movie_review.RData\")"},{"path":"/articles/classification_tasks.html","id":"predicting-new-data","dir":"Articles","previous_headings":"4 Using AI for Classification","what":"4.5 Predicting New Data","title":"02 Classification Tasks","text":"like apply classifier new data, two steps necessary. First, must transform raw text numerical expression using exactly text embedding model used training classifier. resulting object can passed method predict get predictions together estimate certainty class/category.","code":""},{"path":"/articles/classification_tasks.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"02 Classification Tasks","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Logos Verlag Berlin. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1–21. https://doi.org/10.3389/feduc.2022.818365 Campesato, O. (2021). Natural Language Processing Fundamentals Developers. Mercury Learning & Information. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713 Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning R (Second edition). Manning Publications Co.  https://learning.oreilly.com/library/view/-/9781633439849/?ar Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). Advances Analytics LLC. Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th Ed.). SAGE. Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing action: Understanding, analyzing, generating text Python. Manning. Larusson, J. ., & White, B. (Eds.). (2014). Learning Analytics: Research Practice. Springer New York. https://doi.org/10.1007/978-1-4614-3305-7 Papilloud, C., & Hinneburg, . (2018). Qualitative Textanalyse mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler. Springer. https://doi.org/10.1007/978-3-658-21980-2 Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N. (2019). Hierarchical Transformers Long Document Classification. 2019 IEEE Automatic Speech Recognition Understanding Workshop (ASRU) (pp. 838–844). IEEE. https://doi.org/10.1109/ASRU46091.2019.9003958 Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural Language Processing. https://aclanthology.org/D14-1162.pdf Schreier, M. (2012). Qualitative Content Analysis Practice. SAGE. Tunstall, L., Werra, L. von, Wolf, T., & Géron, . (2022). Natural language processing transformers: Building language applications hugging face (Revised edition). O’Reilly. Vaswani, ., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, . N., Kaiser, L., & Polosukhin, . (2017). Attention Need. https://doi.org/10.48550/arXiv.1706.03762 Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, ., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., . . . Dean, J. (2016). Google’s Neural Machine Translation System: Bridging Gap Human Machine Translation. https://doi.org/10.48550/arXiv.1609.08144","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Berding Florian. Author, maintainer. Pargmann Julia. Contributor. Riebenbauer Elisabeth. Contributor. Rebmann Karin. Contributor. Slopinski Andreas. Contributor.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Florian B (2023). aifeducation: Artifical Intelligence Education. R package version 0.1.0.9000.","code":"@Manual{,   title = {aifeducation: Artifical Intelligence for Education},   author = {Berding Florian},   year = {2023},   note = {R package version 0.1.0.9000}, }"},{"path":"/index.html","id":"aifeducation","dir":"","previous_headings":"","what":"Artifical Intelligence for Education","title":"Artifical Intelligence for Education","text":"R package Artificial Intelligence Education (aifeducation) designed special needs educators, educational researchers, social researchers. package supports application Artificial Intelligence (AI) Natural Language Processing tasks text embedding, classification, question answering special conditions educational social sciences. : digital data availability: educational social science data often available hand written form. example, students pupils often solve tasks schools universities creating hand written documents. Thus, educators researchers first transform textual data digital form involving human actions. makes data generation financial expensive time consuming leading small data sets. high privacy policies standards: Furthermore, educational social science data often refers humans /human actions. kind data protected privacy policies many countries limiting access usage data turn results small data sets. long research tradition: Educational social science long research tradition generating insights social phenomenon well learning teaching. insights incorporated applications AI (e.g., Luan et al. 2020; Wong et al. 2019). makes supervised machine learning important technique since provides link educational social theories models one hand machine learning hand (Berding et al. 2022). However, kind machine learning requires humans generating valid data set training AI leading small data sets. complex constructs: Compared classification tasks AI differentiate ‘good’ ‘bad’ movie review constructs educational social science complex. example, research instruments motivation psychology require infer personal motives written essays (e.g., Gruber & Kreuzpointner 2013). reliable valid interpretation kind information requires well qualified human raters making data generation expensive. also limits size data set. imbalanced data: Finally, data educational social science often occurs imbalanced pattern several empirical studies show (Bloemen 2011, Stütz et al. 2022). Imbalanced means categories characteristics data set high absolute frequencies compared categories characteristics. Imbalance training AI guides algorithms focus prioritize categories characteristics high absolute frequencies increasing risk miss categories/characteristics low frequencies (Haixiang et al. 2017). can lead AI prefer special groups persons/materials, imply false recommendations conclusions, miss rare categories characteristics. Currently package focuses classification tasks can used diagnosing characteristics learners written materials estimating properties learning teaching materials. future tasks implemented.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Artifical Intelligence for Education","text":"can install development version aifeducation GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"FBerding/aifeducation\")"},{"path":[]},{"path":"/index.html","id":"transforming-texts-into-numbers","dir":"","previous_headings":"Classification Tasks","what":"Transforming Texts into Numbers","title":"Artifical Intelligence for Education","text":"Classification tasks require transformation raw texts representation numbers. step classification task aifeducation supports newer approaches BERT (Devlin et al. 2019) older approaches GlobalVectors (Pennington, Socher & Manning 2014) Latent Dirichlet Allocation/Topic Modeling. newer approaches ROBERTA (Liu et al. 2019) Longformer (Beltagy, Peters & Cohan 2020) planned future. aifeducation supports usage pre-trained transformer models provided Huggingface creation new transformers allowing educators researchers development specialized domain-specific models. Package supports analysis long texts. Depending method long texts transformed vectors splitting long texts several chunks result sequence vectors.","code":""},{"path":"/index.html","id":"training-ai-under-challenging-conditions","dir":"","previous_headings":"Classification Tasks","what":"Training AI under Challenging Conditions","title":"Artifical Intelligence for Education","text":"second step within classification task aifeducation integrates important statistical mathematical methods dealing main challenges educational social sciences applying AI. order deal problem imbalanced data sets package integrates Synthetic Minority Oversampling Technique learning process. Currently Basic Synthetic Minority Oversampling Technique (Chawla et al. 2002), Density-Bases Synthetic Minority Oversampling Technique (Bunkhumpornpat, Sinapiromsaran & Lursinsap, 2012), Adaptive Synthetic Sampling Approach Imbalanced Learning (Hem Garcia & Li 2008) implemented via R package smotefamiliy. order address problem small data sets training loops AI integrate Pseudo Labeling(e.g., Lee 2013). Pseudo Labeling technique can used supervised learning. , educators researchers rate part data set train AI part data. remaining part data processed humans. Instead, AI uses part data learn . Thus, educators researchers provide additional data learning without working . offers possibility add data training reduce personal costs.","code":""},{"path":"/index.html","id":"evaluating-performance","dir":"","previous_headings":"Classification Tasks","what":"Evaluating Performance","title":"Artifical Intelligence for Education","text":"Classification tasks machine learning comparable empirical method content analysis social science. method looks back long research tradition long discussion evaluate reliability validity generated data. order provide link research tradition order provide educators well educational social researchers performance measures familiar every AI trained packages evaluated following measures concepts: Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa equal weights Fleiss’ Kappa multiple raters exact estimation Light’s Kappa multiple raters","code":""},{"path":"/index.html","id":"sharing-trained-ai","dir":"","previous_headings":"","what":"Sharing Trained AI","title":"Artifical Intelligence for Education","text":"Since package based keras, tensorflow, transformer libraries every trained AI can shared educators researchers. package supports easy use pre-trained AI within R also provides possibility export trained AI environments.","code":""},{"path":"/index.html","id":"tutorial-and-guides","dir":"","previous_headings":"","what":"Tutorial and Guides","title":"Artifical Intelligence for Education","text":"guide install configure package can found via Get started. short introduction using package classification tasks can found vignette 02 clasification tasks","code":""},{"path":"/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Artifical Intelligence for Education","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Logos Verlag Berlin. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1–21. https://doi.org/10.3389/feduc.2022.818365 Bloemen, . (2011). Lernaufgaben Schulbüchern der Wirtschaftslehre: Analyse, Konstruktion und Evaluation von Lernaufgaben für die Lernfelder industrieller Geschäftsprozesse. Hampp. Bunkhumpornpat, C., Sinapiromsaran, K., & Lursinsap, C. (2012). DBSMOTE: Density-Based Synthetic Minority -sampling TEchnique. Applied Intelligence, 36(3), 664–684. https://doi.org/10.1007/s10489-011-0287-y Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority -sampling Technique. Journal Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Gruber, N., & Kreuzpointner, L. (2013). Measuring reliability picture story exercises like TAT. PloS One, 8(11), e79450. https://doi.org/10.1371/journal.pone.0079450 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). Advances Analytics LLC. Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., & Bing, G. (2017). Learning class-imbalanced data: Review methods applications. Expert Systems Applications, 73, 220–239. https://doi.org/10.1016/j.eswa.2016.12.035 , H., Bai, Y., Garcia, E. ., & Li, S. (2008). ADASYN: Adaptive synthetic sampling approach imbalanced learning. 2008 IEEE International Joint Conference Neural Networks (IEEE World Congress Computational Intelligence) (pp. 1322–1328). IEEE. https://doi.org/10.1109/IJCNN.2008.4633969 Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th Ed.). SAGE. Lee, D.‑H. (2013). Pseudo-Label : Simple Efficient Semi-Supervised Learning Method Deep Neural Networks. CML 2013 Workshop : Challenges RepresentationLearning. https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 Luan, H., Geczy, P., Lai, H., Gobert, J., Yang, S. J. H., Ogata, H., Baltes, J., Guerra, R., Li, P., & Tsai, C.‑C. (2020). Challenges Future Directions Big Data Artificial Intelligence Education. Frontiers Psychology, 11, 1–11. https://doi.org/10.3389/fpsyg.2020.580820 Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural Language Processing. https://aclanthology.org/D14-1162.pdf Stütz, S., Berding, F., Reincke, S., & Scheper, L. (2022). Characteristics learning tasks accounting textbooks: AI assisted analysis. Empirical Research Vocational Education Training, 14(1). https://doi.org/10.1186/s40461-022-00138-2 Wong, J., Baars, M., Koning, B. B. de, van der Zee, T., Davis, D., Khalil, M., Houben, G.‑J., & Paas, F. (2019). Educational Theories Learning Analytics: Data Knowledge. D. Ifenthaler, D.-K. Mah, & J. Y.-K. Yau (Eds.), Utilizing Learning Analytics Support Study Success (pp. 3–25). Springer International Publishing. https://doi.org/10.1007/978-3-319-64792-0_1","code":""},{"path":"/reference/basic_text_rep_movie_reviews.html","id":null,"dir":"Reference","previous_headings":"","what":"Example for a Basic Text Representation — basic_text_rep_movie_reviews","title":"Example for a Basic Text Representation — basic_text_rep_movie_reviews","text":"object class \"basic_text_rep\" testing illustration purposes. object created based data set \"data_corpus_moviereviews\" package quanteda.textmodels. udpipe language model english-ewt-ud-2.5-191206 used.","code":""},{"path":"/reference/basic_text_rep_movie_reviews.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example for a Basic Text Representation — basic_text_rep_movie_reviews","text":"","code":"basic_text_rep_movie_reviews"},{"path":"/reference/basic_text_rep_movie_reviews.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example for a Basic Text Representation — basic_text_rep_movie_reviews","text":"object class \"basic_text_rep\".","code":""},{"path":"/reference/bert_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Example for an Object of class EmbeddedText — bert_embeddings","title":"Example for an Object of class EmbeddedText — bert_embeddings","text":"object class EmbeddedText testing illustration purposes. object created based data set \"data_corpus_moviereviews\" package quanteda.textmodels. transformer model bert-base-uncased huggingface used.","code":""},{"path":"/reference/bert_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example for an Object of class EmbeddedText — bert_embeddings","text":"","code":"bert_embeddings"},{"path":"/reference/bert_embeddings.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example for an Object of class EmbeddedText — bert_embeddings","text":"object class EmbeddedText.","code":""},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"function prepares raw texts use TextEmbeddingModel.","code":""},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"","code":"bow_pp_create_basic_text_rep(   data,   vocab_draft,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = FALSE,   split_tags = FALSE,   language_stopwords = \"de\",   use_lemmata = FALSE,   to_lower = FALSE,   min_termfreq = NULL,   min_docfreq = NULL,   max_docfreq = NULL,   window = 5,   weights = 1/(1:5),   trace = TRUE )"},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"data vector containing raw texts. vocab_draft Object created bow_pp_create_vocab_draft. remove_punct bool TRUE punctuation removed. remove_symbols bool TRUE symbols removed. remove_numbers bool TRUE numbers removed. remove_url bool TRUE urls removed. remove_separators bool TRUE separators removed. split_hyphens bool TRUE hyphens splitted several tokens. split_tags bool TRUE tags splitted. use_lemmata bool TRUE lemmas instead original tokens used. to_lower bool TRUE token lemmas used lower cases. min_termfreq int Minimum frequency token part vocabulary. min_docfreq int Minimum appearance token documents part vocabulary. max_docfreq int Maximum appearance token documents part vocabulary. window int size window creating feature co-occurance matrix. weights vector weights corresponding window. vector length must equal window's size. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/bow_pp_create_basic_text_rep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare texts for text embeddings with a bag of word approach. — bow_pp_create_basic_text_rep","text":"Returns list class basic_text_rep following components. dfm: Document-Feature-Matrix. Rows correspond documents. Columns represent number tokens document. fcm: Feature-Co-Occurance-Matrix. information: list containing information used vocabulary. : n_sentence:  Number sentence n_document_segments:  Number document segments/raw texts n_token_init:  Number initial tokens n_token_final:  Number final tokens n_lemmata:  Number lemmas configuration: list containing information vocabulary created lower cases vocabulary uses original tokens lemmas. language_model: list containing information applied language model. : model:  udpipe language model label:  label udpipe language model upos:  applied universal part--speech tags language:  language vocab:  data.frame original vocabulary","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","title":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","text":"function creates list tokens refer specific universal part--speech tags (UPOS) provides corresponding lemmas.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","text":"","code":"bow_pp_create_vocab_draft(   path_language_model,   data,   upos = c(\"NOUN\", \"ADJ\", \"VERB\"),   label_language_model = NULL,   language = NULL,   chunk_size = 100,   trace = TRUE )"},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","text":"path_language_model string Path udpipe language model used tagging lemmatisation. data vector containing raw texts. upos vector containing universal part--speech tags used build vocabulary. label_language_model string Label udpipe language model used. language string Name language (e.g., English, German) chunk_size int Number raw texts processed one time. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","text":"list following components. vocabdata.frame containing tokens, lemmas, tokens lower case, lemmas lower case. language_model ud_language_modeludpipe language model used tagging. label_language_modelLabel udpipe language model. languageLanguage raw texts. uposUsed univerisal part--speech tags. n_sentenceint Estimated number sentence raw texts. n_tokenint Estimated number tokens raw texts. n_document_segmentsint Estimated number document segments/raw texts.","code":""},{"path":"/reference/bow_pp_create_vocab_draft.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a first draft of a vocabulary — bow_pp_create_vocab_draft","text":"list possible tags can found : https://universaldependencies.org/u/pos/index.html. huge number models can found : https://ufal.mff.cuni.cz/udpipe/2/models.","code":""},{"path":"/reference/check_aif_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if all necessary python modules are available — check_aif_py_modules","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function checks necessary python modules available package aifeducation work.","code":""},{"path":"/reference/check_aif_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"","code":"check_aif_py_modules(trace = TRUE)"},{"path":"/reference/check_aif_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"trace bool TRUE list modules availability printed console.","code":""},{"path":"/reference/check_aif_py_modules.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function prints table relevant packages shows modules available . relevant modules available functions returns TRUE. cases FALSE","code":""},{"path":"/reference/check_embedding_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Check of compatible text embedding models — check_embedding_models","title":"Check of compatible text embedding models — check_embedding_models","text":"function checks different objects based text embedding model. necessary order ensure classifiers used data generated compatible embedding models.","code":""},{"path":"/reference/check_embedding_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check of compatible text embedding models — check_embedding_models","text":"","code":"check_embedding_models(object_list, same_class = FALSE)"},{"path":"/reference/check_embedding_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check of compatible text embedding models — check_embedding_models","text":"object_list list object class EmbeddedText TextEmbeddingClassifierNeuralNet. same_class bool TRUE object must class.","code":""},{"path":"/reference/check_embedding_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check of compatible text embedding models — check_embedding_models","text":"Returns TRUE object refer text embedding model. FALSE cases.","code":""},{"path":"/reference/combine_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine Embedded Texts — combine_embeddings","title":"Combine Embedded Texts — combine_embeddings","text":"Function combining embedded texts model","code":""},{"path":"/reference/combine_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine Embedded Texts — combine_embeddings","text":"","code":"combine_embeddings(embeddings_list)"},{"path":"/reference/combine_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine Embedded Texts — combine_embeddings","text":"embeddings_list list objects class EmbeddedText.","code":""},{"path":"/reference/combine_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine Embedded Texts — combine_embeddings","text":"Returns object class EmbeddedText contains unique cases input objects.","code":""},{"path":"/reference/create_bert_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for creating a new transformer based on BERT — create_bert_model","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"function creates transformer configuration based BERT base architecture vocabulary based WordPiece using python libraries 'transformers' 'tokenizers'.","code":""},{"path":"/reference/create_bert_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"","code":"create_bert_model(   model_dir,   vocab_raw_texts = NULL,   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   trace = TRUE )"},{"path":"/reference/create_bert_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"model_dir string Path directory model saved. vocab_raw_texts vector containing raw texts creating vocabulary. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximal position embeddings. parameter also determines maximum length sequence can processes model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attentions heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string name activation function. hidden_dropout_prob double Ratio dropout trace bool TRUE information progress printed console.","code":""},{"path":"/reference/create_bert_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"function return object. Instead configuration vocabulary new model saved disk.","code":""},{"path":"/reference/create_bert_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for creating a new transformer based on BERT — create_bert_model","text":"train model pass directory model function train_tune_bert_model.","code":""},{"path":"/reference/create_iota2_mean_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates in iota2 object — create_iota2_mean_object","title":"Creates in iota2 object — create_iota2_mean_object","text":"Function creates object class iotarelr_iota2 can used package iotarelr. function internal use .","code":""},{"path":"/reference/create_iota2_mean_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates in iota2 object — create_iota2_mean_object","text":"","code":"create_iota2_mean_object(   iota2_list,   free_aem = FALSE,   call = \"aifeducation::te_classifier_neuralnet\",   original_cat_labels )"},{"path":"/reference/create_iota2_mean_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates in iota2 object — create_iota2_mean_object","text":"iota2_list list objects class iotarelr_iota2. free_aem bool TRUE iota2 objects estimated without forcing assumption weak superiority. call string characterizing source estimation. , function within object estimated. original_cat_labels vector containing original labels category.","code":""},{"path":"/reference/create_iota2_mean_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates in iota2 object — create_iota2_mean_object","text":"Returns object class iotarelr_iota2 mean iota2 object.","code":""},{"path":"/reference/create_synthetic_units.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates Synthetic Units — create_synthetic_units","title":"Creates Synthetic Units — create_synthetic_units","text":"Function creating synthetic cases order balance data training TextEmbeddingClassifierNeuralNet. helper function use get_synthetic_cases allow parallel computations.","code":""},{"path":"/reference/create_synthetic_units.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates Synthetic Units — create_synthetic_units","text":"","code":"create_synthetic_units(embedding, target, k, max_k, method, cat, cat_freq)"},{"path":"/reference/create_synthetic_units.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates Synthetic Units — create_synthetic_units","text":"embedding Named data.frame containing text embeddings. cases object taken EmbeddedText$embeddings. target Named factor containing labels/categories corresponding cases. k int number nearest neighbors sampling process. max_k int maximum number nearest neighbors sampling process. method vector containing strings requested methods generating new cases. Currently \"smote\",\"dbsmote\", \"adas\" package smotefamily available. cat string category new cases created. cat_freq Object class \"table\" containing absolute frequencies every category/label.","code":""},{"path":"/reference/create_synthetic_units.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates Synthetic Units — create_synthetic_units","text":"Returns list contains text embeddings new synthetic cases named data.frame labels named factor.","code":""},{"path":"/reference/EmbeddedText.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedded Text — EmbeddedText","title":"Embedded Text — EmbeddedText","text":"Object class R6 stores text embeddings generated object class TextEmbeddingModel via method embed().","code":""},{"path":"/reference/EmbeddedText.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Embedded Text — EmbeddedText","text":"embeddings ('data.frame()') data.frame containing text embeddings chunks. Documents rows. Embeddings dimensions columns.","code":""},{"path":[]},{"path":"/reference/EmbeddedText.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Embedded Text — EmbeddedText","text":"EmbeddedText$new() EmbeddedText$get_model_info() EmbeddedText$get_model_label() EmbeddedText$clone()","code":""},{"path":"/reference/EmbeddedText.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Embedded Text — EmbeddedText","text":"Creates new object representing text embeddings.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded Text — EmbeddedText","text":"","code":"EmbeddedText$new(   model_name,   model_label,   model_date,   model_method,   model_version,   model_language,   param_seq_length,   param_chunks = NULL,   param_overlap = NULL,   param_aggregation = NULL,   embeddings )"},{"path":"/reference/EmbeddedText.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded Text — EmbeddedText","text":"model_name string Name model generates embedding. model_label string Label model generates embedding. model_date string Date embedding generating model created. model_method string Method underlying embedding model. model_version string Version model generated embedding. model_language string Language model generated embedding. param_seq_length int Maximal number tokens processes generating model chunk. param_chunks int Maximal number chunks supported generating model. param_overlap int Number tokens added beginning sequence next chunk model. param_aggregation string Aggregation method hidden states. embeddings data.frame containing text embeddings.","code":""},{"path":"/reference/EmbeddedText.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Embedded Text — EmbeddedText","text":"Method retrieving information model generated embedding.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded Text — EmbeddedText","text":"","code":"EmbeddedText$get_model_info()"},{"path":"/reference/EmbeddedText.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded Text — EmbeddedText","text":"list contain saved information underlying text embedding model.","code":""},{"path":"/reference/EmbeddedText.html","id":"method-get-model-label-","dir":"Reference","previous_headings":"","what":"Method get_model_label()","title":"Embedded Text — EmbeddedText","text":"Method retrieving label model generated embedding.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded Text — EmbeddedText","text":"","code":"EmbeddedText$get_model_label()"},{"path":"/reference/EmbeddedText.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded Text — EmbeddedText","text":"string Lable corresponding text embedding model","code":""},{"path":"/reference/EmbeddedText.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Embedded Text — EmbeddedText","text":"objects class cloneable method.","code":""},{"path":"/reference/EmbeddedText.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded Text — EmbeddedText","text":"","code":"EmbeddedText$clone(deep = FALSE)"},{"path":"/reference/EmbeddedText.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded Text — EmbeddedText","text":"deep Whether make deep clone.","code":""},{"path":"/reference/get_coder_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates reliability measures based on content analysis — get_coder_metrics","title":"Calculates reliability measures based on content analysis — get_coder_metrics","text":"functions calculates different reliability measures based empirical research method content analysis.","code":""},{"path":"/reference/get_coder_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates reliability measures based on content analysis — get_coder_metrics","text":"","code":"get_coder_metrics(true_values, predicted_values)"},{"path":"/reference/get_coder_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates reliability measures based on content analysis — get_coder_metrics","text":"true_values factor containing true labels/categories. predicted_values factor containing predicted labels/categories.","code":""},{"path":"/reference/get_coder_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates reliability measures based on content analysis — get_coder_metrics","text":"Returns vector following reliability measures: #' iota_index: Iota Index Iota Reliability Concept Version 2. min_iota2: Minimal Iota Iota Reliability Concept Version 2. avg_iota2: Average Iota Iota Reliability Concept Version 2. max_iota2: Maximum Iota Iota Reliability Concept Version 2. min_alpha: Minmal Alpha Reliability Iota Reliability Concept Version 2. avg_alpha: Average Alpha Reliability Iota Reliability Concept Version 2. max_alpha: Maximum Alpha Reliability Iota Reliability Concept Version 2. static_iota_index: Static Iota Index Iota Reliability Concept Version 2. dynamic_iota_index: Dynamic Iota Index Iota Reliability Concept Version 2. kalpha_nominal: Krippendorff's Alpha nominal variables. kalpha_ordinal: Krippendorff's Alpha ordinal variables. kendall: Kendall's coefficient concordance W. kappa2: Cohen's Kappa equal weights. kappa_fleiss: Fleiss' Kappa multiple raters exact estimation. kappa_light: Light's Kappa multiple raters. percentage_agreement: Percentage Agreement. gwet_ac: Gwet's AC1/AC2 agreement coefficient.","code":""},{"path":"/reference/get_folds.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates Cross-Validation Samples — get_folds","title":"Creates Cross-Validation Samples — get_folds","text":"Functions creates cross-validation samples ensures relative frequency every category/label within fold equals relative frequency category/label within initial data.","code":""},{"path":"/reference/get_folds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates Cross-Validation Samples — get_folds","text":"","code":"get_folds(target, k_folds)"},{"path":"/reference/get_folds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates Cross-Validation Samples — get_folds","text":"target Named factor containing relevant labels/categories. Missing cases declared NA. k_folds int number folds.","code":""},{"path":"/reference/get_folds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates Cross-Validation Samples — get_folds","text":"Return list following components: val_sample: vector strings containing names cases validation sample. train_sample: vector strings containing names cases train sample. n_folds: int Number realized folds. unlabeled_cases: vector strings containing names unlabeled cases.","code":""},{"path":"/reference/get_folds.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Creates Cross-Validation Samples — get_folds","text":"parameter target allows cases missing categories/labels. declared NA. cases ignored creating different folds. names saved within component unlabeled_cases. cases can used Pseudo Labeling. functions checks absolute frequencies every category/label. absolute frequency sufficient ensure least four cases every fold number folds adjusted. cases warning printed console. least four cases per fold necessary ensure training TextEmbeddingClassifierNeuralNet works well options turned .","code":""},{"path":"/reference/get_stratified_train_test_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a stratified random sample — get_stratified_train_test_split","title":"Creates a stratified random sample — get_stratified_train_test_split","text":"function creates stratified random sample.difference get_train_test_split function require text embeddings split text embeddings train validation sample.","code":""},{"path":"/reference/get_stratified_train_test_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a stratified random sample — get_stratified_train_test_split","text":"","code":"get_stratified_train_test_split(targets, val_size = 0.25)"},{"path":"/reference/get_stratified_train_test_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a stratified random sample — get_stratified_train_test_split","text":"targets Named vector containing labels/categories case. val_size double Value 0 1 indicating many cases label/category part validation sample.","code":""},{"path":"/reference/get_stratified_train_test_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a stratified random sample — get_stratified_train_test_split","text":"list containing names cases belonging train sample validation sample.","code":""},{"path":"/reference/get_synthetic_cases.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates synthetic cases for balancing training data — get_synthetic_cases","title":"Creates synthetic cases for balancing training data — get_synthetic_cases","text":"function creates synthetic cases balancing training object class TextEmbeddingClassifierNeuralNet.","code":""},{"path":"/reference/get_synthetic_cases.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates synthetic cases for balancing training data — get_synthetic_cases","text":"","code":"get_synthetic_cases(embedding, target, method = c(\"smote\"), max_k = 6)"},{"path":"/reference/get_synthetic_cases.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates synthetic cases for balancing training data — get_synthetic_cases","text":"embedding Named data.frame containing text embeddings. cases object taken EmbeddedText$embeddings. target Named factor containing labels corresponding embeddings. method vector containing strings requested methods generating new cases. Currently \"smote\",\"dbsmote\", \"adas\" package smotefamily available. max_k int maximum number nearest neighbors sampling process.","code":""},{"path":"/reference/get_synthetic_cases.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates synthetic cases for balancing training data — get_synthetic_cases","text":"list following components. syntetic_embeddings: Named data.frame containing text embeddings synthetic cases. syntetic_targetsNamed factor containing labels corresponding synthetic cases. n_syntetic_unitstable showing number synthetic cases every label/category.","code":""},{"path":"/reference/get_train_test_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for splitting data into a train and validation sample — get_train_test_split","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"function creates train validation sample based stratified random sampling. relative frequencies category train validation sample equal relative frequencies initial data (proportional stratified sampling).","code":""},{"path":"/reference/get_train_test_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"","code":"get_train_test_split(embedding, target, val_size)"},{"path":"/reference/get_train_test_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"embedding Object class EmbeddedText. target Named factor containing labels every case. val_size double Ratio 0 1 indicating relative frequency cases used validation sample.","code":""},{"path":"/reference/get_train_test_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for splitting data into a train and validation sample — get_train_test_split","text":"Returns list following components. target_train: Named factor containing labels training sample. embeddings_train: Object class EmbeddedText containing text embeddings training sample target_test: Named factor containing labels validation sample. embeddings_test: Object class EmbeddedText containing text embeddings validation sample","code":""},{"path":"/reference/install_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Installing necessary python modules to an environment — install_py_modules","title":"Installing necessary python modules to an environment — install_py_modules","text":"Function installing necessary python modules","code":""},{"path":"/reference/install_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Installing necessary python modules to an environment — install_py_modules","text":"","code":"install_py_modules(envname = \"aifeducation\")"},{"path":"/reference/install_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Installing necessary python modules to an environment — install_py_modules","text":"envname string Name environment packages installed.","code":""},{"path":"/reference/matrix_to_array_c.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape matrix to array — matrix_to_array_c","title":"Reshape matrix to array — matrix_to_array_c","text":"Function written C++ reshaping matrix containing sequential data array use keras.","code":""},{"path":"/reference/matrix_to_array_c.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape matrix to array — matrix_to_array_c","text":"","code":"matrix_to_array_c(matrix, times, features)"},{"path":"/reference/matrix_to_array_c.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape matrix to array — matrix_to_array_c","text":"matrix matrix containing sequential data. times uword Number sequences. features uword Number features within sequence.","code":""},{"path":"/reference/matrix_to_array_c.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape matrix to array — matrix_to_array_c","text":"Returns array. first dimension corresponds cases, second times, third features.","code":""},{"path":"/reference/QAExtractModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Question Answer Models of Type Extraction — QAExtractModel","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"R6 class stores information modeling question answer model. kind model extracts answer given text.","code":""},{"path":"/reference/QAExtractModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"model ('transformers.TFAutoModelForQuestionAnswering') Object class transformers.TFAutoModelForQuestionAnswering transformers python library. Stores qa model. tokenizer ('transformers.AutoTokenizer') Object class transformers.AutoTokenizer transformers python library. Stores tokenizer. qa_pipline ('transformers.QuestionAnsweringPipeline') Object class transformers.QuestionAnsweringPipeline transformers python library.","code":""},{"path":[]},{"path":"/reference/QAExtractModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"QAExtractModel$new() QAExtractModel$save_model() QAExtractModel$load_model() QAExtractModel$answer_question() QAExtractModel$clone()","code":""},{"path":"/reference/QAExtractModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"Method creating new question answer model based pretrained model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$new(   model_name,   model_version,   model_language,   model_license,   model_dir_path )"},{"path":"/reference/QAExtractModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"model_name Character Name new model. model_version Character Version model. model_language Character Language model support. model_license Character License model. model_dir_path string Path directory model stored.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-save-model-","dir":"Reference","previous_headings":"","what":"Method save_model()","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"Method saving question answer model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$save_model(model_dir_path)"},{"path":"/reference/QAExtractModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"model_dir_path string Path directory model tokenizer stored.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-load-model-","dir":"Reference","previous_headings":"","what":"Method load_model()","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"Method loading question answer model.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$load_model(model_dir_path)"},{"path":"/reference/QAExtractModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"model_dir_path string Path directory model tokenizer saved.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-answer-question-","dir":"Reference","previous_headings":"","what":"Method answer_question()","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"Method extracting answers given text.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$answer_question(   question,   knowledge_base,   n_answers = 1,   doc_stride = 128,   max_answer_len = 15,   max_seq_len = 384,   max_question_len = 64,   handle_impossible_answer = FALSE,   align_to_words = TRUE )"},{"path":"/reference/QAExtractModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"question string Question answered. knowledge_base list raw texts search answer. n_answers int Number possible answers generated texts. doc_stride int case knowledge base long model process text divided several overlapping chunks. parameter determines size overlap. max_answer_len int Maximum length token possible answers. answers shorter considered answer. max_seq_len int Maximum length question knowledge base tokenization. context may divided several overlapping chunks. max_question_len int maximum length question tokenization. Longer sequences truncated. handle_impossible_answer bool TRUE impossible answers accepted. align_to_words bool true TRUE algorithm tries align answer real words increases quality results space separated languages.","code":""},{"path":"/reference/QAExtractModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"objects class cloneable method.","code":""},{"path":"/reference/QAExtractModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"","code":"QAExtractModel$clone(deep = FALSE)"},{"path":"/reference/QAExtractModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Question Answer Models of Type Extraction — QAExtractModel","text":"deep Whether make deep clone.","code":""},{"path":"/reference/set_config_cpu_only.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting cpu only for tensorflow — set_config_cpu_only","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"functions configurates tensorflow use cpus.","code":""},{"path":"/reference/set_config_cpu_only.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"","code":"set_config_cpu_only()"},{"path":"/reference/set_config_cpu_only.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"function return anything. used side effects.","code":""},{"path":"/reference/set_config_cpu_only.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting cpu only for tensorflow — set_config_cpu_only","text":"os$environ$setdefault(\"CUDA_VISIBLE_DEVICES\",\"-1\")","code":""},{"path":"/reference/set_config_gpu_low_memory.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting gpus' memory usage — set_config_gpu_low_memory","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function changes memory usage gpus allow computations machine small memory. function computations large models may possible speed computation decreases. #'@return function return anything. used side effects.","code":""},{"path":"/reference/set_config_gpu_low_memory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"","code":"set_config_gpu_low_memory()"},{"path":"/reference/set_config_gpu_low_memory.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function sets TF_GPU_ALLOCATOR \"cuda_malloc_async\" sets memory growth TRUE.","code":""},{"path":"/reference/set_config_tf_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information in tensor flow. — set_config_tf_logger","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"function changes level logging information tensorflow.","code":""},{"path":"/reference/set_config_tf_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"","code":"set_config_tf_logger(level = \"ERROR\")"},{"path":"/reference/set_config_tf_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"string Minimal level printed console. Five levels available: FATAL, ERROR, WARN, INFO, DEBUG.","code":""},{"path":"/reference/set_config_tf_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information in tensor flow. — set_config_tf_logger","text":"function return anything. used side effects.","code":""},{"path":"/reference/split_labeled_unlabeled.html","id":null,"dir":"Reference","previous_headings":"","what":"Splits data into labeled and unlabeled data — split_labeled_unlabeled","title":"Splits data into labeled and unlabeled data — split_labeled_unlabeled","text":"functions splits data labeled unlabeled data.","code":""},{"path":"/reference/split_labeled_unlabeled.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splits data into labeled and unlabeled data — split_labeled_unlabeled","text":"","code":"split_labeled_unlabeled(embedding, target)"},{"path":"/reference/split_labeled_unlabeled.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splits data into labeled and unlabeled data — split_labeled_unlabeled","text":"embedding Object class EmbeddedText. target Named factor containing cases labels missing labels.","code":""},{"path":"/reference/split_labeled_unlabeled.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splits data into labeled and unlabeled data — split_labeled_unlabeled","text":"Returns list following components embeddings_labeled: Object class EmbeddedText containing cases labels. embeddings_unlabeled: Object class EmbeddedText containing cases labels. targets_labeled: Named factor containing labels relevant cases.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Abstract class neural nets keras tensorflow","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"name ('character()') Name classifier. label ('character()') Label classifier used individual title. date ('date()') Date first creation classifier. text_embedding_model ('list()') List storing information underlying text embedding model. information ensures classifier used data correct text embedding model ensures correct handling embeddings. bundeled_model ('bundle object') Object storing keras model neural net. Saved bundled object help package bundle serialization. model_config ('list()') List storing information configuration model. information used predicting new data. model_config$n_gru: Number gru layers. model_config$n_hidden: Number dense layers. model_config$target_levels: Levels target variable. change manually. model_config$input_variables: Order name input variables. change manually. model_config$init_config: List storing parameters passed method new(). last_training ('list()') List storing history results last training. information overwritten new training started. last_training$learning_time: Duration training process. config$history: History last training. config$data: Object class table storing initial frequencies passed data. config$data_pb:l Matrix storing number additional cases (test training) added balanced pseudo labeling. rows refer folds final training. columns refer steps pseudo labeling. config$data_bsc_test: Matrix storing number cases category used testing phase balanced synthetic units. Please note frequencies include original synthetic cases. terms number original synthetic cases exceeds limit majority classes frequency represents number cases created cluster analysis. config$date: Time last training finished. config$config: List storing kind estimation requested last training. config$config$use_bsc:  TRUE  balanced synthetic cases requested. FALSE . config$config$use_baseline: TRUE baseline estimation requested. FALSE . config$config$use_bpl: TRUE  balanced pseudo labeling cases requested. FALSE . reliability ('list()') List storing central reliability measures last training. reliability$val_metric: Array containing reliability measures validation data every fold, method, step (case pseudo labeling). reliability$val_metric_mean: Array containing reliability measures validation data every method step (case pseudo labeling). values represent mean values every fold. reliability$raw_iota_objects: List containing iota_object generated package iotarelr every fold start end last training. reliability$raw_iota_objects$iota_objects_start: List objects class iotarelr_iota2 containing estimated iota reliability second generation baseline model every fold. estimation baseline model requested list set NULL. reliability$raw_iota_objects$iota_objects_end: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold. Depending requested training method values refer baseline model, trained model basis balanced synthetic cases, balanced pseudo labeling combination balanced synthetic cases pseudo labeling. reliability$raw_iota_objects$iota_objects_start_free: List objects class iotarelr_iota2 containing estimated iota reliability second generation baseline model every fold. estimation baseline model requested list set NULL.Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$raw_iota_objects$iota_objects_end_free: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold. Depending requested training method values refer baseline model, trained model basis balanced synthetic cases, balanced pseudo labeling combination balanced synthetic cases pseudo labeling. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$iota_object_start: Object class iotarelr_iota2 mean individual objects every fold. estimation baseline model requested list set NULL. reliability$iota_object_start_free: Object class iotarelr_iota2 mean individual objects every fold. estimation baseline model requested list set NULL. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$iota_object_end: Object class iotarelr_iota2 mean individual objects every fold. Depending requested training method object refers baseline model, trained model basis balanced synthetic cases, balanced pseudo labeling combination balanced synthetic cases pseudo labeling. reliability$iota_object_end_free: Object class iotarelr_iota2 mean individual objects every fold. Depending requested training method object refers baseline model, trained model basis balanced synthetic cases, balanced pseudo labeling combination balanced synthetic cases pseudo labeling. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"TextEmbeddingClassifierNeuralNet$new() TextEmbeddingClassifierNeuralNet$train() TextEmbeddingClassifierNeuralNet$predict() TextEmbeddingClassifierNeuralNet$set_publication_info() TextEmbeddingClassifierNeuralNet$get_publication_info() TextEmbeddingClassifierNeuralNet$set_license() TextEmbeddingClassifierNeuralNet$get_license() TextEmbeddingClassifierNeuralNet$set_model_description() TextEmbeddingClassifierNeuralNet$get_model_description() TextEmbeddingClassifierNeuralNet$export_model() TextEmbeddingClassifierNeuralNet$import_model() TextEmbeddingClassifierNeuralNet$clone()","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Creating new instance class.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$new(   name = NULL,   label = NULL,   text_embeddings = NULL,   targets = NULL,   config = list(hidden = c(128), gru = c(128), dropout = 0.2, recurrent_dropout = 0.4,     l2_regularizer = 0.001, optimizer = \"adam\", act_fct = \"gelu\", act_fct_last =     \"softmax\", err_fct = \"categorical_crossentropy\") )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"name Character Name new classifier. Please refer common name conventions. Free text can used parameter label. label Character Label new classifier. can use free text. text_embeddings object classTextEmbeddingModel. targets factor containing target values classifier. config list containing configuration neural net. config$hidden vector containing number neurons dense layer. length vector determines number dense layers. want dense layer set parameter NULL. config$gru vector containing number neurons gru layer. length vector determines number dense layers. want dense layer set parameter NULL. config$dropout double ranging 0 lower 1 determining dropout gru layer. config$recurrent_dropout double ranging 0 lower 1 determining recurrent dropout gru layer. l2_regularizer double determining l2 regularizer every dense layer. optimizer Object class keras.optimizers. act_fct character naming activation function dense layers. act_fct_last character naming activation function last dense layers. err_fct character naming loss/error function neural net.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method training neural net keras tensorflow.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$train(   data_embeddings,   data_targets,   data_n_valid_samples = 10,   use_baseline = TRUE,   bsl_val_size = 0.25,   use_bsc = TRUE,   bsc_methods = c(\"dbsmote\"),   bsc_max_k = 10,   use_bpl = TRUE,   bpl_max_steps = 10,   bpl_inc_ratio = 0.25,   bpl_anchor = 0.75,   bpl_valid_size = 0.33,   opt_model_reset = TRUE,   epochs = 100,   batch_size = 32,   dir_checkpoint,   trace = TRUE,   view_metrics = FALSE,   keras_trace = 2,   n_cores = 2 )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"data_embeddings Object class TextEmbeddingModel. data_targets Factor containing labels cases stored data_embeddings. Factor must named use names used data_embeddings. data_n_valid_samples int determining number cross-fold samples. use_baseline bool TRUE calculation baseline model requested. option relevant use_bsc=TRUE use_pbl=TRUE. FALSE baseline model calculated. bsl_val_size double 0 1 indicating proportion cases label used validation sample estimation baseline model. remaining cases part training data. use_bsc bool TRUE estimation integrate balanced synthetic cases. FALSE . bsc_methods vector containing methods generating synthetic cases via smotefamily. Multiple methods can passed. Currently bsc_methods=c(\"adas\"), bsc_methods=c(\"smote\") bsc_methods=c(\"dbsmote\") possible. bsc_max_k int determining maximal number k used creating synthetic units. use_bpl bool TRUE estimation integrate balanced pseudo labeling. FALSE . bpl_max_steps int determining maximal number steps every application balanced pseudo labeling. bpl_inc_ratio double ratio 0 1 indicating proportion new cases used training. See notes details. bpl_anchor double 0 1 indicating reference point sorting new cases every label. See notes details. bpl_valid_size double ratio 0 1 determining proportion new cases every label added validation sample training. remaining cases added training sample. opt_model_reset bool TRUE model reseted training. epochs int Number training epochs. batch_size int Size batches. dir_checkpoint string Path directory checkpoint training saved. directory exists created. trace bool TRUE information estimation phase printed console. view_metrics bool TRUE metrics printed RStudie IDE. keras_trace int keras_trace=0 print information training process keras console. keras_trace=1 print progress bar. keras_trace=2 prints one line information every epoch. n_cores int Number cores used creating synthetic units.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"bsc_max_k: values 2 bsc_max_k successively used. number bsc_max_k high value reduced number allows calculating synthetic units. bpl_inc_ratio: ratio applied label smallest number new cases. resulting value used every label ensure balance labels. bpl_anchor: help value new cases sorted. aim distance anchor calculated cases arranged increasing order.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method prediciting new data trained neural net.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$predict(newdata)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"newdata Object class TextEmbeddingModel data.frame predictions made.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Returns data.frame containing predictions probabilities different labels case.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method setting publication information classifier","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_publication_info(   type,   autors,   citation,   url = NULL )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"type string choosing type information added. type=\"developer\" information developer classifier type=\"trainer\" information persons trained classifier. modify already existing classifier please use type=\"modifier\". autors Person list authors. citation Free text citation. url URL corresponding homepage.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method requesting publication information classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_publication_info()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"list saved publication information.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-license-","dir":"Reference","previous_headings":"","what":"Method set_license()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method setting license classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_license(license)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-license-","dir":"Reference","previous_headings":"","what":"Method get_license()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method requesting license classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_license()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"string License classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method setting description classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$set_model_description(   eng = NULL,   native = NULL )"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"eng string text describing training learner, theoretical empirical background, different output labels English. native string text describing training learner, theoretical empirical background, different output labels native language classifier.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method requesting model description.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$get_model_description()"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"list description classifier English native language.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-export-model-","dir":"Reference","previous_headings":"","what":"Method export_model()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method exporting model tensorflow SavedModel format.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$export_model(dir_path)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"dir_path string() Path directory model saved.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-import-model-","dir":"Reference","previous_headings":"","what":"Method import_model()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"Method importing model tensorflow SavedModel format.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$import_model(dir_path)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"dir_path string() Path directory model saved.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"objects class cloneable method.","code":""},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"","code":"TextEmbeddingClassifierNeuralNet$clone(deep = FALSE)"},{"path":"/reference/TextEmbeddingClassifierNeuralNet.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Classifier with a Neural Net — TextEmbeddingClassifierNeuralNet","text":"deep Whether make deep clone.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Text Embedding Model — TextEmbeddingModel","title":"Text Embedding Model — TextEmbeddingModel","text":"R6 class stores text embedding model can used tokenize, encode, decode, embed raw texts. object provides unique interface different text processing methods.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text Embedding Model — TextEmbeddingModel","text":"basic_components ('list()') List storing information can apply methods. basic_components$method: Method underlying text embedding model. basic_components$max_length: Maximum number tokens sequence model processes. general shorter sequences padded longer sequences divided chunks /truncated. bert_components ('list()') List storing information apply bert models. bert_components$model: object class transformers.TFBertModel using transformers library. bert_components$tokenizer: object class transformers.BertTokenizerFast using transformers library. bert_components$aggregation: Aggregation method hidden states. bert_components$use_cls_token: Whether use hidden state representing [CLS] token mean hidden states tokens special functions. bert_components$chunks: Maximal number chunks processed model. bert_components$overlap: Number tokens added beginng sequence next chunk. bow_components ('list()') List storing information apply bow_models. bow_components$model: data.frame describing relationship tokens corresponding text embeddings. bow_components$vocab: data.frame saving tokens, lemmata corresponding integer index. bow_components$configuration: List configuration parameters model. bow_components$configuration$to_lower TRUE tokens transformed lower case. bow_components$configuration$use_lemmata TRUE corresponding lemma used instead token. bow_components$configuration$bow_n_dim Number dimensions GlobalVectors Topic Modeling. bow_components$configuration$bow_n_cluster Number clusters grouping tokens based global vectors. apply method lda. bow_components$configuration$bow_max_iter Maximum number iterations calculation global vectors topics. bow_components$configuration$bow_max_iter_cluster Maximum number iterations creating cluster. Applies method glove. bow_components$configuration$bow_cr_criterion Convergence criterion calculating global vectors topics. bow_components$configuration$bow_learning_rate  Initial learning rate estimating global vectors. bow_components$aggregation: currently apply methods. bow_components$chunks: currently apply methods. bow_components$overlap: currently apply methods.","code":""},{"path":[]},{"path":"/reference/TextEmbeddingModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text Embedding Model — TextEmbeddingModel","text":"TextEmbeddingModel$new() TextEmbeddingModel$load_model() TextEmbeddingModel$save_bert_model() TextEmbeddingModel$encode() TextEmbeddingModel$decode() TextEmbeddingModel$embed() TextEmbeddingModel$set_publication_info() TextEmbeddingModel$get_publication_info() TextEmbeddingModel$set_license() TextEmbeddingModel$get_license() TextEmbeddingModel$set_model_description() TextEmbeddingModel$get_model_description() TextEmbeddingModel$get_model_info() TextEmbeddingModel$clone()","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method creating new text embedding model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$new(   model_name,   model_label,   model_version,   model_language,   method,   max_length = 0,   chunks = 1,   overlap = 0,   aggregation = \"last\",   use_cls_token = TRUE,   model_dir,   bow_basic_text_rep,   bow_n_dim = NULL,   bow_n_cluster = NULL,   bow_max_iter = 500,   bow_max_iter_cluster = NULL,   bow_cr_criterion = 1e-08,   bow_learning_rate = NULL,   trace = FALSE )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"model_name string containing name new model. model_label string containing label/title new model. model_version string version model. model_language string containing language model represents (e.g., English). method string determining kind embedding model. Currently three type supported. method=\"bert\" Bidirectional Encoder Representations Transformers (BERT), method=\"glove\" GlobalVector Clusters, method=\"lda\" topic modeling. See details information. max_length int determining maximums length token sequences uses bert models. relevant methods. chunks int Maximum number chunks. relevant bert models. overlap int determining number tokens added beginning next chunk. relevant bert models. aggregation string method aggregating text embeddings created bert models. See details information. use_cls_token bool TRUE CLS token used aggregation text embedding. FALSE mean corresponding tokens used aggregation text embedding. model_dir string path directory bert model stored. bow_basic_text_rep object class basic_text_rep created via function bow_pp_create_basic_text_rep. relevant method=\"glove\" method=\"lda\". bow_n_dim int Number dimension GlobalVector number topics LDA. bow_n_cluster int Number clusters created basis GlobalVectors. Parameter relevant method=\"lda\" method=\"bert\" bow_max_iter int Maximum Number iterations fitting GlobalVectors Topic Models. bow_max_iter_cluster int Maximum number iterations fitting cluster method=\"glove\". bow_cr_criterion double convergence criterion GlobalVectors. bow_learning_rate double initial learning rate GlobalVectors. trace bool TRUE print information progress. FALSE .","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text Embedding Model — TextEmbeddingModel","text":"method: case method=\"bert\" pretrained bert model must supplied via model_dir. method=\"glove\" method=\"lda\" new model created based data provided via bow_basic_text_rep. original algorithm GlobalVectors provides word embeddings, text embeddings. achieve text embeddings words clusters based word embeddings kmeans. aggregation: creating text embedding bert model serveral options possible: last: aggregation=\"last\" uses hidden states last layer. second_to_last: aggregation=\"second_to_last\" uses hidden states second last layer. fourth_to_last: aggregation=\"fourth_to_last\" uses hidden states fourth last layer. : aggregation=\"\" uses mean hidden states hidden layers. last_four: aggregation=\"last_four\" uses mean hidden states last four layers.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-load-model-","dir":"Reference","previous_headings":"","what":"Method load_model()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method loading transformer model R.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$load_model(model_dir)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"model_dir string containing path relevant model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-save-bert-model-","dir":"Reference","previous_headings":"","what":"Method save_bert_model()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method saving bert model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$save_bert_model(model_dir)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"model_dir string containing path bert model saved.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-encode-","dir":"Reference","previous_headings":"","what":"Method encode()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method encoding words raw texts integers.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$encode(   raw_text,   token_encodings_only = FALSE,   trace = FALSE )"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"raw_text vector containing raw texts. token_encodings_only bool TRUE token encodings returned. FALSE complete encoding returned important bert models. trace bool TRUE information progress printed. FALSE requested.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"list containing integer sequences raw texts special tokens.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-decode-","dir":"Reference","previous_headings":"","what":"Method decode()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method decoding sequence integers tokens","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$decode(int_seqence)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"int_seqence list containing integer sequences transformed tokens single integer sequence vector","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"list token sequences","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-embed-","dir":"Reference","previous_headings":"","what":"Method embed()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method creating text embeddings raw texts","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$embed(raw_text = NULL, doc_id = NULL, trace = FALSE)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"raw_text vector containing raw texts. doc_id vector containing corresponding ids every text. trace bool TRUE information progression printed console.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"Method returns R6 object class EmbeddedText. object contain embeddings data.frame information model creating embeddings.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method setting publication information model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_publication_info(type, autors, citation, url = NULL)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"type string Type information changed/added. type=\"developer\",type=\"trainer\", type=\"modifier\" possible. autors List persons. citation string Citation free text. url string Corresponding URL applicable.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method getting publication information model.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_publication_info()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"list publication information.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-license-","dir":"Reference","previous_headings":"","what":"Method set_license()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method setting license model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_license(license)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-license-","dir":"Reference","previous_headings":"","what":"Method get_license()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method requesting license model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_license()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"string License model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method setting description model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_model_description(eng = NULL, native = NULL)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"eng string text describing training learner, theoretical empirical background, different output labels English. native string text describing training learner, theoretical empirical background, different output labels native language model","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method requesting model description.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_description()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"list description model English native language.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Text Embedding Model — TextEmbeddingModel","text":"Method requesting model information","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_info()"},{"path":"/reference/TextEmbeddingModel.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Text Embedding Model — TextEmbeddingModel","text":"list relevant model information","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text Embedding Model — TextEmbeddingModel","text":"objects class cloneable method.","code":""},{"path":"/reference/TextEmbeddingModel.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Text Embedding Model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$clone(deep = FALSE)"},{"path":"/reference/TextEmbeddingModel.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text Embedding Model — TextEmbeddingModel","text":"deep Whether make deep clone.","code":""},{"path":"/reference/train_tune_bert_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for training and fine-tuning a bert model — train_tune_bert_model","title":"Function for training and fine-tuning a bert model — train_tune_bert_model","text":"function can used training fine-tuning transformer based Bert architecture help python libraries 'transformers', 'datasets', 'tokenizers'.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for training and fine-tuning a bert model — train_tune_bert_model","text":"","code":"train_tune_bert_model(   output_dir,   bert_model_dir_path,   raw_texts,   aug_vocab_by = 100,   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   n_workers = 1,   multi_process = FALSE,   trace = TRUE )"},{"path":"/reference/train_tune_bert_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for training and fine-tuning a bert model — train_tune_bert_model","text":"output_dir string Path directory final model saved. directory exists created. bert_model_dir_path string Path directory original model stored. raw_texts vector containing raw texts training. aug_vocab_by int Number entries extending current vocabulary. See notes details p_mask double Ratio determining number words/tokens masking. whole_word bool TRUE whole word masking applied. FALSE token masking used. val_size double Ratio determining amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. n_workers int Number workers. multi_process bool TRUE multiple process activated. trace bool TRUE information progress printed console.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for training and fine-tuning a bert model — train_tune_bert_model","text":"function return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"/reference/train_tune_bert_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for training and fine-tuning a bert model — train_tune_bert_model","text":"aug_vocab_by > 0 raw texts used training WordPiece tokenizer. end process additional entries added vocabulary part original vocabulary. Pre-Trained models can fine-tuned function available https://huggingface.co/. New models can created via function create_bert_model.","code":""},{"path":"/reference/vocab_draft_movie_review.html","id":null,"dir":"Reference","previous_headings":"","what":"Example for a draft of a Vocabulary — vocab_draft_movie_review","title":"Example for a draft of a Vocabulary — vocab_draft_movie_review","text":"list testing illustration purposes. object created based data set \"data_corpus_moviereviews\" package quanteda.textmodels. udpipe language model english-ewt-ud-2.5-191206 used.","code":""},{"path":"/reference/vocab_draft_movie_review.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example for a draft of a Vocabulary — vocab_draft_movie_review","text":"","code":"vocab_draft_movie_review"},{"path":"/reference/vocab_draft_movie_review.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example for a draft of a Vocabulary — vocab_draft_movie_review","text":"list object.","code":""}]

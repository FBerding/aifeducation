log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
reticulate::py_available(T)
devtools::load_all()
load("~/aifeducation/arguments.rda")
embeddings <- load_from_disk(args$path_to_embeddings)
target_data <- long_load_target_data(file_path=args$path_to_target_data,
selectet_column=args$target_data_column)
# Load feature extractor if provided
if (!is.null(args$path_to_feature_extractor)) {
feature_extractor <- load_from_disk(args$path_to_feature_extractor)
} else {
feature_extractor <- NULL
}
#Check for valid arguments
if(is.null(args$self_attention_heads)){
self_attention_heads=0
}
# Create dir for checkpints
dir_destination <- paste0(
args$destination_path, "/",
args$folder_name
)
dir_checkpoints <- paste0(
args$dir_destination, "/",
"checkpoints"
)
if (dir.exists(dir_destination) == FALSE) {
dir.create(dir_destination)
}
if (dir.exists(dir_checkpoints) == FALSE) {
dir.create(dir_checkpoints)
}
classifier <- TEClassifierProtoNet$new(
ml_framework = "pytorch",
embedding_dim = args$embedding_dim,
name = args$name,
label = args$label,
text_embeddings = embeddings,
feature_extractor = feature_extractor,
targets = target_data,
hidden = args$hidden,
rec = args$rec,
rec_type = args$rec_type,
rec_bidirectional = args$rec_bidirectional,
self_attention_heads = self_attention_heads,
intermediate_size = args$intermediate_size,
attention_type = args$attention_type,
add_pos_embedding = args$add_pos_embedding,
rec_dropout = args$rec_dropout,
repeat_encoder = args$repeat_encoder,
dense_dropout = args$dense_dropout,
recurrent_dropout = args$recurrent_dropout,
encoder_dropout = args$encoder_dropout,
optimizer = args$optimizer
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 0,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
args$embedding_dim
args$rec
args$repeat_encoder
classifier$model
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 0,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
reticulate::py_last_error()
devtools::load_all()
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 0,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
classifier <- TEClassifierProtoNet$new(
ml_framework = "pytorch",
embedding_dim = args$embedding_dim,
name = args$name,
label = args$label,
text_embeddings = embeddings,
feature_extractor = feature_extractor,
targets = target_data,
hidden = args$hidden,
rec = args$rec,
rec_type = args$rec_type,
rec_bidirectional = args$rec_bidirectional,
self_attention_heads = self_attention_heads,
intermediate_size = args$intermediate_size,
attention_type = args$attention_type,
add_pos_embedding = args$add_pos_embedding,
rec_dropout = args$rec_dropout,
repeat_encoder = args$repeat_encoder,
dense_dropout = args$dense_dropout,
recurrent_dropout = args$recurrent_dropout,
encoder_dropout = args$encoder_dropout,
optimizer = args$optimizer
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 0,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
devtools::load_all()
classifier <- TEClassifierProtoNet$new(
ml_framework = "pytorch",
embedding_dim = args$embedding_dim,
name = args$name,
label = args$label,
text_embeddings = embeddings,
feature_extractor = feature_extractor,
targets = target_data,
hidden = args$hidden,
rec = args$rec,
rec_type = args$rec_type,
rec_bidirectional = args$rec_bidirectional,
self_attention_heads = self_attention_heads,
intermediate_size = args$intermediate_size,
attention_type = args$attention_type,
add_pos_embedding = args$add_pos_embedding,
rec_dropout = args$rec_dropout,
repeat_encoder = args$repeat_encoder,
dense_dropout = args$dense_dropout,
recurrent_dropout = args$recurrent_dropout,
encoder_dropout = args$encoder_dropout,
optimizer = args$optimizer
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 0,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 1,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
classifier <- TEClassifierProtoNet$new(
ml_framework = "pytorch",
embedding_dim = args$embedding_dim,
name = args$name,
label = args$label,
text_embeddings = embeddings,
feature_extractor = feature_extractor,
targets = target_data,
hidden = args$hidden,
rec = NULL,
rec_type = args$rec_type,
rec_bidirectional = args$rec_bidirectional,
self_attention_heads = self_attention_heads,
intermediate_size = args$intermediate_size,
attention_type = args$attention_type,
add_pos_embedding = args$add_pos_embedding,
rec_dropout = args$rec_dropout,
repeat_encoder = 1,
dense_dropout = args$dense_dropout,
recurrent_dropout = args$recurrent_dropout,
encoder_dropout = args$encoder_dropout,
optimizer = args$optimizer
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 1,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = args$Ns,
Nq = args$Nq,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
devtools::test_active_file()
args$loss_alpha
args$loss_margin
args$Nq
args$Ns
classifier <- TEClassifierProtoNet$new(
ml_framework = "pytorch",
embedding_dim = args$embedding_dim,
name = args$name,
label = args$label,
text_embeddings = embeddings,
feature_extractor = feature_extractor,
targets = target_data,
hidden = args$hidden,
rec = args$rec,
rec_type = args$rec_type,
rec_bidirectional = args$rec_bidirectional,
self_attention_heads = self_attention_heads,
intermediate_size = args$intermediate_size,
attention_type = args$attention_type,
add_pos_embedding = args$add_pos_embedding,
rec_dropout = args$rec_dropout,
repeat_encoder = args$repeat_encoder,
dense_dropout = args$dense_dropout,
recurrent_dropout = args$recurrent_dropout,
encoder_dropout = args$encoder_dropout,
optimizer = args$optimizer
)
# Train
classifier$train(
data_embeddings = embeddings,
data_targets = target_data,
data_folds = args$data_folds,
data_val_size = args$data_val_size,
use_sc = args$use_sc,
sc_method = args$sc_method,
sc_min_k = args$sc_min_k,
sc_max_k = args$sc_max_k,
use_pl = args$use_pl,
pl_max_steps = args$pl_max_steps,
pl_max = args$pl_max,
pl_anchor = args$pl_anchor,
pl_min = args$pl_min,
sustain_track = TRUE,
sustain_iso_code = args$sustain_iso_code,
sustain_region = NULL,
sustain_interval = 15,
epochs = args$epochs,
batch_size = args$batch_size,
dir_checkpoint = dir_checkpoints,
trace = FALSE,
keras_trace = 0,
pytorch_trace = 1,
log_dir = args$log_dir,
log_write_interval = args$log_write_interval,
Ns = 8,
Nq = 5,
loss_alpha = args$loss_alpha,
loss_margin = args$loss_margin
)
devtools::load_all()
start_studio_new()
library(bslib)
library(shiny)
start_studio_new()
devtools::load_all()
start_studio_new()
future::plan(future::multisession)
load("~/aifeducation/arguments.rda")
ExtendedTask_arguments=args
CurrentTask <- shiny::ExtendedTask$new(long_classifier)
do.call(what = CurrentTask$invoke, args = ExtendedTask_arguments)
CurrentTask$status()
CurrentTask$status
CurrentTask$status()
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$status())
isolate(CurrentTask$result())
devtools::document()
devtools::build()
install.packages("~/aifeducation_0.3.4.tar.gz", repos = NULL, type = "source")
reticulate::py_available(T)
devtools::load_all()
library(shiny)
library(bslib)
start_studio_new()
devtools::load_all()
start_studio_new()
styler:::style_active_file()
styler:::style_active_file()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
model=load_from_disk("C:/Users/WissMit/Desktop/test_protonet")
model$count_parameter()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
devtools::load_all()
start_studio_new()
library(shiny)
library(bslib)
start_studio_new()
devtools::load_all()
devtools::load_all()
start_studio_new()
abc=get_studio_plot_theme
abc=get_studio_plot_theme()
abc==ggplot2::theme_dark
abc=ggplot2::theme_dark
abc
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::document()
devtools::load_all()
start_studio_new()
styler:::style_active_file()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()
devtools::load_all()
start_studio_new()

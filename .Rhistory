id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
head(example_data)
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.data.frame(quanteda.textmodels::data_corpus_moviereviews)
head(example_data)
?bow_pp_create_vocab_draft
devtools::load_all()
4*96
15*512
devtools::build_site()
devtools::build_site()
devtools::build_site()
devtools::build_site()
devtools::build_site()
devtools::build_site()
devtools::document()
usethis::use_testthat()
testthat::skip_if(condition=check_aif_py_modules(),
message = "Necessary python modules not available")
testthat::skip_if_not(condition=check_aif_py_modules(),
message = "Necessary python modules not available")
devtools::test()
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.data.frame(quanteda.textmodels::data_corpus_moviereviews)
table(example_data$label)
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=TRUE)
devtools::load_all()
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=TRUE)
devtools::test()
expect_type(check_aif_py_modules(print = FALSE),
"logical")
check_aif_py_modules()
check_aif_py_modules(print = FALSE)
devtools::load_all()
check_aif_py_modules(print = FALSE)
check_aif_py_modules(print = TRUE)
devtools::check()
devtools::test()
devtools::load_all()
devtools::test()
devtools::test()
expect_type(check_aif_py_modules(print = FALSE),
"logical")
test_that("check_python_modules", {
expect_type(check_aif_py_modules(print = FALSE),
"logical")
})
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::load_all()
devtools::test()
check_aif_py_modules()
check_aif_py_modules(FALSE)
devtools::load_all()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.15,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=12,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
tokens(example_data$text)
tokens(corpus(example_data$text))
class(example_data$text)
class(example_data$text)
class(example_data$text[1])
class(example_data$text[[1]])
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.data.frame(quanteda.textmodels::data_corpus_moviereviews)
quanteda.textmodels::data_corpus_moviereviews
abc<-as.data.frame(quanteda.textmodels::data_corpus_moviereviews)
abc<-as.character(quanteda.textmodels::data_corpus_moviereviews)
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
testthat::skip_on_cran()
testthat::skip_if_not(condition=check_aif_py_modules(),
message = "Necessary python modules not available")
test_that("create_bert_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=TRUE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
})
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.15,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=12,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
devtools::load_all()
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.15,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=12,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.15,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=50,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=100,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=TRUE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
model_dir="tmp"
vocab_raw_texts=example_data$text
vocab_size=30522
vocab_do_lower_case=FALSE
max_position_embeddings=512
hidden_size=768
num_hidden_layer=12
num_attention_heads=12
intermediate_size=3072
hidden_act="gelu"
hidden_dropout_prob=0.1
trace=FALSE
transformers<-reticulate::import("transformers")
datasets<-reticulate::import("datasets")
tok<-reticulate::import("tokenizers")
#Creating a new Tokenizer for Computing Vocabulary
tok_new<-tok$Tokenizer(tok$models$WordPiece())
tok_new$normalizer=tok$normalizers$BertNormalizer(lowercase=vocab_do_lower_case)
tok_new$pre_tokenizer=tok$pre_tokenizers$BertPreTokenizer()
tok_new$decode=tok$decoders$WordPiece()
trainer<-tok$trainers$WordPieceTrainer(
vocab_size=as.integer(vocab_size),
show_progress=trace)
#Calculating Vocabulary
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary"))
}
tok_new$train_from_iterator(vocab_raw_texts,trainer=trainer)
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary - Done"))
}
special_tokens=c("[PAD]","[CLS]","[SEP]","[UNK]","[MASK]")
if(dir.exists(model_dir)==FALSE){
print(paste(date(),"Creating Checkpoint Directory"))
dir.create(model_dir)
}
write(c(special_tokens,names(tok_new$get_vocab)),
file=paste0(model_dir,"/","vocab.txt"))
special_tokens
names(tok_new$get_vocab)
vocab_raw_texts[[1]]
tok_new$train_from_iterator(vocab_raw_texts,trainer=trainer)
lengh(tok_new$get_vocab)
length(tok_new$get_vocab)
tok_new$get_vocab_size
tok_new$get_vocab_size()
write(c(special_tokens,names(tok_new$get_vocab())),
file=paste0(model_dir,"/","vocab.txt"))
testthat::skip_on_cran()
testthat::skip_if_not(condition=check_aif_py_modules(),
message = "Necessary python modules not available")
test_that("create_bert_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=TRUE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
})
devtools::load_all()
devtools::test()
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=FALSE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=FALSE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=0,
p_mask=0.30,
whole_word=FALSE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=100,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
expect_no_error(
create_bert_model(
model_dir="tmp",
vocab_raw_texts=example_data$text,
vocab_size=30522,
vocab_do_lower_case=FALSE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=100,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
devtools::load_all()
expect_no_error(
train_tune_bert_model(output_dir="tmp",
bert_model_dir_path="tmp",
raw_texts= example_data$text,
aug_vocab_by=100,
p_mask=0.30,
whole_word=TRUE,
val_size=0.1,
n_epoch=1,
batch_size=2,
chunk_size=250,
n_workers=1,
multi_process=FALSE,
trace=TRUE))
devtools::test()

preparation_tokens_chunks_masked[[i]]<-tmp
preparation_tokens_chunks_masked[[i]]
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[[i]]
tmp[tmp_sample]<-"[MASK]"
preparation_tokens_chunks_masked[[i]]<-tmp
preparation_tokens_chunks_masked[i]<-tmp
tokens_replace(preparation_tokens_chunks_masked[[i]],
pattern=tmp_sample,replacement="[MASK]")
quanteda::tokens_replace(preparation_tokens_chunks_masked[[i]],
pattern=tmp_sample,replacement="[MASK]")
quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
pattern=tmp_sample,replacement="[MASK]")
quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
pattern=tmp_sample,replacement=rep("[MASK]",times=length(tmp_sample))
tmp[tmp_sample]<-"[MASK]"
preparation_tokens_chunks_masked[i]<-tmp
}
quanteda::tokens_replace()
preparation_tokens_chunks_2<-lapply(preparation_tokens_chunks,paste,collapse=" ")
tokenized_texts=NULL
for(i in 1:length(preparation_tokens_chunks_2)){
tokenized_texts[i]<-list(tokenizer(preparation_tokens_chunks_2[[i]],truncation=FALSE))
print(i)
}
abc<-as.character(preparation_tokens_chunks_2[1:2])
class(abc)
length(abc)
tokenized_texts<-tokenizer(abc)
tokenized_texts["input_ids"][[1]][2]
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts)
test<-vector(length = 2)
test<-c(as.character(preparation_tokens_chunks_2[[1]]),
as.character(preparation_tokens_chunks_2[[2]]))
class(test)
length(test)
as.vector(unlist(unlist(preparation_tokens_chunks_2[1:2])))
tokenized_texts<-tokenizer(test,truncation=FALSE)
data<-tf$data$Dataset$from_tensor_slices(tokenized_texts)
tokenized_texts<-tokenizer(unlist(unlist(preparation_tokens_chunks_2),use.name=FALSE))
data<-tf$data$Dataset$from_tensor_slices(as.vector(tokenized_texts))
tokenized_texts<-tokenizer(preparation_tokens_chunks_2,truncation=FALSE)
abc<-reticulate::py_to_r(tokenizer(preparation_tokens_chunks_2[[i]],truncation=FALSE))
print(tokenized_texts)
test<-unlist(unlist(preparation_tokens_chunks_2[1:2],use.names = FALSE))
test_list<-c(unlist(preparation_tokens_chunks_2[1],use.names = FALSE),
unlist(preparation_tokens_chunks_2[2],use.names = FALSE))
test_list["a"]=list(tokenized_texts[1])
test_list["b"]=list(tokenized_texts[2])
test<-vector(length = 2)
test[1]="Dies ist ein Test."
test[2]="Und ein weiterer Text."
tokenized_texts<-tokenizer(test,truncation=FALSE,return_tensors="tf")
abc<-as.vector(tokenized_texts["input_ids"][1,])
abc[2]<-103
abc
tokenized_texts["input_ids"][1,]<-abc
abc<-reticulate::py_to_r(tokenized_texts)
abcd<-reticulate::dict(list(input_ids=c(102L,1406L,104L,143L,4369L,566L,103L),
token_type_ids=c(0L, 0L, 0L, 0L, 0L, 0L, 0L),
attention_mask=c(1L, 1L, 1L, 1L, 1L, 1L, 1L),
labels=c(102L,1406L,215L,143L,4369L,566L,103L),
word_ids=c(102L,1406L,215L,143L,4369L,566L,103L),
convert=TRUE)
)
reticulate::py_run_string("print(tokenized_texts.keys)")
abc<-as.list(tokenized_texts["input_ids"])
tokenized_texts["input_ids"]
data<-NULL
quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
pattern=tmp_sample,replacement=rep("[MASK]",times=length(tmp_sample)))
i=1
preparation_tokens_chunks_masked<-preparation_tokens_chunks_original
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
preparation_tokens_chunks_masked<-preparation_tokens_chunks_original
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
}
preparation_tokens_chunks_masked<-preparation_tokens_chunks_original
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
}
preparation_tokens_chunks_masked
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
}
preparation_tokens_chunks_masked[[i]]<-quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_original)){
tmp<-preparation_tokens_chunks_original[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_original_2<-lapply(preparation_tokens_chunks_original,paste,collapse=" ")
preparation_tokens_chunks_masked_2<-lapply(preparation_tokens_chunks_masked,paste,collapse=" ")
preparation_tokens_chunks_masked_2[1]
preparation_tokens_chunks_original_2[1]
abc<-as.character(preparation_tokens_chunks_2[1:2])
class(abc)
length(abc)
preparation_tokens_chunks_original_2<-as.character(lapply(preparation_tokens_chunks_original,paste,collapse=" "))
length(preparation_tokens_chunks_original_2)
preparation_tokens_chunks_masked_2<-as.character(lapply(preparation_tokens_chunks_masked,paste,collapse=" "))
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2)
tokenized_texts__masked["input_ids"]
tokenized_texts__masked["input_ids"][1]
tokenizer$mask_token_id
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf")
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf",truncation=TRUE)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf",truncation =TRUE)
abc<-as.character(preparation_tokens_chunks_2[1:2])
class(abc)
length(abc)
tokenized_texts<-tokenizer(abc)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
max_length=128)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
load("Trial/Training_Data.RData")
library(reticulate)
bert_model_dir_path="Trial/Bert_Modelle"
transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
builtin=reticulate::import("builtin")
tokenizer<-transformer$BertTokenizerFast$from_pretrained(bert_model_dir_path)
model<-transformer$TFBertForMaskedLM$from_pretrained(bert_model_dir_path)
preparation_tokens<-quanteda::tokens(paste(training_data$text[1:50],
collapse = " "),
verbose = TRUE)
preparation_tokens_chunks_original<-quanteda::tokens_chunk(
x=preparation_tokens,
size=128,
overlap = 0,
use_docvars = FALSE)
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_original)){
tmp<-preparation_tokens_chunks_original[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_original_2<-as.character(lapply(preparation_tokens_chunks_original,paste,collapse=" "))
preparation_tokens_chunks_masked_2<-as.character(lapply(preparation_tokens_chunks_masked,paste,collapse=" "))
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
training_args = transformer$TFTrainingArguments(
output_dir='./results',           # output directory
num_train_epochs=1,               # total number of training epochs
per_device_train_batch_size=16L,  # batch size per device during training
per_device_eval_batch_size=128L,  # batch size for evaluation
warmup_steps=100L,                # number of warmup steps for learning rate scheduler
weight_decay=0.01,                # strength of weight decay
logging_dir='./logs',             # directory for storing logs
logging_steps=10L
)
training_args = transformer$TFTrainingArguments(
output_dir='./results',           # output directory
num_train_epochs=1,               # total number of training epochs
per_device_train_batch_size=16L,  # batch size per device during training
per_device_eval_batch_size=128L,  # batch size for evaluation
warmup_steps=100L,                # number of warmup steps for learning rate scheduler
weight_decay=0.01,                # strength of weight decay
logging_dir='./logs',             # directory for storing logs
logging_steps=10L
)
tokenized_texts_orignal
train_labels=tokenized_texts_orignal["token_ids"]
train_labels=tokenized_texts_orignal["input_ids"]
train_dataset = tf$data$Dataset$from_tensor_slices(tuple(builtins$dict(tokenized_texts__masked),train_labels))
builtins$dict(tokenized_texts__masked),train_labels)
builtins$dict(tokenized_texts__masked)
train_dataset = tf$data$Dataset$from_tensor_slices(tuple(tokenized_texts__masked,train_labels))
trainer = transformer$TFTrainer(
model=model_BERTje,                  # the instantiated 🤗 Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer = transformer$TFTrainer(
model=model,                  # the instantiated 🤗 Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer$train()
reticulate::py_last_error()
str(train_dataset)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
trainer$train()
trainer = transformer$TFTrainer(
model=model,                  # the instantiated 🤗 Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer$train()
model$compile(optimizer=Adam(3e-5))
model$compile(optimizer= transformer$AdamW(3e-5))
model$compile()
model.fit(train_dataset)
model$fit(train_dataset)
tf_dataset = model$prepare_tf_dataset(train_dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)
tf_dataset = model$prepare_tf_dataset(train_dataset, batch_size=16, shuffle=TRUE, tokenizer=tokenizer)
reticulate::py_install('datasets ', pip = TRUE)
devtools::build()
load("Trial/Training_Data.RData")
load("Trial/embeddings_bert_500.RData")
load("Trial/embeddings_german_glove_cluster_96.RData")
devtools::load_all()
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "Iosmap",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "Isomap",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
table(training_data$basic_web,pred_test$basic_web)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
object.size(test_red$trained_learner$dim_reduction)
object.size(test_red$trained_learner$dim_reduction$model)
object.size(test_red$trained_learner$dim_reduction$model@data)
object.size(test_red$trained_learner$dim_reduction$model@org.data)
object.size(test_red$trained_learner$dim_reduction$model@apply())
object.size(test_red$trained_learner$dim_reduction$model@apply
)
object.size(test_red$trained_learner$dim_reduction$model@inverse)
test_red$trained_learner$dim_reduction$model@has.inverse
test_red$trained_learner$dim_reduction$model@has.inverse<-FALSE
test_red$trained_learner$dim_reduction$model@inverse<-NULL
test_red$trained_learner$dim_reduction$model@inverse=function()
)
test_red$trained_learner$dim_reduction$model@inverse=function
test_red$trained_learner$dim_reduction$model@inverse=function{}
test_red$trained_learner$dim_reduction$model@inverse=function(){}
object.size(test_red$trained_learner$dim_reduction$model@inverse)
object.size(test_red$trained_learner$dim_reduction)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
devtools::load_all()
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
table(training_data$basic_web,pred_test$basic_web)
devtools::build()

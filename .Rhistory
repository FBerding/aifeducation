num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
expect_no_error(
create_roberta_model(
model_dir=testthat::test_path("test_data/roberta"),
vocab_raw_texts=example_data$text,
vocab_size=30522,
add_prefix_space=TRUE,
max_position_embeddings=512,
hidden_size=768,
num_hidden_layer=12,
num_attention_heads=12,
intermediate_size=3072,
hidden_act="gelu",
hidden_dropout_prob=0.1,
trace=FALSE))
})
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:25],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=512,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:25],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=510,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
test_embeddin$
test_embedding$transformer_components$model$config$max_length
test_embedding$transformer_components$model$config$max_position_embeddings
devtools::load_all()
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:25],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=512,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:25],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=510,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:25],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=100,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
devtools::build_site()
model_dir="Trial"
vocab_raw_texts=NULL
vocab_size=30522
add_prefix_space=FALSE
max_position_embeddings=512
hidden_size=768
num_hidden_layer=12
num_attention_heads=12
intermediate_size=3072
hidden_act="gelu"
hidden_dropout_prob=0.1
trace=TRUE
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
example_data$label[c(1:500,1001:1750)]=NA
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id
vocab_raw_texts=example_data$text
#argument checking-----------------------------------------------------------
if(max_position_embeddings>512){
warning("Due to a quadratic increase in memory requirments it is not
recommended to set max_position_embeddings above 512.
If you want to analyse long documents please split your document
into several chunks with an object of class TextEmbedding Model or
use another transformer (e.g. longformer).")
}
#Creating a new Tokenizer for Computing Vocabulary
tok_new<-tok$ByteLevelBPETokenizer()
devtools::load_all()
#Creating a new Tokenizer for Computing Vocabulary
tok_new<-tok$ByteLevelBPETokenizer()
tok_new$enable_truncation(max_length = as.integer(max_position_embeddings))
tok_new$enable_padding(pad_token = "<pad>")
#Calculating Vocabulary
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary"))
}
tok_new$train_from_iterator(
iterator = vocab_raw_texts,
vocab_size = as.integer(vocab_size),
special_tokens=c("<s>","<pad>","</s>","<unk>","<mask>"))
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary - Done"))
}
if(dir.exists(model_dir)==FALSE){
print(paste(date(),"Creating Model Directory"))
dir.create(model_dir)
}
#Saving files
tok_new$save_model(model_dir)
if(trace==TRUE){
print(paste(date(),
"Creating Tokenizer"))
}
tokenizer=transformers$RobertaTokenizerFast(vocab_file = paste0(model_dir,"/","vocab.json"),
merges_file = paste0(model_dir,"/","merges.txt"),
bos_token = "<s>",
eos_token = "</s>",
sep_token = "</s>",
cls_token = "<s>",
unk_token = "<unk>",
pad_token = "<pad>",
mask_token = "<mask>",
add_prefix_space = add_prefix_space)
if(trace==TRUE){
print(paste(date(),
"Creating Tokenizer - Done"))
}
configuration=transformers$RobertaConfig(
vocab_size=as.integer(vocab_size),
max_position_embeddings=as.integer(max_position_embeddings),
hidden_size=as.integer(hidden_size),
num_hidden_layer=as.integer(num_hidden_layer),
num_attention_heads=as.integer(num_attention_heads),
intermediate_size=as.integer(intermediate_size),
hidden_act=hidden_act,
hidden_dropout_prob=hidden_dropout_prob
)
roberta_model=transformers$TFRobertaModel(configuration)
if(trace==TRUE){
print(paste(date(),
"Saving Roberta Model"))
}
roberta_model$save_pretrained(model_dir)
if(trace==TRUE){
print(paste(date(),
"Saving Tokenizer Model"))
}
tokenizer$save_pretrained(model_dir)
if(trace==TRUE){
print(paste(date(),
"Done"))
}
model_dir_path="Trial"
mlm_model=transformes$TFRobertaForMaskedLM$from_pretrained(model_dir_path)
mlm_model=transformers$TFRobertaForMaskedLM$from_pretrained(model_dir_path)
devtools::document()
devtools::check()
devtools::test()
setwd("~/aifeducation/tests/testthat")
classifier<-TextEmbeddingClassifierNeuralNet$new(
name="movie_review_classifier",
label="Classifier for Estimating a Postive or Negative Rating of Movie Reviews",
text_embeddings=current_embeddings,
targets=example_targets,
hidden=NULL,
rec=c(28,28),
dropout=0.2,
recurrent_dropout=0.4,
l2_regularizer=0.01,
optimizer="adam",
act_fct="gelu",
rec_act_fct="tanh")
path="test_data/bert_embeddings.rda"
load(path)
current_embeddings<-bert_embeddings$clone(deep = TRUE)
classifier<-TextEmbeddingClassifierNeuralNet$new(
name="movie_review_classifier",
label="Classifier for Estimating a Postive or Negative Rating of Movie Reviews",
text_embeddings=current_embeddings,
targets=example_targets,
hidden=NULL,
rec=c(28,28),
dropout=0.2,
recurrent_dropout=0.4,
l2_regularizer=0.01,
optimizer="adam",
act_fct="gelu",
rec_act_fct="tanh")
test_that("creation_classifier_neural_net", {
expect_s3_class(classifier,
class="TextEmbeddingClassifierNeuralNet")
})
test_that("training_baseline_only", {
expect_no_error(
classifier$train(
data_embeddings = current_embeddings,
data_targets = example_targets,
data_n_test_samples=2,
use_baseline=TRUE,
bsl_val_size=0.25,
use_bsc=FALSE,
bsc_methods=c("dbsmote"),
bsc_max_k=10,
bsc_val_size=0.25,
use_bpl=FALSE,
bpl_max_steps=2,
bpl_epochs_per_step=1,
bpl_dynamic_inc=FALSE,
bpl_balance=TRUE,
bpl_max=1.00,
bpl_anchor=1.00,
bpl_min=0.00,
bpl_weight_inc=0.02,
bpl_weight_start=0.00,
bpl_model_reset=FALSE,
epochs=2,
batch_size=32,
dir_checkpoint=testthat::test_path("test_data/tmp"),
trace=FALSE,
view_metrics=FALSE,
keras_trace=0,
n_cores=1)
)
})
setwd("~/aifeducation")
test_that("training_baseline_only", {
expect_no_error(
classifier$train(
data_embeddings = current_embeddings,
data_targets = example_targets,
data_n_test_samples=2,
use_baseline=TRUE,
bsl_val_size=0.25,
use_bsc=FALSE,
bsc_methods=c("dbsmote"),
bsc_max_k=10,
bsc_val_size=0.25,
use_bpl=FALSE,
bpl_max_steps=2,
bpl_epochs_per_step=1,
bpl_dynamic_inc=FALSE,
bpl_balance=TRUE,
bpl_max=1.00,
bpl_anchor=1.00,
bpl_min=0.00,
bpl_weight_inc=0.02,
bpl_weight_start=0.00,
bpl_model_reset=FALSE,
epochs=2,
batch_size=32,
dir_checkpoint=testthat::test_path("test_data/tmp"),
trace=FALSE,
view_metrics=FALSE,
keras_trace=0,
n_cores=1)
)
})
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
tmp_path="test_data/roberta"
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
test_that("train_tune_roberta_model", {
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
expect_no_error(
train_tune_roberta_model(output_dir=testthat::test_path("test_data/roberta"),
model_dir_path=testthat::test_path("test_data/roberta"),
raw_texts= example_data$text[1:5],
p_mask=0.30,
val_size=0.1,
n_epoch=1,
batch_size=1,
chunk_size=100,
n_workers=1,
multi_process=FALSE,
trace=FALSE))
})
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
test_that("creation_roberta", {
expect_s3_class(bert_modeling,
class="TextEmbeddingModel")
})
test_that("embedding_roberta", {
embeddings<-bert_modeling$embed(raw_text = example_data$text[1:10],
doc_id = example_data$id[1:10])
expect_s3_class(embeddings, class="EmbeddedText")
})
test_that("encoding_roberta", {
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
expect_length(encodings,10)
expect_type(encodings,type="list")
})
example_data$text[1:10]
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
expect_length(encodings,10)
encodings
devtools::load_all()
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
test_that("encoding_roberta", {
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
expect_length(encodings,10)
expect_type(encodings,type="list")
})
devtools::load_all()
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
test_that("creation_roberta", {
expect_s3_class(bert_modeling,
class="TextEmbeddingModel")
})
test_that("embedding_roberta", {
embeddings<-bert_modeling$embed(raw_text = example_data$text[1:10],
doc_id = example_data$id[1:10])
expect_s3_class(embeddings, class="EmbeddedText")
})
devtools::load_all()
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
test_that("creation_roberta", {
expect_s3_class(bert_modeling,
class="TextEmbeddingModel")
})
test_that("embedding_roberta", {
embeddings<-bert_modeling$embed(raw_text = example_data$text[1:10],
doc_id = example_data$id[1:10])
expect_s3_class(embeddings, class="EmbeddedText")
})
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
encodings
decodings<-bert_modeling$decode(encodings)
decodings
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
decodings<-bert_modeling$decode(encodings)
decodings
devtools::load_all()
#-------------------------------------------------------------------------------
bert_modeling<-TextEmbeddingModel$new(
model_name="roberta_embedding",
model_label="Text Embedding via RoBERTa",
model_version="0.0.1",
model_language="english",
method = "roberta",
max_length = 256,
chunks=4,
overlap=40,
aggregation="last",
model_dir=testthat::test_path(tmp_path))
encodings<-bert_modeling$encode(raw_text = example_data$text[1:10],
token_encodings_only = TRUE)
decodings<-bert_modeling$decode(encodings)
decodings
devtools::test()
gc()
gc()
devtools::build_site()
devtools::document()
devtools::build_site()
usethis::use_github_action_check_standard()
devtools::build_readme()
devtools::document()
devtools::check()
devtools::check()
devtools::check()
devtools::test()
devtools::check()
devtools::test()
devtools::document()
devtools::check()
reticulate::py_config()

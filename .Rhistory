rec=c(64),
self_attention_heads = 2,
dropout=0.5,
l2_regularizer=0.01,
recurrent_dropout=0.4,
optimizer="adam",
act_fct="gelu",
rec_act_fct="tanh")
#-------------------------------------------------------------------------------
devtools::load_all()
test_classifier2<-TextEmbeddingClassifierNeuralNet$new(
name="Test",
label="abc",
#text_embeddings=bert_embeddings,
text_embeddings=embeddings_edda_base,
#targets=example_targets,
targets=debug_targets,
hidden=NULL,
rec=c(64),
self_attention_heads = 2,
dropout=0.5,
l2_regularizer=0.01,
recurrent_dropout=0.4,
optimizer="adam",
act_fct="gelu",
rec_act_fct="tanh")
test<-bundle::unbundle(test_classifier2$bundeled_model)
summary(test)
times=5
features=768
config=list(rec=c(128,64),
dropout=0.4,
recurrent_dropout=0.4,
rec_act="tanh")
config["act_fct_last"]="sigmoid"
config["err_fct"]="binary_crossentropy"
config["metric"]="binary_accuracy"
n_rec=length(config$rec)
#Input Model--------------------------------------------------------------------
if(n_rec>0){
layer_input_ref<-keras::layer_input(shape=list(times,features),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=list(times,features),
name="input_embeddings_obs")
layer_masking_ref<-keras::layer_masking(object=layer_input_ref,
mask_value = 0.0,
name="masking_layer_ref",
input_shape=c(times,features),
trainable=FALSE)
layer_masking_obs<-keras::layer_masking(object=layer_input_obs,
mask_value = 0.0,
name="masking_layer_obs",
input_shape=c(times,features),
trainable=FALSE)
} else {
layer_input_ref<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_obs")
}
#Core Model--------------------------------------------------------------------
core_model=keras::keras_model_sequential(name="core_model")
for(i in 1:n_rec){
if(i<n_rec){
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = TRUE),
name=paste0("bidirectional_",i))
} else {
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = FALSE),
name=paste0("bidirectional_",i))
}
}
tf
devtools::load_all()
return (tf$math$sqrt(sum_square))
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[0]
y=input[1]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
times=5
features=768
config=list(rec=c(128,64),
dropout=0.4,
recurrent_dropout=0.4,
rec_act="tanh")
config["act_fct_last"]="sigmoid"
config["err_fct"]="binary_crossentropy"
config["metric"]="binary_accuracy"
n_rec=length(config$rec)
#Input Model--------------------------------------------------------------------
if(n_rec>0){
layer_input_ref<-keras::layer_input(shape=list(times,features),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=list(times,features),
name="input_embeddings_obs")
layer_masking_ref<-keras::layer_masking(object=layer_input_ref,
mask_value = 0.0,
name="masking_layer_ref",
input_shape=c(times,features),
trainable=FALSE)
layer_masking_obs<-keras::layer_masking(object=layer_input_obs,
mask_value = 0.0,
name="masking_layer_obs",
input_shape=c(times,features),
trainable=FALSE)
} else {
layer_input_ref<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_obs")
}
#Core Model--------------------------------------------------------------------
core_model=keras::keras_model_sequential(name="core_model")
for(i in 1:n_rec){
if(i<n_rec){
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = TRUE),
name=paste0("bidirectional_",i))
} else {
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = FALSE),
name=paste0("bidirectional_",i))
}
}
#Linking Core Model and Input---------------------------------------------------
if(n_rec>0){
model_ref=core_model(layer_masking_ref)
model_obs=core_model(layer_masking_obs)
} else {
model_ref=core_model(layer_input_ref)
model_obs=core_model(layer_input_obs)
}
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[0]
y=input[1]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(model_ref,model_obs)
y=input[[1]]
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[[0]]
y=input[[1]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(model_ref,model_obs)
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[[0]]
y=input[[1]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(model_ref,model_obs)
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[0]
y=input[1]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(model_ref,model_obs)
distance =layer_euclidean_distance(list(model_ref,model_obs))
y=input[[1]]
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[[0]]
y=input[[1]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(list(model_ref,model_obs))
distance =layer_euclidean_distance(x=list(model_ref,model_obs))
test=keras::layer_add(x=list(model_ref,model_obs))
test=keras::layer_add(inputs=list(model_ref,model_obs))
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = test,
name = "Siamese Network")
siamese_net
distance =layer_euclidean_distance(input=list(model_ref,model_obs))
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[[0]]
y=input[[1]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(object=list(model_ref,model_obs))
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[[0L]]
y=input[[1L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(object=list(model_ref,model_obs))
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
f=function(input){
x=input[0L]
y=input[1L]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
distance =layer_euclidean_distance(object=list(model_ref,model_obs))
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[0L]
y=input[1L]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[0L]]
y=input[[1L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[1L]]
y=input[[2L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
})
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = layer_euclidean_distance,
name = "Siamese Network")
siamese_net
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[1L]]
y=input[[2L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
},
name = "euclidean_distance")
normal_layer=keras::layer_batch_normalization(layer_euclidean_distance)
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[1L]]
y=input[[2L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
},
name = "euclidean_distance")
normal_layer=keras::layer_batch_normalization(layer_euclidean_distance)
output_layer=keras::layer_dense(
object = normal_layer,
units = 1L,
activation = "sigmoid")
#Bulding Model-----------------------------------------------------------------
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = layer_euclidean_distance,
name = "Siamese Network")
siamese_net
#Bulding Model-----------------------------------------------------------------
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = output_layer,
name = "Siamese Network")
siamese_net
times=5
features=768
config=list(rec=c(128,64),
dropout=0.4,
recurrent_dropout=0.4,
rec_act="tanh")
config["act_fct_last"]="sigmoid"
config["err_fct"]="binary_crossentropy"
config["metric"]="binary_accuracy"
n_rec=length(config$rec)
#Input Model--------------------------------------------------------------------
if(n_rec>0){
layer_input_ref<-keras::layer_input(shape=list(times,features),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=list(times,features),
name="input_embeddings_obs")
layer_masking_ref<-keras::layer_masking(object=layer_input_ref,
mask_value = 0.0,
name="masking_layer_ref",
input_shape=c(times,features),
trainable=FALSE)
layer_masking_obs<-keras::layer_masking(object=layer_input_obs,
mask_value = 0.0,
name="masking_layer_obs",
input_shape=c(times,features),
trainable=FALSE)
} else {
layer_input_ref<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_obs")
}
#Core Model--------------------------------------------------------------------
core_model=keras::keras_model_sequential(name="core_model")
for(i in 1:n_rec){
if(i<n_rec){
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = TRUE),
name=paste0("bidirectional_",i))
} else {
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = FALSE),
name=paste0("bidirectional_",i))
}
}
#Linking Core Model and Input---------------------------------------------------
if(n_rec>0){
model_ref=core_model(layer_masking_ref)
model_obs=core_model(layer_masking_obs)
} else {
model_ref=core_model(layer_input_ref)
model_obs=core_model(layer_input_obs)
}
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[1L]]
y=input[[2L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
},
name = "euclidean_distance")
#Adding Final Layers-----------------------------------------------------------
normal_layer=keras::layer_batch_normalization(
object=layer_euclidean_distance,
name="normalization")
output_layer=keras::layer_dense(
object = normal_layer,
units = 1L,
activation = "sigmoid")
#Building Model-----------------------------------------------------------------
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = output_layer,
name = "Siamese Network")
siamese_net
times=5
features=768
config=list(rec=c(128,64),
dropout=0.4,
recurrent_dropout=0.4,
rec_act="tanh")
config["act_fct_last"]="sigmoid"
config["err_fct"]="binary_crossentropy"
config["metric"]="binary_accuracy"
n_rec=length(config$rec)
#Input Model--------------------------------------------------------------------
if(n_rec>0){
layer_input_ref<-keras::layer_input(shape=list(times,features),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=list(times,features),
name="input_embeddings_obs")
layer_masking_ref<-keras::layer_masking(object=layer_input_ref,
mask_value = 0.0,
name="masking_layer_ref",
input_shape=c(times,features),
trainable=FALSE)
layer_masking_obs<-keras::layer_masking(object=layer_input_obs,
mask_value = 0.0,
name="masking_layer_obs",
input_shape=c(times,features),
trainable=FALSE)
} else {
layer_input_ref<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_ref")
layer_input_obs<-keras::layer_input(shape=ncol(text_embeddings$embeddings),
name="input_embeddings_obs")
}
#Core Model--------------------------------------------------------------------
core_model=keras::keras_model_sequential(name="core_model")
for(i in 1:n_rec){
if(i<n_rec){
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = TRUE),
name=paste0("bidirectional_",i))
} else {
keras::bidirectional(
object = core_model,
layer=keras::layer_gru(
units=config$rec[i],
input_shape=list(times,features),
dropout = config$dropout,
recurrent_dropout = config$recurrent_dropout,
activation = config$rec_act_fct,
name=paste0("gru_",i),
return_sequences = FALSE),
name=paste0("bidirectional_",i))
}
}
#Linking Core Model and Input---------------------------------------------------
if(n_rec>0){
model_ref=core_model(layer_masking_ref)
model_obs=core_model(layer_masking_obs)
} else {
model_ref=core_model(layer_input_ref)
model_obs=core_model(layer_input_obs)
}
#Adding Similarity Measures----------------------------------------------------
layer_euclidean_distance = keras::layer_lambda(
object=list(model_ref,model_obs),
f=function(input){
x=input[[1L]]
y=input[[2L]]
sum_square = tf$math$reduce_sum(tf$math$square(x - y), axis=1L, keepdims=TRUE)
return (tf$math$sqrt(sum_square))
},
name = "euclidean_distance")
#Adding Final Layers-----------------------------------------------------------
normal_layer=keras::layer_batch_normalization(
object=layer_euclidean_distance,
name="normalization")
output_layer=keras::layer_dense(
object = normal_layer,
units = 1L,
activation = "tanh")
#Building Model-----------------------------------------------------------------
siamese_net<-keras::keras_model(
inputs = list(layer_input_ref,layer_input_obs),
outputs = output_layer,
name = "Siamese Network")
siamese_net %>% keras::compile(
loss = config$err_fct,
optimizer=tf$keras$optimizers$legacy$Adam(),
metric=config$metric)

use_multiprocessing=multi_process,
callbacks=list(callback_checkpoint))
tokenized_dataset
tf_train_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$train,
batch_size = as.integer(batch_size),
collate_fn = data_collator,
shuffle = TRUE
)
data_collator=transformer$DataCollatorForLanguageModeling(
tokenizer = tokenizer,
mlm = TRUE,
mlm_probability = p_mask
)
tf_train_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$train,
batch_size = as.integer(batch_size),
collate_fn = data_collator,
shuffle = TRUE
)
tf_test_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$test,
batch_size = as.integer(batch_size),
#collate_fn = data_collator,
shuffle = TRUE
)
print(paste(date(),"Preparing Training of the Model"))
adam<-tf$keras$optimizers$Adam
if(dir.exists(paste0(output_dir,"/checkpoints"))==FALSE){
print(paste(date(),"Creating Checkpoint Directory"))
dir.create(paste0(output_dir,"/checkpoints"))
}
callback_checkpoint=tf$keras$callbacks$ModelCheckpoint(
filepath = paste0(output_dir,"/checkpoints/"),
monitor="val_loss",
verbose=1L,
mode="auto",
save_best_only=TRUE,
save_freq="epoch",
save_weights_only= TRUE
)
print(paste(date(),"Compile Model"))
mlm_model$compile(optimizer=adam(3e-5))
#Clear session to provide enough resources for computations
tf$keras$backend$clear_session()
print(paste(date(),"Start Fine Tuning"))
mlm_model$fit(x=tf_train_dataset,
validation_data=tf_test_dataset,
epochs=as.integer(n_epoch),
workers=as.integer(n_workers),
use_multiprocessing=multi_process,
callbacks=list(callback_checkpoint))
model_dir="Trial/roberta"
#-------------------------------------------------------------------------------
devtools::load_all()
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
example_data$label[c(1:500,1001:1750)]=NA
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id
table(example_targets)
vocab_raw_texts=example_data$text
vocab_size=30522
add_prefix_space=FALSE
max_position_embeddings=512
hidden_size=768
num_hidden_layer=12
num_attention_heads=12
intermediate_size=3072
hidden_act="gelu"
hidden_dropout_prob=0.1
trace=TRUE
transformers<-reticulate::import("transformers")
datasets<-reticulate::import("datasets")
tok<-reticulate::import("tokenizers")
#Creating a new Tokenizer for Computing Vocabulary
tok_new<-tok$ByteLevelBPETokenizer()
#Calculating Vocabulary
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary"))
}
tok_new$enable_truncation(max_length = as.integer(max_position_embeddings))
tok_new$enable_padding(pad_token = "<pad>")
#Creating a new Tokenizer for Computing Vocabulary
tok_new<-tok$ByteLevelBPETokenizer()
tok_new$enable_truncation(max_length = as.integer(max_position_embeddings))
tok_new$enable_padding(pad_token = "<pad>")
#Calculating Vocabulary
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary"))
}
tok_new$train_from_iterator(
iterator = vocab_raw_texts,
vocab_size = as.integer(vocab_size),
special_tokens=c("<s>","<pad>","</s>","<unk>","<mask>"))
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary - Done"))
}
if(dir.exists(model_dir)==FALSE){
print(paste(date(),"Creating Model Directory"))
dir.create(model_dir)
}
#Saving files
tok_new$save_model(model_dir)
if(trace==TRUE){
print(paste(date(),
"Creating Tokenizer"))
}
tokenizer=transformers$RobertaTokenizerFast(vocab_file = paste0(model_dir,"/","vocab.json"),
merges_file = paste0(model_dir,"/","merges.txt"),
bos_token = "<s>",
eos_token = "</s>",
sep_token = "</s>",
cls_token = "<s>",
unk_token = "<unk>",
pad_token = "<pad>",
mask_token = "<mask>",
add_prefix_space = add_prefix_space)
if(trace==TRUE){
print(paste(date(),
"Creating Tokenizer - Done"))
}
configuration=transformers$RobertaConfig(
vocab_size=as.integer(vocab_size),
max_position_embeddings=as.integer(max_position_embeddings),
hidden_size=as.integer(hidden_size),
num_hidden_layer=as.integer(num_hidden_layer),
num_attention_heads=as.integer(num_attention_heads),
intermediate_size=as.integer(intermediate_size),
hidden_act=hidden_act,
hidden_dropout_prob=hidden_dropout_prob
)
roberta_model=transformers$TFRobertaModel(configuration)
if(trace==TRUE){
print(paste(date(),
"Saving Roberta Model"))
}
roberta_model$save_pretrained(model_dir)
if(trace==TRUE){
print(paste(date(),
"Saving Tokenizer Model"))
}
tokenizer$save_pretrained(model_dir)
if(trace==TRUE){
print(paste(date(),
"Done"))
}
output_dir="Trial/roberts/training"
model_dir_path="Trial/roberta"
raw_texts=example_data$text
aug_vocab_by=0
p_mask=0.15
val_size=0.1
n_epoch=1
batch_size=12
chunk_size=250
n_workers=1
multi_process=FALSE
trace=TRUE
transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
datasets=reticulate::import("datasets")
tok<-reticulate::import("tokenizers")
mlm_model=transformer$TFRobertaForMaskedLM$from_pretrained(model_dir_path)
tokenizer<-transformer$RobertaTokenizerFast$from_pretrained(model_dir_path)
print(paste(date(),"Tokenize Raw Texts"))
prepared_texts<-quanteda::tokens(
x = raw_texts,
what = "word",
remove_punct = FALSE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE,
split_tags = FALSE,
include_docvars = TRUE,
padding = FALSE,
verbose = trace)
if(aug_vocab_by>0){
print(paste(date(),"Augmenting vocabulary"))
#Creating a new Tokenizer for Computing Vocabulary
vocab_size_old=length(tokenizer$get_vocab())
tok_new<-tok$Tokenizer(tok$models$WordPiece())
tok_new$normalizer=tok$normalizers$BertNormalizer(lowercase= tokenizer$do_lower_case)
tok_new$pre_tokenizer=tok$pre_tokenizers$BertPreTokenizer()
tok_new$decode=tok$decoders$WordPiece()
trainer<-tok$trainers$WordPieceTrainer(
vocab_size=as.integer(length(tokenizer$get_vocab())+aug_vocab_by),
show_progress=trace)
#Calculating Vocabulary
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary"))
}
tok_new$train_from_iterator(raw_texts,trainer=trainer)
new_tokens=names(tok_new$get_vocab())
if(trace==TRUE){
print(paste(date(),
"Start Computing Vocabulary - Done"))
}
invisible(tokenizer$add_tokens(new_tokens = new_tokens))
invisible(mlm_model$resize_token_embeddings(length(tokenizer)))
print(paste(date(),"Adding",length(tokenizer$get_vocab())-vocab_size_old,"New Tokens"))
}
print(paste(date(),"Creating Text Chunks"))
prepared_texts_chunks<-quanteda::tokens_chunk(
x=prepared_texts,
size=chunk_size,
overlap = 0,
use_docvars = FALSE)
check_chunks_length=(quanteda::ntoken(prepared_texts_chunks)==chunk_size)
prepared_texts_chunks<-quanteda::tokens_subset(
x=prepared_texts_chunks,
subset = check_chunks_length
)
prepared_text_chunks_strings<-lapply(prepared_texts_chunks,paste,collapse = " ")
prepared_text_chunks_strings<-as.character(prepared_text_chunks_strings)
print(paste(date(),length(prepared_text_chunks_strings),"Chunks Created"))
print(paste(date(),"Creating Input"))
tokenized_texts= tokenizer(prepared_text_chunks_strings,
truncation =TRUE,
padding= TRUE,
max_length=as.integer(chunk_size),
return_tensors="np")
print(paste(date(),"Creating TensorFlow Dataset"))
tokenized_dataset=datasets$Dataset$from_dict(tokenized_texts)
whole_word
print(paste(date(),"Using Token Masking"))
data_collator=transformer$DataCollatorForLanguageModeling(
tokenizer = tokenizer,
mlm = TRUE,
mlm_probability = p_mask)
tokenized_dataset=tokenized_dataset$train_test_split(test_size=val_size)
tf_train_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$train,
batch_size = as.integer(batch_size),
collate_fn = data_collator,
shuffle = TRUE
)
tf_test_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$test,
batch_size = as.integer(batch_size),
#collate_fn = data_collator,
shuffle = TRUE
)
tf_test_dataset=mlm_model$prepare_tf_dataset(
dataset = tokenized_dataset$test,
batch_size = as.integer(batch_size),
collate_fn = data_collator,
shuffle = TRUE
)
print(paste(date(),"Preparing Training of the Model"))
adam<-tf$keras$optimizers$Adam
if(dir.exists(paste0(output_dir,"/checkpoints"))==FALSE){
print(paste(date(),"Creating Checkpoint Directory"))
dir.create(paste0(output_dir,"/checkpoints"))
}
output_dir
output_dir="Trial/roberta/training"
if(dir.exists(paste0(output_dir,"/checkpoints"))==FALSE){
print(paste(date(),"Creating Checkpoint Directory"))
dir.create(paste0(output_dir,"/checkpoints"))
}
callback_checkpoint=tf$keras$callbacks$ModelCheckpoint(
filepath = paste0(output_dir,"/checkpoints/"),
monitor="val_loss",
verbose=1L,
mode="auto",
save_best_only=TRUE,
save_freq="epoch",
save_weights_only= TRUE
)
print(paste(date(),"Compile Model"))
mlm_model$compile(optimizer=adam(3e-5))
#Clear session to provide enough resources for computations
tf$keras$backend$clear_session()
print(paste(date(),"Start Fine Tuning"))
mlm_model$fit(x=tf_train_dataset,
validation_data=tf_test_dataset,
epochs=as.integer(n_epoch),
workers=as.integer(n_workers),
use_multiprocessing=multi_process,
callbacks=list(callback_checkpoint))
devtools::check()
devtools::test()
devtools::test()
devtools::load_all()
devtools::test()
setwd("~/aifeducation/tests/testthat")
tmp_path="test_data/language_models/bert-base-uncased"
bert_modeling<-TextEmbeddingModel$new(
model_name="bert_embedding",
model_label="Text Embedding via BERT",
model_version="0.0.1",
model_language="english",
method = "bert",
max_length = 512,
chunks=15,
overlap=4,
aggregation="last",
use_cls_token=TRUE,
model_dir=(tmp_path))
#-------------------------------------------------------------------------------
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
bert_modeling<-TextEmbeddingModel$new(
model_name="bert_embedding",
model_label="Text Embedding via BERT",
model_version="0.0.1",
model_language="english",
method = "bert",
max_length = 512,
chunks=4,
overlap=40,
aggregation="last",
use_cls_token=TRUE,
model_dir=(tmp_path))
embeddings<-bert_modeling$embed(raw_text = example_data$text,
doc_id = example_data$id)
aifeducation::set_config_gpu_low_memory()
embeddings<-bert_modeling$embed(raw_text = example_data$text,
doc_id = example_data$id)
model_dir="Trial/Bert_Modelle"
transformer = reticulate::import('transformers')
self$transformer_components$tokenizer<-transformer$BertTokenizerFast$from_pretrained(model_dir)
self$transformer_components$model<-transformer$TFBertForMaskedLM$from_pretrained(model_dir)
model_dir="/Trial/Bert_Modelle"
self$transformer_components$tokenizer<-transformer$BertTokenizerFast$from_pretrained(model_dir)
tokenizer<-transformer$BertTokenizerFast$from_pretrained(model_dir)
setwd("~/aifeducation")
tokenizer<-transformer$BertTokenizerFast$from_pretrained(model_dir)
model_dir"Trial/Bert_Modelle"
model_dir="Trial/Bert_Modelle"
tokenizer<-transformer$BertTokenizerFast$from_pretrained(model_dir)
model<-transformer$TFBertForMaskedLM$from_pretrained(model_dir)
#-------------------------------------------------------------------------------
devtools::load_all()
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
example_data$label[c(1:500,1001:1750)]=NA
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id
table(example_targets)
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
tokens<-test_embedding$encode(raw_text = example_data$text[1:40],
token_encodings_only = FALSE,
trace = TRUE)
tokens
modelU(tokens$encodings)
model(tokens$encodings)
test_set<-model$prepare_tf_dataset(tokens$encodings)
datasets<-reticulate::import("datasets")
test<-datasets$Dataset$from_dict(tokens$encodings)
test
test_tf<-model$prepare_tf_dataset(test)
model(test_tf)
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
#------------------------------------------------------------------------------
devtools::load_all()
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
tokens<-test_embedding$encode(raw_text = example_data$text[1:40],
token_encodings_only = FALSE,
batch_size=8,
trace = TRUE)
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
#test_embedding$decode(tokens[1])
test_embeddings_1<-test_embedding$embed(raw_text = example_data$text[1:20],
doc_id = example_data$id[1:20],
batch_size = 8,
trace = TRUE)
#test_embedding$decode(tokens[1])
test_embeddings_1<-test_embedding$embed(raw_text = example_data$text[1:20],
doc_id = example_data$id[1:20],
batch_size = 8,
trace = TRUE)
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
length(example_data$text[1:40])
#test_embedding$decode(tokens[1])
test_embeddings_1<-test_embedding$embed(raw_text = example_data$text[1:20],
doc_id = example_data$id[1:20],
batch_size = 8,
trace = TRUE)
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
#test_embedding$decode(tokens[1])
test_embeddings_1<-test_embedding$embed(raw_text = example_data$text[1:20],
doc_id = example_data$id[1:20],
batch_size = 8,
trace = TRUE)
tokens<-test_embedding$encode(raw_text = example_data$text[1:40],
token_encodings_only = FALSE,
trace = TRUE)
tokens$chunks
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
#------------------------------------------------------------------------------
devtools::load_all()
test_embedding<-TextEmbeddingModel$new(
model_name = "test",
model_label = "test",
model_version = "0.0.1",
model_language = "german",
model_dir="Trial/Bert_Modelle",
method = "bert",
aggregation = "last",
max_length=256,
chunks = 4,
overlap = 10
)
#test_embedding$decode(tokens[1])
test_embeddings_1<-test_embedding$embed(raw_text = example_data$text[1:20],
doc_id = example_data$id[1:20],
batch_size = 8,
trace = TRUE)
devtools::build()

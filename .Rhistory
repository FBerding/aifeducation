preparation_tokens_chunks_masked[i]<-quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
}
preparation_tokens_chunks_masked[[i]]<-quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_masked[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_masked)){
tmp<-preparation_tokens_chunks_masked[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_original)){
tmp<-preparation_tokens_chunks_original[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_original_2<-lapply(preparation_tokens_chunks_original,paste,collapse=" ")
preparation_tokens_chunks_masked_2<-lapply(preparation_tokens_chunks_masked,paste,collapse=" ")
preparation_tokens_chunks_masked_2[1]
preparation_tokens_chunks_original_2[1]
abc<-as.character(preparation_tokens_chunks_2[1:2])
class(abc)
length(abc)
preparation_tokens_chunks_original_2<-as.character(lapply(preparation_tokens_chunks_original,paste,collapse=" "))
length(preparation_tokens_chunks_original_2)
preparation_tokens_chunks_masked_2<-as.character(lapply(preparation_tokens_chunks_masked,paste,collapse=" "))
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2)
tokenized_texts__masked["input_ids"]
tokenized_texts__masked["input_ids"][1]
tokenizer$mask_token_id
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf")
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf",truncation=TRUE)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,return_tensors="tf",truncation =TRUE)
abc<-as.character(preparation_tokens_chunks_2[1:2])
class(abc)
length(abc)
tokenized_texts<-tokenizer(abc)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
max_length=128)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
load("Trial/Training_Data.RData")
library(reticulate)
bert_model_dir_path="Trial/Bert_Modelle"
transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
builtin=reticulate::import("builtin")
tokenizer<-transformer$BertTokenizerFast$from_pretrained(bert_model_dir_path)
model<-transformer$TFBertForMaskedLM$from_pretrained(bert_model_dir_path)
preparation_tokens<-quanteda::tokens(paste(training_data$text[1:50],
collapse = " "),
verbose = TRUE)
preparation_tokens_chunks_original<-quanteda::tokens_chunk(
x=preparation_tokens,
size=128,
overlap = 0,
use_docvars = FALSE)
preparation_tokens_chunks_masked<-NULL
for(i in 1:length(preparation_tokens_chunks_original)){
tmp<-preparation_tokens_chunks_original[[i]]
tmp_size=sample(x=1:min(length(tmp),30),size = 1)
tmp_sample<-sample(x=1:length(tmp),size = floor(tmp_size/100*length(tmp)))
tmp_sample
preparation_tokens_chunks_masked[i]<-list(quanteda::tokens_replace(preparation_tokens_chunks_original[i],
valuetype="fixed",
pattern=tmp[tmp_sample],
replacement=rep("[MASK]",
times=length(tmp_sample)))
)
}
preparation_tokens_chunks_original_2<-as.character(lapply(preparation_tokens_chunks_original,paste,collapse=" "))
preparation_tokens_chunks_masked_2<-as.character(lapply(preparation_tokens_chunks_masked,paste,collapse=" "))
tokenized_texts_orignal=tokenizer(preparation_tokens_chunks_original_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
tokenized_texts__masked=tokenizer(preparation_tokens_chunks_masked_2,
truncation =TRUE,
padding= TRUE,
max_length=128L)
training_args = transformer$TFTrainingArguments(
output_dir='./results',           # output directory
num_train_epochs=1,               # total number of training epochs
per_device_train_batch_size=16L,  # batch size per device during training
per_device_eval_batch_size=128L,  # batch size for evaluation
warmup_steps=100L,                # number of warmup steps for learning rate scheduler
weight_decay=0.01,                # strength of weight decay
logging_dir='./logs',             # directory for storing logs
logging_steps=10L
)
training_args = transformer$TFTrainingArguments(
output_dir='./results',           # output directory
num_train_epochs=1,               # total number of training epochs
per_device_train_batch_size=16L,  # batch size per device during training
per_device_eval_batch_size=128L,  # batch size for evaluation
warmup_steps=100L,                # number of warmup steps for learning rate scheduler
weight_decay=0.01,                # strength of weight decay
logging_dir='./logs',             # directory for storing logs
logging_steps=10L
)
tokenized_texts_orignal
train_labels=tokenized_texts_orignal["token_ids"]
train_labels=tokenized_texts_orignal["input_ids"]
train_dataset = tf$data$Dataset$from_tensor_slices(tuple(builtins$dict(tokenized_texts__masked),train_labels))
builtins$dict(tokenized_texts__masked),train_labels)
builtins$dict(tokenized_texts__masked)
train_dataset = tf$data$Dataset$from_tensor_slices(tuple(tokenized_texts__masked,train_labels))
trainer = transformer$TFTrainer(
model=model_BERTje,                  # the instantiated ðŸ¤— Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer = transformer$TFTrainer(
model=model,                  # the instantiated ðŸ¤— Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer$train()
reticulate::py_last_error()
str(train_dataset)
train_dataset = tf$data$Dataset$from_tensor_slices(tokenized_texts_orignal)
trainer$train()
trainer = transformer$TFTrainer(
model=model,                  # the instantiated ðŸ¤— Transformers model to be trained
args=training_args,                  # training arguments, defined above
train_dataset=train_dataset          # training dataset
)
trainer$train()
model$compile(optimizer=Adam(3e-5))
model$compile(optimizer= transformer$AdamW(3e-5))
model$compile()
model.fit(train_dataset)
model$fit(train_dataset)
tf_dataset = model$prepare_tf_dataset(train_dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)
tf_dataset = model$prepare_tf_dataset(train_dataset, batch_size=16, shuffle=TRUE, tokenizer=tokenizer)
reticulate::py_install('datasets ', pip = TRUE)
devtools::build()
load("Trial/Training_Data.RData")
load("Trial/embeddings_bert_500.RData")
load("Trial/embeddings_german_glove_cluster_96.RData")
devtools::load_all()
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "Iosmap",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "Isomap",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
table(training_data$basic_web,pred_test$basic_web)
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
object.size(test_red$trained_learner$dim_reduction)
object.size(test_red$trained_learner$dim_reduction$model)
object.size(test_red$trained_learner$dim_reduction$model@data)
object.size(test_red$trained_learner$dim_reduction$model@org.data)
object.size(test_red$trained_learner$dim_reduction$model@apply())
object.size(test_red$trained_learner$dim_reduction$model@apply
)
object.size(test_red$trained_learner$dim_reduction$model@inverse)
test_red$trained_learner$dim_reduction$model@has.inverse
test_red$trained_learner$dim_reduction$model@has.inverse<-FALSE
test_red$trained_learner$dim_reduction$model@inverse<-NULL
test_red$trained_learner$dim_reduction$model@inverse=function()
)
test_red$trained_learner$dim_reduction$model@inverse=function
test_red$trained_learner$dim_reduction$model@inverse=function{}
test_red$trained_learner$dim_reduction$model@inverse=function(){}
object.size(test_red$trained_learner$dim_reduction$model@inverse)
object.size(test_red$trained_learner$dim_reduction)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
devtools::load_all()
test_red<-te_classifier$new(
classifier_title=paste("Test_2"),
classifier_version = "0.0.1",
classifier_algorithm="ranger",
learner_name="ranger",
additional_data = NULL,
use_smote = FALSE,
autotuning = FALSE,
smote_K = c(1),
smote_dup = c(10),
text_embeddings=embeddings,
target_data=training_data$basic_web,
use_dim_reduction = TRUE,
dim_red_method = "PCA",
dim_red_n = 30,
category_name = "basic_web",
category_label="basic_web",
normalize_input=FALSE,
normalize_output=FALSE,
n_performance_estimation=2,
ratio_performance_estimation=.66,
tuning_method="random_search",
tune_inner_sampling=mlr3::rsmp("subsampling", ratio= .66, repeats=3),
max_n_tuning=3,
cr_optim=mlr3::msr("classif.measureminiota2"),
#cr_optim=mlr3::msr("classif.measureavgiota2"),
filter_method="jmim",
filter_ratio=1,
verbose=TRUE,
logger_bbotk="warn",
logger_mlr3="warn",
na.rm=TRUE
)
pred_test<-test_red$predict(text_embeddings = embeddings,
verbose = TRUE)
table(pred_test$basic_web)
table(training_data$basic_web,pred_test$basic_web)
devtools::build()
devtools::load_all()
install_py_modules(remove_first = TRUE,cpu_only = TRUE)
reticulate::py_install(packages = "tensorflow-cpu",envname = "aifeducation")
reticulate::py_install(packages = "tensorflow-cpu",envname = "aifeducation",pip = TRUE)
reticulate::use_condaenv("aifeducation")
reticulate::use_condaenv("aifeducation")
devtools::test()
devtools::test_active_file()
testthat::skip_on_cran()
testthat::skip_if_not(condition=check_aif_py_modules(trace = FALSE),
message = "Necessary python modules not available")
if(aifeducation_config$global_framework_set()==FALSE){
aifeducation_config$set_global_ml_backend("tensorflow")
}
aifeducation::set_config_gpu_low_memory()
#transformers$utils$logging$set_verbosity_warning()
transformers$utils$logging$set_verbosity_error()
os$environ$setdefault("TOKENIZERS_PARALLELISM","false")
set_config_tf_logger("ERROR")
set_config_os_environ_logger("ERROR")
transformers$utils$logging$disable_progress_bar()
if(dir.exists(testthat::test_path("test_artefacts"))==FALSE){
dir.create(testthat::test_path("test_artefacts"))
}
if(dir.exists(testthat::test_path("test_artefacts/tmp"))==FALSE){
dir.create(testthat::test_path("test_artefacts/tmp"))
}
ml_frameworks<-c("tensorflow",
"pytorch")
ai_methods=c("bert",
"roberta",
"longformer",
"funnel",
"deberta_v2"
)
ai_framework_matrix<-matrix(
ncol=2,
nrow=length(ai_methods),
data=c(1,1,
1,1,
1,1,
1,1,
1,1),
byrow=TRUE)
colnames(ai_framework_matrix)<-c("tensorflow","pytorch")
rownames(ai_framework_matrix)<-ai_methods
rows_susatainability<-vector(length = length(ai_methods))
names(rows_susatainability)<-ai_methods
rows_susatainability["bert"]=3
rows_susatainability["funnel"]=2
rows_susatainability["roberta"]=2
rows_susatainability["longformer"]=2
rows_susatainability["deberta_v2"]=2
example_data<-data.frame(
id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id1,
label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)
print(check_aif_py_modules())
framework="tensorflow"
ai_method="bert"
base::gc(verbose = FALSE,full = TRUE)
path_01=paste0("test_artefacts/",ai_method)
if(dir.exists(testthat::test_path(path_01))==FALSE){
dir.create(testthat::test_path(path_01))
}
path_02=paste0("test_artefacts/tmp/",ai_method)
path_03=paste0("test_artefacts/tmp_full_models")
if(dir.exists(testthat::test_path(path_02))==FALSE){
dir.create(testthat::test_path(path_02))
}
if(dir.exists(testthat::test_path(path_03))==FALSE){
dir.create(testthat::test_path(path_03))
}
if(dir.exists(testthat::test_path(paste0(path_03,"/tensorflow")))==FALSE){
dir.create(testthat::test_path(paste0(path_03,"/tensorflow")))
}
if(dir.exists(testthat::test_path(paste0(path_03,"/pytorch")))==FALSE){
dir.create(testthat::test_path(paste0(path_03,"/pytorch")))
}
expect_no_error(
train_tune_bert_model(ml_framework = framework,
output_dir=testthat::test_path(paste0(path_01,"/",framework)),
model_dir_path=testthat::test_path(paste0(path_01,"/",framework)),
raw_texts= example_data$text[1:10],
p_mask=0.15,
whole_word=TRUE,
full_sequences_only = TRUE,
val_size=0.25,
n_epoch=2,
batch_size=2,
chunk_size=100,
n_workers=1,
multi_process=FALSE,
sustain_track=TRUE,
sustain_iso_code = "DEU",
sustain_region = NULL,
sustain_interval = 15,
trace=FALSE,
keras_trace = 0))
devtools::load_all()
devtools::test()
devtools::check_win_devel()
reticulate::use_condaenv("aifeducation")
devtools::load_all()
start_aifeducation_studio()
start_aifeducation_studio()
reticulate::use_condaenv("aifeducation")
devtools::load_all()
start_aifeducation_studio()
devtools::load_all()
start_aifeducation_studio()
reticulate::use_condaenv("aifeducation")
devtools::load_all()
start_aifeducation_studio()
reticulate::use_condaenv("aifeducation")

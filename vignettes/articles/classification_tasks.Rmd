---
title: "02 Classification Tasks"
author: "Florian Berding, Julia Pargmann, Andreas Slopinski, Elisabeth Riebenbauer, Karin Rebmann"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{02 Classification Tasks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

load("data/test_classifier.rda")
load("data/bert_embeddings.rda")


classifier<-test_classifier
```

# 1 Introduction and Overview
In educational and social science assigning an observation to scientific concepts
is an important task allowing to understand an observation, to generate new
insights, and to derive recommendations for research and practice. 

In educational science several areas deal with this kind of task. For example, diagnosing
students characteristics is an important aspect of teachers' profession for understanding
and promoting learning. Another example is learning analytics where data about
students is used to provide learning environments adapted to their 
individual needs. On an other level institutions such as schools and universities
can use this information for a data driven decision about their performance
(Laurusson & White 2014) and where and how to improve it. In any case a real world
observations is aligned into scientific models in order to use scientific knowledge
as a technology for improving learning and instruction.

Supervised machine learning is one concept allowing to link real world observations
on the one hand and existing scientific models and theories on the other hand 
(Berding et al. 2022). For
educational sciences this is a great advantage because it allows to use the existing
knowledge and insights for applications of AI. The drawback of this approach is that
the training of AI requires both information about the real world observation on the one hand
and information on the corresponding alignment in scientific models and theories on the other hand.

A valuable source of data in educational science are texts since textual data
can be found everywhere in learning and teaching (Berding et al. 2022). For example, 
teachers often demand students to solve a task which they provide in a written form. 
Students have to create a solution for the tasks which they often document with
a short written essay or a presentation. These data can be used for analyzing learning
and teaching. Teachers' written tasks for their students may provide insights into the quality
of instruction. Students' solutions may provide insights into their learning
outcomes and prerequisites. 

AI can be a helpful assistant in analyzing textual data since the analysis of 
textual data is a challenging and time consuming task for humans because they
have to conduct a content analysis. In this vignette we would like to show you how 
to create an AI that can help
you in such tasks by using the package *aifedcuation*. 

**Please note that an 
introduction in content analysis, natural language processing or machine learning
is behind the scope of this vignette. If you would like to go into details please
refer to the cited literature.**

Before we start we have to introduce a definition of our understanding of basic concepts since applying
AI to educational contexts means to combine the knowledge of different scientific disciplines
using different, sometimes overlapping concepts. Even within a research area
concepts are not unified used. Figure 1 illustrates package's understanding.

![Figure 1: Understanding of Central Concepts](classif_fig_01.png){width=100%}

Since *aifeducation* looks at the application of AI for classification tasks
from the perspective of the empirical method of content analysis there is some 
overlapping between concepts of content analysis and machine learning. In content
analysis phenomenon like performance and colors can be described as a scale/dimension which
is made up by several categories (e.g. Schreier 2012 pp. 59). In our example an exam's performance (scale/dimension) 
could be "good", "average" or "poor". In terms of colors (scale/dimension) categories could be "blue",
"green" etc. Machine learning literature uses other words to describe this kind of data.
In machine learning "scale" and "dimension" corresponds to the term "label" and "categories" refer to 
the term "classes" (Chollet, Kalinowski & Allaire 2022, p. 114).

With these clarifications classification means that a text is assigned to the correct
category of a scale or that the text is labeled with the correct class. To train
an AI to classify a text accordingly based on supervised machine learning 
two kind of data are necessary as Figure 2
illustrates. 

![Figure 2: Basic Structure of Supervised Machine Learning](classif_fig_02.png){width=100%}

By providing AI with both the textual data as input data and the
corresponding information about the class as target data AI can learn
which texts imply a specific class or category. In the example of exams AI can learn
which texts imply a "good", "average" or "poor" judgment. After training
AI can be applied to new texts and predicts the most likely class of every new text.
The generated class can be used for further statistical analysis or
for deriving recommendations about learning and teaching.

To achieve this support by an artificial intelligence several steps are necessary.
Figure 3 provides an overview integrating the functions and objects of *aifeducation*. 

![Figure 3: Overview of the Steps to Perform a Classification](classif_fig_03.png){width=100%}

The first step is to transform raw texts into a form computers can use. That is,
the raw texts must be transformed into numbers. In modern approaches this is done 
by using word embeddings. Campesato (2021, p. 102) describes them as "the collective name for
a set of language modeling and feature learning techniques (...) where words
or phrases from the vocabulary are mapped to vectors of real numbers." Similar
is the definition of word vector: „Word vectors represent the semantic meaning 
of words as vectors in the context of the training corpus.“ (Lane, Howard & Hapke 2019, p. 191)

Campesato (2021, pp. 112) clusters approaches for creating word embeddings into
three groups reflecting their ability to provide context sensitive numerical
representations. Approaches of group one do not account for any context. Typical
methods rely on bag-of-words assumptions. Thus, they are normally not able to
provide a word embedding for single words. Group two consists of approaches such
as word2vec, GloVe (Pennington, Socher % Manning 2014) or fastText which are able to 
provide one embedding for each word regardless of its context. Thus, they do account 
only for one context. The last group consists of approaches such as BERT (Devlin et al. 2019), which are able
to produce multiple word embeddings depending on the context of the words. 

From these different groups *aifedcuation* implements several methods. 

 - **Topic Modeling:** Topic Modeling is an approach that uses frequencies of tokens
 within a text. The frequencies of the tokens are models as the observable variables
 of one more latent topics (Campesato 2021, p. 113). The estimation of a topic model
 is often based on a Latent Dirichlet Analysis (LDA) which describes each text by a
 distribution of topics. The topics themselves are described by a distribution of
 words/tokens (Campesato 2021, p. 114). This relationship between texts, words, and
 topics can be used to create a text embedding by computing the relative amount 
 of every topic in a text on the basis of every token in a text.
 - **GlobalVectorClusters:** GlobalVectors is a newer approach which utilizes the
 co-occurrence of words/tokens to compute GlobalVectors (Campesato 2021, p. 110).
 These vectors are generated in a way
 that tokens/words with a similar meaning are located near to each other (Pennington, Socher & Manning 2014).
 In order to create a *text* embedding from *word* embeddings, *aifeducation* groups tokens 
 into clusters based on their vectors. Thus, tokens with a similar meaning are members of the same cluster. For
 the *text* embedding the tokens of a text are counted for every cluster and the
 frequencies of every cluster for that text are used as a numerical representation
 of that text.
 - **Transformers:** Transformers are the current state-of-art approach for many
 natural language tasks (Tunstall, von Werra & Wolf 2022, p. xv). With help
 of the self attention mechanism (Vaswani et al. 2017) they are able to produce context sensitive
 *word* embeddings (Chollet, Kalinowski & Allaire, 2022 pp.366). In *aifeducation*
 only architecture of BERT are implemented as foundation for classification tasks.

All the approaches are managed and used with a unified interface provided by the object
`TextEmbeddingModel`. With this object you can easily convert raw texts into
a numerical representation which you can use for different classification tasks
at the same time. This makes it possible to reduce computational time. The created
text embedding is stored in an object of class `EmbeddedText`. This object
additionally contains information about the text embedding model that created
this object. 

In the very best case you can apply an existing text embedding model by using 
a transformer from [Huggingface](https://huggingface.co/) or by using a model
from colleagues. If not *aifeducation* provides several functions allowing you 
to create your own models. Depending on the approach you would like to use 
different steps are necessary. In the case of Topic Modeling or GlobalVectorClusters 
you must first create a draft of a vocabulary with the two functions `bow_pp_create_vocab_draft()` and
`bow_pp_create_basic_text_rep()`. When calling these functions you determine central
properties of the resulting model. In the case of transformers you first have to
configure and to train a vocabulary with `create_xxx_model()` and in a next
step you can train your model with `train_tune_xxx_model()`. Every step will be
explained in the next chapters. Please note that `xxx` stands for different
architectures of transformers that are supported with *aifedcuation*.

With an object of class `TextEmbeddingModel` you can create the input data for
the supervised machine learning. Additionally you need the target data which
must be a named factor containing the classes/categories of each text.

With both kind of data you are able to create a new object of class 
`TextEmbeddingClassifierNeuralNet` which is the classifier. For training of the
classifier you have several options which we will cover in detail in chapter 3.
After training the classifier you can share it with other researchers and apply it
on new texts. Please note that the application to new texts requires that the
text is transformed into numbers with *exactly the same text embedding model* before 
passing the text to the classifier. That is, please do not pass the raw texts 
but only the embedded texts to the classifier.

In the next chapters we will guide you to the complete process. Starting with 
the creation of the text embedding models.

**Please not that the creation of a new text embedding model is only necessary
if you can not rely on an existing model or if you can not rely on a pre-trained
transformer.**

# 2 Starting a New Session
Before you can work with *aifeducation* you must set up every new *R* session.
First, it is necessary that you load the library. Second, you must set up
python via reticulate. In the case that you installed python as suggested in the
vignette [01 Get started](articles/aifeducation.html) you may start a new session like
this:
```{r, include = TRUE, eval=FALSE}
library(aifeducation)

#if you use a machine with windows
reticulate::use_condaenv(condaenv = "aifeducation")

#if you use a machine with mac or linux
reticulate::use_virtualenv(virtualenv= "aifedcuation")
```

Now it is a good time to configure tensorflow since some configurations can only
be done **before** tensorflow is used the first time. 

```{r, include = TRUE, eval=FALSE}
#if you would like to use only cpus
set_config_cpu_only()

#if you have a graphic device with low memory
set_config_gpu_low_memory()

#if you would like to reduce the tensorflow output to errors
set_config_os_environ_logger(level = "ERROR")
```

Now everything is ready to start with the preparation tasks.

# 3 Preparation Tasks
## 3.1 Example Data for this Vignette
To illustrate the steps in this vignette we cannot use data from educational
settings since these data is in general protected by privacy policies. Therefore,
we use the data set `data_corpus_moviereviews` from the package quanteda.textmodels
to illustrate the usage of the package. This quanteda.textmodels is automatically installed
when you install *aifeducation*.
```{r, include = TRUE, eval=TRUE}
example_data<-data.frame(
  id=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$id2,
  label=quanteda::docvars(quanteda.textmodels::data_corpus_moviereviews)$sentiment)
example_data$text<-as.character(quanteda.textmodels::data_corpus_moviereviews)

table(example_data$label)
```
We now have a data set with three columns. The first contains the id of the 
movie review, the second contains if the movie was rated positive or negative, and
the third column contains the raw texts. As you can see the data is balanced.
About 1000 reviews imply a positive rating of a movie and about 1000 imply a negative
rating. 

For this tutorial we modify this data set by setting about the half of the 
negative and positive reviews to `NA` indicating that these reviews are not
labeled.

```{r, include = TRUE, eval=TRUE}
example_data$label[c(1:500,1001:1500)]=NA
summary(example_data$label)
```

Furthermore, we will bring some imbalance by setting 250 positive reviews to `NA`.
```{r, include = TRUE, eval=TRUE}
example_data$label[1501:1750]=NA
summary(example_data$label)
```

We will now use this data to show you how to use the different objects
and functions in *aifeducation*.

## 3.2 Topic Modeling and GlobalVectorClusters
If you would like to create a new text embedding model with Topic Modeling or
GlobalVectorClusters you first have to create a draft of a vocabulary. You can
do this by calling the function `bow_pp_create_vocab_draft()`. The main input 
of this function is a vector of texts. The function's aims are 

 - to create a list of all tokens of the texts, 
 - to reduce the tokens to tokens that carry semantic meaning,
 - to provide the lemma of every token.
 
Since Topic Modeling depends on a bag-of-word approach the reason for this 
preprocess step is to reduce the tokens to tokens that really carry semantic
meaning. In general these are tokens of words that are either nouns, verbs or 
adjectives (Papilloud & Hinneburg 2018, p. 32). With our example data an 
application of that function could be:

```{r, include = TRUE, eval=FALSE}
vocab_draft<-bow_pp_create_vocab_draft(
  path_language_model="language_model/english-gum-ud-2.5-191206.udpipe",
  data=example_data$text,
  upos=c("NOUN", "ADJ","VERB"),
  label_language_model="english-gum-ud-2.5-191206",
  language="english",
  trace=TRUE)
```

As you can see there is an additional parameter `path_language_model`. Here you
must insert the path to an udpipe pre-trained language model since this function
uses the *udpipe* package for part-of-speech tagging and lemmataziation. A collection of pre-trained
models for about 65 languages can be found here 
[https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131]. Just download
the relevant model to your machine and provide the path to the model.

With the parameter `upos` you can select which tokens should be selected. In this
example only tokens that represent a noun, an adjective or a verb will remain after
the analysis. A list of possible tags can be found here: 
[https://universaldependencies.org/u/pos/index.html].

Please do not forget do provide a label for the udpipe model you use and
please also provide the language your are analyzing. This information is important
since this will be transferred to the text embedding model. Other researchers/users
will need this information in order to estimate if this model could help for their
own work.

In the next step we can use our draft of a vocabulary to create a basic
text representation with the function `bow_pp_create_basic_text_rep()`.
This function takes raw texts and the draft of a vocabulary as main input. The
function aims 

 - to remove tokens referring to stopwords,
 - to clean the data (e.g., removing punctuation, numbers),
 - to lower case all tokens if requested,
 - to remove tokens with a specific minimal frequency,
 - to remove tokens that occur in to few or to many documents
 - to create a document-feature-matrix (dfm),
 - to create a feature co-occurrence matrix (fcm).
 
Applied to the example the call of the function could look like this:
```{r, include = TRUE, eval=FALSE}
basic_text_rep<-bow_pp_create_basic_text_rep(
  data = example_data$text,
  vocab_draft = vocab_draft,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE,
  split_hyphens = FALSE,
  split_tags = FALSE,
  language_stopwords="eng",
  use_lemmata = FALSE,
  to_lower=FALSE,
  min_termfreq = NULL,
  min_docfreq= NULL,
  max_docfreq=NULL,
  window = 5,
  weights = 1 / (1:5),
  trace=TRUE)
```

`data` takes the raw texts while `vocab_draft` takes the draft of a vocabulary
we created in the first step. 

The main goal is to create a document-feature-matrix(dfm)
and a feature co-occurrence matrix (fcm). The dfm is a matrix that reports the
texts in the rows and the number of tokens in the columns. This matrix is later
used to create a text embedding model based on Topic Modeling. The dfm is
reduced to tokens that correspond to the part-of-speech tags of the vocabulary draft.
Punctuation, symbols, numbers etc. are removed from this matrix if
you set the corresponding parameter to `TRUE`. If you set `use_lemmata = TRUE` 
you can reduce the dimensionality of this matrix further by
using the lemmata instead of the tokens (Papilloud & Hinneburg 2018, p.33). 
If you set `to_lower = TRUE` all tokens
are transformed to lower case. At the end you get an matrix that tries to
represent the semantic meaning of the text with a minimum of tokens possible.

The same applies for the fcm. Here the tokens/features are reduced in the same way.
However, before the features are reduced the tokens co-occurrence is calculated.
For this aim a window is used and shifted across the text counting the tokens left
and right from a token under investigation. The size of this window can be determined
with `window`. With `weights` you can provide weights for counting. For example that tokens
which are far away from the token under investigation count less as tokens that 
are near to the token under investigation. The fcm is later used for creating
a text embedding model based on GlobalVectorClusters.

As you may notices the dfm counts only the words in a text. Thus, their position in the
text or within a sentence does not matter. If you further lower case tokens or use
lemmata more syntactic information is lost for the advantage that the dfm has
a lower dimensionality while losing only few semantic meaning. In contrast, the 
fcm is a matrix that describes how often different tokens occur together.
Thus, fcm recovers part of the position of words in a sentence and in a text.

Now everything is ready to create a new text embedding model based on Topic
Modeling or GlobalVectorClusters. Before we show you how to create the new model
we will have a look on the preparation of a new transformer.

## 3.3 Creating a New Transformer
In general it is recommended to use a pre-trained model since the creation of a
new transformers requires a large data set of texts and is computational intensive.
In this vignette we will illustrated the process with a BERT model. However, for
many other transformers the process is the same.

The creation of a new transformer requires at least two steps. First you must
decide about the architecture of your transformer. This includes to create
a vocabulary. In *aifedcuation* you can do this by calling the function
`create_bert_model()`. For our example this could look like this: 

```{r, include = TRUE, eval=FALSE}
basic_text_rep<-bow_pp_create_basic_text_rep(
create_bert_model(
    model_dir = "my_own_transformer",
    vocab_raw_texts=example_data$text,
    vocab_size=30522,
    vocab_do_lower_case=FALSE,
    max_position_embeddings=512,
    hidden_size=768,
    num_hidden_layer=12,
    num_attention_heads=12,
    intermediate_size=3072,
    hidden_act="gelu",
    hidden_dropout_prob=0.1,
    trace=TRUE)
```

For this function to work you must provide a path to a directory where your new transformer should be
saved. Furthermore, you must provide raw texts. These texts are **not** used for training
the transformer but for training the vocabulary. The maximum size of the vocabulary
is determined by `vocab_size`. Please do not provide a size about 50000 to 60000 since 
this kind of vocabulary works different to the approachs described in section 2.2. Modern
tokenizer such as *WordPiece* (Wu et al. 2016) use algorithms that splits tokens into smaller elements
allowing them to build a huge number of words with a small number of elements. Thus,
even with only small number of about 30000 tokens they are able to represent a very large number
of words. As a consequence, these kind of vocabulary are many times smaller as the
vocabularies built in section 2.2.

The other parameters allow you to customize your BERT model. For example, you could
increase the number of hidden layers from 12 to 24 or reduce the hidden size from 768
to 256 allowing you to built and to test larger or smaller transformers.

Please note that with `max_position_embeddings` you determine how many tokens
your transformer can process. If your text has more tokens *after* tokenaization these
tokens are ignored. However, if you would like to analyze long documents please
avoid to increase this number significantly because the computational times does not
increase linear but quadratic (Beltagy, Peters & Cohan 2020). For long documents you 
can use another architecture of BERT
(e.g. Longformer from Beltagy, Peters & Cohan 2020) or split a long document 
into several chunks which are seuential used for classification (e.g., Pappagari et al. 2019). Using chunks is supported with *aifedcuation*.

After calling the function you will find your new model in your model directory.
The next step is to train your model by calling `train_tune_bert_model()`.

```{r, include = TRUE, eval=FALSE}
train_tune_bert_model(
  output_dir = "my_own_transformer_trained",
  bert_model_dir_path = "my_own_transformer",
  raw_texts = example_data$text,
  aug_vocab_by=0,
  p_mask=0.15,
  whole_word=TRUE,
  val_size=0.1,
  n_epoch=1,
  batch_size=12,
  chunk_size=250,
  n_workers=1,
  multi_process=FALSE,
  trace=TRUE)
```
Here, it is important that you provide the path to the directory where your new
transformer is stored. Furthermore, it is important that you provide *another*
directory where your trained transformer should be saved to avoid reading
and writing collisions. 

Now the provides raw data is used for training your model by using 
Masked Language Modeling. First, you can set the length of token sequences with `chunk_size`.
With `whole_word` you can chose between masking single
tokens or masking complete words (Please remember that modern tokenizers split
words into several tokens. Thus, tokens and word are not forced to match each other
directly). With `p_mask` you can determine how many tokens
should be masked. Finally, with `val_size` you set how many chunks should be used
for the validation sample. 

If you work on a machine and your graphic device has only a small memory please reduce
the batch size significantly. We also recommend to change the usage of memory with
`set_config_gpu_low_memory()`.

After the training finishes you can find the transformer ready to use in your 
output_directory. Now you are able to create a text embedding model.

# 4 Text Embedding 
## 4.1 Introduction
In *aifedcuation* a text embedding model is stored as an object of class 
`TextEmbeddingModel`. This object contains all relevant information for transforming
raw texts into a numeric representation that can be used for machine learning. 

In *aifedcuation* the transformation of raw texts into numbers is a separate step
from downstream task such as classification. The reason is to reduce computational time
on machines with low performance. By separating text embedding from other tasks
the text embedding has to be calculated only once and can be used for different
tasks at the same time. Another advantage 
is that the training of the downstream tasks involves only the downstream tasks an not
the parameters of the embedding model making training less time consuming and 
decreases the computational insensitivity. Finally, this approach allows the analysis of
long documents by applying the same algorithm on different parts of a long text.

The text embedding model provides a unified interface. That is, after creating
the model with different methods the handling of the model is always the same.

In the following we will show you how to use this object. We start with Topic Modeling.

## 4.2 Creating Text Embedding Models
### 4.2.1 Topic Modeling
For creating a new text embedding model based on Topic Modeling you only need
a basic text representation generated with the function `bow_pp_create_basic_text_rep()`
(see section 2.2). Now you can create a new instance of a text embedding model by
calling `TextEmbeddingModel$new()`.

```{r, include = TRUE, eval=FALSE}
topic_modeling<-TextEmbeddingModel$new(
  model_name="topic_model_embedding",
  model_label="Text Embedding via Topic Modeling",
  model_version="0.0.1",
  model_language="english",
  method="lda",
  bow_basic_text_rep=basic_text_rep,
  bow_n_dim=12,
  bow_max_iter=500,
  bow_cr_criterion=1e-8,
  trace=TRUE
)
```

First you have to provide a name for your new model (`model_name`). This should be 
a unique but short name without any spaces. With `model_label` you can provide
a label for your model with more freedom. It is important that you provide a version
for your model for the case that you will create an improved version in the future.
With `model_language` you provide users the information for which language your model
is designed. This is very important if you plan to share your model to a wider 
community.

With `method` you determine which approach should be used for your model. If you 
would like to use Topic Modeling you have to set `method = "lda"`. the number of
topics is set via `bow_n_dim`. In this example we would like to create a
topic model with 12 topics. The number of topics also determines the dimensionality
for our text embedding. That is, every text will be characterized by these 12 topics.

Please do not forget to pass your basic text representation to `bow_basic_text_rep`.

After the model is estimated it is stored as `topic_modeling` in our example.

### 4.2.2 GlobealVectorClusters
The creation of a text embedding model based on GlobalVectorClusters is very similar
to a model based on Topic Modeling. There are only two differences.

```{r, include = TRUE, eval=FALSE}
global_vector_clusters_modeling<-TextEmbeddingModel$new(
  model_name="global_vector_clusters_embedding",
  model_label="Text Embedding via Clusters of GlobalVectors",
  model_version="0.0.1",
  model_language="english",
  method="glove_cluster",
  bow_basic_text_rep=basic_text_rep,
  bow_n_dim=96,
  bow_n_cluster=384,
  bow_max_iter=500,
  bow_max_iter_cluster=500,
  bow_cr_criterion=1e-8,
  trace=TRUE
)
```

First, you request a model based on GlobalVectorCluster by setting `method="glove_cluster"`.
Second, you have to determine the dimensionalty of the global vectors with
`bow_n_dim` and the number of clusters by `bow_n_cluster`. When creating a new
text embedding model the global vector of each token is calculated based on the
feature-co-occurrence matrix (fcm) you provide with `basic_text_rep`. That is, for 
very token a vector is calculated with the length of `bow_n_dim`. Since these
vectors are **word** embeddings and not **text** embeddings an additional step is
necessary to create text embedding. In *aifedcuation* the word embeddings are used
to group the word into clusters. The number of cluster is set with `bow_n_cluster`.
Now, the text embedding results by counting the tokens of every cluster for every text.

The final model is stored as `global_vector_clusters_modeling`.

### 4.2.3 Transformers
Using a transformer for creating a text embedding model is similar to the other 
both approaches.  

```{r, include = TRUE, eval=FALSE}
bert_modeling<-TextEmbeddingModel$new(
  model_name="bert_embedding",
  model_label="Text Embedding via BERT",
  model_version="0.0.1",
  model_language="english",
  method = "bert",
  max_length = 512,
  chunks=4,
  overlap=30,
  aggregation="last",
  use_cls_token=TRUE,
  model_dir="my_own_transformer_trained"
  )
```

To request a model based on a transformer you must set `method` accordingly.
Since we use a BERT model in our example we have to set `method = "bert"`. Next,
you have to provide the directory where your model is stored. In this example
this would be `bert_model_dir_path="my_own_transformer_trained`. Of course 
you cane use any other pre-trained model from Huggingface which addresses your needs.

Using a BERT model for text embedding is no problem since your text do not provide
more tokens as the transformer can process. This maximal value is set in the config
of the transformer (see section 2.3). If the text produces more tokens the last tokens
are ignored. In some cases you may want to analyze long texts. In these situations
reducing the text to the first tokens (e.g. only the first 512 tokens) could result in a problematic lose of
information. To deal with these situations you can config a text embedding model in 
*aifecuation* to split long texts into several chunks which are processed by the transformer.
The maximal number of chunks is set with `chunks`. In our example above, the 
text embedding model would split a text consisting of 1024 tokens into 2 chunk with every chunk
consisting of 512 tokens. For every chunk a text embedding is calculated. As a results you receive a sequence
of embeddings. The first embeddings characterizes the first part of the text and
the second embedding characterizes the second part of the text (and so on). 
Thus, our example text embedding model is able to process texts with about 4*512=2048 tokens.
This approach is inspired by the work by Pappagari et al. (2019). 

Since transformers are able to account for the context it may be useful to connect
every chunk in order to bring the context into the calculations. This can be done
with `overlap` determining how many tokens of the end of a prior chunk should be 
added to the next chunk. 

Finally, you have to decide if you would like the embedding of the classification
token [CLS] as text embedding or the mean of all token embeddings as text embedding
(`use_cls_token`). You can further decide from which hidden layer or layers
the embeddings should be drawn (`aggregation="last"`). In their initial
work Devlin et al. (2019) used the hidden states of different layers for classification. 

After deciding about the configuration you can use your model.

## 4.3 Transforming Raw Texts into Embedded Texts
Although the mechanics within a text embedding model are different the usage is 
always the same. To transform raw text into a numeric representation you only
have to use the embed method of your model. Therefore, you must provide the
raw texts to `raw_text`. In addition, it is necessary that you provide
a character vector containing the id of every text. The ids must be unique.

```{r, include = TRUE, eval=FALSE}
topic_embeddings<-topic_modeling$embed(
  raw_text=example_data$text,
  doc_id=example_data$id, 
  trace = TRUE)

cluster_embeddings<-global_vector_clusters_modeling$embed(
  raw_text=example_data$text,
  doc_id=example_data$id, 
  trace = TRUE)

bert_embeddings<-bert_modeling$embed(
  raw_text=example_data$text,
  doc_id=example_data$id, 
  trace = TRUE)
```

The method `embed`creates an object of class `EmbeddedText`. This is just
a data.frame consisting the embedding of every text. Depending on the method
the data.frame has a different meaning:

 - **Topic Modeling:** In the case of Topic Modeling the rows represent the texts 
 and the columns represent the percentage of every topic within a text.
 - **GlobalVectorClusters:** In this case the rows represent the texts and the columns
 represent the absolute frequencies of tokens belonging to a semantic cluster.
 - **Transformer - Bert:** In the case of BERT the rows represent the texts and
 the columns represents the contextualized text embedding. That is, Bert's understanding
 of the relevant text chunk.
 
Please not that in the case of Bert models the embeddings of every chunks are concatenated.

With the embedded texts you now have the input to train a new classifier or
to apply a pre-trained classifier for predicting categories/classes. In
the next chapter we will show you how to use these classifiers. But before
we start we will show you how to save and load your model.

## 4.4 Saving and Loading Text Embedding Models
Saving a created text embedding model is very easy. However, the saving
and loading process is different for models based on Topic Modeling and 
GlobalVectorClusters on the hand and models based on transformers on the other hand.

For models using Topic Modeling or GlobalVectorClusters you can call `save()` to
write your model to disk.
```{r, include = TRUE, eval=FALSE}
save(topic_modeling, 
     file="models/embedding_model_topic.RData")
save(global_vector_clusters_modeling, 
     file="models/embedding_model_gvc.RData")
```
If you want to load your model just call `load()` and you can continue using
your model.
```{r, include = TRUE, eval=FALSE}
load(file="models/embedding_model_topic.RData")
load(file="models/embedding_model_gvc.RData")
```

If your text embedding model is based on a transformer saving and loading 
requires some other steps. In this case your text embedding models serves as
an interface to *R*. The original model is saved in a model directory. Thus,
you have to save your interface to that model directory.
```{r, include = TRUE, eval=FALSE}
save(bert_modeling,
     file="my_own_transformer_trained/r_interface.RData")
```
Loading your model requires two steps. First, load the interface.
```{r, include = TRUE, eval=FALSE}
load(file="my_own_transformer_trained/r_interface.RData")
```
Now the text embedding model is available in *R.*. Next you must
re-initialize the transformer by calling the corresponding method `load_model` 
of your model.
```{r, include = TRUE, eval=FALSE}
bert_modeling$load_model(model_dir="my_own_transformer_trained")
```
Now you can use your text embedding model.

# 5 Using AI for Classification 
## 5.1 Creating a New Classifier
In *aifedcuation* classifiers are based on neural nets and stored in objects
of class `TextEmbeddingClassifierNeuralNet`. You can create a new classifier
by calling `TextEmbeddingClassifierNeuralNet$new()`.

```{r, include = TRUE, eval=FALSE}
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id

classifier<-TextEmbeddingClassifierNeuralNet$new(
  name="movie_review_classifier",
  label="Classifier for Estimating a Postive or Negative Rating of Movie Reviews",
  text_embeddings=bert_embeddings,
  targets=example_targets,
  hidden=NULL,
  rec=c(128,128),
  self_attention_heads = 0,
  dropout=0.3,
  recurrent_dropout=0.4,
  l2_regularizer=0.001,
  optimizer="adam",
  act_fct="tanh",
  rec_act_fct="tanh")
```

Similar to the text embedding model you should provide a name (`name`) and a
label (`label`) for your new classifier. With `text_embeddings` you have to provide
an embedded text. We would like to recommend that you use the embedding you would
like to use for training. We here continue our example and use the embedding
produced by our BERT model.

`targets` takes the target data for the supervised learning. Please do not
omit cases which have no category/class since they can be used with a special 
training technique we will show you later. It is very important that you provide the
target data as factor. Otherwise an error will occur. It is also important
that you name your factor. That is, the entries of the factor mus have names
that correspond to the ids of the corresponding texts. Without these names
the method cannot match text embeddings as input data to the target data. 

With the other parameters  you decide about the structure of your classifier. Figure
4 illustrates this.

![Figure 4: Overview of Possible Structure iof a Classifier](classif_fig_04.png){width=100%}

`hidden` takes a vector of integers determining the number of layers and the number
of neurons. In our example this are no dense layers. `rec` also takes a vector of
integers determining the number
and size of the Gated Recurrent Unit. In this example we use two layers with 128 each.

Since the classifiers in *aifeducation* use a standardized scheme for their 
creation, dense layers are used after the gru layers. If you want to omit gru layers
or dense layers set the corresponding argument to `NULL`. 

If you use a text embedding model that processes more than 1 chunk we would like
to recommend to use recurrent layers since they are able to use the sequential structure 
of your data. In all other cases you can rely on dense layers only.

If you use text embeddings it is a good idea to try self attention layer in order to
take the context of all chunks into account. To add a self attention layer you must
provide an integer greater 0 to `self_attention_heads`. This will add a self 
attention layer. Additionally a normalization layer and a recurrent layer are also added
behind self attention to process the contextualized sequences. 

Masking, normalization, and the creation of the input layer as well as the output
layer is done automatically.

After you have created a new classifier you can begin training.

## 5.2 Training a Classifier
For starting the training of your classifier you have to call the `train` method.
Similar for the creation of the classifier you must provide the text embedding to
`data_embeddings` and the categories/classes as target data to `data_targets`.
Please remember that `data_targets` expects a named factor where the names 
corresponds to the ids of the corresponding text embeddings. Text embeddings
and target data that cannot be matched are ommited from training. 

For training a classifier it is necessary that you provide a path to
`dir_checkpoint`. This directory stores the best set of weights during each
training step. After training these weights are automatically used as final weights
for the classifier. 

For performance estimation training splits the data into several chunky based on
cross fold validation. The number of folds is set with `data_n_test_samples`. 
In every case one fold is not used for training and serves as *test* sample. The
remaining data is used for creating a *training* and a *validation* sample. All
performance values saved in the trained classifier refer to the test sample. That
is, this data has never been used during training and provides a more realistic
estimation of classifier`s performance.

```{r, include = TRUE, eval=FALSE}
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id

classifier$train(
   data_embeddings = bert_embeddings,
   data_targets = example_targets,
   data_n_test_samples=5,
   use_baseline=TRUE,
   bsl_val_size=0.25,
   use_bsc=TRUE,
   bsc_methods=c("dbsmote"),
   bsc_max_k=10,
   bsc_val_size=0.25,
   use_bpl=TRUE,
   bpl_max_steps=30,
   bpl_epochs_per_step=1,
   bpl_dynamic_inc=FALSE,
   bpl_balance=TRUE,
   bpl_max=1.00,
   bpl_anchor=1.00,
   bpl_min=0.00,
   bpl_weight_inc=0.02,
   bpl_weight_start=0.00,
   bpl_model_reset=FALSE,
   epochs=40,
   batch_size=32,
   trace=TRUE,
   view_metrics=FALSE,
   keras_trace=0,
   n_cores=2,
   dir_checkpoint="training/classifier")
```

Since *aifedcuation* tries to address the special needs in educational and
social science some special training steps are integrated in this method. 

 - **Baseline:** If you are interested in training your classifier without applying
 any additional statistical techniques you should set `use_baseline = TRUE`. In this
 case the classifier is trained with the provided data as it is. Cases with missing values 
 in target data are omitted. Even if you would like 
 to apply further statistical adjustments it makes sense to compute a baseline model
 for comparing the effect of the modified training process with an unmodified
 training. By using `bsl_val_size` you can determine how many data should be
 used as training data and how many data should be used as validation data.
 - **Balanced Synthetic Cases:** In case of imbalanced data it is recommended to
 set `use_bsc=TRUE`. Now, before training a number of synthetic units is created
 via different techniques. Currently you can request *Basic Synthetic Minority Oversampling Technique*,
 *Density-Bases Synthetic Minority Oversampling Technique*, and
 *Adaptive Synthetic Sampling Approach for Imbalanced Learning*. The aim is to 
 create new cases that fill the gap to the majority class. Multiclass problems
 are reduced to a two class problem (class under investigation vs. each other) 
 for generating these units.
 You can even request several techniques at once. If the number of synthetic units
 and original minority units exceed the number of cases of the majority class a
 random sample is drawn. If the technique allows to set the number of neighbors
 during generation `k = bsc_max_k` is used.
 - **Balanced Pseudo Labeling:** This technique is relevant if you have a labeled
 target data and a large number of unlabeled target data. This option activates
 an implementation of pseudo labeling. That is, the classifier is trained with
 the cases for which labeled data is available. After training the classifier
 predicts the classes/categories for the unlabeled cases. The classifier is trained with the
 extended data and after each epoch it predicts the classes/categories of the
 unlabeled data again. This process is iterated as long `bpl_step_max` is reached. Please note
 that the number of cases added within each step is determined for *all* classes/categories
 by the category with the *lowest absolute frequency* in the case of
 `bpl_balance=TRUE`. If more cases are available the cases are sorted
 by their distance to `bpl_anchor`. This is a value describing the certainty of the 
 pseudo labels. 0 equals random guessing, 1 equals perfect certainty. It is recommended
 to include cases which have a high but not a perfect certainty to improve the
 quality of the classifier. Cases with a very high certainty provide only 
 little new information for the training while cases with a low certainty are
 may provide to much noise to the training. You can set the minimal and maximal 
 limits for considering cases with `bpl_min` and `bpl_max` for both 
 `bpl_balance=TRUE` and `bpl_balance=FALSE`. The value for increasing the weights
 for every step is determined with `bpl_weight_inc`.
 This technique requires `use_baseline = TRUE` or `use_bsc=TRUE`. 
 If both is set to true a classifier
 trained on the basis of balanced synthetic cases is used.
 
Figure 5 illustrates the training loop for the cases that all three options
are set to `TRUE`.

![Figure 5: Overview of the Steps to Perform a Classification](classif_fig_05.png){width=100%}

The example above applies the algorithm proposed by Lee (2013). That is, after
training the classifier on the labeled data the unlabeled data is introduced
into the training with an increasing weight in every step. While Lee (2013)
suggests to recalculate the pseudo labels of the unlabeled data after every
weight actualization in *aifeducation* the pseudo labels are recalculated after
every epoch. The model is never re-initialized during this training. Thus, to achieve
a training that is near to the work of Lee (2013) `steps` must be treated as epochs,
`epochs_per_steps` must be set to 1 and model re-initialization has to be turned off
(`bpl_model_reset = FALSE`). The second change to Lee`s (2013) algorithm is that in our example
`bpl_balance = TRUE`. Thus, not all unlabeled data is used. That is, for
every class/category the same number of pseudo labels is used and at the same time
the cases which have the highest classification certainty.
 
Finally, `trace`, `view_metrics`, and `keras_trace` allow you to control how
many information about the training progress are printed to the console. Please
note that training the classifier can take some time. 

Please note that after performance estimation the final training of the classifier
makes use of all data available. That is, the test sample is left empty.

## 5.3 Evaluating Classifier's Performance
After finishing training you can evaluate the performance of the classifier. For
every fold the classifier is applied to the test sample and the results are
compared with the true categories/class. Since the test sample is never be part
of the training all performance measures provide a more realistic idea of
classifier`s performance.

To support researchers in judging the quality of the predictions *aifeducation*
utilizes several measures and concepts from content analysis. These are 

- Iota Concept of the Second Generation (Berding & Pargmann 2022)
- Krippendorff's Alpha (Krippendorff 2019)
- Percentage Agreement
- Gwet's AC1/AC2 (Gwet 2014)
- Kendall's coefficient of concordance W
- Cohen's Kappa with equal weights
- Fleiss' Kappa for multiple raters with exact estimation
- Light's Kappa for multiple raters

You can access the concrete values by accessing the field `reliability` which
stores all relevant information. In this list you will find the reliability values
for every fold and for every requested training configuration. In addition, the
reliability of every step within balanced pseudo labeling is reported.

The central estimates for the reliabilites can be found via `reliability$test_metric_mean`.
In our example this would be:


```{r, include = TRUE, eval=TRUE}
classifier$reliability$test_metric_mean
```
You know have a table with all relevant values. Of particular interest are the
values for alpha from the Iota Concept since they represent a measure of reliability
which is independent from the frequency distribution of the classes/categories.
The alpha values describe the probability that a case of a specific class is
recognized as the specific class. As you can see compared to the baseline model
applying *Balanced Synthetic Cases increased* increases the minimal value of alpha
reducing the risk to miss cases which belong to a seldom class (see row with "BSC"). On the opposite the
alpha values for the major category decreases slightly losing its unjustified bonus
from a high number of cases in the training set. Thus, this provides a more realistic
performance estimation of the classifier. 

Furthermore, you can see that the application of pseudo labeling increases the
alpha values for the minor class further up to step 3. 

Finally, you can plot a coding stream scheme showing how the cases of different classes
are labeled. Her we use the package *iotarelr*.

```{r, fig.height = 6, fig.width = 7.2, fig.align = "center", fig.cap = "Figure 6: Coding Stream of the Classifier",include = TRUE, eval=TRUE}
library(iotarelr)
iotarelr::plot_iota2_alluvial(test_classifier$reliability$iota_object_end_free)
```
Here you can see that a small number of the negative reviews is treated as a good
review while a larger number of the positive reviews is treated as a bad review.
Thus, the data for the major class (negative reviews) is more reliable and valid 
as the the data for the minor class (positive reviews).

Evaluating the performance of a classifier is a complex task and 
and behind the scope of this vignette. Here, we
would like to refer to the cited literature of content analysis and machine
learning if you would like to dive into the deep of this topic.

## 5.4 Saving and Loading a Classifier
If you have created a classifier saving and loading is very easy due to the *R* package
*bundle*. You can just use `save()` and `load()`. In our example this could be

```{r, include = TRUE, eval=FALSE}
save(classifier,
     file="classifiers/movie_review.RData")

load(file="classifiers/movie_review.RData")
```

## 5.5 Predicting New Data
If you would like to apply your classifier to new data, two steps are necessary.
First, you must transform the raw text into a numerical expression by using
*exactly* the same text embedding model that was used for training your classifier.
The resulting object can be passed to the method `predict` and you will get
the predictions together with an estimate of certainty for each class/category. 

# References

Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The Long-Document Transformer. 
https://doi.org/10.48550/arXiv.2004.05150

Berding, F., & Pargmann, J. (2022). Iota Reliability Concept of the Second Generation. 
Logos Verlag Berlin. https://doi.org/10.30819/5581

Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A., & Rebmann, K. (2022). 
Performance and Configuration of Artificial Intelligence in Educational Settings.: 
Introducing a New Reliability Concept Based on Content Analysis. 
Frontiers in Education, 1–21. https://doi.org/10.3389/feduc.2022.818365

Campesato, O. (2021). Natural Language Processing Fundamentals for Developers. 
Mercury Learning & Information. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713 

Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning with R 
(Second edition). Manning Publications Co. 
https://learning.oreilly.com/library/view/-/9781633439849/?ar 

Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. 
In J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings of the 2019 Conference 
of the North (pp. 4171–4186). Association for Computational Linguistics. 
https://doi.org/10.18653/v1/N19-1423

Gwet, K. L. (2014). Handbook of inter-rater reliability: The definitive guide to 
measuring the extent of agreement among raters (Fourth edition). Advances Analytics LLC. 

Krippendorff, K. (2019). Content Analysis: An Introduction to Its Methodology (4th Ed.). SAGE. 

Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing in 
action: Understanding, analyzing, and generating text with Python. Manning. 

Larusson, J. A., & White, B. (Eds.). (2014). Learning Analytics: 
From Research to Practice. Springer New York. https://doi.org/10.1007/978-1-4614-3305-7

Lee, D.‑H. (2013). Pseudo-Label : The Simple and Efficient Semi-Supervised Learning
Method for Deep Neural Networks. CML 2013 Workshop : Challenges in RepresentationLearning. https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks

Papilloud, C., & Hinneburg, A. (2018). Qualitative Textanalyse mit Topic-Modellen: 
Eine Einführung für Sozialwissenschaftler. Springer. https://doi.org/10.1007/978-3-658-21980-2

Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N. (2019). 
Hierarchical Transformers for Long Document Classification. In 2019 IEEE Automatic 
Speech Recognition and Understanding Workshop (ASRU) (pp. 838–844). IEEE. 
https://doi.org/10.1109/ASRU46091.2019.9003958

Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for 
Word Representation. Proceedings of the 2014 Conference on Empirical Methods in 
Natural Language Processing. https://aclanthology.org/D14-1162.pdf

Schreier, M. (2012). Qualitative Content Analysis in Practice. SAGE. 

Tunstall, L., Werra, L. von, Wolf, T., & Géron, A. (2022). Natural language processing 
with transformers: Building language applications with hugging face (Revised edition). O'Reilly. 

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). 
Attention Is All You Need. https://doi.org/10.48550/arXiv.1706.03762

Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., 
Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y.,
Kudo, T., Kazawa, H., . . . Dean, J. (2016). Google's Neural Machine Translation System: 
Bridging the Gap between Human and Machine Translation. https://doi.org/10.48550/arXiv.1609.08144


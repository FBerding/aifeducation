---
title: "03 Using R syntax"
author: "Florian Berding, Julia Pargmann, Andreas Slopinski, Elisabeth Riebenbauer, Karin Rebmann"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{03 Using R syntax}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(aifeducation)
```

# 1 Introduction and Overview

## 1.1 Preface

This vignette introduces the package *aifeducation* and its usage with *R* syntax.
For users who are unfamiliar with *R* or those who do not have coding skills in relevant languages
(e.g., python) we recommend to start with the graphical user interface *Aifeducation - Studio*
which is described in the vignette 
[02 Using the graphical user interface Aifeducation - Studio](gui_aife_studio.html).

We assume that *aifeducation* is installed as described
in vignette 
[01 Get Started](https://fberding.github.io/aifeducation/articles/aifeducation.html).
The introduction starts with a brief explanation of basic concepts,
which are necessary to work with this package.

## 1.2 Basic Concepts

In the educational and social sciences, assigning an observation to
scientific concepts is an important task that allows researchers to
understand an observation, to generate new insights, and to derive
recommendations for research and practice.

In educational science, several areas deal with this kind of task. For
example, diagnosing students' characteristics is an important aspect of
a teachers' profession and necessary to understand and promote learning.
Another example is the use of learning analytics, where data about
students is used to provide learning environments adapted to their
individual needs. On another level, educational institutions such as
schools and universities can use this information for data-driven
performance decisions (Laurusson & White 2014) as well as where and how
to improve it. In any case, a real-world observation is aligned with
scientific models to use scientific knowledge as a technology for
improved learning and instruction.

Supervised machine learning is one concept that allows a link between
real-world observations and existing scientific models and theories
(Berding et al. 2022). For the educational sciences, this is a great
advantage because it allows researchers to use the existing knowledge
and insights to apply AI. The drawback of this approach is that the
training of AI requires both information about the real world
observations and information on the corresponding alignment with
scientific models and theories.

A valuable source of data in educational science are written texts,
since textual data can be found almost everywhere in the realm of
learning and teaching (Berding et al. 2022). For example, teachers often
require students to solve a task which they provide in a written form.
Students have to create a solution for the tasks which they often
document with a short written essay or a presentation. This data can be
used to analyze learning and teaching. Teachers' written tasks for their
students may provide insights into the quality of instruction while
students' solutions may provide insights into their learning outcomes
and prerequisites.

AI can be a helpful assistant in analyzing textual data since the
analysis of textual data is a challenging and time-consuming task for
humans.

> Please note that an introduction to content analysis, natural language
> processing or machine learning is beyond the scope of this vignette.
> If you would like to learn more, please refer to the cited literature.

Before we start, it is necessary to introduce a definition of our
understanding of some basic concepts, since applying AI to educational
contexts means to combine the knowledge of different scientific
disciplines using different, sometimes overlapping concepts. Even within
a single research area, concepts are not unified. Figure 1 illustrates
this package's understanding.

![Figure 1: Understanding of Central Concepts](img_articles/classif_fig_01.png){width="100%"}

Since *aifeducation* looks at the application of AI for classification
tasks from the perspective of the empirical method of content analysis,
there is some overlapping between the concepts of content analysis and
machine learning. In content analysis, a phenomenon like performance or
colors can be described as a scale/dimension which is made up by several
categories (e.g. Schreier 2012, pp. 59). In our example, an exam's
performance (scale/dimension) could be "good", "average" or "poor". In
terms of colors (scale/dimension) categories could be "blue", "green",
etc. Machine learning literature uses other words to describe this kind
of data. In machine learning, "scale" and "dimension" correspond to the
term "label" while "categories" refer to the term "classes" (Chollet,
Kalinowski & Allaire 2022, p. 114).

With these clarifications, classification means that a text is assigned
to the correct category of a scale or, respectively, that the text is
labeled with the correct class. As Figure 2 illustrates, two kinds of
data are necessary to train an AI to classify text in line with
supervised machine learning principles.

![Figure 2: Basic Structure of Supervised Machine Learning](img_articles/classif_fig_02.png){width="100%"}

By providing AI with both the textual data as input data and the
corresponding information about the class as target data, AI can learn
which texts imply a specific class or category. In the above exam
example, AI can learn which texts imply a "good", an "average" or a
"poor" judgment. After training, AI can be applied to new texts and
predict the most likely class of every new text. The generated class can
be used for further statistical analysis or to derive recommendations
about learning and teaching.

In use cases as described in this vignette, AI has to "understand"
natural language: „Natural language processing is an area of research in
computer science and artificial intelligence (AI) concerned with
processing natural languages such as English and Mandarin. This
processing generally involves translating natural language into data
(numbers) that a computer can use to learn about the world. (…)” (Lane ,
Howard & Hapke 2019, p. 4)

Thus, the first step is to transform raw texts into a a form that is
usable for a computer, hence raw texts must be transformed into numbers.
In modern approaches, this is usually done through word embeddings.
Campesato (2021, p. 102) describes them as "the collective name for a
set of language modeling and feature learning techniques (...) where
words or phrases from the vocabulary are mapped to vectors of real
numbers." The definition of a word vector is similar: „Word vectors
represent the semantic meaning of words as vectors in the context of the
training corpus." (Lane, Howard & Hapke 2019, p. 191). In the next step,
the words or text embeddings can be used as input data and the labels as
target data for training AI to classify a text.

In *aifeducation,* these steps are covered with three different types of
models, as shown in Figure 3.

![Figure 3: Modells Types in aifeducation](img_articles/classif_model_hierachy.png){width="100%"}

-   **Base Models:** The base models are the models which contain the
    capacities to understand natural language. In general, these are
    transformers such as BERT, RoBERTa, etc. A huge number of
    pre-trained models can be found on
    [Huggingface](https://huggingface.co/).

-   **Text Embedding Models:** The modes are built on top of base models
    and store directions on how to use these base models for converting
    raw texts into sequences of numbers. Please note that the same base
    model can be used to create different text embedding models.

-   **Classifiers:** Classifiers are used on top of a text embedding
    model. They are used to classify a text into categories/classes
    based on the numeric representation provided by the corresponding
    text embedding model. Please note that a text embedding model can be
    used to create different classifiers (e.g. one classifier for
    colors, one classifier to estimate the quality of a text, etc.).

# 2 Start Working
## 2.1 Starting a New Session
Before you can work with *aifeducation* you must set up a new *R*
session. First, it is necessary that you load the library. Second, you
must set up python via reticulate. In case you installed python as
suggested in vignette [01 Get started](aifeducation.html) you
may start a new session like this:

```{r, include = TRUE, eval=FALSE}
reticulate::use_condaenv(condaenv = "aifeducation")
library(aifeducation)
```

Next you have to choose the machine learning framework you would like to use.
You can set the framework for the complete session with

```{r, include = TRUE, eval=FALSE}
#For tensorflow
aifeducation_config$set_global_ml_backend("tensorflow")
set_transformers_logger("ERROR")

#For PyTorch
aifeducation_config$set_global_ml_backend("pytorch")
```

Setting the global machine learning framework is only for convenience. You can
change the framework at any time during a session by calling this method again or
by setting the argument 'ml_framework' of methods and functions manually.

In the case that you would like to use *tensorflow* now is a good time to 
configure that backend, since some configurations
can only be done **before** *tensorflow* is used the first time.

```{r, include = TRUE, eval=FALSE}
#if you would like to use only cpus
set_config_cpu_only()

#if you have a graphic device with low memory
set_config_gpu_low_memory()

#if you would like to reduce the tensorflow output to errors
set_config_os_environ_logger(level = "ERROR")
```

> **Note:** Please remember: Every time you start a new session in *R* you have to
to set the correct conda environment, to load the library *aifeducation*, and
to chose your machine learning framework.

## 2.2  Reading Texts into *R*

For most applications of  *aifeducation* it's necessary to read the text you would like to use into
*R*. For this task, several packages are available on CRAN. Our experience has been good
with the package [readtext](https://cran.r-project.org/package=readtext)
since it allows you to process different kind of sources for textual data.
Please refer to readtext's documentation for more details. If you have not installed
this package on your machine, you can request it by
```{r, include = TRUE, eval=FALSE}
install.packages("readtext")
```

For example, if you have stored your texts in an excel sheet with two columns
(*texts* for the texts and *id* for the texts' id) you can read the data by
```{r, include = TRUE, eval=FALSE}
#for excel files
textual_data<-readtext::readtext(
  file="text_data.xlsx",
  text_field = "texts",
  docid_field = "id"
)
```
Here it is crucial that you pass the file path to `file` and the name of
the column for the texts to `text_field` and the name of the column for the id
to `docid_field`.

In other cases you may have stored each text in a separate file (e.g., .txt or .pdf).
For these cases you can pass the directory of the files and read the data. In the following 
example the files are stored in the directory "data".
```{r, include = TRUE, eval=FALSE}
#read all files with the extension .txt in the directory data
textual_data<-readtext::readtext(
  file="data/*.txt"
)

#read all files with the extension .pdf in the directory data
textual_data<-readtext::readtext(
  file="data/*.pdf"
)
```
If you read texts for several files you do not need to specify the arguments
`docid_field` and `text_field`. The id of the texts is automatically set to
the file names. 

After the text is read we recommend to do some text cleaning.
```{r, include = TRUE, eval=FALSE}
#remove multiple spaces and new lines
textual_data$text=stringr::str_replace_all(textual_data$text,pattern = "[:space:]{1,}",replacement = " ")

#remove hyphenation
textual_data$text=stringr::str_replace_all(textual_data$text,pattern = "-(?=[:space:])",replacement = "")
```


Please refer to the documentation of the function readtext within the readtext
library for more information.

## 2.3 Example Data for this Vignette

To illustrate the steps in this vignette, we cannot use data from
educational settings since these data is generally protected by privacy
policies. Therefore, we use a subset of the Standford Movie Review Dataset provided by
Maas et al. (2011) which is part of the package. You can access the data set with
`imdb_movie_reviews`.

We now have a data set with three columns. The first contains contains the raw text,
the second contains the rating of the movie (positive
or negative), and the third column the ID of the movie review.
About 200 reviews imply a positive rating
of a movie and about 100 imply a negative rating.

For this tutorial, we modify this data set by setting about 100 positive reviews to `NA`,
indicating that these reviews are not labeled.

```{r, include = TRUE, eval=TRUE}
example_data=imdb_movie_reviews
example_data$label<-as.character(example_data$label)
example_data$label[c(201:300)]=NA
example_targets<-as.factor(example_data$label)
table(example_data$label)
```

We will now use this data to show you how to use the different objects
and functions in *aifeducation*.

# 3 Base Models
## 3.1 Overview
Base models are the foundation of all further models in *aifeducation*.
At the moment, these are transformer models such as BERT (Devlin et al.
2019), RoBERTa (Liu et al. 2019), DeBERTa version 2 (He et al. 2020),
Funnel-Transformer (Dai et al. 2020), and Longformer (Beltagy, Peters &
Cohan 2020). In general, these models are trained on a large corpus of
general texts in the first step. In the next step, the models are
fine-tuned to domain-specific texts and/or fine-tuned for specific
tasks. Since the creation of base models requires a huge number of texts
resulting in high computational time, it is recommended to use
pre-trained models. These can be found on
[Huggingface](https://huggingface.co/). Sometimes, however, it is more
straightforward to create a new model to fit a specific purpose.
Aifeducation Studio supports the opportunity to both create and
train/fine-tune base models.

## 3.2 Creation of Base Models

Every transformer model is composed of two parts: 1) the tokenizer which
splits raw texts into smaller pieces to model a large number of words
with a limited, small number of tokens and 2) the neural network that is
used to model the capabilities for understanding natural language.

At the beginning you can choose between the different supported
transformer architectures. Depending on the architecture, you
have different options determining the shape of your neural network. For this 
vignette we use a BERT (Devlin et al. 2019) model which can be created
with the function `create_bert_model`.

```{r, include = TRUE, eval=FALSE}
create_bert_model(
    ml_framework=aifeducation_config$get_framework(),
    model_dir = "my_own_transformer",
    vocab_raw_texts=example_data$text,
    vocab_size=30522,
    vocab_do_lower_case=FALSE,
    max_position_embeddings=512,
    hidden_size=768,
    num_hidden_layer=12,
    num_attention_heads=12,
    intermediate_size=3072,
    hidden_act="gelu",
    hidden_dropout_prob=0.1,
    sustain_track=TRUE,
    sustain_iso_code="DEU",
    sustain_region=NULL,
    sustain_interval=15,
    trace=TRUE)
```

First, the function receives the machine learning framework you chose at the 
start of the session. However, you can change this by setting `ml_framework="tensorflow"`
or by `ml_framework="pytorch"`.

For this function to work, you must provide a path to a directory where
your new transformer should be saved (`model_dir`). Furthermore, you must provide raw
texts. These texts are **not** used for training the transformer but for
training the vocabulary. The maximum size of the vocabulary is
determined by `vocab_size`. Modern tokenizers such as
*WordPiece* (Wu et al. 2016) use algorithms that splits tokens into
smaller elements, allowing them to build a huge number of words with a
small number of elements. Thus, even with only small number of about
30,000 tokens, they are able to represent a very large number of words.

The other parameters allow you to customize your BERT model. For
example, you could increase the number of hidden layers from 12 to 24 or
reduce the hidden size from 768 to 256, allowing you to build and to
test larger or smaller models.

> The vignette [Optimal model configuration](model_configuration.html) provides
details on how to configurate a base model.

Please note that with `max_position_embeddings` you determine how many
tokens your transformer can process. If your text has more tokens, these tokens 
are ignored. However, if you would
like to analyze long documents, please avoid to increase this number too
significantly because the computational time does not increase in a
linear way but quadratic (Beltagy, Peters & Cohan 2020). For long
documents you can use another architecture of BERT (e.g. Longformer from
Beltagy, Peters & Cohan 2020) or split a long document into several
chunks which are used sequentially for classification (e.g., Pappagari
et al. 2019). Using chunks is supported by *aifedcuation* for all models.

Since creating a transformer model is energy consuming *aifeducation* allows 
you to estimate its ecological impact with help of the python library
`codecarbon`. Thus, `sustain_track` is set to `TRUE` by default. If you
use the sustainability tracker you must provide the alpha-3 code
for the country where your computer is located (e.g., "CAN"="Canada", "Deu"="Germany").
A list with the codes
can be found on [wikipedia](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3).
The reason is that different countries use different sources and techniques for 
generating their energy resulting in a specific impact on CO2 emissions. For USA
and Canada you can additionally specify a region by setting `sustain_region`. 
Please refer to the documentation of codecarbon for more information.

After calling the function, you will find your new model in your model
directory. 

## 3.3 Train/Tune a Base Model
If you would like to train a new base model (see section 3.2) for the
first time or want to adapt a pre-trained model to a domain-specific
language or task, you have to can call the corresponding training function. In case
of a BERT model this is `train_tune_bert_model()`.

```{r, include = TRUE, eval=FALSE}
train_tune_bert_model(
  ml_framework=aifeducation_config$get_framework(),
  output_dir = "my_own_transformer_trained",
  model_dir_path = "my_own_transformer",
  raw_texts = example_data$text,
  p_mask=0.15,
  whole_word=TRUE,
  val_size=0.1,
  n_epoch=1,
  batch_size=12,
  chunk_size=250,
  n_workers=1,
  multi_process=FALSE,
  sustain_track=TRUE,
  sustain_iso_code="DEU",
  sustain_region=NULL,
  sustain_interval=15,
  trace=TRUE)
```

Here it is important that you provide the path to the directory where
your new transformer is stored. Furthermore, it is important that you
provide *another* directory where your trained transformer should be
saved to avoid reading and writing collisions.

Now, the provided raw data is used to train your model. In the case of a BERT model
the learning objective is *Masked Language Modeling*. Other models may use other
learning objectives. Please refer to the documentation for more detials on every model.

First, you can set the length of token sequences with
`chunk_size`. With `whole_word` you can choose between masking single
tokens or masking complete words (Please remember that modern tokenizers
split words into several tokens. Thus, tokens and words are not forced
to match each other directly). With `p_mask` you can determine how many
tokens should be masked. Finally, with `val_size`, you set how many
chunks of tokens should be used for the validation sample.

Please remember to set the correct alpha-3 code for tracking the ecological impact
of training your model (`sustain_iso_code`).

If you work on a machine and your graphic device only has small memory,
please reduce the batch size significantly. We also recommend to change
the usage of memory with `set_config_gpu_low_memory()` at the beginning of the session
if you use *tensorflow* as framework. 

After the training finishes, you can find the transformer ready to use
in your output_directory. Now you are able to create a text embedding
model.

Again you can change the machine learning framework by setting `ml_framework="tensorflow"`
or by `ml_framework="pytorch"`. If you do not change this argument the framework
you chose at the beginning is used.

# 4 Text Embedding Models
## 4.1 Introduction
The text embedding model is the interface to *R* in *aifeducation*. In
order to create a new model, you need a base model that provides the
ability to understand natural language. A text embedding model is stored as an object of
class `TextEmbeddingModel`. This object contains all relevant
information for transforming raw texts into a numeric representation
that can be used for machine learning.

In *aifedcuation*, the transformation of raw texts into numbers is a
separate step from downstream tasks such as classification. This is to
reduce computational time on machines with low performance. By
separating text embedding from other tasks, the text embedding has to be
calculated only once and can be used for different tasks at the same
time. Another advantage is that the training of the downstream tasks
involves only the downstream tasks an not the parameters of the
embedding model, making training less time-consuming, thus decreasing
computational intensity. Finally, this approach allows the analysis of
long documents by applying the same algorithm to different parts.

The text embedding model provides a unified interface: After creating
the model with different methods, the handling of the model is always
the same.

## 4.2 Create a Text Embedding Model
First you have to choose the base model that should form the foundation
of your new text embedding model.Since we use a BERT model in our example, 
we have to set `method = "bert"`.

```{r, include = TRUE, eval=FALSE}
bert_modeling<-TextEmbeddingModel$new(
  ml_framework=aifeducation_config$get_framework(),
  model_name="bert_embedding",
  model_label="Text Embedding via BERT",
  model_version="0.0.1",
  model_language="english",
  method = "bert",
  max_length = 512,
  chunks=4,
  overlap=30,
  emb_layer_min="middle",
  emb_layer_max="2_3_layer",
  emb_pool_type="average",
  model_dir="my_own_transformer_trained"
  )
```

Next, you have to provide the directory where your base
model is stored. In this example this would be
`model_dir="my_own_transformer_trained`. Of course you can use
any other pre-trained model from Huggingface which addresses your needs.

Using a BERT model for text embedding is not a problem since your text
does not provide more tokens than the transformer can process. This
maximal value is set in the configuration of the transformer (see
section 3.2). If the text produces more tokens the last tokens are
ignored. In some instances you might want to analyze long texts. In
these situations, reducing the text to the first tokens (e.g. only the
first 512 tokens) could result in a problematic loss of information. To
deal with these situations you can configure a text embedding model in
*aifecuation* to split long texts into several chunks which are
processed by the base model. The maximal number of chunks is set with
`chunks`. In our example above, the text embedding model would split a
text consisting of 1024 tokens into two chunks with every chunk
consisting of 512 tokens. For every chunk a text embedding is
calculated. As a result, you receive a sequence of embeddings. The first
embeddings characterizes the first part of the text and the second
embedding characterizes the second part of the text (and so on). Thus,
our example text embedding model is able to process texts with about
4*512=2048 tokens. This approach is inspired by the work by Pappagari
et al. (2019).

Since transformers are able to account for the context, it may be useful
to interconnect every chunk to bring context into the calculations. This
can be done with `overlap` to determine how many tokens of the end of a
prior chunk should be added to the next. In our example the last 30 tokens of the prior 
chunks are added at the beginning of the following chunk. This can help to
add the correct context of the text sections into the analysis. Altogether,
this example model can analyse a maximum of 512+(4-1)*(512-30)=1958 tokens of a 
text.

Finally, you have to decide from which hidden layer or layers the embeddings 
should be drawn. With `emb_layer_min` and `emb_layer_max` you can decide over which layers
the average value for every token should be calculated. Please note that the calculation
considers all layers between `emb_layer_min` and `emb_layer_max`. 
In their initial work, Devlin et al. (2019) used
the hidden states of different layers for classification.

With `emb_pool_type` you decide which tokens are used for pooling within every layer. 
In the case of `emb_pool_type="cls"` only the cls token is used. In the case of
`emb_pool_type="average"` all tokens within a layer are averaged except padding tokens.

> The vignette [Optimal model configuration](model_configuration.html) provides
details on how to configurate a text embedding model.

After deciding about the configuration, you can use your model.

> **Note:** With version 0.3.1 of aifeducation every transformer can be used with both
machine learning frameworks. Even the pre-trained weights can be used across
backends. However, in the future models my be implemented that are available only
for a specific framework. 

## 4.3 Transforming Raw Texts into Embedded Texts

To transform raw text into a numeric
representation you only have to use the `embed` method of your model. To
do this, you must provide the raw texts to `raw_text`. In addition, it
is necessary that you provide a character vector containing the ID of
every text. The IDs must be unique.

```{r, include = TRUE, eval=FALSE}
bert_embeddings<-bert_modeling$embed(
  raw_text=example_data$text,
  doc_id=example_data$id, 
  trace = TRUE)
```

The method `embed`creates an object of class `EmbeddedText`. This is
just a array consisting the embedding of every text. The first dimension of 
the array refers to specific texts, the second dimension refers to chunks/sequences, 
and the third dimension refers to the features. 

With the embedded texts you now have the input to train a new classifier
or to apply a pre-trained classifier for predicting categories/classes.
In the next chapter we will show you how to use these classifiers. But
before we start, we will show you how to save and load your model.

## 4.4 Saving and Loading Text Embedding Models

Saving a created text embedding model is very easy in *aifeducation* by using the
function `save_ai_model`. This function provides a unique interface for all
text embedding models. For saving your work you can pass your model to `model` and
the directory where to save the model to `model_dir`. Please do only pass 
the path of a directory and not the path of a file to this function. Internally
the function creates a new folder in the directory where all files belonging
to a model are stored.

```{r, include = TRUE, eval=FALSE}
save_ai_model(
  model=bert_modeling, 
  model_dir="text_embedding_models",
  dir_name="model_transformer_bert",
  save_format="default",
  append_ID=FALSE)
```

As you can see the text embedding model is saved within a
directory named "text_embedding_models". Within this directory the function
creates a unique folder for every model if it does not exist. The name of this folder is
specified with `dir_name`.

If you set `dir_name=NULL` and `append_ID=FALSE` the the name of the folder is created 
by using the models' name. 
If you change the argument `append_ID` to `append_ID=TRUE` and set `dir_name=NULL`
the unique ID of the model is added to the directory. The ID is added automatically to ensure
that every model has a unique name. This is important if you would like to share your 
work with other persons. 

> Since the files are stored with
a special structure please do **not** change the files manually.

If you want to load your model, just call the function `load_ai_model` and you can continue
using your model.

```{r, include = TRUE, eval=FALSE}
bert_modeling<-load_ai_model(
  model_dir="text_embedding_models/model_transformer_bert",
  ml_framework=aifeducation_config$get_framework())
```

With `ml_framework` you can decide which framework the model should use. 
If you set `ml_framework="auto"` the models will be initialized with the same framework
during saving the model. Please note
that at the moment all implemented text embedding models can be used with both frameworks.
However, this may change in the future. 

## 4.5 Sustainability

In the case the underlying model was trained with an active sustainability tracker
(section 3.2 and 3.3) you can receive a table showing you the energy consumption, CO2 emissions,
and hardware used during training by calling the method `get_sustainability_data()`.
For our example this would be `bert_modeling$get_sustainability_data()`.

# 5 Classifiers
## 5.1 Create a Classifier
Classifiers are built on top of a text embedding model. You can create
a new classifier by calling `TEClassifierRegular$new()`. The `TE` in the object class
refers to the idea that the classifiers uses text embeddings instead of raw texts.

```{r, include = TRUE, eval=FALSE}
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id

classifier<-TEClassifierRegular$new(
  ml_framework=aifeducation_config$get_framework(),
  name="movie_review_classifier",
  label="Classifier for Estimating a Postive or Negative Rating of Movie Reviews",
  text_embeddings=bert_embeddings,
  targets=example_targets,
  use_fe=TRUE,
  fe_features=128,
  fe_method="lstm",
  fe_noise_factor=0.2,
  hidden=c(40,20),
  rec=c(40,40),
  rec_type="gru",
  self_attention_heads=1,
  intermediate_size=NULL,
  attention_type="fourier",
  add_pos_embedding=FALSE,
  rec_dropout=0.3,
  repeat_encoder=0,
  dense_dropout=0.3,
  recurrent_dropout=0.1,
  encoder_dropout=0.3,
  optimizer="adam")
```

Similar to the text embedding model you should provide a name (`name`)
and a label (`label`) for your new classifier. With `text_embeddings`
you have to provide an embedded text. The embedded texts is created with a text
embedding model as described in section 4. We here
continue our example and use the embedding produced by our BERT model.

`targets` takes the target data for the supervised learning. Please do
not omit cases which have no category/class since they can be used with
a special training technique we will show you later. It is very
important that you provide the target data as factors. Otherwise an
error will occur. It is also important that you name your elements of the factor. That
is, the entries of the factor mus have names that correspond to the IDs
of the corresponding texts. Without these names the method cannot match
the input data (text embeddings) to the target data.

With `use_fe` you can request a feature extractor. The aim of the feature extractor
is to reduce the number of dimensions of the text embeddings and saving as much
information as possible. Reducing the number of dimensions can speed up training
and inference and at the same time increase the performance of your classifier.
For more details please refer to vignette [Optimal model configuration](model_configuration.html).
With `fe_features` you can decide to which number of dimensions the text embeddings
should be compressed. `fe_method` allows you to select a model for the reduction
("lstm" or "dense"). Sine all feature extractors in *aifeducation* are based
on denoising autoencoder you can decide how many noise should be added during 
training the extractor (`fe_noise_factor`).

With the other parameters you decide about the structure of your
classifier. Figure 4 illustrates this.

![Figure 4: Overview of Possible Structure of a
Classifier](img_articles/classif_fig_04.png){width="100%"}

`hidden` takes a vector of integers, determining the number of layers
and the number of neurons. In our example, there two dense layers. The first
with 40 and the second with 20 neurons.
`rec` also takes a vector of integers determining the number and size of
the recurrent layers. In this example, we use two layer with
40 neurons each. With `rec_type` you can choose between two types of recurrent layers.
`rec_type="gru"` implements a Gated Recurrent Unit (GRU) network and 
`rec_type="lstm"` implements a Long Short-Term Memory layer.

Since the classifiers in *aifeducation* use a standardized scheme for
their creation, dense layers are used after the gru layers. If you want
to omit gru layers or dense layers, set the corresponding argument to
`NULL`.

If you use a text embedding model that processes more than one chunk we
would like to recommend to use recurrent layers since they are able to
use the sequential structure of your data. In all other cases you can
rely on dense layers only.

If you use text embeddings with more than one chunk, it is a good idea to try self-attention
layering in order to take the context of all chunks into account. To add self-attention
you have two choices:

-   You can use the attention mechanism used in classic transformer models as 
    multihead attention (Vaswani et al. 2017). For this variant you have to set
    `attention_type="multihead"`, `repeat_encoder` to a value of at least 1, and
    `self_attention_heads` to a value of at least 1. 
    
-   Furthermore you can use the attention mechanism described in Lee-Thorp et al. (2021) 
    of the FNet model which allows much fast computations at low accuracy costs. To use
    this kind of attention you have to set `attention_type="fourier` and 
    `repeat_encoder` to a value of at least 1.

With `repeat_encoder` you can chose how many times an encoder layer should be added.
The encoder is implemented as described by Chollet, Kalinowski, and
Allaire (2022, pp. 373) for both variants of attention. In our example we have only
300 cases altogether and only 4 chunks. Thus, we do not use any encoder layer.

You can further extend the abilities of your network by adding positional embeddings.
Positional embeddings take care for the order of your chunks. Thus, adding such a 
layer may increase performance if the order of information is important. You can 
add this layer by setting `add_pos_embedding=TRUE`. The layer is created as
described by Chollet, Kalinowski, and Allaire (2022, pp. 378).

> The vignette [Optimal model configuration](model_configuration.html) provides
details on how to configurate a classifier.

Masking, normalization, and the creation of the input layer as well as
the output layer are done automatically.

After you have created a new classifier, you can begin training.

> **Note:** In contrast to the text embedding models your decision about the 
machine learning framework is more important since the classifier can only be used
with the framework you created and trained the model. 

## 5.2 Training a Classifier
To start the training of your classifier, you have to call the `train`
method. Similarly, for the creation of the classifier, you must provide
the text embedding to `data_embeddings` and the categories/classes as
target data to `data_targets`. Please remember that `data_targets`
expects a **named** factor where the names correspond to the IDs of the
corresponding text embeddings. Text embeddings and target data that
cannot be matched are omitted from training.

To train a classifier, it is necessary that you provide a path to
`dir_checkpoint`. This directory stores the best set of weights during
each training epoch. After training, these weights are automatically
used as final weights for the classifier.

For performance estimation, training splits the data into several chunks
based on cross-fold validation. The number of folds is set with
`data_folds`. In every case, one fold is not used for training
and serves as a *test* sample. The remaining data is used to create a
*training* and a *validation* sample. 
The percentage of cases within each fold used as a validation sample is determined with
`data_val_size`. This sample is used to determine the state of the model that generalizes best.
All performance values saved in the trained classifier refer to the test sample. This data has never
been used during training and provides a more realistic estimation of a
classifier`s performance.

```{r, include = TRUE, eval=FALSE}
example_targets<-as.factor(example_data$label)
names(example_targets)=example_data$id

classifier$train(
   data_embeddings = bert_embeddings,
   data_targets = example_targets,
   data_folds=5,
   data_val_size=0.25,
   fe_epochs=1000,
   fe_val_size=0.25,
   balance_class_weights=TRUE,
   balance_sequence_length=TRUE,
   use_sc=TRUE,
   sc_method="dbsmote",
   sc_min_k=1,
   sc_max_k=10,
   use_pl=TRUE,
   pl_max_steps=5,
   pl_max=1.00,
   pl_anchor=1.00,
   pl_min=0.00,
   sustain_track=TRUE,
   sustain_iso_code="DEU",
   sustain_region=NULL,
   sustain_interval=15,
   epochs=60,
   batch_size=32,
   dir_checkpoint="training/classifier",
   trace=TRUE,
   keras_trace=2,
   pytorch_trace=1)
```
```{r, include = FALSE, eval=TRUE}
classifier=vignette_classifier
```

You can further modify the training process with different arguments. 
With `balance_class_weights=TRUE` the absolute frequencies of the classes/categories
is adjusted according to the 'Inverse Class Frequency' method. This option
should be activated if you have to deal with imbalanced data.
    
With `balance_sequence_length=TRUE` you can 
increase performance if you have to deal with texts that differ in their lengths 
and have an imbalanced frequency. If this option is enabled, the loss is adjusted 
to the absolute frequencies of length of your texts according to the 'Inverse Class Frequency' method. 
    
`epochs` determines the maximal number of epochs. During training, the model
with the best balanced accuracy is saved and used.
    
`batch_size`sets the number of cases that should be processed
simultaneously. Please adjust this value to your machine's
capacities. Please note that the batch size can have an impact on
the classifier's performance.

Since *aifedcuation* tries to address the special needs in educational
and social science, some special training steps are integrated into this
method.

-   **Synthetic Cases:** In case of imbalanced data, it is
    recommended to set `use_sc=TRUE`. Before training, a number of
    synthetic units is created via different techniques. Currently you
    can request *Basic Synthetic Minority Oversampling Technique*,
    *Density-Bases Synthetic Minority Oversampling Technique*, and
    *Adaptive Synthetic Sampling Approach for Imbalanced Learning*. The
    aim is to create new cases that fill the gap to the majority class.
    Multi-class problems are reduced to a two class problem (class under
    investigation vs. each other) for generating these units. 
    If the technique allows to set the number of neighbors during generation, you
    can configure the data generation with `sc_min_k` and `sc_max_k`. 
    The synthetic cases for every class a generated for all *k* between `sc_min_k` and
    `sc_max_k`. Every *k* contributes proportional to the synthetic cases.
    To apply the addition of synthetic data, you have to set `use_sc=TRUE`

-   **Pseudo-Labeling:** This technique is relevant if you have
    labeled target data and a large number of unlabeled target data.
    With the different parameter starting with "pl_", you can configure the process
    of pseudo-labeling. Implementation of pseudo-labeling is based on 
    Cascante-Bonilla et al. (2020). To apply
    pseudo-labeling, you have to set `use_pl=TRUE`. `pl_max=1.00`, 
    `pl_anchor=1.00`, and `pl_min=0.00` are used to
    describe the certainty of a prediction. 0 refers to random guessing
    while 1 refers to perfect certainty. `pl_anchor` is used as a reference
    value. The distance to `pl_anchor` is calculated for every case. Then,
    they are sorted with an increasing distance from `pl_anchor`. The proportion
    of added pseudo-labeled data into training increases with every step. 
    The maximum number of steps is determined with `pl_max_steps`. 

Figure 5 illustrates the training loop for the cases that all options are set to `TRUE`.

![Figure 5: Overview of the Steps to Perform a
Classification](img_articles/classif_fig_05.png){width="100%"}

The example above applies the generation of synthetic cases and the algorithm 
proposed by Cascante-Bonilla et al. (2020). For every fold the training starts 
with generating synthetic cases to fill
the gab between the classes and the majority class. After this an initial training
of the classifiers starts. The trained classifier is used to predict pseudo-labels
for the unlabeled part of the data and adds 20% of the cases
with the highest certainty for their pseudo-labels to the training data set. 
Now new synthetic cases are generated based on both the labeled data and the new
added pseudo-labeled data. The classifier is re-initialized and trained again. After training, the
classifier predicts the potential labels of *all* originally unlabeled
data and adds 40% of the pseudo-labeled data to the training data with the highest
certainty. Again new synthetic cases are generated on both the labeled and added pseudo-labeled
data. The model is again re-initialized and trained again until the maximum number of
steps for pseudo labeling (`pl_max_steps`) is reached. After this, the logarithm is
restated for the next fold until the number of folds (`data_folds`) is reached.
All of these steps are used only to estimate the performance of the classifier to
evaluate for the classifier unknown data.

The last phase of the training begins after the last fold. In the final training
the data set is split only into a train and validation set without a test set to provide
the maximal amount of data for the best performance in final training. 

In the case options like the generation of synthetic cases (`use_sc`) or 
pseudo-labeling (`use_pl`) are disabled the training process is shorter.

In the case your classifier uses a feature extractor you can configure the
training process with `fe_epochs` and `fe_val_size`. The first argument 
determines the maximal number of training steps while the last determines
the number of text embeddings used as a validation sample. The model
with the lowest loss on the validation sample is applied for compressing
the text embeddings.

Since training a neural net is energy consuming *aifeducation* allows 
you to estimate its ecological impact with help of the python library
`codecarbon`. Thus, `sustain_track` is set to `TRUE` by default. If you
use the sustainability tracker you must provide the alpha-3 code
for the country where your computer is located (e.g., "CAN"="Canada", "Deu"="Germany").
A list with the codes
can be found on [wikipedia](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3).
The reason is that different countries use different sources and techniques for 
generating their energy resulting in a specific impact on CO2 emissions. For USA
and Canada you can additionally specify a region by setting `sustain_region`. 
Please refer to the documentation of codecarbon for more information.

Finally, `trace`, `pytorch_trace`, and `keras_trace` allow you to control
how much information about the training progress is printed to the
console. Please note that training the classifier can take some time.

Please note that after performance estimation, the final training of the
classifier makes use of all data available. That is, the test sample is
left empty.

## 5.3 Evaluating Classifier's Performance

After finishing training, you can evaluate the performance of the
classifier. For every fold, the classifier is applied to the test sample
and the results are compared to the true categories/classes. Since the
test sample is never part of the training, all performance measures
provide a more realistic idea of the classifier`s performance.

To support researchers in judging the quality of the predictions,
*aifeducation* utilizes several measures and concepts from content
analysis. These are

-   Iota Concept of the Second Generation (Berding & Pargmann 2022)
-   Krippendorff's Alpha (Krippendorff 2019)
-   Percentage Agreement
-   Gwet's AC1/AC2 (Gwet 2014)
-   Kendall's coefficient of concordance W
-   Cohen's Kappa unweighted
-   Cohen's Kappa with equal weights
-   Cohen's Kappa with squared weights
-   Fleiss' Kappa for multiple raters without exact estimation

You can access the concrete values by accessing the field `reliability`
which stores all relevant information. In this list you will find the
reliability values for every fold. In addition, the reliability of every step within
pseudo-labeling is reported.

The central estimates for the reliability values can be found via
`reliability$test_metric_mean`. In our example this would be:

```{r, include = TRUE, eval=TRUE}
classifier$reliability$test_metric_mean
```


Of particular interest
are the values for alpha from the Iota Concept since they represent a
measure of reliability which is independent from the frequency
distribution of the classes/categories. The alpha values describe the
probability that a case of a specific class is recognized as that
specific class. As you can see, compared to the baseline model, applying
*Balanced Synthetic Cases increased* increases the minimal value of
alpha, reducing the risk to miss cases which belong to a rare class (see
row with "BSC"). On the contrary, the alpha values for the major category
decrease slightly, thus losing its unjustified bonus from a high number of
cases in the training set. This provides a more realistic
performance estimation of the classifier.

An addition, standard measures from machine learning are reported. These are

-   Precision
-   Recall
-   F1-Score

You can acces these values as follows:
```{r, include = TRUE, eval=TRUE}
classifier$reliability$standard_measures_mean
```
Finally, you can plot a coding stream scheme showing how the cases of
different classes are labeled. Here we use the package *iotarelr*.

```{r, include = TRUE, eval=TRUE, fig.cap="Figure 6: Coding Stream of the Classifier",fig.align="center"}
library(iotarelr)
iotarelr::plot_iota2_alluvial(classifier$reliability$iota_object_end_free)
```

Here you can see that a small number of negative reviews is treated
as a good review while a larger number of positive reviews is
treated as a bad review. Thus, the data for the major class (negative
reviews) is more reliable and valid as the the data for the minor class
(positive reviews).

Evaluating the performance of a classifier is a complex task and and
beyond the scope of this vignette. Instead, we would like to refer to the
cited literature of content analysis and machine learning if you would
like to dive deeper into this topic.

## 5.4 Sustainability

In the case the classifier was trained with an active sustainability tracker
you can receive information on sustainability by calling `classifier$get_sustainability_data()`.

```{r, include = TRUE, eval=TRUE}
classifier$get_sustainability_data()
```

## 5.5 Saving and Loading a Classifier

If you have created a classifier, saving and loading is very easy. The process
for saving a model is similar to the process for text embedding models. You only have 
to pass the model and a directory path to the function `save_ai_model`.

```{r, include = TRUE, eval=FALSE}
save_ai_model(
  model=classifier,
  model_dir="classifiers",
  dir_name="movie_classifier",
  save_format = "default",
  append_ID=FALSE)
```

In contrast to text embedding models you can specify the additional argument `save_format`.
In the case of pytorch models this arguments allows you to choose between
`save_format = "safetensors"` and `save_format = "pt"`.
We recommend to chose `save_format = "safetensors"` since this is a safer method
to save your models.
In the case of tensorflow models this argument allows you to choose between
`save_format = "keras"`, `save_format = "tf"` and  `save_format = "h5"`.
We recommend to chose `save_format = "keras"` since this is the recommended format
by keras. 
If you set `save_format = "default"` .safetensors is used for pytorch models and
.keras is used for tensorflow models.

If you would like to load a model you can call the function `load_ai_model`.
```{r, include = TRUE, eval=FALSE}
classifier<-load_ai_model(
  model_dir="classifiers/movie_classifier")
```

> **Note:** Classifiers depend on the framework which was used during creation. 
Thus, a classifier is always initalized with its original framework. The argument
`ml_framework` has no effect. 

## 5.6 Predicting New Data

If you would like to apply your classifier to new data, two steps are
necessary. You must first transform the raw text into a numerical
expression by using *exactly* the same text embedding model that was
used for training your classifier (see section 4). In the case of our example classifier we
use our BERT model.

```{r, include = TRUE, eval=FALSE}
# If our mode is not loaded
bert_modeling<-load_ai_model(
  model_dir="text_embedding_models/bert_embedding")

# Create a numerical representation of the text
text_embeddings<-bert_modeling$embed(
  raw_text = textual_data$texts,
  doc_id = textual_data$doc_id,
  batch_size=8,
  trace=TRUE)
```

To transform raw texts into a numeric representation 
just pass the raw texts and the IDs of every text to the method `embed` of the loaded model.
This is very easy if you used the package [readtext](https://cran.r-project.org/package=readtext)
to read raw text from disk, since the object resulting from `readtext` always 
stores the texts in the column "texts" and the IDs in the column "doc_id".

Depending on your machine, embedding raw texts may take some time. In case
you use a machine with a graphic device, it is possible that an "out of memory"
error occurs. In this case reduce the batch size. If the error still occurs,
restart the *R* session, switch to cpu-only mode *directly* after
loading the libraries with `aifeducation::set_config_cpu_only()` and request the 
embedding again.

In the example above, the text embeddings are stored in `text_embedding`. Since
embedding texts may take some time, it is a good idea to save the embeddings 
for future analysis (use the `save` function of *R*). This allows you to load
the embeddings without the need to apply the text embedding model on the same 
raw texts again.

The resulting object can then be passed to
the method `predict` of our classifier and you will get the predictions together with an
estimate of certainty for each class/category.

```{r, include = TRUE, eval=FALSE}
# If your classifier is not loaded
classifier<-load_ai_model(
  model_dir="classifiers/movie_review_classifier")

# Predict the classes of new texts
predicted_categories<-classifier$predict(
  newdata = text_embeddings,
  batch_size=8,
  verbose=0)
```

After the classifier finishes the prediction, the estimated categories/classes
are stored as `predicted_categories`. This object is a `data.frame` containing
texts' IDs in the rows and the probabilities of the different categories/classes
in the columns. The last column with the name `expected_category` represents the 
category which is assigned to a text due the highest probability.

The estimates can be used in further analysis with common methods of the educational
and social sciences such as correlation analysis, regression analysis, structural 
equation modeling, latent class analysis or analysis of variance.

# References

Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The
Long-Document Transformer. <https://doi.org/10.48550/arXiv.2004.05150>

Berding, F., & Pargmann, J. (2022). Iota Reliability Concept of the
Second Generation. Berlin: Logos. <https://doi.org/10.30819/5581>

Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, A., &
Rebmann, K. (2022). Performance and Configuration of Artificial
Intelligence in Educational Settings.: Introducing a New Reliability
Concept Based on Content Analysis. Frontiers in Education, 1--21.
<https://doi.org/10.3389/feduc.2022.818365>

Campesato, O. (2021). Natural Language Processing Fundamentals for
Developers. Mercury Learning & Information.
<https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713>

Cascante-Bonilla, P., Tan, F., Qi, Y. & Ordonez, V. (2020). Curriculum
Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning.
<https://doi.org/10.48550/arXiv.2001.06001>

Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning with
R (Second edition). Manning Publications Co.
<https://learning.oreilly.com/library/view/-/9781633439849/?ar>

Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer:
Filtering out Sequential Redundancy for Efficient Language Processing.
<https://doi.org/10.48550/arXiv.2006.03236>

Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, & T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171--4186).
Association for Computational Linguistics.
<https://doi.org/10.18653/v1/N19-1423>

Gwet, K. L. (2014). Handbook of inter-rater reliability: The definitive
guide to measuring the extent of agreement among raters (Fourth
edition). Gaithersburg: STATAXIS.

He, P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced
BERT with Disentangled Attention.
<https://doi.org/10.48550/arXiv.2006.03654>

Krippendorff, K. (2019). Content Analysis: An Introduction to Its
Methodology (4th ed.). Los Angeles: SAGE.

Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing
in action: Understanding, analyzing, and generating text with Python.
Shelter Island: Manning.

Larusson, J. A., & White, B. (Eds.). (2014). Learning Analytics: From
Research to Practice. New York: Springer.
<https://doi.org/10.1007/978-1-4614-3305-7>

Lee, D.‑H. (2013). Pseudo-Label: The Simple and Efficient
Semi-Supervised Learning Method for Deep Neural Networks. CML 2013
Workshop: Challenges in Representation Learning.

Lee-Thorp, J., Ainslie, J., Eckstein, I. & Ontanon, S. (2021). 
FNet: Mixing Tokens with Fourier Transforms. <https://doi.org/10.48550/arXiv.2105.03824>

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: A Robustly
Optimized BERT Pretraining Approach.
<https://doi.org/10.48550/arXiv.1907.11692>

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D.,
Ng, A. Y., & Potts, C. (2011). Learning Word Vectors for Sentiment
Analysis. In D. Lin, Y. Matsumoto, & R. Mihalcea (Eds.),
Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies (pp. 142–150).
Association for Computational Linguistics.
<https://aclanthology.org/P11-1015>

Papilloud, C., & Hinneburg, A. (2018). Qualitative Textanalyse mit
Topic-Modellen: Eine Einführung für Sozialwissenschaftler. Wiesbaden: Springer.
<https://doi.org/10.1007/978-3-658-21980-2>

Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N.
(2019). Hierarchical Transformers for Long Document Classification. In
2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)
(pp. 838--844). IEEE. <https://doi.org/10.1109/ASRU46091.2019.9003958>

Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global
Vectors for Word Representation. Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing.
<https://aclanthology.org/D14-1162.pdf>

Schreier, M. (2012). Qualitative Content Analysis in Practice. Los Angeles: SAGE.

Tunstall, L., Werra, L. von, Wolf, T., & Géron, A. (2022). Natural
language processing with transformers: Building language applications
with hugging face (Revised edition). Heidelberg: O'Reilly.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need.
<https://doi.org/10.48550/arXiv.1706.03762>

Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A.,
Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa,
H., . . . Dean, J. (2016). Google's Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation.
<https://doi.org/10.48550/arXiv.1609.08144>

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TEClassifierProtoNet.R
\name{TEClassifierProtoNet}
\alias{TEClassifierProtoNet}
\title{Text embedding classifier with a ProtoNet}
\value{
Objects of this class are used for assigning texts to classes/categories. For
the creation and training of a classifier an object of class \link{EmbeddedText} and a \code{factor}
are necessary. The object of class \link{EmbeddedText} contains the numerical text
representations (text embeddings) of the raw texts generated by an object of class
\link{TextEmbeddingModel}. The \code{factor} contains the classes/categories for every
text. Missing values (unlabeled cases) are supported. For predictions an object of class
\link{EmbeddedText} has to be used which was created with the same text embedding model as
for training.
}
\description{
Abstract class for neural nets with 'keras'/'tensorflow' and
'pytorch'.

This object represents in implementation of a prototypical network for
few-shot learning as described by Snell, Swersky, and Zemel (2017). The network
uses a multi way contrastive loss described by Zhang et al. (2019). The network
learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018)
}
\references{
Oreshkin, B. N., Rodriguez, P. & Lacoste, A. (2018). TADAM:
Task dependent adaptive metric for improved few-shot learning. https://doi.org/10.48550/arXiv.1805.10123

Snell, J., Swersky, K. & Zemel, R. S. (2017). Prototypical Networks
for Few-shot Learning. https://doi.org/10.48550/arXiv.1703.05175

Zhang, X., Nie, J., Zong, L., Yu, H. & Liang, W. (2019). One Shot
Learning with Margin. In Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang & S.-J. Huang (Eds.),
Lecture Notes in Computer Science. Advances in Knowledge Discovery and Data Mining (Vol. 11440, pp. 305â€“317).
Springer International Publishing. https://doi.org/10.1007/978-3-030-16145-3_24
}
\seealso{
Other Classification: 
\code{\link{DataManagerClassifier}},
\code{\link{TEClassifierRegular}},
\code{\link{TextEmbeddingClassifierNeuralNet}}
}
\concept{Classification}
\section{Super class}{
\code{\link[aifeducation:TEClassifierRegular]{aifeducation::TEClassifierRegular}} -> \code{TEClassifierProtoNet}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-TEClassifierProtoNet-new}{\code{TEClassifierProtoNet$new()}}
\item \href{#method-TEClassifierProtoNet-train}{\code{TEClassifierProtoNet$train()}}
\item \href{#method-TEClassifierProtoNet-embed}{\code{TEClassifierProtoNet$embed()}}
\item \href{#method-TEClassifierProtoNet-clone}{\code{TEClassifierProtoNet$clone()}}
}
}
\if{html}{\out{
<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="check_embedding_model"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-check_embedding_model'><code>aifeducation::TEClassifierRegular$check_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="extract_features"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-extract_features'><code>aifeducation::TEClassifierRegular$extract_features()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_documentation_license"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_documentation_license'><code>aifeducation::TEClassifierRegular$get_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_ml_framework"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_ml_framework'><code>aifeducation::TEClassifierRegular$get_ml_framework()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_model_description"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_model_description'><code>aifeducation::TEClassifierRegular$get_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_model_info"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_model_info'><code>aifeducation::TEClassifierRegular$get_model_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_package_versions"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_package_versions'><code>aifeducation::TEClassifierRegular$get_package_versions()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_publication_info"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_publication_info'><code>aifeducation::TEClassifierRegular$get_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_software_license"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_software_license'><code>aifeducation::TEClassifierRegular$get_software_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_sustainability_data"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_sustainability_data'><code>aifeducation::TEClassifierRegular$get_sustainability_data()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="get_text_embedding_model"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-get_text_embedding_model'><code>aifeducation::TEClassifierRegular$get_text_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="load_model"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-load_model'><code>aifeducation::TEClassifierRegular$load_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="predict"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-predict'><code>aifeducation::TEClassifierRegular$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="save_model"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-save_model'><code>aifeducation::TEClassifierRegular$save_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="set_documentation_license"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-set_documentation_license'><code>aifeducation::TEClassifierRegular$set_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="set_model_description"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-set_model_description'><code>aifeducation::TEClassifierRegular$set_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="set_publication_info"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-set_publication_info'><code>aifeducation::TEClassifierRegular$set_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="set_software_license"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-set_software_license'><code>aifeducation::TEClassifierRegular$set_software_license()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TEClassifierProtoNet-new"></a>}}
\if{latex}{\out{\hypertarget{method-TEClassifierProtoNet-new}{}}}
\subsection{Method \code{new()}}{
Creating a new instance of this class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TEClassifierProtoNet$new(
  ml_framework = aifeducation_config$get_framework(),
  name = NULL,
  label = NULL,
  text_embeddings = NULL,
  use_fe = TRUE,
  fe_features = 128,
  fe_method = "lstm",
  fe_noise_factor = 0.2,
  targets = NULL,
  hidden = c(128),
  rec = c(128),
  rec_type = "gru",
  rec_bidirectional = FALSE,
  embedding_dim = 3,
  self_attention_heads = 0,
  intermediate_size = NULL,
  attention_type = "fourier",
  add_pos_embedding = TRUE,
  rec_dropout = 0.1,
  repeat_encoder = 1,
  dense_dropout = 0.4,
  recurrent_dropout = 0.4,
  encoder_dropout = 0.1,
  optimizer = "adam"
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\code{ml_framework="tensorflow"} for 'tensorflow' and \code{ml_framework="pytorch"}
for 'pytorch'}

\item{\code{name}}{\code{Character} Name of the new classifier. Please refer to
common name conventions. Free text can be used with parameter \code{label}.}

\item{\code{label}}{\code{Character} Label for the new classifier. Here you can use
free text.}

\item{\code{text_embeddings}}{An object of class\code{TextEmbeddingModel}.}

\item{\code{use_fe}}{\code{bool} If \code{TRUE} a feature extractor is applied in
order to reduce the dimensionality of the text embeddings.}

\item{\code{fe_features}}{\code{int} determining the number of dimensions to which
the dimension of the text embedding should be reduced.}

\item{\code{fe_method}}{\code{string} Method to use for the feature extraction.
\code{"lstm"} for an extractor based on LSTM-layers or \code{"dense"} for
dense layers.}

\item{\code{fe_noise_factor}}{\code{double} between 0 and a value lower 1 indicating
how much noise should be added for the training of the feature extractor.}

\item{\code{targets}}{\code{factor} containing the target values of the classifier.}

\item{\code{hidden}}{\code{vector} containing the number of neurons for each dense layer.
The length of the vector determines the number of dense layers. If you want no dense layer,
set this parameter to \code{NULL}.}

\item{\code{rec}}{\code{vector} containing the number of neurons for each recurrent layer.
The length of the vector determines the number of dense layers. If you want no dense layer,
set this parameter to \code{NULL}.}

\item{\code{rec_type}}{\code{string} Type of the recurrent layers. \code{rec_type="gru"} for
Gated Recurrent Unit and \code{rec_type="lstm"} for Long Short-Term Memory.}

\item{\code{rec_bidirectional}}{\code{bool} If \code{TRUE} a bidirectional version of the reccurent
layers is used.}

\item{\code{embedding_dim}}{\code{Int} determining the dimensionality of the embedding.}

\item{\code{self_attention_heads}}{\code{integer} determining the number of attention heads
for a self-attention layer. Only relevant if \code{attention_type="multihead"}}

\item{\code{intermediate_size}}{\code{int} determining the size of the projection layer within
a each transformer encoder.}

\item{\code{attention_type}}{\code{string} Choose the relevant attention type. Possible values
are \code{"fourier"} and \code{multihead}.}

\item{\code{add_pos_embedding}}{\code{bool} \code{TRUE} if positional embedding should be used.}

\item{\code{rec_dropout}}{\code{double} ranging between 0 and lower 1, determining the
dropout between bidirectional gru layers.}

\item{\code{repeat_encoder}}{\code{int} determining how many times the encoder should be
added to the network.}

\item{\code{dense_dropout}}{\code{double} ranging between 0 and lower 1, determining the
dropout between dense layers.}

\item{\code{recurrent_dropout}}{\code{double} ranging between 0 and lower 1, determining the
recurrent dropout for each recurrent layer. Only relevant for keras models.}

\item{\code{encoder_dropout}}{\code{double} ranging between 0 and lower 1, determining the
dropout for the dense projection within the encoder layers.}

\item{\code{optimizer}}{Object of class \code{keras.optimizers}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Returns an object of class \link{TextEmbeddingClassifierNeuralNet} which is ready for
training.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TEClassifierProtoNet-train"></a>}}
\if{latex}{\out{\hypertarget{method-TEClassifierProtoNet-train}{}}}
\subsection{Method \code{train()}}{
Method for training a neural net.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TEClassifierProtoNet$train(
  data_embeddings,
  data_targets,
  data_folds = 5,
  data_val_size = 0.25,
  use_sc = TRUE,
  sc_method = "dbsmote",
  sc_min_k = 1,
  sc_max_k = 10,
  use_pl = TRUE,
  pl_max_steps = 3,
  pl_max = 1,
  pl_anchor = 1,
  pl_min = 0,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  epochs = 40,
  batch_size = 35,
  fe_epochs = 1000,
  fe_val_size = 0.25,
  Ns = 5,
  Nq = 3,
  loss_alpha,
  loss_margin,
  dir_checkpoint,
  trace = TRUE,
  keras_trace = 2,
  pytorch_trace = 1
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data_embeddings}}{Object of class \code{TextEmbeddingModel}.}

\item{\code{data_targets}}{\code{Factor} containing the labels for cases
stored in \code{data_embeddings}. Factor must be named and has to use the
same names used in \code{data_embeddings}.}

\item{\code{data_folds}}{\code{int} determining the number of cross-fold
samples.}

\item{\code{data_val_size}}{\code{double} between 0 and 1, indicating the proportion of cases of each class
which should be used for the validation sample during the estimation of the baseline model.
The remaining cases are part of the training data.}

\item{\code{use_sc}}{\code{bool} \code{TRUE} if the estimation should integrate
balanced synthetic cases. \code{FALSE} if not.}

\item{\code{sc_method}}{\code{vector} containing the methods for generating
synthetic cases via 'smotefamily'. Multiple methods can
be passed. Currently \code{sc_method=c("adas")}, \code{sc_method=c("smote")}
and \code{sc_method=c("dbsmote")} are possible.}

\item{\code{sc_min_k}}{\code{int} determining the minimal number of k which is used
for creating synthetic units.}

\item{\code{sc_max_k}}{\code{int} determining the maximal number of k which is used
for creating synthetic units.}

\item{\code{use_pl}}{\code{bool} \code{TRUE} if the estimation should integrate
balanced pseudo-labeling. \code{FALSE} if not.}

\item{\code{pl_max_steps}}{\code{int} determining the maximum number of steps during
pseudo-labeling.}

\item{\code{pl_max}}{\code{double} between 0 and 1, setting the maximal level of
confidence for considering a case for pseudo-labeling.}

\item{\code{pl_anchor}}{\code{double} between 0 and 1 indicating the reference
point for sorting the new cases of every label. See notes for more details.}

\item{\code{pl_min}}{\code{double} between 0 and 1, setting the minimal level of
confidence for considering a case for pseudo-labeling.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked
during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable
must be set if sustainability should be tracked. A list can be found on
Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{Region within a country. Only available for USA and
Canada See the documentation of codecarbon for more information.
\url{https://mlco2.github.io/codecarbon/parameters.html}}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power
usage.}

\item{\code{epochs}}{\code{int} Number of training epochs.}

\item{\code{batch_size}}{\code{int} Size of the batches for the feature extractor.}

\item{\code{fe_epochs}}{\code{int} Number of training epochs for the feature extractor.}

\item{\code{fe_val_size}}{\code{double} between 0 and 1, indicating the proportion of cases
which should be used for the validation sample during the training of the feature extractor.}

\item{\code{Ns}}{\code{int} Number of cases for every class in the sample.}

\item{\code{Nq}}{\code{int} Number of cases for every class in the query.}

\item{\code{loss_alpha}}{\code{double} Value between 0 and 1 indicating how strong
the loss should focus on pulling cases to its corresponding prototypes or
pushing cases away from other prototypes. The higher the value the more the loss
concentrates on pulling cases to its corresponding prototypes.}

\item{\code{loss_margin}}{\code{dobule} Value greater 0 indicating the minimal
distance of every case from prototypes of other classes.}

\item{\code{dir_checkpoint}}{\code{string} Path to the directory where
the checkpoint during training should be saved. If the directory does not
exist, it is created.}

\item{\code{trace}}{\code{bool} \code{TRUE}, if information about the estimation
phase should be printed to the console.}

\item{\code{keras_trace}}{\code{int} \code{keras_trace=0} does not print any
information about the training process from keras on the console.}

\item{\code{pytorch_trace}}{\code{int} \code{pytorch_trace=0} does not print any
information about the training process from pytorch on the console.
\code{pytorch_trace=1} prints a progress bar.}

\item{\code{balance_class_weights}}{\code{bool} If \code{TRUE} class weights are
generated based on the frequencies of the training data with the method
Inverse Class Frequency'. If \code{FALSE} each class has the weight 1.}

\item{\code{balance_sequence_length}}{\code{bool} If \code{TRUE} sample weights are
generated for the length of sequences based on the frequencies of the training data with the method
Inverse Class Frequency'. If \code{FALSE} each sequences length has the weight 1.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
\itemize{

\item{\code{sc_max_k: }All values from sc_min_k up to sc_max_k are successively used. If
the number of sc_max_k is too high, the value is reduced to a number that
allows the calculating of synthetic units.}

\item{\code{pl_anchor: }With the help of this value, the new cases are sorted. For
this aim, the distance from the anchor is calculated and all cases are arranged
into an ascending order.
}
}
}

\subsection{Returns}{
Function does not return a value. It changes the object into a trained
classifier.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TEClassifierProtoNet-embed"></a>}}
\if{latex}{\out{\hypertarget{method-TEClassifierProtoNet-embed}{}}}
\subsection{Method \code{embed()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TEClassifierProtoNet$embed(
  embeddings_q = NULL,
  classes_q = NULL,
  embeddings_s = NULL,
  classes_s = NULL
)}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TEClassifierProtoNet-clone"></a>}}
\if{latex}{\out{\hypertarget{method-TEClassifierProtoNet-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TEClassifierProtoNet$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dotAIFEBaseTransformer.R
\name{.AIFEBaseTransformer}
\alias{.AIFEBaseTransformer}
\title{Base \code{R6} class for creation and definition of \code{.AIFE*Transformer-like} classes}
\description{
This base class is used to create and define \code{.AIFE*Transformer-like} classes. It serves as a skeleton
for a future concrete transformer and cannot be used to create an object of itself (an attempt to call \code{new}-method
will produce an error).
}
\section{Private attributes}{
 Developers should know the purpose of all the private attributes.
\subsection{\strong{Attribute \code{title}}}{

\code{string} The title for a transformer. This title is displayed in the progress bar. Can be set with \code{set_title()}.
}

\subsection{\strong{Attribute \code{params}}}{

\code{list()} containing all the transformer parameters. Can be set with \code{set_model_param()}.
\subsection{\strong{'Static' parameters}}{

Regardless of the transformer, the following parameters are always included:
\itemize{
\item \code{ml_framework}
\item \code{sustain_track}
\item \code{sustain_iso_code}
\item \code{sustain_region}
\item \code{sustain_interval}
\item \code{trace}
\item \code{pytorch_safetensors}
}
}

\subsection{\strong{'Dynamic' parameters}}{

In the case of \strong{create} it also contains (see \code{create}-method for details):
\itemize{
\item \code{model_dir}
\item \code{vocab_raw_texts}
\item \code{vocab_size}
\item \code{max_position_embeddings}
\item \code{hidden_size}
\item \code{hidden_act}
\item \code{hidden_dropout_prob}
\item \code{attention_probs_dropout_prob}
\item \code{intermediate_size}
\item \code{num_attention_heads}
}

In the case of \strong{train} it also contains (see \code{train}-method for details):
\itemize{
\item \code{output_dir}
\item \code{model_dir_path}
\item \code{raw_texts}
\item \code{p_mask}
\item \code{whole_word}
\item \code{val_size}
\item \code{n_epoch}
\item \code{batch_size}
\item \code{chunk_size}
\item \code{min_seq_len}
\item \code{full_sequences_only}
\item \code{learning_rate}
\item \code{n_workers}
\item \code{multi_process}
\item \code{keras_trace}
\item \code{pytorch_trace}
}
}

\subsection{\strong{'Dependent' parameters}}{

Depending on the transformer and the method used class may contain different parameters:
\itemize{
\item \code{vocab_do_lower_case}
\item \code{num_hidden_layer}
\item \code{add_prefix_space}
\item etc.
}

\figure{transformer_params_list.png}

Figure 1: Possible parameters in the params list
}

}

\subsection{\strong{Attribute \code{temp}}}{

\code{list()} containing all the temporary local variables that need to be accessed between the step functions. Can be
set with \code{set_model_temp()}.

For example, it can be a variable \code{tok_new} that stores the tokenizer from
\code{steps_for_creation$create_tokenizer_draft}. To train the tokenizer, access the variable \code{tok_new} in
\code{steps_for_creation$calculate_vocab} through the \code{temp} list of this class.

\figure{transformer_temp_list.png}

Figure 2: Possible parameters in the temp list
}

\subsection{\strong{Attribute \code{steps_for_creation}}}{

\code{list()} that stores required and optional steps (functions) for creating a new transformer.

To access (input) parameters of the transformer, use the \code{params} list (e.g. \code{params$ml_framework}). To access a
local variable outside of a function, put it in the \code{temp} list.
\subsection{Required}{

The \strong{required steps} to be defined in each child transformer are:
\itemize{
\item \code{create_tokenizer_draft}: \verb{function()} that creates a tokenizer draft. In this function a new tokenizer must
be created and stored as an element of a list \code{temp} (e.g. \code{temp$tok_new}). This function can include the
definition of special tokens and/or trainers (\code{tokenizers.trainers.WordPieceTrainer}). See the
\code{create_WordPiece_tokenizer()} and \code{create_ByteLevelBPE_tokenizer()} functions to create a new tokenizer object
(\code{tokenizers.Tokenizer}) based on the \code{tokenizers.models.WordPiece} and \code{tokenizers.models.ByteLevel} models
respectively.
\item \code{calculate_vocab}: \verb{function()} for calculating a vocabulary. The tokenizer created in the
\code{create_tokenizer_draft()} function is trained. See \code{tokenizers.Tokenizer.train_from_iterator()} for details.
\item \code{save_tokenizer_draft}: \verb{function()} that saves a tokenizer draft to a model directory (e.g. to a \code{vocab.txt} file).
See \code{tokenizers.Tokenizer.save_model()} for details.
\item \code{create_final_tokenizer}: \verb{function()} that creates a new transformer tokenizer object. The tokenizer must be stored
in the \code{tokenizer} parameter of the \code{temp} list. See \code{transformers.PreTrainedTokenizerFast},
\code{transformers.LongformerTokenizerFast} and \code{transformers.RobertaTokenizerFast} for details.
\item \code{create_transformer_model}: \verb{function()} that creates a transformer model. The model must be passed to the \code{model}
parameter of the \code{temp} list. See \verb{transformers.(TF)BertModel}, \verb{transformers.(TF)DebertaV2ForMaskedLM},
\verb{transformers.(TF)FunnelModel}, \verb{transformers.(TF)LongformerModel}, \verb{transformers.(TF)RobertaModel}, etc. for
details.
}
}

\subsection{Optional}{

\strong{Optional step} is:
\itemize{
\item \code{check_max_pos_emb}: \verb{function()} that checks transformer parameter \code{max_position_embeddings}.
Leave \code{NULL} to skip the check.
}
}

\subsection{Other}{

\strong{Required and already defined step} is:
\itemize{
\item \code{save_transformer_model}: \verb{function()} that saves a newly created transformer. Uses the temporary \code{model}
and \code{pt_safe_save} parameters of the \code{temp} list. See \verb{transformers.(TF)PreTrainedModel.save_pretrained()} for
details.
}

Use the \verb{set_SFC_*()} methods to set required/optional steps for creation, where * is the name of the step.

Use the \code{set_required_SFC()} method to set all required steps at once.
}

}

\subsection{\strong{Attribute \code{steps_for_training}}}{

\code{list()} that stores required and optional steps (functions) for training the transformer.

To access (input) parameters of the transformer, use the \code{params} list (e.g. \code{params$ml_framework}). To access a
local variable outside of a function, put it in the \code{temp} list.
\subsection{Required}{

The \strong{required step} in each child transformer is:
\itemize{
\item \code{load_existing_model}: \verb{function()} that loads the model and its tokenizer. The model and the transformer must be
stored to the \code{model} and \code{tokenizer} parameters respectively of the private \code{temp} list. See
\verb{transformers.(TF)PreTrainedModel} for details.
}
}

\subsection{Optional}{

\strong{Optional step} is:
\itemize{
\item \code{cuda_empty_cache}: \verb{function()} to empty the cache if \code{torch.cuda} is available.
}
}

\subsection{Other}{

\strong{Required and already defined steps} are:
\itemize{
\item \code{check_chunk_size}: \verb{function()} that checks transformer's parameter \code{chunk_size} and adjusts it. Uses
the \code{model} parameter of the private \code{temp} list and modifies the \code{chunk_size} parameter of the private \code{params}
list.
\item \code{create_chunks_for_training}: \verb{function()} that creates chunks of the sequenses for the trainining. Uses the
\code{tokenizer} parameter and adds \code{tokenized_dataset} parameter to the private \code{temp} list.
\item \code{prepare_train_tune}: \verb{function()} that prepares the data for the training. For \code{tensorflow}: uses the \code{model}
and \code{tokenizer} parameters, adds the \code{tf_train_dataset}, \code{tf_test_dataset}, \code{callbacks} parameters to the private
\code{temp} list. For \code{pytorch}: uses the \code{model}, \code{tokenizer} parameters, adds the \code{trainer} parameter to the private
\code{temp} list.
\item \code{start_training}: \verb{function()} that starts the training. For \code{tensorflow}: uses the \code{model}, \code{tf_train_dataset},
\code{tf_test_dataset}, \code{callbacks} parameters of the private \code{temp} list. For \code{pytorch}: uses the \code{trainer} parameter
of the \code{temp} list.
\item \code{save_model}: \verb{function()} that saves the model. For \code{tensorflow}: uses the \code{model} parameter of the \code{temp} list.
For \code{pytorch}: uses the \code{model}, \code{pt_safe_save} and \code{trainer} parameters of the \code{temp} list.
}

Use the \verb{set_SFT_*()} methods to set required/optional steps for creation, where * is the name of the step.
}

}
}

\section{Create}{
 The \code{create}-method is a basic algorithm that is used to create a new transformer, but cannot be
called directly. It has some required and optional steps stored in a private \code{steps_for_creation} list.
}

\section{Train}{
 The \code{train}-method is a basic algorithm that is used to train and tune the transformer but cannot be
called directly. It has some required and optional steps stored in a private \code{steps_for_training} list.
}

\section{Concrete transformer implementation}{
 There are already implemented concrete (child) transformers (e.g.
\code{BERT}, \code{DeBERTa-V2}, etc.).

To implement a new one, do the following steps:

1 Create a new \code{R}-file with a name like \code{dotAIFECustomTransformer}.

2 Open the file and write the creation of a new \code{R6::R6Class()} inside of it (see the code below). The name of the
class must be defined here \strong{(1)}. Remember to inherit the base transformer class \strong{(2)}. Use the \code{private} list
for the private attributes \strong{(3)} like \code{title}, \code{steps_for_creation}, etc. (which will be explained later) and the
\code{public} list \strong{(4)} for the \code{initialize}, \code{create}, \code{train} methods.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{.AIFECustomTransformer <- R6::R6Class(
   classname = ".AIFECustomTransformer", # (1)
   inherit = .AIFEBaseTransformer,       # (2)
   private = list(),                     # (3)
   public = list()                       # (4)
)
}\if{html}{\out{</div>}}

3 Define the private \code{title} attribute \strong{(1)} and set it in the \code{initialize} method \strong{(2)} using the inherited
\code{super$set_title()} base method \strong{(3)} in the base class.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{.AIFECustomTransformer <- R6::R6Class(
   classname = ".AIFECustomTransformer",
   inherit = .AIFEBaseTransformer,
   private = list(
     title = "Custom Model"           # (1)
   ),
   public = list(
     initialize = function() \{        # (2)
       super$set_title(private$title) # (3)
     \}
   )
)
}\if{html}{\out{</div>}}

4 Define the private \code{steps_for_creation} list \strong{(1)} to implement the required steps (functions) \strong{(2)}-\strong{(6)},
and \strong{(7)} if needed.

\strong{Note} that local variables created inside of functions can be used through the private \code{temp} list. Put the
local \code{tok_new} variable \strong{(8)} in the \code{temp} list in the \code{create_tokenizer_draft} step \strong{(2)} and use it in the
\code{calculate_vocab} step \strong{(3)} like \strong{(9)}.

Similarly, use the input parameters of the transformer such as \code{ml_framework} using the private \code{params} list like
\strong{(10)}.

\strong{Important!}

In the \code{create_final_tokenizer} step \strong{(5)} store the tokenizer in the \code{private$temp$tokenizer} variable \strong{(11)}.

In the \code{create_transformer_model} step \strong{(6)} store the transformer model in the \code{private$temp$model} variable
\strong{(12)}.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{.AIFECustomTransformer <- R6::R6Class(
   classname = ".AIFECustomTransformer",
   inherit = .AIFEBaseTransformer,
   private = list(
     title = "Custom Model",
     steps_for_creation = list(                   # (1)
       # required
       create_tokenizer_draft = function() \{      # (2)
         # The implementation must be here
         # private$temp$tok_new <- ...         # (8)
       \},
       calculate_vocab = function() \{             # (3)
         # The implementation must be here
         # ... private$temp$tok_new ...        # (9)
       \},
       save_tokenizer_draft = function() \{        # (4)
         # The implementation must be here
       \},
       create_final_tokenizer = function() \{      # (5)
         # The implementation must be here
         # private$temp$tokenizer <- ... # (!!!) (11)
       \},
       create_transformer_model = function() \{    # (6)
         # The implementation must be here
         # ... private$params$ml_framework ... # (10)
         # private$temp$model <- ...     # (!!!) (12)
       \},
       # optional: omit this element if do not needed
       check_max_pos_emb = function() \{           # (7)
         # The implementation must be here
       \}
     )
   ),
   public = list(
     initialize = function() \{
       super$set_title(private$title)
     \}
   )
)
}\if{html}{\out{</div>}}

5 Define the \code{create} method \strong{(1)} with all the input parameters \strong{(2)} of the \code{create} method of the base
class. Add all the dependent parameters of the custom transformer to the input parameters \strong{(3)}. Dependent
parameters are parameters that depend on the transformer and are not present in the base class. Set these dependent
parameters to the base class using the \code{super$set_model_param()} method \strong{(4)}. Set required and optional steps to
the base class using the \code{super$set_required_SFC()} and \code{super$set_SFC_check_max_pos_emb()} methods respectively
\strong{(5)}. Finally run the basic \code{create} algorithm using \code{super$create()} \strong{(6)} with all the input parameters
\strong{(2)}.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{.AIFECustomTransformer <- R6::R6Class(
   classname = ".AIFECustomTransformer",
   inherit = .AIFEBaseTransformer,
   private = list(
     title = "Custom Model",
     steps_for_creation = list(
       # required
       create_tokenizer_draft = function() \{ \},
       calculate_vocab = function() \{ \},
       save_tokenizer_draft = function() \{ \},
       create_final_tokenizer = function() \{ \},
       create_transformer_model = function() \{ \},
       # optional: omit this element if do not needed
       check_max_pos_emb = function() \{ \}
     )
   ),
   public = list(
     initialize = function() \{ \},
     # (1)
     create = function(# (2) --------------------------
                       ml_framework,
                       model_dir,
                       vocab_raw_texts,
                       vocab_size,
                       # ...
                       trace,
                       pytorch_safetensors,

                       # (3) --------------------------
                       dep_param1,
                       dep_param2,
                       # ...
                       dep_paramN) \{
       # (4) -----------------------------------------
       super$set_model_param("dep_param1", dep_param1)
       super$set_model_param("dep_param2", dep_param2)
       # ...
       super$set_model_param("dep_paramN", dep_paramN)

       # (5) -----------------------------------------
       super$set_required_SFC(private$steps_for_creation)

       # optional, can be omitted if do not needed
       super$set_SFC_check_max_pos_emb(private$steps_for_creation$check_max_pos_emb)

       # (6) -----------------------------------------
       super$create(
         ml_framework = ml_framework,
         model_dir = model_dir,
         vocab_raw_texts = vocab_raw_texts,
         vocab_size = vocab_size,
         # ...
         trace = trace,
         pytorch_safetensors = pytorch_safetensors)
     \}
   )
)
}\if{html}{\out{</div>}}

6 Define \code{train} method \strong{(1)} similarly to the step 5. Implement steps (functions) in the private
\code{steps_for_training} list \strong{(2)}. Set the dependent parameters \strong{(4)} in the base class using
\code{super$set_model_param()} method \strong{(5)}. Set the implemented steps for training in the base class using
\verb{super$set_SFT_*()} methods \strong{(6)}. Finally run the basic \code{train} algorithm \strong{(7)} of the base class with all the
(input) parameters \strong{(3)}.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{.AIFECustomTransformer <- R6::R6Class(
   classname = ".AIFECustomTransformer",
   inherit = .AIFEBaseTransformer,
   private = list(
     title = "Custom Model",
     steps_for_creation = list(
       # required
       create_tokenizer_draft = function() \{ \},
       calculate_vocab = function() \{ \},
       save_tokenizer_draft = function() \{ \},
       create_final_tokenizer = function() \{ \},
       create_transformer_model = function() \{ \},
       # optional: omit this element if do not needed
       check_max_pos_emb = function() \{ \}
     ),
     # (2)
     steps_for_training = list(
       # required
       load_existing_model = function() \{ \},
       # optional
       cuda_empty_cache = function() \{ \}
     )
   ),
   public = list(
     initialize = function() \{ \},
     create = function( ) \{
       # ---------------------------
       # super$set_model_param(...)
       # ...
       # ---------------------------
       # super$set_required_SFC(...)
       # super$set_SFC_*(...)
       # ...
       # ---------------------------
       # super$create(...)
     \},

     # (1)
     train = function(# (3) --------
                      ml_framework,
                      # ...

                      # (4) --------
                      dep_param1,
                      # ...
                      dep_paramN) \{
       # (5) -----------------------------------------
       super$set_model_param("dep_param1", dep_param1)
       # ...
       super$set_model_param("dep_paramN", dep_paramN)

       # (6) -----------------------------------------
       super$set_SFT_load_existing_model(private$steps_for_creation$load_existing_model)
       # optional
       super$set_SFT_cuda_empty_cache(private$steps_for_creation$cuda_empty_cache)

       # (7) -----------------------------------------
       super$train(
         ml_framework = ml_framework,
         # ...
       )
     \}
   )
)
}\if{html}{\out{</div>}}
}

\references{
Hugging Face transformers documantation:
\itemize{
\item \href{https://huggingface.co/docs/transformers/model_doc/bert}{BERT}
\item \href{https://huggingface.co/docs/transformers/model_doc/deberta-v2}{DeBERTa}
\item \href{https://huggingface.co/docs/transformers/model_doc/funnel}{Funnel}
\item \href{https://huggingface.co/docs/transformers/model_doc/longformer}{Longformer}
\item \href{https://huggingface.co/docs/transformers/model_doc/roberta}{RoBERTa}
}
}
\seealso{
Other Transformers for developers: 
\code{\link{.AIFEBertTransformer}},
\code{\link{.AIFEDebertaTransformer}},
\code{\link{.AIFEFunnelTransformer}},
\code{\link{.AIFELongformerTransformer}},
\code{\link{.AIFERobertaTransformer}}
}
\concept{Transformers for developers}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-.AIFEBaseTransformer-new}{\code{.AIFEBaseTransformer$new()}}
\item \href{#method-.AIFEBaseTransformer-set_title}{\code{.AIFEBaseTransformer$set_title()}}
\item \href{#method-.AIFEBaseTransformer-set_model_param}{\code{.AIFEBaseTransformer$set_model_param()}}
\item \href{#method-.AIFEBaseTransformer-set_model_temp}{\code{.AIFEBaseTransformer$set_model_temp()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb}{\code{.AIFEBaseTransformer$set_SFC_check_max_pos_emb()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft}{\code{.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_calculate_vocab}{\code{.AIFEBaseTransformer$set_SFC_calculate_vocab()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft}{\code{.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer}{\code{.AIFEBaseTransformer$set_SFC_create_final_tokenizer()}}
\item \href{#method-.AIFEBaseTransformer-set_SFC_create_transformer_model}{\code{.AIFEBaseTransformer$set_SFC_create_transformer_model()}}
\item \href{#method-.AIFEBaseTransformer-set_required_SFC}{\code{.AIFEBaseTransformer$set_required_SFC()}}
\item \href{#method-.AIFEBaseTransformer-set_SFT_load_existing_model}{\code{.AIFEBaseTransformer$set_SFT_load_existing_model()}}
\item \href{#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache}{\code{.AIFEBaseTransformer$set_SFT_cuda_empty_cache()}}
\item \href{#method-.AIFEBaseTransformer-create}{\code{.AIFEBaseTransformer$create()}}
\item \href{#method-.AIFEBaseTransformer-train}{\code{.AIFEBaseTransformer$train()}}
\item \href{#method-.AIFEBaseTransformer-clone}{\code{.AIFEBaseTransformer$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-new"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-new}{}}}
\subsection{Method \code{new()}}{
An object of this class cannot be created. Thus, method's call will produce an error.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$new()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
This method returns an error.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_title"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_title}{}}}
\subsection{Method \code{set_title()}}{
Setter for the title. Sets a new value for the \code{title} private attribute.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_title(title)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{title}}{\code{string} A new title.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_model_param"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_model_param}{}}}
\subsection{Method \code{set_model_param()}}{
Setter for the parameters. Adds a new parameter and its value to the private \code{params} list.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_model_param(param_name, param_value)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{param_name}}{\code{string} Parameter's name.}

\item{\code{param_value}}{\code{any} Parameter's value.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_model_temp"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_model_temp}{}}}
\subsection{Method \code{set_model_temp()}}{
Setter for the temporary model's parameters. Adds a new temporary parameter and its value to the
private \code{temp} list.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_model_temp(temp_name, temp_value)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{temp_name}}{\code{string} Parameter's name.}

\item{\code{temp_value}}{\code{any} Parameter's value.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb}{}}}
\subsection{Method \code{set_SFC_check_max_pos_emb()}}{
Setter for the \code{check_max_pos_emb} element of the private \code{steps_for_creation} list. Sets a new
\code{fun} function as the \code{check_max_pos_emb} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_check_max_pos_emb(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft}{}}}
\subsection{Method \code{set_SFC_create_tokenizer_draft()}}{
Setter for the \code{create_tokenizer_draft} element of the  private \code{steps_for_creation} list. Sets a
new \code{fun} function as the \code{create_tokenizer_draft} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_create_tokenizer_draft(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_calculate_vocab"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_calculate_vocab}{}}}
\subsection{Method \code{set_SFC_calculate_vocab()}}{
Setter for the \code{calculate_vocab} element of the private \code{steps_for_creation} list. Sets a new \code{fun}
function as the \code{calculate_vocab} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_calculate_vocab(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft}{}}}
\subsection{Method \code{set_SFC_save_tokenizer_draft()}}{
Setter for the \code{save_tokenizer_draft} element of the private \code{steps_for_creation} list. Sets a new
\code{fun} function as the \code{save_tokenizer_draft} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_save_tokenizer_draft(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer}{}}}
\subsection{Method \code{set_SFC_create_final_tokenizer()}}{
Setter for the \code{create_final_tokenizer} element of the private \code{steps_for_creation} list. Sets a new
\code{fun} function as the \code{create_final_tokenizer} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_create_final_tokenizer(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFC_create_transformer_model"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFC_create_transformer_model}{}}}
\subsection{Method \code{set_SFC_create_transformer_model()}}{
Setter for the \code{create_transformer_model} element of the private \code{steps_for_creation} list. Sets a
new \code{fun} function as the \code{create_transformer_model} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFC_create_transformer_model(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_required_SFC"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_required_SFC}{}}}
\subsection{Method \code{set_required_SFC()}}{
Setter for all required elements of the private \code{steps_for_creation} list. Executes setters for all
required creation steps.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_required_SFC(required_SFC)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{required_SFC}}{\code{list()} A list of all new required steps.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFT_load_existing_model"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFT_load_existing_model}{}}}
\subsection{Method \code{set_SFT_load_existing_model()}}{
Setter for the \code{load_existing_model} element of the private \code{steps_for_training} list. Sets a new
\code{fun} function as the \code{load_existing_model} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFT_load_existing_model(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache}{}}}
\subsection{Method \code{set_SFT_cuda_empty_cache()}}{
Setter for the \code{cuda_empty_cache} element of the private \code{steps_for_training} list. Sets a new
\code{fun} function as the \code{cuda_empty_cache} step.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$set_SFT_cuda_empty_cache(fun)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{fun}}{\verb{function()} A new function.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-create"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-create}{}}}
\subsection{Method \code{create()}}{
This method creates a transformer configuration based on the child-transformer architecture and a
vocabulary using the python libraries \code{transformers} and \code{tokenizers}.

This method \strong{adds} \code{raw_text_dataset} and \code{pt_safe_save}, \strong{uses} \code{tokenizer} temporary parameters of the
\code{temp} list.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$create(
  ml_framework,
  model_dir,
  vocab_raw_texts,
  vocab_size,
  max_position_embeddings,
  hidden_size,
  num_attention_heads,
  intermediate_size,
  hidden_act,
  hidden_dropout_prob,
  attention_probs_dropout_prob,
  sustain_track,
  sustain_iso_code,
  sustain_region,
  sustain_interval,
  trace,
  pytorch_safetensors
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{model_dir}}{\code{string} Path to the directory where the model should be saved.}

\item{\code{vocab_raw_texts}}{\code{vector} containing the raw texts for creating the vocabulary.}

\item{\code{vocab_size}}{\code{int} Size of the vocabulary.}

\item{\code{max_position_embeddings}}{\code{int} Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.}

\item{\code{hidden_size}}{\code{int} Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.}

\item{\code{num_attention_heads}}{\code{int} Number of attention heads.}

\item{\code{intermediate_size}}{\code{int} Number of neurons in the intermediate layer of the attention mechanism.}

\item{\code{hidden_act}}{\code{string} Name of the activation function.}

\item{\code{hidden_dropout_prob}}{\code{double} Ratio of dropout.}

\item{\code{attention_probs_dropout_prob}}{\code{double} Ratio of dropout for attention probabilities.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{Region within a country. Only available for USA and Canada See the documentation of codecarbon for more information.
\url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-train"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-train}{}}}
\subsection{Method \code{train()}}{
This method can be used to train or fine-tune a transformer based on \code{BERT} architecture with the
help of the python libraries \code{transformers}, \code{datasets}, and \code{tokenizers}.

This method \strong{adds} \code{from_pt}, \code{from_tf}, \code{load_safe} and \code{pt_safe_save}, \strong{uses} \code{tokenized_dataset} and
\code{tokenizer} temporary parameters of the \code{temp} list.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$train(
  ml_framework,
  output_dir,
  model_dir_path,
  raw_texts,
  p_mask,
  whole_word,
  val_size,
  n_epoch,
  batch_size,
  chunk_size,
  full_sequences_only,
  min_seq_len,
  learning_rate,
  n_workers,
  multi_process,
  sustain_track,
  sustain_iso_code,
  sustain_region,
  sustain_interval,
  trace,
  keras_trace,
  pytorch_trace,
  pytorch_safetensors
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{output_dir}}{\code{string} Path to the directory where the final model should be saved. If the directory does not exist, it will be created.}

\item{\code{model_dir_path}}{\code{string} Path to the directory where the original model is stored.}

\item{\code{raw_texts}}{\code{vector} containing the raw texts for training.}

\item{\code{p_mask}}{\code{double} Ratio that determines the number of words/tokens used for masking.}

\item{\code{whole_word}}{\code{bool}
\itemize{
\item \code{TRUE}: whole word masking should be applied.
\item \code{FALSE}: token masking is used.
}}

\item{\code{val_size}}{\code{double} Ratio that determines the amount of token chunks used for validation.}

\item{\code{n_epoch}}{\code{int} Number of epochs for training.}

\item{\code{batch_size}}{\code{int} Size of batches.}

\item{\code{chunk_size}}{\code{int} Size of every chunk for training.}

\item{\code{full_sequences_only}}{\code{bool} \code{TRUE} for using only chunks with a sequence length equal to \code{chunk_size}.}

\item{\code{min_seq_len}}{\code{int} Only relevant if \code{full_sequences_only = FALSE}. Value determines the minimal sequence length included in
training process.}

\item{\code{learning_rate}}{\code{double} Learning rate for adam optimizer.}

\item{\code{n_workers}}{\code{int} Number of workers. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{multi_process}}{\code{bool} \code{TRUE} if multiple processes should be activated. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{Region within a country. Only available for USA and Canada See the documentation of codecarbon for more information.
\url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{keras_trace}}{\code{int}
\itemize{
\item \code{keras_trace = 0}: does not print any information about the training process from keras on the console.
\item \code{keras_trace = 1}: prints a progress bar.
\item \code{keras_trace = 2}: prints one line of information for every epoch. Only relevant if \code{ml_framework = "tensorflow"}.
}}

\item{\code{pytorch_trace}}{\code{int}
\itemize{
\item \code{pytorch_trace = 0}: does not print any information about the training process from pytorch on the console.
\item \code{pytorch_trace = 1}: prints a progress bar.
}}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEBaseTransformer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEBaseTransformer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEBaseTransformer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}

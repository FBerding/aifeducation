% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AIFETransformerMaker.R
\docType{data}
\name{aife_transformer_maker}
\alias{aife_transformer_maker}
\title{\code{R6} object of the \code{AIFETransformerMaker} class}
\format{
An object of class \code{AIFETransformerMaker} (inherits from \code{R6}) of length 3.
}
\usage{
aife_transformer_maker
}
\description{
Object for creating the transformers with different types. See \link{AIFETransformerMaker} class for
details.
}
\section{Transformer parameters}{

\subsection{'Static' parameters}{\tabular{lll}{
   Name \tab Type \tab Description \cr
   ml_framework \tab \code{string} \tab Framework to use for training and inference\if{html}{\out{<sup>}}1\if{html}{\out{</sup>}} \cr
   sustain_track \tab \code{bool} \tab If \code{TRUE} energy consumption is tracked during training\if{html}{\out{<sup>}}2\if{html}{\out{</sup>}} \cr
   sustain_iso_code \tab \code{string} \tab ISO code (Alpha-3-Code) for the country\if{html}{\out{<sup>}}3\if{html}{\out{</sup>}} \cr
   sustain_region \tab \code{string} \tab Region within a country. Only available for USA and Canada\if{html}{\out{<sup>}}4\if{html}{\out{</sup>}} \cr
   sustain_interval \tab \code{integer} \tab Interval in seconds for measuring power usage \cr
   trace \tab \code{bool} \tab \code{TRUE} if information about the progress should be printed to the console \cr
   pytorch_safetensors \tab \code{bool} \tab Choose between safetensors and standard pytorch format\if{html}{\out{<sup>}}5\if{html}{\out{</sup>}} \cr
}


\if{html}{\out{<sup>}}1 Available frameworks are "tensorflow" and "pytorch"\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}2 Via the python library codecarbon\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}3 This variable must be set if sustainability should be tracked. A list can be found on Wikipedia:
\url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}4 See the documentation of codecarbon for more information
\url{https://mlco2.github.io/codecarbon/parameters.html}\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}5 Only relevant for pytorch models. \code{TRUE}: a 'pytorch' model is saved in safetensors format; \code{FALSE}
(or 'safetensors' is not available): model is saved in the standard pytorch format (.bin)\if{html}{\out{</sup>}}
}

\subsection{'Dynamic' parameters for creation}{\tabular{lll}{
   Name \tab Type \tab Description \cr
   model_dir \tab \code{string} \tab Path to the directory where the model should be saved \cr
   vocab_raw_texts \tab \code{vector} \tab Contains the raw texts for creating the vocabulary \cr
   vocab_size \tab \code{int} \tab Size of the vocabulary \cr
   max_position_embeddings \tab \code{int} \tab Number of maximum position embeddings\if{html}{\out{<sup>}}1\if{html}{\out{</sup>}} \cr
   hidden_size \tab \code{int} \tab Number of neurons in each layer\if{html}{\out{<sup>}}2\if{html}{\out{</sup>}} \cr
   hidden_act \tab \code{string} \tab Name of the activation function \cr
   hidden_dropout_prob \tab \code{double} \tab Ratio of dropout \cr
   attention_probs_dropout_prob \tab \code{double} \tab Ratio of dropout for attention probabilities \cr
   intermediate_size \tab \code{int} \tab Number of neurons in the intermediate layer of the attention mechanism \cr
   num_attention_heads \tab \code{int} \tab Number of attention heads \cr
   vocab_do_lower_case \tab \code{bool} \tab \code{TRUE} if all words/tokens should be lower case \cr
   num_hidden_layer \tab \code{int} \tab Number of hidden layers \cr
   target_hidden_size \tab \code{int} \tab Number of neurons in the final layer\if{html}{\out{<sup>}}2\if{html}{\out{</sup>}} \cr
   block_sizes \tab \code{vector} \tab Contains \code{int}s that determine the number and sizes of each block \cr
   num_decoder_layers \tab \code{int} \tab Number of decoding layers \cr
   activation_dropout \tab \code{float} \tab Dropout probability between the layers of the feed-forward blocks \cr
   pooling_type \tab \code{string} \tab Type of pooling\if{html}{\out{<sup>}}3\if{html}{\out{</sup>}} \cr
   add_prefix_space \tab \code{bool} \tab \code{TRUE} if an additional space should be inserted to the leading words \cr
   trim_offsets \tab \code{bool} \tab \code{TRUE} trims the whitespaces from the produced offsets \cr
   attention_window \tab \code{int} \tab Size of the window around each token for attention mechanism in every layer \cr
}


\if{html}{\out{<sup>}}1 This parameter also determines the maximum length of a sequence which can be processed with the model\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}2 This parameter determines the dimensionality of the resulting text embedding\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}3 \code{"mean"} and \code{"max"} for pooling with mean and maximum values respectively\if{html}{\out{</sup>}}
}

\subsection{'Dynamic' parameters for training}{\tabular{lll}{
   Name \tab Type \tab Description \cr
   output_dir \tab \code{string} \tab Path to the directory where the final model should be saved\if{html}{\out{<sup>}}1\if{html}{\out{</sup>}} \cr
   model_dir_path \tab \code{string} \tab Path to the directory where the original model is stored \cr
   raw_texts \tab \code{vector} \tab Contains the raw texts for training \cr
   p_mask \tab \code{double} \tab Ratio that determines the number of words/tokens used for masking \cr
   whole_word \tab \code{bool} \tab Choose a type of masking\if{html}{\out{<sup>}}2\if{html}{\out{</sup>}} \cr
   val_size \tab \code{double} \tab Ratio that determines the amount of token chunks used for validation \cr
   n_epoch \tab \code{int} \tab Number of epochs for training \cr
   batch_size \tab \code{int} \tab Size of batches \cr
   chunk_size \tab \code{int} \tab Size of every chunk for training \cr
   min_seq_len \tab \code{int} \tab Value determines the minimal sequence length included in training process\if{html}{\out{<sup>}}3\if{html}{\out{</sup>}} \cr
   full_sequences_only \tab \code{bool} \tab \code{TRUE} for using only chunks with a sequence length equal to \code{chunk_size} \cr
   learning_rate \tab \code{double} \tab Learning rate for adam optimizer \cr
   n_workers \tab \code{int} \tab Number of workers\if{html}{\out{<sup>}}4\if{html}{\out{</sup>}} \cr
   multi_process \tab \code{bool} \tab \code{TRUE} if multiple processes should be activated\if{html}{\out{<sup>}}4\if{html}{\out{</sup>}} \cr
   keras_trace \tab \code{int} \tab Controls the information about the training process from keras on the console\if{html}{\out{<sup>}}4,5\if{html}{\out{</sup>}} \cr
   pytorch_trace \tab \code{int} \tab Controls the information about the training process from pytorch on the console \cr
}


\if{html}{\out{<sup>}}1 If the directory does not exist, it will be created\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}2 \code{TRUE}: whole word masking should be applied; \code{FALSE}: token masking is used\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}3 Only relevant if \code{full_sequences_only = FALSE}\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}4 Only relevant if \code{ml_framework = "tensorflow"}\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}5 \code{keras_trace = 0}: does not print any information; \code{keras_trace = 1}: prints a progress bar;
\code{keras_trace = 2}: prints one line of information for every epoch\if{html}{\out{</sup>}}

\if{html}{\out{<sup>}}6 \code{pytorch_trace = 0}: does not print any information; \code{pytorch_trace = 1}: prints a progress bar\if{html}{\out{</sup>}}
}
}

\section{Allowed transformers}{
 Using the transformers will be shown in this section.
\subsection{\code{BERT}}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer <- aife_transformer_maker$make(AIFETrType$bert)
}\if{html}{\out{</div>}}
\subsection{Create}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$create(ml_framework = ml_framework,
                   model_dir = model_dir,
                   vocab_raw_texts = NULL,
                   vocab_size = 30522,
                   vocab_do_lower_case = FALSE,
                   max_position_embeddings = 512,
                   hidden_size = 768,
                   num_hidden_layer = 12,
                   num_attention_heads = 12,
                   intermediate_size = 3072,
                   hidden_act = "gelu",
                   hidden_dropout_prob = 0.1,
                   attention_probs_dropout_prob = 0.1,
                   sustain_track = TRUE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

\subsection{Train}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$train(ml_framework = ml_framework,
                  output_dir = output_dir,
                  model_dir_path = model_dir_path,
                  raw_texts = raw_texts,
                  p_mask = 0.15,
                  whole_word = TRUE,
                  val_size = 0.1,
                  n_epoch = 1,
                  batch_size = 12,
                  chunk_size = 250,
                  full_sequences_only = FALSE,
                  min_seq_len = 50,
                  learning_rate = 3e-3,
                  n_workers = 1,
                  multi_process = FALSE,
                  sustain_track = TRUE,
                  sustain_iso_code = NULL,
                  sustain_region = NULL,
                  sustain_interval = 15,
                  trace = TRUE,
                  keras_trace = 1,
                  pytorch_trace = 1,
                  pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

}

\subsection{\code{DeBERTa-v2}}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer <- aife_transformer_maker$make(AIFETrType$deberta)
}\if{html}{\out{</div>}}
\subsection{Create}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$create(ml_framework = ml_framework,
                   model_dir = model_dir,
                   vocab_raw_texts = vocab_raw_texts,
                   vocab_size = 128100,
                   vocab_do_lower_case = FALSE,
                   max_position_embeddings = 512,
                   hidden_size = 1536,
                   num_hidden_layer = 24,
                   num_attention_heads = 24,
                   intermediate_size = 6144,
                   hidden_act = "gelu",
                   hidden_dropout_prob = 0.1,
                   attention_probs_dropout_prob = 0.1,
                   sustain_track = TRUE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

\subsection{Train}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$train(ml_framework = ml_framework,
                  output_dir = output_dir,
                  model_dir_path = model_dir_path,
                  raw_texts = raw_texts,
                  p_mask = 0.15,
                  whole_word = TRUE,
                  val_size = 0.1,
                  n_epoch = 1,
                  batch_size = 12,
                  chunk_size = 250,
                  full_sequences_only = FALSE,
                  min_seq_len = 50,
                  learning_rate = 3e-2,
                  n_workers = 1,
                  multi_process = FALSE,
                  sustain_track = TRUE,
                  sustain_iso_code = NULL,
                  sustain_region = NULL,
                  sustain_interval = 15,
                  trace = TRUE,
                  keras_trace = 1,
                  pytorch_trace = 1,
                  pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

}

\subsection{\code{RoBERTa}}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer <- aife_transformer_maker$make(AIFETrType$roberta)
}\if{html}{\out{</div>}}
\subsection{Create}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$create(ml_framework = ml_framework,
                   model_dir = model_dir,
                   vocab_raw_texts = vocab_raw_texts,
                   vocab_size = 30522,
                   add_prefix_space = FALSE,
                   trim_offsets = TRUE,
                   max_position_embeddings = 512,
                   hidden_size = 768,
                   num_hidden_layer = 12,
                   num_attention_heads = 12,
                   intermediate_size = 3072,
                   hidden_act = "gelu",
                   hidden_dropout_prob = 0.1,
                   attention_probs_dropout_prob = 0.1,
                   sustain_track = TRUE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

\subsection{Train}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$train(ml_framework = ml_framework,
                  output_dir = output_dir,
                  model_dir_path = model_dir_path,
                  raw_texts = raw_texts,
                  p_mask = 0.15,
                  val_size = 0.1,
                  n_epoch = 1,
                  batch_size = 12,
                  chunk_size = 250,
                  full_sequences_only = FALSE,
                  min_seq_len = 50,
                  learning_rate = 3e-2,
                  n_workers = 1,
                  multi_process = FALSE,
                  sustain_track = TRUE,
                  sustain_iso_code = NULL,
                  sustain_region = NULL,
                  sustain_interval = 15,
                  trace = TRUE,
                  keras_trace = 1,
                  pytorch_trace = 1,
                  pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

}

\subsection{\code{Funnel}}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer <- aife_transformer_maker$make(AIFETrType$funnel)
}\if{html}{\out{</div>}}
\subsection{Create}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$create(ml_framework = ml_framework,
                   model_dir = model_dir,
                   vocab_raw_texts = vocab_raw_texts,
                   vocab_size = 30522,
                   vocab_do_lower_case = FALSE,
                   max_position_embeddings = 512,
                   hidden_size = 768,
                   target_hidden_size = 64,
                   block_sizes = c(4, 4, 4),
                   num_attention_heads = 12,
                   intermediate_size = 3072,
                   num_decoder_layers = 2,
                   pooling_type = "mean",
                   hidden_act = "gelu",
                   hidden_dropout_prob = 0.1,
                   attention_probs_dropout_prob = 0.1,
                   activation_dropout = 0.0,
                   sustain_track = TRUE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

\subsection{Train}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$train(ml_framework = ml_framework,
                  output_dir = output_dir,
                  model_dir_path = model_dir_path,
                  raw_texts = raw_texts,
                  p_mask = 0.15,
                  whole_word = TRUE,
                  val_size = 0.1,
                  n_epoch = 1,
                  batch_size = 12,
                  chunk_size = 250,
                  full_sequences_only = FALSE,
                  min_seq_len = 50,
                  learning_rate = 3e-3,
                  n_workers = 1,
                  multi_process = FALSE,
                  sustain_track = TRUE,
                  sustain_iso_code = NULL,
                  sustain_region = NULL,
                  sustain_interval = 15,
                  trace = TRUE,
                  keras_trace = 1,
                  pytorch_trace = 1,
                  pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

}

\subsection{\code{Longformer}}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer <- aife_transformer_maker$make(AIFETrType$longformer)
}\if{html}{\out{</div>}}
\subsection{Create}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$create(ml_framework = ml_framework,
                   model_dir = model_dir,
                   vocab_raw_texts = NULL,
                   vocab_size = 30522,
                   add_prefix_space = FALSE,
                   trim_offsets = TRUE,
                   max_position_embeddings = 512,
                   hidden_size = 768,
                   num_hidden_layer = 12,
                   num_attention_heads = 12,
                   intermediate_size = 3072,
                   hidden_act = "gelu",
                   hidden_dropout_prob = 0.1,
                   attention_probs_dropout_prob = 0.1,
                   attention_window = 512,
                   sustain_track = TRUE,
                   sustain_iso_code = NULL,
                   sustain_region = NULL,
                   sustain_interval = 15,
                   trace = TRUE,
                   pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

\subsection{Train}{

\if{html}{\out{<div class="sourceCode r">}}\preformatted{transformer$train(ml_framework = ml_framework,
                  output_dir = output_dir,
                  model_dir_path = model_dir_path,
                  raw_texts = raw_texts,
                  p_mask = 0.15,
                  val_size = 0.1,
                  n_epoch = 1,
                  batch_size = 12,
                  chunk_size = 250,
                  full_sequences_only = FALSE,
                  min_seq_len = 50,
                  learning_rate = 3e-2,
                  n_workers = 1,
                  multi_process = FALSE,
                  sustain_track = TRUE,
                  sustain_iso_code = NULL,
                  sustain_region = NULL,
                  sustain_interval = 15,
                  trace = TRUE,
                  keras_trace = 1,
                  pytorch_trace = 1,
                  pytorch_safetensors = TRUE)
}\if{html}{\out{</div>}}
}

}
}

\examples{
# Use 'make' method of the 'aifeducation::aife_transformer_maker' object
# Pass string with the type of transformers
# Allowed types are "bert", "deberta_v2", "funnel", "longformer", "roberta"
my_bert <- aife_transformer_maker$make("bert")

# Or use elements of the 'aifeducation::AIFETrType' list
my_longformer <- aife_transformer_maker$make(AIFETrType$longformer)

# Run 'create' or 'train' methods of the transformer in order to create a
# new transformer or train the newly created one, respectively
# my_bert$create(...)
# my_bert$train(...)

# my_longformer$create(...)
# my_longformer$train(...)

}
\seealso{
Other Transformer: 
\code{\link{AIFETrType}},
\code{\link{AIFETransformerMaker}}
}
\concept{Transformer}
\keyword{datasets}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/transformer_longformer.R
\name{train_tune_longformer_model}
\alias{train_tune_longformer_model}
\title{Function for training and fine-tuning a Longformer model}
\usage{
train_tune_longformer_model(
  output_dir,
  model_dir_path,
  raw_texts,
  p_mask = 0.15,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  trace = TRUE
)
}
\arguments{
\item{output_dir}{\code{string} Path to the directory where the final model
should be saved. If the directory does not exist, it will be created.}

\item{model_dir_path}{\code{string} Path to the directory where the original
model is stored.}

\item{raw_texts}{\code{vector} containing the raw texts for training.}

\item{p_mask}{\code{double} Ratio determining the number of words/tokens for masking.}

\item{val_size}{\code{double} Ratio determining the amount of token chunks used for
validation.}

\item{n_epoch}{\code{int} Number of epochs for training.}

\item{batch_size}{\code{int} Size of batches.}

\item{chunk_size}{\code{int} Size of every chunk for training.}

\item{learning_rate}{\code{bool} Learning rate for adam optimizer.}

\item{n_workers}{\code{int} Number of workers.}

\item{multi_process}{\code{bool} \code{TRUE} if multiple processes should be activated.}

\item{trace}{\code{bool} \code{TRUE} if information on the progress should be printed
to the console.}
}
\value{
This function does not return an object. Instead the trained or fine-tuned
model is saved to disk.
}
\description{
This function can be used to train or fine-tune a transformer
based on BERT architecture with the help of the python libraries 'transformers',
'datasets', and 'tokenizers'.
}
\note{
Pre-Trained models which can be fine-tuned with this function are available
at \url{https://huggingface.co/}. New models can be created via the function
\link{create_roberta_model}.

Training of this model makes use of dynamic masking.
}
\references{
Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The
Long-Document Transformer. \url{https://doi.org/10.48550/arXiv.2004.05150}

Hugging Face Documentation
\url{https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig}
}

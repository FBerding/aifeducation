[{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://fberding.github.io/aifeducation/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"introduction","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Introduction","title":"01 Get started","text":"Several packages allow users use machine learning directly R nnet single layer neural nets, rpart decision trees, ranger random forests. Furthermore, mlr3verse series packages exists managing different algorithms unified interface. packages can used ‘normal’ computer provide easy installation. terms natural language processing, approaches currently limited. State---art approaches rely neural nets multiple layers consist huge number parameters making computationally demanding. specialized libraries keras, PyTorch tensorflow, graphical processing units (gpu) can help speed computations significantly. However, many specialized libraries machine learning written python. Fortunately, interface python provided via R package reticulate. R package Artificial Intelligence Education (aifeducation) aims provide educators, educational researchers, social researchers convincing interface state---art models natural language processing tries address special needs challenges educational social sciences. package currently supports application Artificial Intelligence (AI) tasks text embedding classification. Since state---art approaches natural language processing rely large models compared classical statistical methods (e.g., latent class analysis, structural equation modeling) based largely python, additional installation steps necessary. like train develop models AIs, compatible graphic device necessary. Even low performing graphic device can speed computations significantly. prefer use pre-trained models however, necessary. case, ‘normal’ office computer without graphic device sufficient cases. get ready using package, two steps necessary.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"step-1---install-the-r-package","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 1 - Install the R package","title":"01 Get started","text":"First need install package. can done : command, aifeducation installed machine.","code":"install.packages(\"aifeducation\")"},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"step-2---install-python-and-optional-r-packages","dir":"Articles","previous_headings":"1) Installation and Technical Requirements","what":"Step 2 - Install Python and optional R packages","title":"01 Get started","text":"Since natural language processing neural nets based models computationally intensive, PyTorch used within package together specialized python libraries. straightforward method getting started call function install_aifeducation follows: function install python, miniconda, relevant python libraries conda environment called “aifeducation”. addition, recommend set install_aifeducation_studio=TRUE since install optional R packages necessary use AI Education - Studio. AI Education - Studio graphical user interface applying packages. recommend use everyone unfamiliar R machine learning. want use studio, can set argument FALSE. case use package R syntax. suitable machine like use graphic card computations need install software. can find information : https://pytorch.org/get-started/locally/ can check python working using function reticulate::py_available(). return TRUE. can check necessary python packages successfully installed calling function check_aif_py_modules Now everything ready use package.","code":"install_aifeducation(   install_aifeducation_studio = TRUE ) reticulate::py_available(initialize = TRUE) aifeducation::check_aif_py_modules()"},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"starting-a-new-session","dir":"Articles","previous_headings":"","what":"2) Starting a new session","title":"01 Get started","text":"convenient way work package use AI Education - Studio can start calling aifeducation::start_aifeducation_studio(). case want use graphical user interface, prepare R sessions. First, necessary set python via ‘reticulate’ choose conda environment necessary python libraries available. can load aifeducation. case installed python suggested vignette, may start new session like : Note: Please remember: Every time start new session R, set correct conda environment load library aifeducation. necessary use AI Education - Studio.","code":"reticulate::use_condaenv(condaenv = \"aifeducation\") library(aifeducation)"},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"tutorials-and-guides","dir":"Articles","previous_headings":"","what":"3) Tutorials and Guides","title":"01 Get started","text":"guide use graphical user interface can found vignette 02 Using graphical user interface Aifeducation - Studio. short introduction package examples classification tasks can found vignette 03 Using R syntax. Documenting sharing work described vignette 04 Sharing using trained AI/models.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/aifeducation.html","id":"update-aifeducation","dir":"Articles","previous_headings":"","what":"4) Update aifeducation","title":"01 Get started","text":"case already use aifeducation want update newer version package, recommended update used python libraries well. easiest way remove conda environment “aifeducation” install libraries fresh environment. can done setting remove_first=TRUE install_py_modules.","code":"aifeducation::install_py_modules(   remove_first = TRUE )"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"preface","dir":"Articles","previous_headings":"1 Introduction and Overview","what":"1.1 Preface","title":"03 Using R syntax","text":"vignette introduces package aifeducation usage R syntax. users unfamiliar R coding skills relevant languages (e.g., python), recommend start graphical user interface Aifeducation - Studio, described vignette 02 Using graphical user interface Aifeducation - Studio. assume aifeducation installed described vignette 01 Get Started. introduction starts brief explanation basic concepts, necessary work package.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"basic-concepts","dir":"Articles","previous_headings":"1 Introduction and Overview","what":"1.2 Basic Concepts","title":"03 Using R syntax","text":"educational social sciences, assigning scientific concepts observation important task allows researchers understand observation, generate new insights, derive recommendations research practice. educational science, several areas deal kind task. example, diagnosing students’ characteristics important aspect teachers’ profession necessary understand promote learning. Another example use learning analytics, data students used provide learning environments adapted individual needs. another level, educational institutions schools universities can use information data-driven performance decisions (Laurusson & White 2014) well improve . case, real-world observation aligned scientific models use scientific knowledge technology improved learning instruction. Supervised machine learning one concept allows link real-world observations existing scientific models theories (Berding et al. 2022). educational science, great advantage allows researchers use existing knowledge insights apply AI. drawback approach training AI requires information real world observations information corresponding alignment scientific models theories. valuable source data educational science written texts, since textual data can found almost everywhere realm learning teaching (Berding et al. 2022). example, teachers often require students solve task provide written form. Students create solution tasks often document short written essay presentation. data can used analyze learning teaching. Teachers’ written tasks students may provide insights quality instruction students’ solutions may provide insights learning outcomes prerequisites. AI can helpful assistant analyzing textual data since analysis textual data challenging time-consuming task humans. Please note introduction content analysis, natural language processing machine learning beyond scope vignette. like learn , please refer cited literature. start, necessary introduce definition understanding basic concepts, since applying AI educational contexts means combine knowledge different scientific disciplines using different, sometimes overlapping, concepts. Even within single research area, concepts unified. Figure 1 illustrates package’s understanding. Since aifeducation looks application AI classification tasks perspective empirical method content analysis, overlapping concepts content analysis machine learning. content analysis, phenomenon like performance colors can described scale/dimension made several categories (e.g. Schreier 2012, pp. 59). example, exam’s performance (scale/dimension) “good”, “average” “poor”. terms colors (scale/dimension) categories “blue”, “green”, etc. Machine learning literature uses words describe kind data. machine learning, “scale” “dimension” correspond term “label” “categories” refer term “classes” (Chollet, Kalinowski & Allaire 2022, p. 114). clarifications, classification means text assigned correct category scale , respectively, text labeled correct class. Figure 2 illustrates, two kinds data necessary train AI classify text line supervised machine learning principles. providing AI textual data input data corresponding information class target data, AI can learn texts imply specific class category. exam example, AI can learn texts imply “good”, “average” “poor” judgment. training, AI can applied new texts predict likely class every new text. generated class can used statistical analysis derive recommendations learning teaching. use cases described vignette, AI “understand” natural language: „Natural language processing area research computer science artificial intelligence (AI) concerned processing natural languages English Mandarin. processing generally involves translating natural language data (numbers) computer can use learn world. (…)” (Lane , Howard & Hapke 2019, p. 4) Thus, first step transform raw texts form usable computer, hence raw texts must transformed numbers. modern approaches, usually done word embeddings. Campesato (2021, p. 102) describes “collective name set language modeling feature learning techniques (…) words phrases vocabulary mapped vectors real numbers.” definition word vector similar: „Word vectors represent semantic meaning words vectors context training corpus.” (Lane, Howard & Hapke 2019, p. 191). next step, words text embeddings can used input data labels target data training AI classify text. aifeducation, steps covered three different types models, shown Figure 3. Base Models: base models contain capacities understand natural language. general, transformers BERT, RoBERTa, etc. huge number pre-trained models can found Hugging Face. Text Embedding Models: modes built top base models store directions use base models converting raw texts sequences numbers. Please note base model can used create different text embedding models. Classifiers: Classifiers used top text embedding model. used classify text categories/classes based numeric representation provided corresponding text embedding model. Please note text embedding model can used create different classifiers (e.g. one classifier colors, one classifier estimate quality text, etc.).","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"starting-a-new-session","dir":"Articles","previous_headings":"2 Start Working","what":"2.1 Starting a New Session","title":"03 Using R syntax","text":"can work aifeducation, must set new R session. First, necessary set python via ‘reticulate’ chose conda environment necessary python libraries available. Second, can load aifeducation. case installed python suggested vignette 01 Get started may start new session like : Note: Please remember: Every time start new session R, set correct conda environment load library aifeducation.","code":"reticulate::use_condaenv(condaenv = \"aifeducation\") library(aifeducation)"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"introducation","dir":"Articles","previous_headings":"2 Start Working > 2.2 Data Management","what":"2.2.1 Introducation","title":"03 Using R syntax","text":"context use cases aifeducation, three different types data necessary: raw texts, text embeddings, target data represent categories/classes text. deal first two types allow use large data sets may fit memory machine, packages ships two specialized objects. first LargeDataSetForText. Objects class used read raw texts .txt, .pdf, .xlsx files store computations. second LargeDataSetForTextEmbeddings used store text embeddings raw texts generated TextEmbeddingModels. describe transformation raw texts text embeddings later.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"raw-texts","dir":"Articles","previous_headings":"2 Start Working > 2.2 Data Management","what":"2.2.2 Raw Texts","title":"03 Using R syntax","text":"creation LargeDataSetForText necessary like create train base model generate text embeddings. case like create data set first time call method: Now empty data set. fill object raw texts different methods available depending file type use storing raw texts. .txt files first alternative store raw texts .txt files. use structure data specific way: Create main folder storing data. Store every raw text/document single .txt file folder within main folder. every folder one file raw text/document. Add additional .txt file folder named bib_entry.txt. file contains bibliographic information raw text. Add additional .txt file folder named license.txt contains short statement license text “CC ”. Add additional .txt file folder named url_license.txt contains url/link license’ text “https://creativecommons.org/licenses//4.0/”. Add additional .txt file folder named text_license.txt contains full license raw texts. Add additional .txt file folder named url_source.txt contains url/link text file internet. Applying rules may result data structure follows: text_a.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_b.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_C.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt Now can call method add_from_files_txt passing path directory main folder dir_path. data set now read raw texts main folder assign every text corresponding bib entry license. Please note adding bib_entry.txt, license.txt, url_license.txt, text_license.txt, url_soruce.text every folder optional. file corresponding folder, empty entry data set. However, backdrop European AI Act, recommend provide license bibliographic information make documentation models straightforward. Furthermore, licenses provided Creative Commons require statements creators, copyright note, URL link source material (possible), license material URL link license’s text internet license text . Please check licenses material using requirements. .pdf files second alternative use .pdf files source raw texts. , necessary structure similar .txt files: Create main folder storing data. Store every raw text/document single .pdf file folder within main folder. every folder one file raw text/document. Add additional .txt file folder named bib_entry.txt. file contains bibliographic information raw text. Add additional .txt file folder named license.txt contains short statement license text “CC ”. Add additional .txt file folder named url_license.txt contains URL/link license text “https://creativecommons.org/licenses//4.0/”. Add additional .txt file folder named text_license.txt contains full license raw texts. Add additional .txt file folder named url_source.txt contains url/link text file internet. Applying rules may result data structure follows: text_a.pdf bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_b.pdf bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_C.pdf bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt Please files except text file must .txt, .pdf. Now can call method add_from_files_pdf passing path directory main folder dir_path. stated , bib_entry.txt, license.txt, url_license.txt, text_license.txt, url_soruce.text optional. .xlsx files third alternative store raw texts .xlsx files. alternative useful many small raw texts. raw texts large books papers recommend store .txt .pdf files. order add raw texts .xlsx files, files need special structure: Create main folder storing .xlsx files like read. .xlsx files must contain names columns first row names must identical column across .xslx files like read. Every .xslx files must contain column storing text ID must contain column storing raw text. Every text must unique ID across .xlsx files. Every .xslx file can contain additional column bib entry. Every .xslx file can contain additional column license. Every .xslx file can contain additional column license’s URL. Every .xslx file can contain additional column license text. Every .xslx file can contain additional column source’s URL. .xlsx file may look like Now can call method add_from_files_xlsx passing path directory main folder dir_path. Please forget specify column names ID, text well bibliographic license information. Saving loading data set create LargeDataSetForText can save data disk calling function save_to_disk. example code : argument object requires object like save. case raw_texts. dir_path specific location save object folder_name define name folder created within directory. folder data set saved. load existing data set, can call function load_from_disk directory path stored data. case . Now can work data.","code":"raw_texts <- LargeDataSetForText$new() raw_texts$add_from_files_txt(   dir_path = \"main folder\" ) raw_texts$add_from_files_pdf(   dir_path = \"main folder\" ) raw_texts$add_from_files_xlsx(   dir_path = \"main folder\",   id_column = \"id\",   text_column = \"text\",   bib_entry_column = \"bib_entry\",   license_column = \"license\",   url_license_column = \"url_license\",   text_license_column = \"text_license\",   url_source_column = \"url_source\" ) save_to_disk(   object = raw_texts,   dir_path = \"C:/\",   folder_name = \"raw_texts\" ) raw_text_dataset <- load_from_disk(\"C:/raw_texts\")"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"text-embeddings","dir":"Articles","previous_headings":"2 Start Working > 2.2 Data Management","what":"2.2.3 Text Embeddings","title":"03 Using R syntax","text":"numerical representations raw texts (called text embeddings) stored objects class LargeDataSetForTextEmbeddings. kinds data sets generated models TextEmbeddingModels. Thus, never need create data set manually. However, need kind data set train classifier predict categories/classes raw texts. Thus, may advantageous save already transformed data. can save load object class functions save_to_disk load_from_disk. Let us assume LargeDataSetForTextEmbeddings text_embeddings. Saving object may look like: data set saved C:/text_embeddings. Loading data set may look like:","code":"save_to_disk(   object = text_embeddings,   dir_path = \"C:/\",   folder_name = \"text_embeddings\" ) new_text_embeddings <- load_from_disk(\"C:/text_embeddings\")"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"target-data","dir":"Articles","previous_headings":"2 Start Working > 2.2 Data Management","what":"2.2.4 Target Data","title":"03 Using R syntax","text":"last data type necessary working aifeducation categories/classes given raw texts. kind data currently provide special object. just need named factor storing classes/categories dimension. also important names equal ID corresponding raw texts/text embeddings since matching classes/categories texts done help names. Saving loading can done R’s functions save load.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"example-data-for-this-vignette","dir":"Articles","previous_headings":"2 Start Working","what":"2.3 Example Data for this Vignette","title":"03 Using R syntax","text":"illustrate steps vignette, use data educational settings since data generally protected privacy policies. Therefore, use subset Standford Movie Review Dataset provided Maas et al. (2011) part package. can access data set imdb_movie_reviews. now data set three columns. first column contains raw text, second contains rating movie (positive negative), third column ID movie review. 200 reviews imply positive rating movie 100 imply negative rating. tutorial, modify data set setting 50 positive 25 negative reviews NA, indicating reviews labeled. now create LargeDataSetForText data.frame. can must ensure data.set necessary columns: Now add two columns. tutorial add bibliographic license information although recommended practice. Now data.frame ready input data set. “label” column included data set. save categories/labels within separate factor. now use data show use different objects functions aifeducation.","code":"example_data <- imdb_movie_reviews example_data$label <- as.character(example_data$label) example_data$label[c(76:100)] <- NA example_data$label[c(201:250)] <- NA example_targets <- as.factor(example_data$label) table(example_data$label) #>  #> neg pos  #>  75 150 colnames(example_data) #> [1] \"text\"  \"label\" \"id\" example_data$bib_entry <- NA example_data$license <- NA colnames(example_data) #> [1] \"text\"      \"label\"     \"id\"        \"bib_entry\" \"license\" data_set_reviews_text <- LargeDataSetForText$new() data_set_reviews_text$add_from_data.frame(example_data) review_labels <- example_data$label names(review_labels) <- example_data$id"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"overview","dir":"Articles","previous_headings":"3 Base Models","what":"3.1 Overview","title":"03 Using R syntax","text":"Base models foundation models aifeducation. moment, transformer models MPNet(), BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version 2 (et al. 2020), Funnel-Transformer (Dai et al. 2020), Longformer (Beltagy, Peters & Cohan 2020). general, models trained large corpus general texts first step. next step, models fine-tuned domain-specific texts /fine-tuned specific tasks. Since creation base models requires huge number texts resulting high computational time, recommended use pre-trained models. can found Hugging Face. Sometimes, however, straightforward create new model fit specific purpose. aifeducation supports option create train/fine-tune base models.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"creation-of-base-models","dir":"Articles","previous_headings":"3 Base Models","what":"3.2 Creation of Base Models","title":"03 Using R syntax","text":"Every transformer model composed two parts: 1) tokenizer splits raw texts smaller pieces model large number words limited, small number tokens 2) neural network used model capabilities understanding natural language. beginning can choose different supported transformer architectures. Depending architecture, different options determining shape neural network. vignette use BERT (Devlin et al. 2019) model can created create-method Transformer class. Use aife_transformer_maker create transformer object. See p. 3 Transformer Maker 01 Transformers Developers details. First, function receives machine learning framework chose start session. However, can change setting ml_framework=\"tensorflow\" ml_framework=\"pytorch\". function work, must provide path directory new transformer saved (model_dir). Furthermore, must provide raw texts. texts used train transformer vocabulary. maximum size vocabulary determined vocab_size. Modern tokenizers WordPiece (Wu et al. 2016) use algorithms splits tokens smaller elements, allowing build huge number words small number elements. Thus, even small number 30,000 tokens, able represent large number words. parameters allow customize BERT model. example, increase number hidden layers 12 24 reduce hidden size 768 256, allowing build test larger smaller models. vignette 04 Model configuration provides details configure base model. Please note max_position_embeddings determine many tokens transformer can process. text tokens, tokens ignored. However, like analyze long documents, please avoid increase number significantly computational time increase linear way quadratic (Beltagy, Peters & Cohan 2020). long documents can use another architecture BERT (e.g. Longformer Beltagy, Peters & Cohan 2020) split long document several chunks used sequentially classification (e.g., Pappagari et al. 2019). Using chunks supported aifedcuation models. Since creating transformer model energy consuming, aifeducation allows estimate ecological impact help python library codecarbon. Thus, sustain_track set TRUE default. use sustainability tracker must provide alpha-3 code country computer located (e.g., “CAN”=“Canada”, “Deu”=“Germany”). list codes can found Wikipedia. reason different countries use different sources techniques generating energy resulting specific impact CO2 emissions. USA Canada can additionally specify region setting sustain_region. Please refer documentation codecarbon information. calling function, find new model model directory.","code":"base_model<-aife_transformer_maker$make(\"bert\") base_model$create(   ml_framework = \"pytorch\",   model_dir = \"my_own_transformer\",   text_dataset = LargeDataSetForText$new(example_data),   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   sustain_track = TRUE,   sustain_iso_code = \"DEU\",   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"trainfine-tune-a-base-model","dir":"Articles","previous_headings":"3 Base Models","what":"3.3 Train/Fine-Tune a Base Model","title":"03 Using R syntax","text":"like train new base model (see section 3.2) first time want adapt pre-trained model domain-specific language task, can call corresponding train-method. See p. 3 Transformer Maker 01 Transformers Developers details. important provide path directory new transformer stored. Furthermore, important provide another directory trained transformer saved avoid reading writing collisions. Now, provided raw data used train model. case BERT model, learning objective Masked Language Modeling. models may use learning objectives. Please refer documentation details every model. First, can set length token sequences chunk_size. whole_word can choose masking single tokens masking complete words (Please remember modern tokenizers split words several tokens. Thus, tokens words forced match directly). p_mask can determine many tokens masked. Finally, val_size, set many chunks tokens used validation sample. Minimum 2. Please remember set correct alpha-3 code tracking ecological impact training model (sustain_iso_code). work machine graphic device small memory capacity, please reduce batch size significantly. also recommend change usage memory set_config_gpu_low_memory() beginning session use tensorflow framework. training finishes, can find transformer ready use output_directory. Now able create text embedding model. can change machine learning framework setting ml_framework=\"tensorflow\" ml_framework=\"pytorch\". change argument, framework chose beginning used.","code":"base_model$train(   ml_framework = \"pytorch\",   output_dir = \"my_own_transformer_trained\",   model_dir_path = \"my_own_transformer\",   text_dataset = LargeDataSetForText$new(example_data[1:10, ]),   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   n_workers = 1,   multi_process = FALSE,   sustain_track = TRUE,   sustain_iso_code = \"DEU\",   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"introduction","dir":"Articles","previous_headings":"4 Text Embedding Models","what":"4.1 Introduction","title":"03 Using R syntax","text":"text embedding model interface R aifeducation. order create new model, need base model provides ability understand natural language. text embedding model stored object class TextEmbeddingModel. object contains relevant information transforming raw texts numeric representation can used machine learning. aifedcuation, transformation raw texts numbers separate step downstream tasks classification. reduce computational time machines low performance. separating text embedding tasks, text embedding calculated can used different tasks time. Another advantage training downstream tasks involves downstream tasks parameters embedding model, making training less time-consuming, thus decreasing computational intensity. Finally, approach allows analysis long documents applying algorithm different parts. text embedding model provides unified interface: creating model different methods, handling model always .","code":""},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"create-a-text-embedding-model","dir":"Articles","previous_headings":"4 Text Embedding Models","what":"4.2 Create a Text Embedding Model","title":"03 Using R syntax","text":"First choose base model forms foundation new text embedding model. Since use BERT model example, set method = \"bert\". Next, provide directory base model stored. example model_dir=\"my_own_transformer_trained. course can use pre-trained model Hugging Face addresses needs. Using BERT model text embedding problem since text provide tokens transformer can process. maximum value set configuration transformer (see section 3.2). text produces tokens, last tokens ignored. instances might want analyze long texts. situations, reducing text first tokens (e.g. first 512 tokens) result problematic loss information. deal situations, can configure text embedding model aifecuation split long texts several chunks processed base model. maximum number chunks set chunks. example , text embedding model split text consisting 1024 tokens two chunks every chunk consisting 512 tokens. every chunk, text embedding calculated. result, receive sequence embeddings. first embedding characterizes first part text second embedding characterizes second part text (). Thus, sample text embedding model able process texts 4*512=2048 tokens. approach inspired work Pappagari et al. (2019). Since transformers able account context, may useful interconnect every chunk bring context calculations. can done overlap determine many tokens end prior chunk added next. example last 30 tokens prior chunks added beginning following chunk. can help add correct context text sections analysis. Altogether, sample model can analyse maximum 512+(4−1)*(512−30)=1958512+(4-1)*(512-30)=1958 tokens text. Finally, decide hidden layer(s) embeddings drawn. emb_layer_min emb_layer_max can decide layers average value every token calculated. Please note calculation considers layers emb_layer_min emb_layer_max. initial work, Devlin et al. (2019) used hidden states different layers classification. emb_pool_type, decide tokens used pooling within every layer. case emb_pool_type=\"cls\", cls token used. case emb_pool_type=\"average\" tokens within layer averaged except padding tokens. vignette 04 Model configuration provides details configure text embedding model. deciding configuration, can use model.","code":"bert_modeling <- TextEmbeddingModel$new() bert_modeling$configure(   model_name = \"bert_embedding\",   model_label = \"Text Embedding via BERT\",   model_language = \"english\",   method = \"bert\",   max_length = 512,   chunks = 4,   overlap = 30,   emb_layer_min = \"middle\",   emb_layer_max = \"2_3_layer\",   emb_pool_type = \"average\",   model_dir = \"my_own_transformer_trained\" )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"transforming-raw-texts-into-embedded-texts","dir":"Articles","previous_headings":"4 Text Embedding Models","what":"4.3 Transforming Raw Texts into Embedded Texts","title":"03 Using R syntax","text":"transform raw text numeric representation, use embed_large method model. , must provide LargeDataSetForText large_datas_set. Relying sample data section 2.3, can use movie reviews raw texts. method embed_largecreates object class LargeDataSetForTextEmbeddings. just data set consisting embeddings every text. embeddings array, first dimension refers specific texts, second dimension refers chunks/sequences, third dimension refers features. embedded texts now input train new classifier apply pre-trained classifier predicting categories/classes. next chapter show use classifiers. start, show save load model.","code":"review_embeddings <- bert_modeling$embed_large(   large_datas_set = data_set_reviews_text,   trace = TRUE )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"saving-and-loading-text-embedding-models","dir":"Articles","previous_headings":"4 Text Embedding Models","what":"4.4 Saving and Loading Text Embedding Models","title":"03 Using R syntax","text":"Saving created text embedding model easy aifeducation using function save_to_disk. function provides unique interface text embedding models. saving work can pass model object directory save model dir_path. folder_name can determine name folder created directory store model. example model saved folder location C:/text_embedding_models/bert_model. want load model can call load_from_disk.","code":"save_ai_model(   object = bert_modeling,   dir_path = \"C:/text_embedding_models\",   folder_name = \"bert_model\" ) bert_modeling <- load_from_disk(\"C:/text_embedding_models/bert_model\")"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"sustainability","dir":"Articles","previous_headings":"4 Text Embedding Models","what":"4.5 Sustainability","title":"03 Using R syntax","text":"case underlying model trained active sustainability tracker (section 3.2 3.3) can receive table showing energy consumption, CO2 emissions, hardware used training calling method get_sustainability_data(). example bert_modeling$get_sustainability_data().","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"create-a-classifier","dir":"Articles","previous_headings":"5 Classifiers","what":"5.1 Create a Classifier","title":"03 Using R syntax","text":"Classifiers built top TextEmbeddingModel. can create new classifier calling TEClassifierRegular$new(). TE object class refers idea classifiers uses text embeddings instead raw texts. sample data section 2.3 text embeddings section 4.3, creation new classifier may look like: Similarly text embedding model, provide name (name) label (label) new classifier. text_embeddings provide LargeDataSetForTextEmbeddings. data set created TextEmbeddingModel described section 4. continue example use embedding produced BERT model. target_levels take categories/classes classifier predict. can numbers even words. case like use ordinal data, important provide classes/categories correct order. , classes/categories representing “higher” level must stated categories/classes lower level. provide wrong order, performance indices valid. case nominal data order matter. feature_extractor can add feature extractor tries reduce number features text embedding passing embeddings classifier. can read Section 6.2. parameters decide structure classifier. Figure 4 illustrates . dense_layers takes vector integers, determining number layers dense_size determines number neurons dense layers. example, two dense layers 5 neurons. rec_layers also takes vector integers determining number layers rec_size determines size recurrent layers. example, use two layer 10 neurons . rec_type can choose two types recurrent layers. rec_type=\"gru\" implements Gated Recurrent Unit (GRU) network rec_type=\"lstm\" implements Long Short-Term Memory layer. rec_bidirectional can decide whether recurrent layer unidirectional bidirectional. Since classifiers aifeducation use standardized scheme creation, dense layers used gru layers. want omit gru layers dense layers, set corresponding argument number layers 0 (dense_layers=0, rec_layers=0). use text embedding model processes one chunk recommend use recurrent layers, since use sequential structure data. cases can rely dense layers . use text embeddings one chunk, can try self-attention layering order take context chunks account. add self-attention two choices: can use attention mechanism used classic transformer models multi-head attention (Vaswani et al. 2017). variant set attention_type=\"multihead\", repeat_encoder value least 1, self_attention_heads value least 1. Furthermore can use attention mechanism described Lee-Thorp et al. (2021) FNet model allows much fast computations low accuracy costs. use kind attention set attention_type=\"fourier repeat_encoder value least 1. repeat_encoder can choose many times encoder layer added. encoder implemented described Chollet, Kalinowski, Allaire (2022, pp. 373) variants attention. example 300 cases altogether 4 chunks. Thus, use encoder layers. can extend abilities network adding positional embeddings. Positional embeddings take care order chunks. Thus, adding layer may increase performance order information important. can add layer setting add_pos_embedding=TRUE. layer created described Chollet, Kalinowski, Allaire (2022, pp. 378). vignette 04 Model configuration provides details configure classifier. Masking, normalization, creation input layer well output layer done automatically. created new classifier, can begin training.","code":"classifier <- TEClassifierRegular$new() classifier$configure(   name = \"movie_review_classifier\",   label = \"Classifier for Estimating a Postive or Negative Rating of Movie Reviews\",   text_embeddings = review_embeddings,   feature_extractor = NULL,   target_levels = c(\"neg\", \"pos\"),   dense_layers=2,   dense_size=5,   rec_layers=2,   rec_size=10,   rec_type = \"gru\",   rec_bidirectional = FALSE,   self_attention_heads = 0,   intermediate_size = NULL,   attention_type = \"fourier\",   add_pos_embedding = FALSE,   rec_dropout = 0.5,   repeat_encoder = 0,   dense_dropout = 0.2,   recurrent_dropout = 0.6,   encoder_dropout = 0.1,   optimizer = \"adam\" )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"training-a-classifier","dir":"Articles","previous_headings":"5 Classifiers","what":"5.2 Training a Classifier","title":"03 Using R syntax","text":"start training classifier, call train method. Similarly, creation classifier, must provide text embedding data_embeddings categories/classes target data data_targets. Please remember data_targets expects named factor names correspond IDs corresponding text embeddings. Text embeddings target data matched omitted training. train classifier, necessary provide path dir_checkpoint. directory stores best set weights training epoch. training, weights automatically used final weights classifier. performance estimation, training splits data several chunks based cross-fold validation. number folds set data_folds. every case, one fold used training serves test sample. remaining data used create training validation sample. percentage cases within fold used validation sample determined data_val_size. sample used determine state model generalizes best. performance values saved trained classifier refer test sample. data never used training provides realistic estimation classifier’s performance. can modify training process different arguments. balance_class_weights=TRUE absolute frequencies classes/categories adjusted according ‘Inverse Class Frequency’ method. option activated deal imbalanced data. balance_sequence_length=TRUE can increase performance deal texts differ lengths imbalanced frequency. option enabled, loss adjusted absolute frequencies length texts according ‘Inverse Class Frequency’ method. epochs determines maximal number epochs. training, model best balanced accuracy saved used. batch_sizesets number cases processed simultaneously. Please adjust value machine’s capacities. Please note batch size can impact classifier’s performance. Since aifedcuation tries address special needs educational social science, special training steps integrated method. Synthetic Cases: case imbalanced data, recommended set use_sc=TRUE. training, number synthetic units created via different techniques. Currently can request Basic Synthetic Minority Oversampling Technique, Density-Bases Synthetic Minority Oversampling Technique, Adaptive Synthetic Sampling Approach Imbalanced Learning. aim create new cases fill gap majority class. Multi-class problems reduced two class problem (class investigation vs. ) generating units. technique allows set number neighbors generation, can configure data generation sc_min_k sc_max_k. synthetic cases every class generated k sc_min_k sc_max_k. Every k contributes proportionally synthetic cases. Pseudo-Labeling: technique relevant labeled target data large number unlabeled target data. different parameters starting “pl_”, can configure process pseudo-labeling. Implementation pseudo-labeling based Cascante-Bonilla et al. (2020). apply pseudo-labeling, set use_pl=TRUE. pl_max=1.00, pl_anchor=1.00, pl_min=0.00 used describe certainty prediction. 0 refers random guessing 1 refers perfect certainty. pl_anchor used reference value. distance pl_anchor calculated every case. , sorted increasing distance pl_anchor. proportion added pseudo-labeled data training increases every step. maximum number steps determined pl_max_steps. Figure 5 illustrates training loop cases options set TRUE. example applies generation synthetic cases algorithm proposed Cascante-Bonilla et al. (2020). every fold, training starts generating synthetic cases fill gap classes majority class. , initial training classifiers starts. trained classifier used predict pseudo-labels unlabeled part data adds 20% cases highest certainty pseudo-labels training data set. Now new synthetic cases generated based labeled data newly added pseudo-labeled data. classifier re-initialized trained . training, classifier predicts potential labels originally unlabeled data adds 40% pseudo-labeled data training data highest certainty. , new synthetic cases generated labeled added pseudo-labeled data. model re-initialized trained maximum number steps pseudo labeling (pl_max_steps) reached. , logarithm restated next fold number folds (data_folds) reached. steps used estimate performance classifier evaluate classifier’s unknown data. last phase training begins last fold. final training, data set split training validation set without test set provide maximum amount data best performance final training. case options like generation synthetic cases (use_sc) pseudo-labeling (use_pl) disabled, training process shorter. Since training neural net energy consuming, aifeducation allows estimate ecological impact help python library codecarbon. Thus, sustain_track set TRUE default. use sustainability tracker must provide alpha-3 code country computer located (e.g., “CAN”=“Canada”, “Deu”=“Germany”). list codes can found Wikipedia. reason different countries use different sources techniques generating energy resulting specific impact CO2 emissions. USA Canada, can additionally specify region setting sustain_region. Please refer documentation codecarbon information. Finally, trace, ml_trace allow control much information training progress printed console. Please note training classifier can take time. Please note performance estimation, final training classifier makes use data available. , test sample left empty.","code":"classifier$train(   data_embeddings = review_embeddings,   data_targets = review_labels,   data_folds = 10,   data_val_size = 0.25,   balance_class_weights = TRUE,   balance_sequence_length = TRUE,   use_sc = FALSE,   sc_method = \"dbsmote\",   sc_min_k = 1,   sc_max_k = 10,   use_pl = FALSE,   pl_max_steps = 5,   pl_max = 1.00,   pl_anchor = 1.00,   pl_min = 0.00,   sustain_track = TRUE,   sustain_iso_code = \"DEU\",   sustain_region = NULL,   sustain_interval = 15,   epochs = 300,   batch_size = 32,   dir_checkpoint = \"training/classifier\",   trace = TRUE,   ml_trace = 1 )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"evaluating-classifiers-performance","dir":"Articles","previous_headings":"5 Classifiers","what":"5.3 Evaluating Classifier’s Performance","title":"03 Using R syntax","text":"finishing training, can evaluate performance classifier. every fold, classifier applied test sample results compared true categories/classes. Since test sample never part training, performance measures provide realistic idea classifier’s performance. support researchers judging quality predictions, aifeducation utilizes several measures concepts content analysis. Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa unweighted Cohen’s Kappa equal weights Cohen’s Kappa squared weights Fleiss’ Kappa multiple raters without exact estimation can access concrete values accessing field reliability, stores relevant information. list find reliability values every fold. addition, reliability every step within pseudo-labeling reported. central estimates reliability values can found via reliability$test_metric_mean. example : particular interest values alpha Iota Concept, since represent measure reliability independent frequency distribution classes/categories. alpha values describe probability case specific class recognized specific class. can see, compared baseline model, applying Balanced Synthetic Cases increased increases minimal value alpha, reducing risk miss cases belong rare class (see row “BSC”). contrary, alpha values major category decrease slightly, thus losing unjustified bonus high number cases training set. provides realistic performance estimation classifier. addition, standard measures machine learning reported. Precision Recall F1-Score can access values follows: Finally, can plot coding stream scheme showing cases different classes labeled. use package iotarelr. Figure 6: Coding Stream Classifier can see small number negative reviews treated good review, larger number positive reviews treated bad review. Thus, data major class (negative reviews) reliable valid data minor class (positive reviews). Evaluating performance classifier complex task beyond scope vignette. Instead, like refer cited literature content analysis machine learning like dive deeper topic.","code":"classifier$reliability$test_metric_mean #>              iota_index               min_iota2               avg_iota2  #>               0.5606719               0.4584235               0.5869457  #>               max_iota2               min_alpha               avg_alpha  #>               0.7154678               0.5785714               0.7226190  #>               max_alpha       static_iota_index      dynamic_iota_index  #>               0.8666667               0.2620308               0.4736155  #>          kalpha_nominal          kalpha_ordinal                 kendall  #>               0.4654527               0.4654527               0.7369689  #>       kappa2_unweighted   kappa2_equal_weighted kappa2_squared_weighted  #>               0.4613610               0.4613610               0.4613610  #>            kappa_fleiss    percentage_agreement       balanced_accuracy  #>               0.4533283               0.7693676               0.7226190  #>                 gwet_ac           avg_precision              avg_recall  #>               0.5980910               0.7610960               0.7226190  #>                  avg_f1  #>               0.7266641 classifier$reliability$standard_measures_mean #>     precision    recall        f1 #> neg 0.7155556 0.5785714 0.6209740 #> pos 0.8066364 0.8666667 0.8323543 library(iotarelr) iotarelr::plot_iota2_alluvial(classifier$reliability$iota_object_end_free)"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"sustainability-1","dir":"Articles","previous_headings":"5 Classifiers","what":"5.4 Sustainability","title":"03 Using R syntax","text":"case classifier trained active sustainability tracker, can receive information sustainability calling classifier$get_sustainability_data().","code":"classifier$get_sustainability_data() #> $sustainability_tracked #> [1] TRUE #>  #> $date #> [1] \"Tue Oct  1 20:56:37 2024\" #>  #> $sustainability_data #> $sustainability_data$duration_sec #> [1] 343.5135 #>  #> $sustainability_data$co2eq_kg #> [1] 0.0005406515 #>  #> $sustainability_data$cpu_energy_kwh #> [1] 0.001012826 #>  #> $sustainability_data$gpu_energy_kwh #> [1] 0 #>  #> $sustainability_data$ram_energy_kwh #> [1] 0.0004664782 #>  #> $sustainability_data$total_energy_kwh #> [1] 0.001479304 #>  #>  #> $technical #> $technical$tracker #> [1] \"codecarbon\" #>  #> $technical$py_package_version #> [1] \"2.3.4\" #>  #> $technical$cpu_count #> [1] 8 #>  #> $technical$cpu_model #> [1] \"11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz\" #>  #> $technical$gpu_count #> [1] NA #>  #> $technical$gpu_model #> [1] NA #>  #> $technical$ram_total_size #> [1] 15.73279 #>  #>  #> $region #> $region$country_name #> [1] \"Germany\" #>  #> $region$country_iso_code #> [1] \"DEU\" #>  #> $region$region #> [1] NA"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"saving-and-loading-a-classifier","dir":"Articles","previous_headings":"5 Classifiers","what":"5.5 Saving and Loading a Classifier","title":"03 Using R syntax","text":"Saving loading follows pattern objects aifeducation. can save classifier calling save_to_disk. example may : classifier saved C:/classifiers/imdb_movie_reviews. load model call load_from_disk.","code":"save_to_disk(   object = classifier,   dir_path = \"C:/classifiers\",   folder_name = \"imdb_movie_reviews\" ) classifier <- load_from_disk(\"C:/classifiers/imdb_movie_reviews\")"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"predicting-new-data","dir":"Articles","previous_headings":"5 Classifiers","what":"5.6 Predicting New Data","title":"03 Using R syntax","text":"like apply classifier new data, two steps necessary. must first transform raw text numerical expression using exactly text embedding model used train classifier (see section 4). case example classifier, use BERT model. transform raw texts numeric representation just pass raw texts method embed_large loaded model. raw texts object class LargeDataSetForText. create data set, please refer section 2. example , text embeddings stored review_embeddings. Since embedding texts may take time, good idea save embeddings future analysis (see section 2 details). allows load embeddings without need apply text embedding model raw texts . resulting object can passed method predict classifier get predictions together estimate certainty class/category. classifier finishes prediction, estimated categories/classes stored predicted_categories. object data.frame containing texts’ IDs rows probabilities different categories/classes columns. last column name expected_category represents category assigned text due highest probability. estimates can used analysis common methods educational social sciences correlation analysis, regression analysis, structural equation modeling, latent class analysis analysis variance. Now ready use aifeducation. section 6 describe models classification tasks improving model performance.","code":"# If our mode is not loaded bert_modeling <- load_from_disk(\"C:/text_embedding_models/bert_model\")  # Create a numerical representation of the text review_embeddings <- bert_modeling$embed_large(   large_datas_set = data_set_reviews_text,   trace = TRUE ) # If your classifier is not loaded classifier <- load_from_disk(\"C:/classifiers/imdb_movie_reviews\")  # Predict the classes of new texts predicted_categories <- classifier$predict(   newdata = review_embeddings,   batch_size = 8 )"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"classifiers-protonet","dir":"Articles","previous_headings":"6 Extensions","what":"6.1 Classifiers: ProtoNet","title":"03 Using R syntax","text":"classifier introduced section 5 regular classifier comes traditional challenges deep learning, need large number training data, expensive hardware requirements, limited possibility interpret model’s parameters (Jadon & Garg 2020, pp.13-14). Since educational social sciences data bottle neck, classifier can work small data sets preferable. types models discussed literature terms “meta-learning” (Zou 2023) “-shot learning” (Jadon & Garg 2020). basic idea behind approaches model learns use supporting data set predict output query data set (e.g., Zou 2023, pp. 2-3). However, model explicitly trained query data set. One type models within area Prototypical Networks (ProtoNet) initially proposed Snell, Swersky, Zemel (2017). type network developed create classifiers able generalize new classes model see training, using information examples class provided network (support data set). achieve goal, networks learn create prototype every class support data set help examples every class. , network compares new data prototypes assigns class nearest prototype new data. Since network calculates distance every new case every prototype, belongs metric-based meta-learning approaches (Zhou 2023, pp. 48). Since ProtoNet simple, easy understand approach provie3w good performance, several extensions suggested. aifeducation replaces original loss function loss function suggested Zhang et al. (2019) adds learnable metric described Oreshkin, Rodriguez, Lacoste (2019) increase performance. implementation provided aifeducation currently applies fixed set classes prototypes learned training using available training data. extended/changed future allow selection support data set user. application classifier based ProtoNet similar regular classifiers. difference embedding_dim. ProtoNet classifier uses network project similarity differences single cases prototypes n-dimensional space. Similar cases located near different cases located away. number dimensions space determined embedding_dim. case embedding_dim set 1,2 3 position every case prototypes can easily visualized. example use data section 5. Let us first create configure new classifier. Now can plot untrained classifiers embeds different cases prototypes. create corresponding plot can call method plot_embeddings. argument embeddings_q takes embeddings different cases input classifier. case true classes cases, can add plot using argument classes_q. resulting plot shown following Figure. large triangles represent prototypes every class dots refer labeled cases data set. , color represents true class. unlabeled cases, square used. , color indicates class estimates. can see, cases located similarly seems clear structure. Let us see changes train model. arguments requesting balance class weights (balance_class_weights) balancing sequence length (balance_sequence_length), four new arguments available. Ns determine many examples every class used training within support sample. examples used calculate prototypes every class. Nq determine many examples every class part query sample. training network tries predict correct classes examples. arguments loss_alpha loss_margin refer configuration loss function describes Zhang et al. (2019). loss_margin refers minimal distance examples query sample prototypes represent class. loss_alpha determines loss pay attention minimize distance examples corresponding prototype pay attention maximize distance prototypes represent class. set loss_alpha=1, loss tries minimize distance examples corresponding prototype. set loss_alpha=0, loss pays tries maximize distance examples prototypes reflect class. next two important arguments refer sampling strategies training. sampling_separate=TRUE, cases sample query drawn pool cases. Thus, specific case can sample case one epoch query case another epoch. However, ensured specific cases never occurs sample query training step. addition, ensured every case exists training step. set sampling_separate=FALSE, training data set split one data pool sample one data pool query. Thus, case can sample case query case. shuffle can request every training step random sample chosen training data set, resulting different combinations sample query cases. training highly recommend set shuffle=TRUE, since result better performing classifiers. training can request visualization data . first omit unlabeled cases setting inc_unlabeled=FALSE order get impression quality training. shown figure, cases now sorted. Cases class “neg” located close prototype “neg”, cases class “pos” located near prototype “pos”. Since use data training, result expected. small number cases located near wrong prototype. can seen red dot close prototype “pos” green dot close red prototype “neg”. Let us now add unlabeled cases plot setting inc_unlabeled=TRUE. following figure shows, model estimates class cases according distance two prototypes. Cases close prototype “pos” assigned “pos”, cases near prototype “neg” assigned “neg”. Finally, let us report reliability classifier.","code":"classifier <- TEClassifierProtoNet$new() classifier$configure(   name = \"proto_net_movie_review_classifier\",   label = \"ProtoNet classifier for Estimating a Postive or Negative Rating of Movie Reviews\",   text_embeddings = review_embeddings,   feature_extractor = NULL,   target_levels = c(\"neg\", \"pos\"),   hidden = c(5),   rec = c(6, 6),   rec_type = \"gru\",   rec_bidirectional = FALSE,   embedding_dim = 2,   self_attention_heads = 0,   intermediate_size = NULL,   attention_type = \"fourier\",   add_pos_embedding = TRUE,   rec_dropout = 0.3,   repeat_encoder = 0,   dense_dropout = 0.4,   recurrent_dropout = 0.4,   encoder_dropout = 0.1,   optimizer = \"adam\" ) plot_untrained<-classifier$plot_embeddings(   embeddings_q = review_embeddings,   classes_q=review_labels, ) plot_untrained classifier$train(   data_embeddings = review_embeddings,   data_targets = review_labels,   data_folds = 5,   data_val_size = 0.25,   use_sc = TRUE,   sc_method = \"dbsmote\",   sc_min_k = 1,   sc_max_k = 10,   use_pl = TRUE,   pl_max_steps = 5,   pl_max = 1.00,   pl_anchor = 1.00,   pl_min = 0.00,   sustain_track = TRUE,   sustain_iso_code = \"DEU\",   sustain_region = NULL,   sustain_interval = 15,   epochs = 400,   batch_size = 32,   Ns = 2,   Nq = 10,   loss_alpha = 0.5,   loss_margin = 0.5,   sampling_separate=FALSE,   sampling_shuffle = TRUE,   dir_checkpoint = \"training/classifier\",   trace = TRUE,   ml_trace=1 ) plot_trained_1<-classifier$plot_embeddings(   embeddings_q = review_embeddings,   classes_q=review_labels,   inc_unlabeled=FALSE ) plot_trained_1 plot_trained_2<-classifier$plot_embeddings(   embeddings_q = review_embeddings,   classes_q=review_labels,   inc_unlabeled=FALSE ) plot_trained_2 classifier$reliability$test_metric_mean #>              iota_index               min_iota2               avg_iota2  #>               0.4375494               0.3161485               0.4643332  #>               max_iota2               min_alpha               avg_alpha  #>               0.6125178               0.4013095               0.6123214  #>               max_alpha       static_iota_index      dynamic_iota_index  #>               0.8233333               0.1946912               0.3727251  #>          kalpha_nominal          kalpha_ordinal                 kendall  #>               0.2297705               0.2297705               0.6241177  #>       kappa2_unweighted   kappa2_equal_weighted kappa2_squared_weighted  #>               0.2345552               0.2345552               0.2345552  #>            kappa_fleiss    percentage_agreement       balanced_accuracy  #>               0.2122470               0.6717391               0.6123214  #>                 gwet_ac           avg_precision              avg_recall  #>               0.4255430               0.6485922               0.6123214  #>                  avg_f1  #>               0.6061235"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"feature-extractors","dir":"Articles","previous_headings":"6 Extensions","what":"6.2 Feature Extractors","title":"03 Using R syntax","text":"Another option increase model’s performance /increase computational speed apply feature extractor. example, work Ganesan et al. (2021) indicates reduction hidden size can increase model’s accuracy. aifeducation, feature extractor model tries reduce number features given text embeddings feeding embeddings input classifier. feature extractors implemented aifeducation auto-encoders support sequential data sequences different length. basic architecture extractors shown following figure. learning objective feature extractors first compress information reducing number features number features latent space (Frochte 2019, p.281). figure , mean reduce number features 8 4 store much information possible 8 dimensions 4 dimensions. next step, extractor tries reconstruct original information compressed information latent space (Frochte 2019, pp.280-281). information extended 4 dimensions 8. training, hidden representation latent space used compression original input. can create feature extractor follows. Similarly models, can use name model’s name label model’s label. argument text_embeddings takes object class EmbeddedText LargeDataSetForTextEmbeddings. object connect feature extractor specific TextEmbeddingModel. , feature extractor works embeddings exactly TextEmbeddingModel. features determines number features compressed representation. lower number, higher requested compression. value corresponds features latent space figure . method determine type layer feature extractor use. set method=\"lstm\", layers model long short-term memory layers. set method=\"dense\" layers standard dense layers. Independently choice, models try generate latent space co-variance features zero. Thus, features represent unique information. addition, methods except \"lstm\" use orthogonal parameterization prevent -fitting apply parameter sharing. opposite layers use parameters. details please refer Ranjan (2019). noise_factor can add noise training making feature extractor perform denoising auto-encoder, can provide robust generalizations. Training extractor identical models aifeducation. Please note text embeddings provided data_embeddings must generated TextEmbeddingModels embeddings provided configuration model. trained feature extractor, can use every classifier. Just pass feature extractor feature_extractor configuration classifier. classifier described section 5 look like: . Now can use train classifier way without feature extractor. Even saving loading done automatically.","code":"feature_extractor<-TEFeatureExtractor$new() feature_extractor$configure(    name = \"feature_extractor_bert_movie_reviews\",    label = \"Feature extractor for Text Embeddings via BERT\",    text_embeddings = review_embeddings,    features = 128,    method = \"lstm\",    noise_factor = 0.2,    optimizer = \"adam\" ) feature_extractor$train(  data_embeddings=review_embeddings,  data_val_size = 0.25,  sustain_track = TRUE,  sustain_iso_code = \"DEU\",  sustain_region = NULL,  sustain_interval = 15,  epochs = 40,  batch_size = 32,  dir_checkpoint,  trace = TRUE,  ml_trace = 1, ) classifier <- TEClassifierRegular$new() classifier$configure(   name = \"movie_review_classifier\",   label = \"Classifier for Estimating a Postive or Negative Rating of Movie Reviews\",   text_embeddings = review_embeddings,   feature_extractor = feature_extractor,   target_levels = c(\"neg\", \"pos\"),   hidden = c(5),   rec = c(6, 6),   rec_type = \"gru\",   rec_bidirectional = FALSE,   self_attention_heads = 0,   intermediate_size = NULL,   attention_type = \"fourier\",   add_pos_embedding = TRUE,   rec_dropout = 0.1,   repeat_encoder = 0,   dense_dropout = 0.4,   recurrent_dropout = 0.4,   encoder_dropout = 0.1,   optimizer = \"adam\" )"},{"path":"https://fberding.github.io/aifeducation/articles/classification_tasks.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"03 Using R syntax","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Berlin: Logos. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1–21. https://doi.org/10.3389/feduc.2022.818365 Campesato, O. (2021). Natural Language Processing Fundamentals Developers. Mercury Learning & Information. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713 Cascante-Bonilla, P., Tan, F., Qi, Y. & Ordonez, V. (2020). Curriculum Labeling: Revisiting Pseudo-Labeling Semi-Supervised Learning. https://doi.org/10.48550/arXiv.2001.06001 Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning R (Second edition). Manning Publications Co. https://learning.oreilly.com/library/view/-/9781633439849/?ar Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer: Filtering Sequential Redundancy Efficient Language Processing. https://doi.org/10.48550/arXiv.2006.03236 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Frochte, J. (2019). Maschinelles Lernen: Grundlagen und Algorithmen Python (2., aktualisierte Auflage). Hanser. Ganesh, P., Chen, Y., Lou, X., Khan, M. ., Yang, Y., Sajjad, H., Nakov, P., Chen, D., & Winslett, M. (2021). Compressing Large-Scale Transformer-Based Models: Case Study BERT. Transactions Association Computational Linguistics, 9, 1061–1080. https://doi.org/10.1162/tacl_a_00413 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). Gaithersburg: STATAXIS. , P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT Disentangled Attention. https://doi.org/10.48550/arXiv.2006.03654 Jadon, S., & Garg, . (2020). Hands-One-shot Learning Python: Learn Implement Fast Accurate Deep Learning Models Fewer Training Samples Using Pytorch. Packt Publishing Limited. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6175328 Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th ed.). Los Angeles: SAGE. Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing action: Understanding, analyzing, generating text Python. Shelter Island: Manning. Larusson, J. ., & White, B. (Eds.). (2014). Learning Analytics: Research Practice. New York: Springer. https://doi.org/10.1007/978-1-4614-3305-7 Lee, D.‑H. (2013). Pseudo-Label: Simple Efficient Semi-Supervised Learning Method Deep Neural Networks. CML 2013 Workshop: Challenges Representation Learning. Lee-Thorp, J., Ainslie, J., Eckstein, . & Ontanon, S. (2021). FNet: Mixing Tokens Fourier Transforms. https://doi.org/10.48550/arXiv.2105.03824 Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 Maas, . L., Daly, R. E., Pham, P. T., Huang, D., Ng, . Y., & Potts, C. (2011). Learning Word Vectors Sentiment Analysis. D. Lin, Y. Matsumoto, & R. Mihalcea (Eds.), Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies (pp. 142–150). Association Computational Linguistics. https://aclanthology.org/P11-1015 Oreshkin, B. N., Rodriguez, P., & Lacoste, . (2018). TADAM: Task dependent adaptive metric improved -shot learning. Advance online publication. https://doi.org/10.48550/arXiv.1805.10123 Papilloud, C., & Hinneburg, . (2018). Qualitative Textanalyse mit Topic-Modellen: Eine Einführung für Sozialwissenschaftler. Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-21980-2 Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N. (2019). Hierarchical Transformers Long Document Classification. 2019 IEEE Automatic Speech Recognition Understanding Workshop (ASRU) (pp. 838–844). IEEE. https://doi.org/10.1109/ASRU46091.2019.9003958 Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural Language Processing. https://aclanthology.org/D14-1162.pdf Ranjan, & Chitta. (2019). Build right Autoencoder — Tune Optimize using PCA principles.: Part . https://towardsdatascience.com/build--right-autoencoder-tune--optimize-using-pca-principles-part--1f01f821999b Schreier, M. (2012). Qualitative Content Analysis Practice. Los Angeles: SAGE. Snell, J., Swersky, K., & Zemel, R. S. (2017). Prototypical Networks -shot Learning. https://doi.org/10.48550/arXiv.1703.05175 Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.‑Y. (2020). MPNet: Masked Permuted Pre-training Language Understanding. https://doi.org/10.48550/arXiv.2004.09297 Tunstall, L., Werra, L. von, Wolf, T., & Géron, . (2022). Natural language processing transformers: Building language applications hugging face (Revised edition). Heidelberg: O’Reilly. Vaswani, ., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, . N., Kaiser, L., & Polosukhin, . (2017). Attention Need. https://doi.org/10.48550/arXiv.1706.03762 Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, ., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., . . . Dean, J. (2016). Google’s Neural Machine Translation System: Bridging Gap Human Machine Translation. https://doi.org/10.48550/arXiv.1609.08144 Zhang, X., Nie, J., Zong, L., Yu, H., & Liang, W. (2019). One Shot Learning Margin. Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang, & S.-J. Huang (Eds.), Lecture Notes Computer Science. Advances Knowledge Discovery Data Mining (Vol. 11440, pp. 305–317). Springer International Publishing. https://doi.org/10.1007/978-3-030-16145-3_24 ou, L. (2023). Meta-Learning: Theory, Algorithms Applications. Elsevier Science & Technology. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=7134465","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"preface","dir":"Articles","previous_headings":"1 Introduction and Overview","what":"1.1 Preface","title":"02 Using the graphical user interface Aifeducation - Studio","text":"vignette introduces Aifeducation - Studio graphical user interface creating, training, documenting, analyzing, applying artificial intelligence (AI). made users unfamiliar R coding skills relevant languages (e.g., python). experienced users, interface provides convenient way working AI educational context. article overlaps vignette 03 Using R syntax, explains use package R syntax. assume aifeducation installed described vignette 01 Get Started. introduction starts brief explanation basic concepts, necessary work package.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"basic-concepts","dir":"Articles","previous_headings":"1 Introduction and Overview","what":"1.2 Basic Concepts","title":"02 Using the graphical user interface Aifeducation - Studio","text":"educational social sciences, assigning scientific concepts observations important task allows researchers understand observation, generate new insights, derive recommendations research practice. educational science, several areas deal kind task. example, diagnosing students’ characteristics important aspect teachers’ profession necessary understand promote learning. Another example use learning analytics, data students used provide learning environments adapted individual needs. another level, educational institutions schools universities can use information data-driven performance decisions (Laurusson & White 2014) well improve . case, real-world observation aligned scientific models use scientific knowledge technology improved learning instruction. Supervised machine learning one concept allows link real-world observations existing scientific models theories (Berding et al. 2022). educational sciences, great advantage allows researchers use existing knowledge insights apply AI. drawback approach training AI requires information real world observations information corresponding alignment scientific models theories. valuable source data educational science written texts, since textual data can found almost everywhere realm learning teaching (Berding et al. 2022). example, teachers often require students solve task provide written form. Students create solution tasks often document short written essay presentation. data can used analyze learning teaching. Teachers’ written tasks students may provide insights quality instruction students’ solutions may provide insights learning outcomes prerequisites. AI can helpful assistant analyzing textual data since analysis textual data challenging time-consuming task humans. Please note introduction content analysis, natural language processing machine learning beyond scope vignette. like learn , please refer cited literature. start, necessary introduce definition understanding basic concepts, since applying AI educational contexts means combine knowledge different scientific disciplines using different, sometimes overlapping concepts. Even within single research area, concepts unified. Figure 1 illustrates package’s understanding. Since aifeducation looks application AI classification tasks perspective empirical method content analysis, overlapping concepts content analysis machine learning. content analysis, phenomenon like performance colors can described scale/dimension made several categories (e.g. Schreier 2012, pp. 59). example, exam’s performance (scale/dimension) “good”, “average” “poor”. terms colors (scale/dimension) categories “blue”, “green”, etc. Machine learning literature uses words describe kind data. machine learning, “scale” “dimension” correspond term “label” “categories” refer term “classes” (Chollet, Kalinowski & Allaire 2022, p. 114). clarifications, classification means text assigned correct category scale , respectively, text labeled correct class. Figure 2 illustrates, two kinds data necessary train AI classify text line supervised machine learning principles. providing AI textual data input data corresponding information class target data, AI can learn texts imply specific class category. exam example, AI can learn texts imply “good”, “average” “poor” judgment. training, AI can applied new texts predict likely class every new text. generated class can used statistical analysis derive recommendations learning teaching. use cases described vignette, AI “understand” natural language: „Natural language processing area research computer science artificial intelligence (AI) concerned processing natural languages English Mandarin. processing generally involves translating natural language data (numbers) computer can use learn world. (…)” (Lane , Howard & Hapke 2019, p. 4) Thus, first step transform raw texts form usable computer, hence raw texts must transformed numbers. modern approaches, usually done word embeddings. Campesato (2021, p. 102) describes “collective name set language modeling feature learning techniques (…) words phrases vocabulary mapped vectors real numbers.” definition word vector similar: „Word vectors represent semantic meaning words vectors context training corpus.” (Lane, Howard & Hapke 2019, p. 191). next step, words text embeddings can used input data labels target data training AI classify text. aifeducation, steps covered three different types models, shown Figure 3. Base Models: base models models contain capacities understand natural language. general, transformers BERT, RoBERTa, etc. huge number pre-trained models can found Huggingface. Text Embedding Models: modes built top base models store directions use base models converting raw texts sequences numbers. Please note base model can used create different text embedding models. Classifiers: Classifiers used top text embedding model. used classify text categories/classes based numeric representation provided corresponding text embedding model. Please note text embedding model can used create different classifiers (e.g. one classifier colors, one classifier estimate quality text, etc.). help overview can start introduction Aifeducation Studio.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"starting-aifeducation-studio","dir":"Articles","previous_headings":"","what":"2 Starting Aifeducation Studio","title":"02 Using the graphical user interface Aifeducation - Studio","text":"recommend start clean R session. can start Aifeducation Studio entering following console: Please note can take moment. beginning see home page (Figure 4). choose framework (pytorch activated default).","code":"aifeducation::start_aifeducation_studio()"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"creating-a-dataset","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.1 Preparing Data > 3.1.1 Collection of Raw Texts","what":"3.1.1.1 Creating a Dataset","title":"02 Using the graphical user interface Aifeducation - Studio","text":"fist step working AI gather structure data. scope aifeducation, data can either collection raw texts, sequences numbers representing texts (text embeddings) texts’ labels. Collections raw texts necessary two cases: First, train fine-tune base models. Second, transform texts texts embeddings can used input training classifier predicting texts’ labels via classifiers. create collection raw texts, choose Data Management page top panel shown Figure 5. resulting page (see Figure 6), first choose directory texts stored (box Text Sources). recommend store texts like use single folder. Within folder, can structure data sub-folders. next step, can decide file formats included (box File Types). Currently, aifeducation supports .txt, .pdf, .xlsx files. enabled, files requested file format included data collection. .txt files first alternative store raw texts .txt files. use structure data specific way: Create main folder storing data. Store every raw text/document single .txt file folder within main folder. every folder one file raw text/document. Add additional .txt file folder named bib_entry.txt. file contains bibliographic information raw text. Add additional .txt file folder named license.txt contains short statement license text “CC ”. Add additional .txt file folder named url_license.txt contains URL/link license’ text “https://creativecommons.org/licenses//4.0/”. Add additional .txt file folder named text_license.txt contains full license raw texts. Add additional .txt file folder named url_source.txt contains URL/link text file internet. Applying rules may result data structure follows: text_a.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_b.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt text_c.txt bib_entry.txt license.txt url_license.txt text_license.txt url_source.txt Please note, adding bib_entry.txt, license.txt, url_license.txt, text_license.txt, url_soruce.text every folder optional. file corresponding folder, empty entry data set. However, backdrop European AI Act, recommend provide license bibliographic information make documentation models straightforward. Furthermore, licenses, provided Creative Commons, require statements creators, copyright notice, URL link source material (possible), license material URL link license’s text internet license text . Please check license requirements material using, . .pdf files second alternative use .pdf files source raw texts. necessary structure similar .txt files. Please note files except text file must .txt, .pdf. stated bib_entry.txt, license.txt, url_license.txt, text_license.txt, url_soruce.text optional. .xlsx files third alternative store raw texts .xlsx files. alternative practicable many small raw texts. raw texts large, books papers, recommend store .txt .pdf files. order add raw texts .xlsx files, files need special structure: Create main folder storing .xlsx files like read. .xlsx files must contain names columns first row (Figure 7) names must identical column across .xslx files like read. Every .xslx files must contain column storing text ID must contain column storing raw text. Every text must unique ID across .xlsx files. Every .xslx file can contain additional column bib entry. Every .xslx file can contain additional column license. Every .xslx file can contain additional column license’s URL. Every .xslx file can contain additional column license’s text. Every .xslx file can contain additional column source’s URL. .xlsx file may look like last step choose folder collection raw texts saved. Please select path folder name LargeDataSet (Create Data Set button Control Panel) (Figure 8). Directory field specify location save object Folder Name define name folder created within directory. folder data set saved. Finally, can start creating collection (Continue button Figure 8). Please note can take time. can see creation progress progress bars (Figure 9). process finishes can see message like Figure 10 (process completed without errors). now folder files can used tasks like creation training/tuning transformers (Base Models tab top panel), etc. object contains dataset stores texts together IDs. case .xlsx files, texts’ IDs set IDs stored corresponding column ID. case .pdf .txt files, file names used ID (without file extension, see Figure 11). Please note consequence two files text_01.txt text_01.pdf ID, allowed. Please ensure use unique IDs across file formats. IDs important since used match corresponding class/category, available.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"exploring-a-dataset","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.1 Preparing Data > 3.1.1 Collection of Raw Texts","what":"3.1.1.2 Exploring a Dataset","title":"02 Using the graphical user interface Aifeducation - Studio","text":"explore large dataset raw texts saved disk can use DataSetExplorer. DataSetExplorer can found tab Data Management. rights side (Control Panel) can choose folder dataset stored. loading can explore texts using widgets within Control Panel. Please note explorer used displaying raw texts changing data. widget Documents allows select document. help widget Pages can browse different pages document.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"collections-of-texts-labels","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.1 Preparing Data","what":"3.1.2 Collections of Texts’ Labels","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Labels necessary like train classifier. easiest way create table contains column texts’ ID one multiple columns contain texts’ categories/classes. Supported file formats .xlsx, .csv, .rda/.rdata. Figure 13 illustrates example .xslx file. case, table must contain column name “id” contains texts’ IDs. columns must also unique names. Please pay attention use “id” “ID” “Id”. like create dataset labels without using Excel can use TableEditor. TableEditor located tab Data Management. right side can either create new empty table load edit existing table. TableEditor can used .csv files .rda .rdata files contain matrix data.frame. Change values: can add change value within cell clicking specific cell. Add rows: can add row clicking button Add Row. Remove rows: Rows can removed entering number rows like remove. entering number clicking Remove Row delete row. Add column: add new column just click button Add Column. Remove column: remove column please enter name column like remove. click Remove Column. Rename column: rename column please enter name column right side tab. next step enter new name click Rename Column. save table disk can click Save .","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"overview","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.2 Base Models","what":"3.2.1 Overview","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Base models foundation models aifeducation. moment, transformer models BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version 2 (et al. 2020), Funnel-Transformer (Dai et al. 2020), Longformer (Beltagy, Peters & Cohan 2020), MPNet. general first step, models trained large corpus general texts. next step, models fine-tuned domain-specific texts /fine-tuned specific tasks. Since creation base models requires huge number texts resulting high computational time, recommended use pre-trained models. can found Huggingface. Sometimes, however, straightforward create new model fit specific purpose. Aifeducation Studio supports opportunity create train/fine-tune base models.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"creation-of-base-models","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.2 Base Models","what":"3.2.2 Creation of Base Models","title":"02 Using the graphical user interface Aifeducation - Studio","text":"order create new base model choose option (tab) Create tab Base Models top panel app. Figure 13 shows corresponding page. Figure 15: Base Models - Create Transformer (click image enlarge) Every transformer model composed two parts: 1) tokenizer splits raw texts smaller pieces model large number words limited, small number tokens 2) neural network used model capabilities understanding natural language. beginning can choose different supported transformer architectures (Base Architecture combobox section Model-based parameters box Model Architecture). Depending architecture, different options determining shape neural network. also independent (general) options transformers (box General parameters). right-bottom side box Model Architecture, find box named Vocabulary. must provide path folder contains files data LargeDataSet. data set used calculate vocabulary transformer. file created Aifeducation Studio (tab Data Management) ensure compatibility. See section 3.1.1 details. important provide number many tokens vocabulary include. Depending transformer method, can set additional options, affecting transformer’s vocabulary. Transform Lower: option enabled, words raw text transformed lower cases. instance, resulting token Learners learners . disabled, Learners learners different tokenization. Add Prefix Spaces: enabled, space added first word already one. Thus, enabling option leads similar tokenization word learners cases: 1) “learners need high motivation high achievement.” 2) “high motivation necessary learners achieve high performance.”. Trim Offsets: option enabled, white spaces produced offsets trimmed. last step choose folder new base model saved (Choose Folder button Control Panel). Finally, can start creation model clicking button “Start Creation”. creation model may take time. can see progress creation progress bars.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"traintune-a-base-model","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.2 Base Models","what":"3.2.3 Train/Tune a Base Model","title":"02 Using the graphical user interface Aifeducation - Studio","text":"like train new base model (see section 3.2.2) first time want adapt pre-trained model domain-specific language task, click Train section Base Models tab. Figure 16: Base Models - Train/Tune Transformer (click image enlarge) first step, choose base model like train/tune (box Base Model). Please note every base model consists several files. Thus, provide neither single multiple files. Instead provide folder stores entire model. Compatible models base models created Aifeducation Studio. addition can use model Huggingface uses architecture implemented aifeducation BERT, DeBERTa, etc. choosing base model, new boxes appear shown Figure 14. train model, must first provide folder dataset (collection raw texts) (box Dataset). recommend create collection texts described section 3.1.1. Next can configure training base model (box Train Tune Settings). Possible options depend kind model. Chunk Size: training validating base model, raw texts split several smaller texts. value determines maximum length smaller text pieces number tokens. value exceed maximum size set creation base model. Minimal Sequence Length: value determines minimal length text chunk order part training validation data. Full Sequences : option enabled, text chunks number tokens equal “chunk size” included data. Disable option lot small text chunks like use training validation. Probability Token Masking: option determines many tokens every sequence masked. Probability Token Permutation: option determines many tokens every sequence used permutation. Whole Word Masking: option activated, tokens belonging single word masked. options disabled available token masking used. Validation Size: option determines many sequences used validating performance base model. Sequences used validation available training. Batch Size: option determines many sequences processed time. Please adjust value computation capacities machine. n Epochs: maximum number epochs training. training, model best validation loss saved disk used final model. Learning Rate: initial learning rate. last step, provide directory trained model saved training (Choose Folder button Control Panel). corresponding folder also contain checkpoints training. important directory directory one stored original model . clicking button “Start Training/Tuning”, training starts. Please note training base model can last days even weeks, depending size kind model, amount data, capacities machine. can see training progress progress bars plot (Figure 17).","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"create-a-text-embedding-model","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.3 Text Embedding Models","what":"3.3.1 Create a Text Embedding Model","title":"02 Using the graphical user interface Aifeducation - Studio","text":"text embedding model interface R aifeducation. order create new model, need base model provides ability understand natural language. object contains relevant information transforming raw texts numeric representation can used machine learning. aifedcuation, transformation raw texts numbers separate step downstream tasks classification. reduce computational time machines low performance. separating text embedding tasks, text embedding calculated can used different tasks time. Another advantage training downstream tasks involves downstream tasks parameters embedding model, making training less time-consuming, thus decreasing computational intensity. Finally, approach allows analysis long documents applying algorithm different parts. can open creation page clicking “Create” tab section “Text Embedding Models”. Figure 18 shows corresponding page. Figure 18: Text Embedding Model - Create (click image enlarge) First choose base model form foundation new text embedding model. Please select folder contains entire model single files (button Choose Base Model Control Panel). choosing model, new boxes appear allow customize interface. important give model unique name label (Control Panel). difference Name Label Name used computer Label users. Thus, Name contain spaces special characters. Label restrictions. Think Label title book paper. Label can also provide version number create newer version model. case create new model, recommend use “0.0.1”. also important choose language model created , English, French, German, etc. right side box Interface Setting can set interface process raw text: N chunks: Sometimes texts long. value, can decide many chunks longer texts divided. maximum length every chunk determined value provided “Maximum Sequence Length”. Minimum chunks two. Maximal Sequence Length: value determines maximum number tokens model processes every chunk. N Token Overlap: value determines many tokens form prior chunk included current chunk. overlap can useful provide correct context every chunk. Layers Embeddings - Min: Base models transform raw data sequence numbers using different layers’ hidden states. option can decide layer use first. Layers Embeddings - Max: option can decide last layer use. hidden states layers min max averaged form embedding text chunk. Pooling Type: option can decide hidden states cls-token used embedding. set option “average”, hidden states tokens averaged within layer except hidden states padding tokens. maximum number tokens model can process provide downstream tasks can calculated MaxTokens=NChunks*MaximalSequenceLength−(NChunks−1)*NOverlapMax Tokens = NChunks*MaximalSequenceLength-(NChunks-1)*NOverlap text longer, remaining tokens ignored lost analysis. Please note can create multiple text embedding models different configuration based base model. last step provide name folder save model (button Save Model).","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"using-a-text-embedding-model","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.3 Text Embedding Models","what":"3.3.2 Using a Text Embedding Model","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Using text embedding model central aspect applying artificial intelligence aifeducation. corresponding page can found clicking “Use” tab “Text Embedding Models”. start choose model like use. Please select folder contains entire model instead selecting single files (button Choose Model). selecting loading model, new box appears shows different aspects model can use . tab Description (Figure 19) provides documentation model. Figure 19: Text Embedding Model - Description (click image enlarge) tab Training shows development loss validation loss last training corresponding base model. plot displayed, history data available. tab Embedd Text (Figure 20) allows transform raw texts numerical representation texts, called text embeddings. text embeddings can used downstream tasks classifying texts. order transform raw texts embedded texts, first select collection raw texts (folder containing several files). recommend create collection according section 3.1.1. Next provide folder embeddings stored name . Batch Size can determine many raw texts processed simultaneously. Please adjust value machine’s capacities. clicking button “Save Embeddings” transformation texts begins. Figure 20: Text Embedding Model - Embeddings (click image enlarge) tab Tokenize/Encode/Decode (Figure 21) offers insights way text embedding model processes data. box Encode can insert raw text clicking Encode, can see text divided tokens corresponding IDs. IDs passed base model used generate numeric representation text. box Decode allows reverse process. can insert sequence numbers (separated comma spaces) clicking Decode, corresponding tokens raw text appear. Figure 21: Text Embedding Model - Tokenize/Encode/Decode (click image enlarge) Finally, tab Fill Mask (Figure 22) allows request underlying base model text embedding model calculate solution fill---blank text. box Text can insert raw text. gap signaled insert corresponding masking token. token can found table row “mask_token”. insert gap/mask_token please ensure correct spelling. N Solutions per mask can determine many tokens model calculate every gap/mask_token. clicking “Calculate Tokens”, find image right side box, showing reasonable token selected gap. tokens ordered certainty; perspective model, reasonable tokens top less reasonable tokens bottom. Figure 22: Text Embedding Model - Fill Mask (click image enlarge)","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"documenting-a-text-embedding-model","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.3 Text Embedding Models","what":"3.3.3 Documenting a Text Embedding Model","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Creating “good” AI models requires lot effort. Thus, sharing work users important support progress discipline. Thus, meaningful documentation required. addition, well written documentation makes AI model transparent, allowing others understand AI model generated solution. also important order judge limitations model. support developers documenting work, Aifeducation Studio provides easy way add comprehensive description. find part app clicking “Document” tab Text Embedding Models. First, choose text embedding model like document (base model!). choosing model, new box appears, allowing insert necessary information. Via tabs Developers Modifiers, can provide names email addresses relevant contributors. Developers refer people created model, Modifiers refers people adapted pre-trained model another domain task. Figure 23: Text Embedding Model - Documentation (click image enlarge) tabs Abstract Description, can provide abstract detailed description work English /native language text embedding model (e.g., French, German, etc.), allowing reach broader audience (Figure 23). four tabs can provide documentation plain text, html, /markdown allowing insert tables highlight parts documentation. like see documentation look like internet, can click button “Preview”. Saving changes possible clicking Save. information document model, please refer vignette 03 Sharing Using Trained AI/Models.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"create-a-classifier","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.4 Classifiers","what":"3.4.1 Create a Classifier","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Classifiers built top text embedding model. create classifier, click Create tab Classifiers. Figure 22 shows corresponding page. Figure 24: Classifier - Creation Part 1 (click image enlarge) Creating classifier requires two kinds data. First, text embedding collection texts. embeddings created text embedding model described section 3.3.2. Second, table labels every text. kind data created described section 3.1.2. can provide text embeddings opening corresponding file first box (Input Data) clicking Choose Embeddings button. selecting embeddings, see summary underlying text embedding model generated embeddings. addition, can see many documents file. Please note classifier bound text embedding model generated embeddings. , classifier can used access corresponding text embedding model. text embedding model necessary transform raw texts format classifier can understand. second box (Target Data), can select file contains corresponding labels. loading file, can select column table like use target data training. addition, can see short summary absolute frequencies single classes/categories. Please note can create multiple classifiers different target data based text embedding model. Thus, need create new text embedding model new classifier. particular, can use text embeddings training different classifiers. important provide model’s name label Control Panel. Model Name used internal purposes machine Model Label used title classifiers users. Thus, Model Name contain spaces special characters. Model Label, restrictions. third box (Architecture) create architecture neural network (Figure 25). Figure 25: Classifier - Creation Part 2 (click image enlarge) Since detailed explanation every option beyond scope introduction, give short version: Positional Embedding: activating option, add positional embedding classifier. provides neural network ability take account order within sequence. Encoding Layers: layers similar encoding layers used transformer models, allowing calculate context-sensitive text embeddings. provide classifier ability take surrounding text chunks (see section 3.3.1) sequences account. Recurrent Layers: section allows add recurrent layers classifier. layers able account order within sequence. order add layers, just pass numbers input field Reccurent Layers separate comma space. Every number represents layer number determines number neurons. field can see Aifeducation Studio input. helpful avoid invalid specifications layers. Dense Layers: section can add dense layers network. process add layers similar process recurrent layers. Optimizer: can choose different optimizers training. next box (Training Settings) contains setting training classifier (Figure 26). Going detail beyond scope introduction. can provide overview. Section: General Setting Balance Class Weights: option enabled, loss adjusted absolute frequencies classes/categories according ‘Inverse Class Frequency’ method. option activated deal imbalanced data. Balance Sequence Length: Activating option can increase performance deal texts differ lengths imbalanced frequency. option enabled, loss adjusted absolute frequencies length texts according ‘Inverse Class Frequency’ method. Number Folds: number folds used estimating performance classifier. Proportion Validation Sample: percentage cases within fold used validation sample. sample used determine state model generalizes best. Epochs: Maximal number epochs. training, model best balanced accuracy saved used. Batch Size: number cases processed simultaneously. Please adjust value machine’s capacities. Please note batch size can impact classifier’s performance. Section: Synthetic Cases Add Synthetic Cases: active, creation additional synthetic cases applied training. added train data. way cases generated can configured following parameters: Method: method used generating cases. Min k: minimal number neighbors used generating synthetic cases. Max k: maximum number neighbors used generating synthetic cases. algorithm generate number synthetic cases every class ensure number cases every class equals number cases majority class. synthetic cases every class generated k Min k Max k. Every k contributes proportional synthetic cases. Section: Pseudo-Labeling Add Pseudo Labeling: activated, pseudo-labeling used training described Cascante-Bonilla et al. (2020). way pseudo-labeling applied can configured following parameters: Max Steps: number steps pseudo-labeling. example, first step, 1/Max Steps pseudo-labeled cases added, second step, 2/Max Steps pseudo-labeled cases added, etc.. cases added can influenced Balance Pseudo-Labels, Certainty Anchor, Max Certainty Value, Min Certainty Value. Certainty Anchor: value determines reference point choosing pseudo-labeled cases. 1 refers perfect certainty, 0 refers certainty similar random guessing. Selected cases closest value. Max Certainty Value: Pseudo-labeled cases exceeding value included training. Min Certainty Value: Pseudo-labeled cases falling bellow value included training. recommend use pseudo-labeling described Cascante-Bonilla et al. (2020). Therefore, following parameters set: Max Steps = 5 Max Certainty Value = 1.00 Certainty Anchor = 1.00 Min Certainty Value = 0.00 Figure 26: Classifier - Creation Part 3 (click image enlarge) start training, can check many cases can matched text embeddings target data clicking button Test Data Matching (Control Panel Figure 22). allows check structure data working. everything okay can provide directory like save model start training model clicking Train Model. Please note training classifier can take several hours. can see progress progress bars.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"using-a-classifier","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.4 Classifiers","what":"3.4.2 Using a Classifier","title":"02 Using the graphical user interface Aifeducation - Studio","text":"case trained classifier using classifier trained users, can analyze model’s performance use model classify new texts. , select Use tab tab Classifiers. Similar functions app, first select classifier providing folder contains entire model. Please note classifier made several files. Thus, Aifeducation Studio asks select folder containing files single files. loading classifier, new box appears. first tab, Description (Figure 27), find documentation model. Figure 27: Classifier - Description (click image enlarge) second tab, Training (Figure 28), receive summary training process model. includes visualization loss, accuracy, balanced accuracy every fold every epoch. Depending applied training techniques (Balanced Pseudo-Labeling), can request additional images. Figure 28: Classifier - Training (click image enlarge) third tab, Reliability (Figure 27), provides information model’s quality. can find visualizations giving insights classifier able generate reliable results. addition, measures content analysis well machine learning allow analyze specific aspects model’s performance. Figure 29: Classifier - Reliability (click image enlarge) last tab, Prediction (Figure 30), allows apply trained model new data. can use trained model assign classes/categories new texts. purpose, must first provide file contains text embeddings documents like classify. can create embeddings text embedding model used providing training data classifier. necessary steps described section 3.3.2. Figure 30: Classifier - Prediction (click image enlarge) embeddings must created text embedding model created text embeddings training. , error occur. See section 3.4.1 3.3.2 details. next step provide folder like save predictions provide file name. default case store predictions .csv file. resulting data table may look like shown Figure 31.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"documenting-a-classifier","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.4 Classifiers","what":"3.4.3 Documenting a Classifier","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Documenting classifier similar documentation text embedding model (section 3.3.3, see Figure 22). support developers documenting work, Aifeducation Studio provides easy way add comprehensive description. find part app clicking Document tab tab Classifiers. First, choose classifier like document. choosing model, new box appears, allowing insert necessary information. Via tab Developers, can provide names email addressees relevant contributors. tabs Abstract Description, can provide abstract detailed description work English /native language classifier (e.g., French, German, etc.), allowing reach broader audience. four tabs can provide documentation plain text, html, /markdown allowing insert tables highlight parts documentation. like see documentation look like internet, can click button “Preview”. Saving changes possible clicking Save information document model, please refer vignette 03 Sharing Using Trained AI/Models.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"feature-extractors","dir":"Articles","previous_headings":"3 Using Aifeducation Studio > 3.5 Extensions","what":"3.5.1 Feature Extractors","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Another option increase model’s performance /increase computational speed apply feature extractor. example, work Ganesan et al. (2021) indicates reduction hidden size can increase model accuracy. aifeducation, feature extractor model tries reduce number features given text embeddings feeding embeddings input classifier. can create feature extractor follows Figure 30 (Create tab tab Feature Extractors). First, need choose embeddings can see choosing box Input Data (Choose Embeddings button Control Panel). can specify name label model Model Name Model Label fields, respectively. Specify architecture training settings Architecture Training Settings boxes. Target Features determines number features compressed representation. lower number, higher requested compression. value corresponds features latent space figure . Method determine type layer feature extractor use. set “lstm”, layers model long short-term memory layers. set “dense”, layers standard dense layers. Noise Factor can add noise training, making feature extractor denoising auto encoder, can provide robust generalizations. ready start training, just click Train Model button. choose path folder folder name want store model. Please note training classifier can take several hours. can see progress progress bars. trained feature extractor, can use every classifier. Just pass feature extractor path Feature Extractor configuration classifier (Figure 25). . Now can use train classifier way without feature extractor.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/gui_aife_studio.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"02 Using the graphical user interface Aifeducation - Studio","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1–21. https://doi.org/10.3389/feduc.2022.818365 Campesato, O. (2021). Natural Language Processing Fundamentals Developers. Mercury Learning & Information. https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6647713 Cascante-Bonilla, P., Tan, F., Qi, Y. & Ordonez, V. (2020). Curriculum Labeling: Revisiting Pseudo-Labeling Semi-Supervised Learning. https://doi.org/10.48550/arXiv.2001.06001 Chollet, F., Kalinowski, T., & Allaire, J. J. (2022). Deep learning R (Second edition). Manning Publications Co. https://learning.oreilly.com/library/view/-/9781633439849/?ar Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer: Filtering Sequential Redundancy Efficient Language Processing. https://doi.org/10.48550/arXiv.2006.03236 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 , P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT Disentangled Attention. https://doi.org/10.48550/arXiv.2006.03654 Lane, H., Howard, C., & Hapke, H. M. (2019). Natural language processing action: Understanding, analyzing, generating text Python. Shelter Island: Manning. Larusson, J. ., & White, B. (Eds.). (2014). Learning Analytics: Research Practice. New York: Springer. https://doi.org/10.1007/978-1-4614-3305-7 Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 Schreier, M. (2012). Qualitative Content Analysis Practice. Los Angeles: SAGE.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"introduction-and-overview","dir":"Articles","previous_headings":"","what":"1 Introduction and Overview","title":"04 Model configuration","text":"Training AI model requires lot data, consumes time energy. general, several model configurations tested best performing model achieved. Thus, important choose good starting configuration avoid unnecessary computations time investments. help vignette like present research results provide rules thumb creating AI models efficient computation offer potential good performance. vignette structured according three main objects used aifeducation. base models, text embedding models classifiers.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"base-models","dir":"Articles","previous_headings":"","what":"2 Base Models","title":"04 Model configuration","text":"base models core models understanding natural language. Assuming researchers educational social sciences access limited data computational resources, AI models small efficient possible. recent years, researchers generated insights language models can reduced size without losing much performance. following, present concepts can realized aifeducation. Vocabulary size embedding matrix first step creating language model generate vocabulary used split text tokens. help embedding matrix, tokens translated numerical representation: Every token transformed vector dimension. number rows embedding matrix equals number tokens, number columns can chosen developer. original study Devlin et al. (2019, p. 4174), BERT model used vocabulary size 30,000 tokens. study conducted Zhao et al. (2019, p.2), calculated vocabulary 5,000 tokens able completely cover textual data. Furthermore, calculated vocabulary 30,000 tokens included 94% tokens small vocabulary. Thus, smaller vocabulary potential represent textual data efficient way. Chen et al. (2019, p. 3494) report study results showing classification tasks, vocabulary size 100 999 tokens can enough reasonable performance vocabulary size 1,000 10,000 required natural language inference. Gowda May (2020, p. 3960) revealed small medium data sizes, vocabulary 8,000 tokens provides good performance. large vocabulary able represent rare words better, words higher frequency even covered well smaller vocabulary (Ganesh et al. 2021, p. 1070). Thus, recommend try vocabulary size 10,000. important note vocabulary size impact words split tokens. Kaya Tantug (2024, p. 5) illustrate, higher vocabulary size allows tokenizer split words smaller number tokens smaller number requires tokenizer use tokens. Thus, length token sequence generated given chunk text longer tokenizer small vocabulary compared tokenizer large vocabulary. order describe effect, Kaya Tantug (2024, p. 5) propose tokenization granularity rate, calculated number tokens divided words. consequence, reducing vocabulary size requires increase maximal sequence length transformer order allow transformer process number words. study Wies et al. (2021) investigates relationship vocabulary size, dimension embedding matrix, width/depth transformer model (hidden size number layers). able show size dimension embedding matrix equal larger hidden size transformer model. explained , vocabulary size generally greater 1,000. can treated given parameter. Thus, dimension embedding matrix equal larger hidden size. Since aifeducation relies transformers library, base models implemented aifeducation use hidden size dimension embedding matrix, ensuring equal size. Thus, recommendation always satisfied. Width vs depth Levine et al. (2020, p. 2) investigate architecture transformers reveal minimal depth transformer encoder multi-head attention Lmin=log(d)L_{min}=log(d), dd hidden size. example, hidden size attention layer 768, formula suggests least Lmin=log(768)=6.64379L_{min}=log(768)=6.64379, seven layers. addition, work offers formula estimating optimal depth depending hidden size (Levine et al. 2020, p. 8): Loptim(d)=log(d)−5.0390.0555L_{optim}(d)=\\frac{log(d)-5.039}{0.0555} hidden size 768, Loptim(768)=28.91513033L_{optim}(768)=28.91513033 29 layers. Number attention heads hidden size (width layers) transformer influence well attention mechanism can used. Wies et al. (2021) showed product number attention head HH dimension internal attention representation dad_{} equal dimension hidden size dd transformer. case product greater hidden dimension dd, bottleneck occurs - reducing performance model. aifeducation, transformers determine dimension dad_{} da=d/Hd_{}=d/H, ensuring rule always fulfilled. Please confuse internal attention representation dad_{} intermediate size multi-head attention layer. Regarding number attention heads, Liu, Liu, Han (2021) develop single-head attention show transformer single-head attention achieves better performance transformer multi-head attention similar model size. study, Michel, Omer, Neubig (2019, p. 4) revealed test time, one head enough stable performance even model trained 12 16 heads. Based findings, Ganesh et al. (2021, p. 1068) conclude 1 2 heads encoder layers can sufficient high accuracy. Voita et al. 2019 (p. 5802) showed high number attention heads can removed training without significant decrease model’s performance. study also reveals training model scratch reduced number attention heads results lower performance, compared model trained higher number heads pruning training. However, difference small (Voita et al. 2019, p. 5803). sum , recommend start modeling 1 2 attention heads per layer.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"text-embedding-models","dir":"Articles","previous_headings":"","what":"3 Text Embedding Models","title":"04 Model configuration","text":"Text embedding models built top base model. used create numerical representation raw texts able represent semantic meaning text best possible. representations used downstream tasks classification. Rogers, Kovaleva, Rumshisky (2020) summarize knowledge BERT models work, providing good starting point deriving recommendations “good” configuration text embedding model. review provides evidence information linear word order represented lower layers, middle layers represent mainly syntactic information. clear semantic knowledge located seems semantic information spread across layers. final layer task-specific layer, changes fine-tuning. Since text embedding model aims provide numerical representation can used varying tasks, final layer may best choice due connection learning objective (e.g., masked language modeling). study conducted Liu et al. (2019, p. 1078) investigates performance models 16 linguistic tasks, revealing transformers, single best layer, best layers located area middle two-thirds layer. original study done Devlin et al. (2019, p. 4179), BERT model performed best representation drawn second--last hidden layer, weighted sum last four layers, concatenation last four layers named entity recognition. usual approach generate representations texts use representation [CLS] token final layer. However, stated , representation layers may easily transferable varying tasks. Furthermore, instead using representation [CLS] token, representations tokens mean representations can used. Tanaka et al.’s (2020, p. 151) study, mean representations tokens (except special tokens) performs better classification task representation [CLS] token. representations drawn final layer. Toshniwal et al. (2020, p. 168) use weighted average layers generate token representations reduced number dimensions. compare six different methods aggregating different token representations single text representation reveal average pooling inferior methods, max pooling simple competitive method (Toshniwal et al. 2020, p. 169), “max pooling takes maximum value time dimension contextualized embeddings within span.” (Toshniwal et al. 2020, p. 168). contrast, study conducted Ma et al. (2019) reveals max pooling better CLS mean pooling superior max pooling. However, results Ma et al.’s (2019) study averaged across different layers, providing limited information combine different pooling methods different layers. sum , recommend use embeddings middle two-thirds layer combination max mean pooling.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"classifiers","dir":"Articles","previous_headings":"","what":"4 Classifiers","title":"04 Model configuration","text":"Classifiers built top text embedding model represent final step classification tasks. Although underlying transformer part training, classifier still challenge approach used aifeducation transformers’ hidden size. example, hidden size original BERT model 768 base 1024 large variation (Devlin et al. 2019, p. 4173). Since data availability low educational social sciences, low performance expected. solution solve problem reduce dimensions, proposed Ganesan et al. (2021). study investigate relationship sample size, dimension, dimension reduction method. text representations built calculating mean tokens second last layer (Ganesan et al. 2021, p. 4517). central findings fine-tuning transformer training examples (10,000) results lower performance using fine-tuned transformer (Ganesan et al. 2021, p. 4519). Principal component analysis performed best, multi-layer non-linear auto-encoders (NLAE) also good choice (Ganesan et al. 2021, p. 4520). number dimensions depends specific task. However, larger training sample allows higher number dimension. cases, 1/121/12 1/61/6 dimensions sufficient (Ganesan et al. 2021, p. 4522). Ganesan et al. (2021) work shows reduction dimensions necessary case transformer model uses large hidden size. Since models aifeducation work sequential data, package contains LSTM (fe_method=\"lstm\") dense feature extractor (fe_method=\"dense\"). use classifier training, set use_fe=TRUE creation object specify desired number dimensions fe_features.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"limitations","dir":"Articles","previous_headings":"","what":"5 Limitations","title":"04 Model configuration","text":"Please note findings presented vignette refer different architectures AI models. general, results transferred directly model architectures. Thus, recommendations can serve rule thumb.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/model_configuration.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"04 Model configuration","text":"Chen, W., Su, Y., Shen, Y., Chen, Z., Yan, X., & Wang, W. Y. (2019). Large Vocabulary Text Classification Need? Variational Approach Vocabulary Selection. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 3487–3497). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1352 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Ganesan, . V., Matero, M., Ravula, . R., Vu, H., & Schwartz, H. . (2021). Empirical Evaluation Pre-trained Transformers Human-Level NLP: Role Sample Size Dimensionality. Proceedings Conference. Association Computational Linguistics. North American Chapter. Meeting, 2021, 4515–4532. https://doi.org/10.18653/v1/2021.naacl-main.357 Ganesh, P., Chen, Y., Lou, X., Khan, M. ., Yang, Y., Sajjad, H., Nakov, P., Chen, D., & Winslett, M. (2021). Compressing Large-Scale Transformer-Based Models: Case Study BERT. Transactions Association Computational Linguistics, 9, 1061–1080. https://doi.org/10.1162/tacl_a_00413 Gowda, T., & May, J. (2020). Finding Optimal Vocabulary Size Neural Machine Translation. T. Cohn, Y. , & Y. Liu (Eds.), Findings Association Computational Linguistics: EMNLP 2020 (pp. 3955–3964). Association Computational Linguistics. https://doi.org/10.18653/v1/2020.findings-emnlp.352 Kaya, Y. B., & Tantuğ, . C. (2024). Effect tokenization granularity Turkish large language models. Intelligent Systems Applications, 21, 200335. https://doi.org/10.1016/j.iswa.2024.200335 Levine, Y., Wies, N., Sharir, O., Bata, H., & Shashua, . (2020). Limits Depth Efficiencies Self-Attention. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, & H. Lin (Eds.), Advances Neural Information Processing Systems (Vol. 33, pp. 22640–22651). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf Liu, L., Liu, J., & Han, J. (2021). Multi-head Single-head? Empirical Comparison Transformer Training. https://doi.org/10.48550/arXiv.2106.09650 Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E., & Smith, N. . (2019). Linguistic Knowledge Transferability Contextual Representations. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 1073–1094). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1112 Ma, X., Wang, Z., Ng, P., Nallapati, R., & Xiang, B. (2019). Universal Text Representation BERT: Empirical Study. https://doi.org/10.48550/arXiv.1910.07973 Michel, P., Levy, O., & Neubig, G. (2019). Sixteen Heads Really Better One? https://doi.org/10.48550/arXiv.1905.10650 Rogers, ., Kovaleva, O., & Rumshisky, . (2020). Primer BERTology: Know BERT Works. Transactions Association Computational Linguistics, 8, 842–866. https://doi.org/10.1162/tacl_a_00349 Tanaka, H., Shinnou, H., Cao, R., Bai, J., & Ma, W. (2020). Document Classification Word Embeddings BERT. L.-M. Nguyen, X.-H. Phan, K. Hasida, & S. Tojo (Eds.), Communications Computer Information Science. Computational Linguistics (Vol. 1215, pp. 145–154). Springer Singapore. https://doi.org/10.1007/978-981-15-6168-9_13 Toshniwal, S., Shi, H., Shi, B., Gao, L., Livescu, K., & Gimpel, K. (2020). Cross-Task Analysis Text Span Representations. S. Gella, J. Welbl, M. Rei, F. Petroni, P. Lewis, E. Strubell, M. Seo, & H. Hajishirzi (Eds.), Proceedings 5th Workshop Representation Learning NLP (pp. 166–176). Association Computational Linguistics. https://doi.org/10.18653/v1/2020.repl4nlp-1.20 Voita, E., Talbot, D., Moiseev, F., Sennrich, R., & Titov, . (2019). Analyzing Multi-Head Self-Attention: Specialized Heads Heavy Lifting, Rest Can Pruned. . Korhonen, D. Traum, & L. Màrquez (Eds.), Proceedings 57th Annual Meeting Association Computational Linguistics (pp. 5797–5808). Association Computational Linguistics. https://doi.org/10.18653/v1/P19-1580 Wies, N., Levine, Y., Jannai, D., & Shashua, . (2021). transformer architecture fits data? vocabulary bottleneck self-attention. https://doi.org/10.48550/arXiv.2105.03928 Zhao, S., Gupta, R., Song, Y., & Zhou, D. (2019). Extremely Small BERT Models Mixed-Vocabulary Training. https://doi.org/10.48550/arXiv.1909.11687","code":""},{"path":"https://fberding.github.io/aifeducation/articles/sharing_and_publishing.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1 Introduction","title":"05 Sharing and Using Trained AI/Models","text":"educational social sciences, common practice share research instruments questionnaires tests. example, Open Test Archive provides access large number open access instruments. aifeducation assumes AI-based classifiers shareable, similarly research instruments, empower educational social science researchers support application AI educational purposes. Thus, aifeducation aims make sharing process convenient possible. aim, every model generated aifeducation can prepared publication basic steps. vignette, like show make AI ready publication use models others. save, load, apply models described 02 Using graphical user interface Aifeducation - Studio 03 Using R syntax.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/sharing_and_publishing.html","id":"creating-documentation-with-ai-for-education---studio","dir":"Articles","previous_headings":"","what":"2 Creating Documentation with AI for Education - Studio","title":"05 Sharing and Using Trained AI/Models","text":"convenience way document work use AI Education - Studio. first step start user interface calling start_aifeducation_studio: next steps depend model like document. case like document TextEmbeddingModel, first select TextEmbeddingModels top app. next window, please select tab Document. Now can load model like document clicking button “Choose Model”. loading model can see different tabs different parts documentation. first tab Developer. can add names e-mail addresses model’s developers. addition, can add suggested citation URL link relevant sites model. help tabs can write abstract English, abstract native language model French German, can add detailed description languages. tab abstracts allows set keywords work, help others find work search engines. documentation goes corresponding text field left side within tab. click button “Preview”, see preview documentation right side. finished documenting part model, please click “Save” button save changes. last tab allows set license model documentation. documentation models TEFeautreExtractors, TEClassifierRegular, TEClassifierProtoNet works exactly . difference select corresponding tab top app. TextEmbeddingModels additional tab called “Modifiers”. tab relevant develop base model rather modify base model created . modification can adaption specific tasks specific domains. case recommend add people developed base model via tab “Developers” research group via tab “Modifiers”.","code":"start_aifeducation_studio()"},{"path":"https://fberding.github.io/aifeducation/articles/sharing_and_publishing.html","id":"creating-documentation-with-r-syntax","dir":"Articles","previous_headings":"","what":"3 Creating Documentation with R Syntax","title":"05 Sharing and Using Trained AI/Models","text":"process documenting model similar models aifeducation, since models use methods. , illustrate process TextEmbeddingModel. First, every model needs clear description developed, modified can used. can add description via method set_model_description. method allows provide description English native language model make distribution model easier. abstract_eng abstract_native can provide summary description. important like share work repository. keywords_eng keywords_native can set vector keywords, helps others find work search engines. can access model’s description using method get_model_description Besides description work, necessary provide information people involved creating model. can done method set_publication_info. First , decide type information like add. two choices: “developer”, “modifier”, set type. type=\"developer\" stores information people involved process developing model. use transformer model Hugging Face, contributors description model entered developers. cases can use type providing description developed model. cases might wish modify existing model. might case use transformer model adapt model specific domain task. case rely work people modify work. , can describe modifications setting type=modifier. every type contributor can add relevant individuals via authors. Please use R function personList() . citation can provide short text cite work different contributors. url can provide link relevant sites model. can access information using get_publication_info. Finally, must provide license using model. can done set_model_license get_model_license. documentation work part software. can set another software. can set license documentation using method set_documentation_license. Now able share work. Please remember save now fully described object described vignette 03 Using R syntax. documentation process models TEFeautreExtractors, TEClassifierRegular, TEClassifierProtoNet. one difference. TextEmbeddingModelsyou can differentiate “developers” “modifiers”. possible models. models need argument type. Calling method look like:","code":"example_model$set_model_description(   eng = NULL,   native = NULL,   abstract_eng = NULL,   abstract_native = NULL,   keywords_eng = NULL,   keywords_native = NULL ) example_model$get_model_description() example_model$set_publication_info(   type,   authors,   citation,   url = NULL ) example_model$get_publication_info() example_model$set_model_license(\"GPL-3\") example_model$set_documentation_license(\"CC BY-SA\") example_model$set_publication_info(   authors,   citation,   url = NULL )"},{"path":"https://fberding.github.io/aifeducation/articles/sharing_and_publishing.html","id":"content-and-style-of-a-documentation","dir":"Articles","previous_headings":"","what":"4 Content and Style of a Documentation","title":"05 Sharing and Using Trained AI/Models","text":"necessary structure content documentation depends kind model like document, national laws (European AI Act), research standards discipline. scientific point view, recommend every model abstract, keywords, detailed description English. additional abstract, keywords, description native language model may helpful reaching broad audience corresponding language community. can write abstracts descriptions HTML R Markdown allows add links sources publications, add tables highlight important aspects model. models recommend description answers least following questions: kind data used create model? much data used create model? steps performed method used? kinds tasks materials can model used? kind information necessary others form opinion model. case classifiers, recommend add descriptions: short reference theoretical models guided development. clear detailed description every single category/class. short statement classifier can used. description kind quantity data used training. Information potential bias data. possible, information inter-coder-reliability coding process data. possible, provide link corresponding text embedding model least state potential users can get text embedding model. statement get text embedding model important since classifier can used corresponding text embedding model. Please report performance values classifier description.** displayed automatically AI Education - Studio can accessed directly via example_classifier$reliability$test_metric_mean. Please consider native language example classifier:","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"overview","dir":"Articles","previous_headings":"1 Base Transformer Class","what":"1.1 Overview","title":"01 Transformers","text":"See Base Transformer Class Documentation details. UML-diagram Base Transformer Class: class : private attributes: title, sustainability_tracker, steps_for_creation steps_for_training (see 1.2 Private attributes). public attributes: params temp lists (see 1.3 Public attributes). private methods: clear_variables method sets params, temp sustainability_tracker attributes NULL. check_required_SFC check_required_SFT methods check required steps (functions) SFC SFT lists NULL respectively. Steps set NULL default must functions. init_common_model_params method contains set_model_params method calls set common model’s parameters (listed ‘static’ parameters p. 1.3 Public attributes -> param list). define_required_SFC_functions define_required_SFT_functions methods contains definitions required creation/training steps (functions) respectively. public methods: initialize, setters title attribute, params temp lists elements, steps creation (SFC) training (STF) functions, main create train user-methods. See documentation details.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"private-attributes","dir":"Articles","previous_headings":"1 Base Transformer Class","what":"1.2 Private attributes","title":"01 Transformers","text":"Developers know purpose private attributes.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"attribute-title","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes","what":"Attribute title","title":"01 Transformers","text":"string title transformer. title displayed progress bar. default title set \"Transformer Model\". Can set set_title() method (example implementing new child-transformer initialize-method): Use super access public methods base class (1).","code":".AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\",   inherit = .AIFEBaseTransformer,   public = list(     initialize = function() {       super$set_title(\"Custom Transformer\") # (1)     }   ) )"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"attribute-sustainability_tracker","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes","what":"Attribute sustainability_tracker","title":"01 Transformers","text":"codecarbon.OfflineEmissionsTracker object used track sustainability demand. can created private create_sustain_tracker() method.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"attribute-steps_for_creation","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes","what":"Attribute steps_for_creation","title":"01 Transformers","text":"list() stores required optional steps (functions) creating new transformer. access (input) parameters transformer, use params list (e.g. params$ml_framework). access local variable outside function, put temp list. Use set_SFC_*() methods set required/optional steps creation, * name step. Use set_required_SFC() method set required steps .","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"required","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_creation","what":"Required","title":"01 Transformers","text":"required steps defined child transformer : create_tokenizer_draft: function() creates tokenizer draft. function new tokenizer must created stored element list temp (e.g. temp$tok_new). function can include definition special tokens /trainers (tokenizers.trainers.WordPieceTrainer). See create_WordPiece_tokenizer() create_ByteLevelBPE_tokenizer() functions create new tokenizer object (tokenizers.Tokenizer) based tokenizers.models.WordPiece tokenizers.models.ByteLevel models respectively. calculate_vocab: function() calculating vocabulary. tokenizer created create_tokenizer_draft() function trained. See tokenizers.Tokenizer.train_from_iterator() details. save_tokenizer_draft: function() saves tokenizer draft model directory (e.g. vocab.txt file). See tokenizers.Tokenizer.save_model() details. create_final_tokenizer: function() creates new transformer tokenizer object. tokenizer must stored tokenizer parameter temp list. See transformers.PreTrainedTokenizerFast, transformers.LongformerTokenizerFast transformers.RobertaTokenizerFast details. create_transformer_model: function() creates transformer model. model must passed model parameter temp list. See transformers.(TF)BertModel, transformers.(TF)DebertaV2ForMaskedLM, transformers.(TF)FunnelModel, transformers.(TF)LongformerModel, transformers.(TF)RobertaModel, etc. details.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"optional","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_creation","what":"Optional","title":"01 Transformers","text":"Optional step : check_max_pos_emb: function() checks transformer parameter max_position_embeddings. Leave NULL skip check.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"other","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_creation","what":"Other","title":"01 Transformers","text":"Required already defined step : save_transformer_model: function() saves newly created transformer. Uses temporary model pt_safe_save parameters temp list. See transformers.(TF)PreTrainedModel.save_pretrained() details.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"attribute-steps_for_training","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes","what":"Attribute steps_for_training","title":"01 Transformers","text":"list() stores required optional steps (functions) training transformer. access (input) parameters transformer, use params list (e.g. params$ml_framework). access local variable outside function, put temp list. Use set_SFT_*() methods set required/optional steps creation, * name step.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"required-1","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_training","what":"Required","title":"01 Transformers","text":"required step child transformer : load_existing_model: function() loads model tokenizer. model transformer must stored model tokenizer parameters respectively temp list. See transformers.(TF)PreTrainedModel details.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"optional-1","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_training","what":"Optional","title":"01 Transformers","text":"Optional step : cuda_empty_cache: function() empty cache torch.cuda available.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"other-1","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.2 Private attributes > Attribute steps_for_training","what":"Other","title":"01 Transformers","text":"Required already defined steps : check_chunk_size: function() checks transformer’s parameter chunk_size adjusts . Uses model parameter temp list modifies chunk_size parameter params list. create_chunks_for_training: function() creates chunks sequenses trainining. Uses tokenizer parameter adds tokenized_dataset parameter temp list. prepare_train_tune: function() prepares data training. tensorflow: uses model tokenizer parameters, adds tf_train_dataset, tf_test_dataset, callbacks parameters temp list. pytorch: uses model, tokenizer parameters, adds trainer parameter temp list. start_training: function() starts training. tensorflow: uses model, tf_train_dataset, tf_test_dataset, callbacks parameters temp list. pytorch: uses trainer parameter temp list. save_model: function() saves model. tensorflow: uses model parameter temp list. pytorch: uses model, pt_safe_save trainer parameters temp list. Required already defined step, can overwritten custom version: create_data_collator: function() creates data collator model. temp list uses tokenizer return_tensors parameters, adds data_collator parameter list. params list uses whole_word p_mask.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"params-list","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.3 Public attributes","what":"params list","title":"01 Transformers","text":"list containing transformer’s parameters (‘static’, ‘dynamic’ ‘dependent’ parameters). Can set set_model_param().","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"static-parameters","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.3 Public attributes > params list","what":"‘Static’ parameters","title":"01 Transformers","text":"Regardless transformer, following parameters always included: ml_framework text_dataset sustain_track sustain_iso_code sustain_region sustain_interval trace pytorch_safetensors log_dir log_write_interval","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"dynamic-parameters","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.3 Public attributes > params list","what":"‘Dynamic’ parameters","title":"01 Transformers","text":"case create also contains (see create-method details): model_dir vocab_size max_position_embeddings hidden_size hidden_act hidden_dropout_prob attention_probs_dropout_prob intermediate_size num_attention_heads case train also contains (see train-method details): output_dir model_dir_path p_mask whole_word val_size n_epoch batch_size chunk_size min_seq_len full_sequences_only learning_rate n_workers multi_process keras_trace pytorch_trace","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"dependent-parameters","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.3 Public attributes > params list","what":"‘Dependent’ parameters","title":"01 Transformers","text":"Depending transformer method used, class may contain different parameters: vocab_do_lower_case num_hidden_layer add_prefix_space etc.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"temp-list","dir":"Articles","previous_headings":"1 Base Transformer Class > 1.3 Public attributes","what":"temp list","title":"01 Transformers","text":"list containing temporary transformer’s parameters list() containing temporary local variables need accessed step functions. Can set set_model_temp(). example, can variable tok_new stores tokenizer steps_for_creation$create_tokenizer_draft. train tokenizer, access variable tok_new steps_for_creation$calculate_vocab temp list class.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"overview-1","dir":"Articles","previous_headings":"2 Allowed Transformers","what":"2.1 Overview","title":"01 Transformers","text":"UML-diagram Transformer Classes: Base Transformer Class following child-classes: .AIFEBertTransformer .AIFEDebertaTransformer .AIFEFunnelTransformer .AIFELongformerTransformer .AIFERobertaTransformer .AIFEMpnetTransformer object Base Transformer Class created, thus create train methods called directly. However objects child classes can created new-method (1). Use create (2) train (3) methods create/train concrete transformer respectively:","code":"# Example of using .AIFEBertTransformer class # For the other one - analogically  bert_transformer <- .AIFEBertTransformer$new() # (1)  # See .AIFEBertTransformer documentation to get input parameters # for create and train methods instead of ... bert_transformer$create(...)                   # (2) bert_transformer$train(...)                    # (3)"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"static-parameters-1","dir":"Articles","previous_headings":"2 Allowed Transformers > 2.2 Transformer Parameters","what":"‘Static’ parameters","title":"01 Transformers","text":"1 Available frameworks  “tensorflow”  “pytorch” 2 Via  python library codecarbon 3  variable must  set  sustainability   tracked.  list can  found  Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes 4 See  documentation  codecarbon   information https://mlco2.github.io/codecarbon/parameters.html 5  relevant  pytorch models. TRUE:  ‘pytorch’ model  saved  safetensors format; FALSE ( ‘safetensors’   available): model  saved   standard pytorch format (.bin) 6  relevant  log_dir   NULL","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"dynamic-parameters-for-creation","dir":"Articles","previous_headings":"2 Allowed Transformers > 2.2 Transformer Parameters","what":"‘Dynamic’ parameters for creation","title":"01 Transformers","text":"1  parameter also determines  maximum length   sequence  can  processed   model 2  parameter determines  dimensionality   resulting text embedding 3 \"mean\"  \"max\"  pooling  mean  maximum values respectively","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"dynamic-parameters-for-training","dir":"Articles","previous_headings":"2 Allowed Transformers > 2.2 Transformer Parameters","what":"‘Dynamic’ parameters for training","title":"01 Transformers","text":"1   directory   exist,    created 2 TRUE: whole word masking   applied; FALSE: token masking  used 3  relevant  full_sequences_only = FALSE 4  relevant  ml_framework = \"tensorflow\" 5 keras_trace = 0:   print  information; keras_trace = 1: prints  progress bar; keras_trace = 2: prints one line  information  every epoch 6 pytorch_trace = 0:   print  information; pytorch_trace = 1: prints  progress bar","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"overview-2","dir":"Articles","previous_headings":"3 Transformer Maker","what":"3.1 Overview","title":"01 Transformers","text":"See Transformer Maker Class details. Transformer Maker Class make-method create Transformer object. UML-diagram Transformer Maker Class:","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"bert","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"BERT","title":"01 Transformers","text":"See BERT Transformer Developers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$bert) # or # transformer <- aife_transformer_maker$make(\"bert\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > BERT","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 30522,                    vocab_do_lower_case = FALSE,                    max_position_embeddings = 512,                    hidden_size = 768,                    num_hidden_layer = 12,                    num_attention_heads = 12,                    intermediate_size = 3072,                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > BERT","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   whole_word = TRUE,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-3,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"deberta-v2","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"DeBERTa-v2","title":"01 Transformers","text":"See DeBERTa-v2 Transformer Develovers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$deberta_v2) # or # transformer <- aife_transformer_maker$make(\"deberta_v2\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create-1","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > DeBERTa-v2","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 128100,                    vocab_do_lower_case = FALSE,                    max_position_embeddings = 512,                    hidden_size = 1536,                    num_hidden_layer = 24,                    num_attention_heads = 24,                    intermediate_size = 6144,                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train-1","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > DeBERTa-v2","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   whole_word = TRUE,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-2,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"roberta","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"RoBERTa","title":"01 Transformers","text":"See RoBERTa Transformer Develovers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$roberta) # or # transformer <- aife_transformer_maker$make(\"roberta\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create-2","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > RoBERTa","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 30522,                    add_prefix_space = FALSE,                    trim_offsets = TRUE,                    max_position_embeddings = 512,                    hidden_size = 768,                    num_hidden_layer = 12,                    num_attention_heads = 12,                    intermediate_size = 3072,                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train-2","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > RoBERTa","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-2,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"funnel","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"Funnel","title":"01 Transformers","text":"See Funnel Transformer Develovers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$funnel) # or # transformer <- aife_transformer_maker$make(\"funnel\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create-3","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > Funnel","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 30522,                    vocab_do_lower_case = FALSE,                    max_position_embeddings = 512,                    hidden_size = 768,                    target_hidden_size = 64,                    block_sizes = c(4, 4, 4),                    num_attention_heads = 12,                    intermediate_size = 3072,                    num_decoder_layers = 2,                    pooling_type = \"mean\",                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    activation_dropout = 0.0,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train-3","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > Funnel","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   whole_word = TRUE,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-3,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"longformer","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"Longformer","title":"01 Transformers","text":"See Longformer Transformer Develovers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$longformer) # or # transformer <- aife_transformer_maker$make(\"longformer\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create-4","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > Longformer","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 30522,                    add_prefix_space = FALSE,                    trim_offsets = TRUE,                    max_position_embeddings = 512,                    hidden_size = 768,                    num_hidden_layer = 12,                    num_attention_heads = 12,                    intermediate_size = 3072,                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    attention_window = 512,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train-4","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > Longformer","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-2,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"mpnet","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage","what":"MPNet","title":"01 Transformers","text":"See MPNet Transformer Develovers details.","code":"transformer <- aife_transformer_maker$make(AIFETrType$mpnet) # or # transformer <- aife_transformer_maker$make(\"mpnet\")"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"create-5","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > MPNet","what":"Create","title":"01 Transformers","text":"","code":"transformer$create(ml_framework = ml_framework,                    model_dir = model_dir,                    text_dataset = text_dataset,                    vocab_size = 30522,                    vocab_do_lower_case = FALSE,                    max_position_embeddings = 512,                    hidden_size = 768,                    num_hidden_layer = 12,                    num_attention_heads = 12,                    intermediate_size = 3072,                    hidden_act = \"gelu\",                    hidden_dropout_prob = 0.1,                    attention_probs_dropout_prob = 0.1,                    sustain_track = TRUE,                    sustain_iso_code = NULL,                    sustain_region = NULL,                    sustain_interval = 15,                    trace = TRUE,                    pytorch_safetensors = TRUE,                    log_dir = NULL,                    log_write_interval = 2)"},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"train-5","dir":"Articles","previous_headings":"3 Transformer Maker > 3.2 Usage > MPNet","what":"Train","title":"01 Transformers","text":"","code":"transformer$train(ml_framework = ml_framework,                   output_dir = output_dir,                   model_dir_path = model_dir_path,                   text_dataset = text_dataset,                   p_mask = 0.15,                   p_perm = 0.15,                   whole_word = TRUE,                   val_size = 0.1,                   n_epoch = 1,                   batch_size = 12,                   chunk_size = 250,                   full_sequences_only = FALSE,                   min_seq_len = 50,                   learning_rate = 3e-3,                   n_workers = 1,                   multi_process = FALSE,                   sustain_track = TRUE,                   sustain_iso_code = NULL,                   sustain_region = NULL,                   sustain_interval = 15,                   trace = TRUE,                   keras_trace = 1,                   pytorch_trace = 1,                   pytorch_safetensors = TRUE,                   log_dir = NULL,                   log_write_interval = 2)"},{"path":[]},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"overview-3","dir":"Articles","previous_headings":"4 Implement A Custom Transformer","what":"4.1 Overview","title":"01 Transformers","text":"Custom Transformer template (“dotAIIFECustomTransformer.R”) located R-folder project.","code":""},{"path":"https://fberding.github.io/aifeducation/articles/transformers.html","id":"implementation-steps","dir":"Articles","previous_headings":"4 Implement A Custom Transformer","what":"4.2 Implementation Steps","title":"01 Transformers","text":"implement new transformer, following steps: Create new R-file name like dotAIFECustomTransformer. Open file write creation new R6::R6Class() inside (see code ). name class must defined (1). Remember inherit base transformer class (2). Use private list private attributes (3) like title, steps_for_creation, etc. (explained later) public list (4) initialize, create, train methods. Define private title attribute (1) set initialize method (2) using inherited super$set_title() base method (3) base class. Define private steps_for_creation list (1) implement required steps (functions) (2)-(6), (7) needed. forget pass self input parameter functions. Note local variables created inside functions can used inherited temp list. Put local tok_new variable (8) temp list create_tokenizer_draft step (2) use calculate_vocab step (3) like (9). Similarly, use input parameters transformer ml_framework using inherited params list like (10). Important! create_final_tokenizer step (5) store tokenizer self$temp$tokenizer variable (11). create_transformer_model step (6) store transformer model self$temp$model variable (12). Define create method (1) input parameters (2) create method base class. Add dependent parameters custom transformer input parameters (3). Dependent parameters parameters depend transformer present base class. Set dependent parameters base class using super$set_model_param() method (4). Set required optional steps base class using super$set_required_SFC() super$set_SFC_check_max_pos_emb() methods respectively (5). Finally run basic create algorithm using super$create() (6) input parameters (2). Define train method (1) similarly step 5. Implement steps (functions) private steps_for_training list (2). forget pass self input parameter required function. Set dependent parameters (4) base class using super$set_model_param() method (5). Set implemented steps training base class using super$set_SFT_*() methods (6). Finally run basic train algorithm (7) base class (input) parameters (3). Now use transformer (created p. 4.2) like code example p. 2.1. file “R/AIFETransformerMaker.R” find definition AIFETrType list add new element end (1): end file “R/dotAIFECustomTransformer.R” put following line use transformer Transformer Maker: Now use Transformer Maker class create custom transformer like p. 3.2.","code":".AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\", # (1)   inherit = .AIFEBaseTransformer, # (2)   private = list(), # (3)   public = list() # (4) ) .AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\",   inherit = .AIFEBaseTransformer,   private = list(     title = \"Custom Model\" # (1)   ),   public = list(     initialize = function() { # (2)       super$set_title(private$title) # (3)     }   ) ) {r} .AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\",   inherit = .AIFEBaseTransformer,   private = list(     title = \"Custom Model\",     steps_for_creation = list(                   # (1)       # required       create_tokenizer_draft = function(self) {      # (2)         # The implementation must be here         # self$temp$tok_new <- ...         # (8)       },       calculate_vocab = function(self) {             # (3)         # The implementation must be here         # ... self$temp$tok_new ...        # (9)       },       save_tokenizer_draft = function(self) {        # (4)         # The implementation must be here       },       create_final_tokenizer = function(self) {      # (5)         # The implementation must be here         # self$temp$tokenizer <- ... # (!!!) (11)       },       create_transformer_model = function(self) {    # (6)         # The implementation must be here         # ... self$params$ml_framework ... # (10)         # self$temp$model <- ...     # (!!!) (12)       },       # optional: omit this element if do not needed       check_max_pos_emb = function(self) {           # (7)         # The implementation must be here       }     )   ),   public = list(     initialize = function() {       super$set_title(private$title)     }   ) ) {r} .AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\",   inherit = .AIFEBaseTransformer,   private = list(     title = \"Custom Model\",     steps_for_creation = list(       # required       create_tokenizer_draft = function(self) { },       calculate_vocab = function(self) { },       save_tokenizer_draft = function(self) { },       create_final_tokenizer = function(self) { },       create_transformer_model = function(self) { },       # optional: omit this element if do not needed       check_max_pos_emb = function(self) { }     )   ),   public = list(     initialize = function() { },     # (1)     create = function(# (2) --------------------------                       ml_framework,                       model_dir,                       text_dataset,                       vocab_size,                       # ...                       trace,                       pytorch_safetensors,                       # ...                        # (3) --------------------------                       dep_param1,                       dep_param2,                       # ...                       dep_paramN) {       # (4) -----------------------------------------       super$set_model_param(\"dep_param1\", dep_param1)       super$set_model_param(\"dep_param2\", dep_param2)       # ...       super$set_model_param(\"dep_paramN\", dep_paramN)        # (5) -----------------------------------------       super$set_required_SFC(private$steps_for_creation)        # optional, can be omitted if do not needed       super$set_SFC_check_max_pos_emb(private$steps_for_creation$check_max_pos_emb)        # (6) -----------------------------------------       super$create(         ml_framework = ml_framework,         model_dir = model_dir,         text_dataset = text_dataset,         vocab_size = vocab_size,         # ...         trace = trace,         pytorch_safetensors = pytorch_safetensors         # ...       )     }   ) ) {r} .AIFECustomTransformer <- R6::R6Class(   classname = \".AIFECustomTransformer\",   inherit = .AIFEBaseTransformer,   private = list(     title = \"Custom Model\",     steps_for_creation = list(       # required       create_tokenizer_draft = function(self) { },       calculate_vocab = function(self) { },       save_tokenizer_draft = function(self) { },       create_final_tokenizer = function(self) { },       create_transformer_model = function(self) { },       # optional: omit this element if do not needed       check_max_pos_emb = function(self) { }     ),     # (2)     steps_for_training = list(       # required       load_existing_model = function(self) { },       # optional       cuda_empty_cache = function() { },       create_data_collator = function() { }     )   ),   public = list(     initialize = function() { },     create = function() {       # ---------------------------       # super$set_model_param(...)       # ...       # ---------------------------       # super$set_required_SFC(...)       # super$set_SFC_*(...)       # ...       # ---------------------------       # super$create(...)     },      # (1)     train = function(# (3) --------                      ml_framework,                      # ...                       # (4) --------                      dep_param1,                      # ...                      dep_paramN) {       # (5) -----------------------------------------       super$set_model_param(\"dep_param1\", dep_param1)       # ...       super$set_model_param(\"dep_paramN\", dep_paramN)        # (6) -----------------------------------------       super$set_SFT_load_existing_model(private$steps_for_training$load_existing_model)       # optional       super$set_SFT_cuda_empty_cache(private$steps_for_training$cuda_empty_cache)       super$set_SFT_create_data_collator(private$steps_for_training$create_data_collator)        # (7) -----------------------------------------       super$train(         ml_framework = ml_framework,         # ...       )     }   ) ) AIFETrType <- list(   bert = \"bert\",   roberta = \"roberta\",   # ...   mpnet = \"mpnet\",   custom = \"custom\"   # (1) ) .AIFETrObj[[AIFETrType$custom]] <- .AIFECustomTransformer$new"},{"path":"https://fberding.github.io/aifeducation/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Berding Florian. Author, maintainer. Tykhonova Yuliia. Author. Pargmann Julia. Contributor. Leube Anna. Contributor. Riebenbauer Elisabeth. Contributor. Rebmann Karin. Contributor. Slopinski Andreas. Contributor.","code":""},{"path":"https://fberding.github.io/aifeducation/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Florian Berding, Yuliia Tykhonova, Julia Pargmann, Elisabeth Riebenbauer,   Karin Rebmann, Andreas Slopinski (2024). AI Education (aifeducation).   R package educators researchers educational social sciences.   URL=https://fberding.github.io/aifeducation/index.html","code":"@Manual{,   title = {AI for Education (aifeducation). A R package for educators and       reserachers of the educational and social sciences.},   author = {Florian Berding and Yuliia Tykhonova and Julia Pargmann and Elisabeth Riebenbauer and Karin Rebmann and Andreas Slopinski},   year = {2024},   url = {https://fberding.github.io/aifeducation/index.html}, }"},{"path":"https://fberding.github.io/aifeducation/index.html","id":"aifeducation-","dir":"","previous_headings":"","what":"Artificial Intelligence for Education","title":"Artificial Intelligence for Education","text":"R package Artificial Intelligence Education (aifeducation) designed special requirements educators, educational researchers, social researchers. target audience package educators researchers coding skills like develop models, well people like use models created researchers/educators. package supports application Artificial Intelligence (AI) Natural Language Processing tasks text embedding classification special conditions educational social sciences.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"features-overview","dir":"","previous_headings":"","what":"Features Overview","title":"Artificial Intelligence for Education","text":"Simple usage artificial intelligence providing routines important tasks educators researchers social educational sciences. Provides graphical user interface (Aifeducation Studio), allowing users work AI without need coding skills. Supports ‘PyTorch’ core machine learning framework widely used research. Implements advantages python library ‘datasets’, increasing computational speed allowing use large data sets. Uses safetensors saving models ‘PyTorch’. Supports pre-trained language models Hugging Face. Supports MPNet, BERT, RoBERTa, DeBERTa, Longformer, Funnel Transformer creating context-sensitive text embedding. Makes sharing pre-trained models easy. Integrates sustainability tracking. Integrates special statistical techniques dealing data structures common social educational sciences. Supports classification long text documents. Currently, package focuses classification tasks can either used diagnose characteristics learners written material estimate properties learning teaching material. future, tasks implemented.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Artificial Intelligence for Education","text":"can install latest stable version package CRAN : can install development version aifeducation GitHub : instructions installation can found vignette 01 Get Started. Please note update version aifeducation may require update python libraries. Refer 01 Get Started details. use package R syntax described vignette 03 Using R syntax.","code":"install.packages(\"aifeducation\") install.packages(\"devtools\") devtools::install_github(repo=\"FBerding/aifeducation\",                          ref=\"master\",                          dependencies = \"Imports\")"},{"path":"https://fberding.github.io/aifeducation/index.html","id":"graphical-user-interface-ai-for-education---studio","dir":"","previous_headings":"","what":"Graphical User Interface AI for Education - Studio","title":"Artificial Intelligence for Education","text":"package ships shiny app serves graphical user interface. Figure 1: Aifeducation Studio AI Education - Studio allows users easily develop, train, apply, document, analyse AI models without coding skills. See corresponding vignette details: 02 Using graphical user interface Aifeducation - Studio.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"sustainability","dir":"","previous_headings":"","what":"Sustainability","title":"Artificial Intelligence for Education","text":"Training AI models consumes time energy. help researchers estimate ecological impact work, sustainability tracker implemented. based python library ‘codecarbon’ Courty et al. (2023). tracker allows estimate energy consumption CPUs, GPUs RAM training derives value CO2 emission. value based energy mix country computer located.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"pytorch-and-tensorflow-compatibility","dir":"","previous_headings":"","what":"PyTorch and Tensorflow Compatibility","title":"Artificial Intelligence for Education","text":"core machine learning framework package ‘PyTorch’, providing broad support graphical devices accelerate computations, access new unique model architectures, high compatibility models across different versions machine learning framework. ‘Tensorflow’ also supported version 2.15 models. Please refer appendix A01 Supported Machine Learning Frameworks detailed overview.Tensorflow support removed version 1.1.0 package.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"model-life-cycle","dir":"","previous_headings":"","what":"Model Life Cycle","title":"Artificial Intelligence for Education","text":"Research requires reproducibility traceability. Thus, starting version 1.0.0 package, top priority ensure already trained models work future versions package.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/index.html","id":"transforming-texts-into-numbers","dir":"","previous_headings":"Classification Tasks","what":"Transforming Texts into Numbers","title":"Artificial Intelligence for Education","text":"Classification tasks require transformation raw texts representation numbers. step, aifeducation supports new approaches MPNet (Song et al. 2020), BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), DeBERTa version 2 (et al. 2020), Funnel-Transformer (Dai et al. 2020), Longformer (Beltagy, Peters & Cohan 2020). aifeducation supports use pre-trained transformer models provided Hugging Face creation new transformers, allowing educators researchers develop specialized domain-specific models. See 04 Model configuration details configuration new model. package supports analysis long texts. Depending method, long texts transformed vectors , , long, split several chunks results sequence vectors.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"training-ai-under-challenging-conditions","dir":"","previous_headings":"Classification Tasks","what":"Training AI under Challenging Conditions","title":"Artificial Intelligence for Education","text":"second step within classification task, aifeducation integrates important statistical mathematical methods dealing main challenges educational social sciences applying AI. : digital data availability: educational social sciences, data often available handwritten form. example, schools universities, students often solve tasks creating handwritten documents. Thus, educators researchers first transform analogue data digital form, involving human action. makes data generation expensive time-consuming, leading small data sets. high privacy policy standards: Furthermore, educational social sciences, data often refers humans /actions. kinds data protected privacy policies many countries, limiting access usage data, also results small data sets. long research tradition: Educational social sciences long research tradition generating insights social phenomena well learning teaching. insights incorporated applications AI (e.g., Luan et al. 2020; Wong et al. 2019). makes supervised machine learning important technology since provides link educational social theories models one hand machine learning hand (Berding et al. 2022). However, kind machine learning requires humans generate valid data set training process, leading small data sets. complex constructs: Compared classification tasks , instance, AI differentiate ‘good’ ‘bad’ movie review, constructs educational social sciences complex. example, research instruments motivational psychology require infer personal motifs written essays (e.g., Gruber & Kreuzpointner 2013). reliable valid interpretation kind information requires well qualified human raters, making data generation expensive. also limits size data set. imbalanced data: Finally, data educational social sciences often occurs imbalanced pattern several empirical studies show (Bloemen 2011; Stütz et al. 2022). Imbalanced means categories characteristics data set high absolute frequencies compared categories characteristics. Imbalance AI training guides algorithms focus prioritize categories characteristics high absolute frequencies, increasing risk miss categories/characteristics low frequencies (Haixiang et al. 2017). can lead AI prefer special groups people/material, imply false recommendations conclusions, miss rare categories characteristics. order deal problem imbalanced data sets, package integrates Synthetic Minority Oversampling Technique learning process. Currently, Basic Synthetic Minority Oversampling Technique (Chawla et al. 2002), Density-Bases Synthetic Minority Oversampling Technique (Bunkhumpornpat, Sinapiromsaran & Lursinsap 2012), Adaptive Synthetic Sampling Approach Imbalanced Learning (Hem Garcia & Li 2008) implemented via R package smotefamiliy. order address problem small data sets, training loops AI integrate pseudo-labeling (e.g., Lee 2013). Pseudo-labeling technique can used supervised learning. specifically, educators researchers rate part data set train AI part. remainder data processed humans. Instead, AI uses part data learn . Thus, educators researchers provide additional data AI’s learning process without coding . offers possibility add data training process reduce labor cost.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"evaluating-performance","dir":"","previous_headings":"Classification Tasks","what":"Evaluating Performance","title":"Artificial Intelligence for Education","text":"Classification tasks machine learning comparable empirical method content analysis social sciences. method looks back long research tradition ongoing discussion evaluate reliability validity generated data. order provide link research tradition provide educators well educational social researchers performance measures familiar , every AI trained package evaluated following measures concepts: Iota Concept Second Generation (Berding & Pargmann 2022) Krippendorff’s Alpha (Krippendorff 2019) Percentage Agreement Gwet’s AC1/AC2 (Gwet 2014) Kendall’s coefficient concordance W Cohen’s Kappa unweighted (Cohen 1960) Cohen’s Kappa equal weights (Cohen 1968) Cohen’s Kappa squared weights (Cohen 1968)  addition, traditional measures machine learning literature also available: Precision Recall F1-Score","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"sharing-trained-ai","dir":"","previous_headings":"","what":"Sharing Trained AI","title":"Artificial Intelligence for Education","text":"Since package based ‘PyTorch’ transformer library, every trained AI can shared educators researchers. package supports easy use pre-trained AI within R, also provides possibility export trained AI environments. Using pre-trained AI classification requires classifier corresponding text embedding model. Use AI Education Studio just load R start predictions. Vignette 02 Using graphical user interface Aifeducation - Studio describes use user interface. Vignette 03 Using R syntax describes save load objects R syntax. vignette 05 Sharing Using Trained AI/Models can find detailed guide document share models.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"tutorial-and-guides","dir":"","previous_headings":"","what":"Tutorial and Guides","title":"Artificial Intelligence for Education","text":"01 Get Started: Installation configuration package. 02 Using graphical user interface Aifeducation - Studio: Introduction graphical user interface Aifeducation Studio. 03 Using R syntax: short introduction using package R syntax examples classification tasks. 04 Model configuration: Summary studies finding good configuration model. 05 Sharing Using Trained AI/Models: Guidance share models.","code":""},{"path":"https://fberding.github.io/aifeducation/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Artificial Intelligence for Education","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. https://doi.org/10.48550/arXiv.2004.05150 Berding, F., & Pargmann, J. (2022). Iota Reliability Concept Second Generation. Berlin: Logos. https://doi.org/10.30819/5581 Berding, F., Riebenbauer, E., Stütz, S., Jahncke, H., Slopinski, ., & Rebmann, K. (2022). Performance Configuration Artificial Intelligence Educational Settings.: Introducing New Reliability Concept Based Content Analysis. Frontiers Education, 1-21. https://doi.org/10.3389/feduc.2022.818365 Bloemen, . (2011). Lernaufgaben Schulbüchern der Wirtschaftslehre: Analyse, Konstruktion und Evaluation von Lernaufgaben für die Lernfelder industrieller Geschäftsprozesse. Hampp. Bunkhumpornpat, C., Sinapiromsaran, K., & Lursinsap, C. (2012). DBSMOTE: Density-Based Synthetic Minority -sampling Technique. Applied Intelligence, 36(3), 664–684. https://doi.org/10.1007/s10489-011-0287-y Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority -sampling Technique. Journal Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953 Cohen, J (1968). Weighted kappa: Nominal scale agreement provision scaled disagreement partial credit. Psychological Bulletin, 70(4), 213–220. https://doi.org/10.1037/h0026256 Cohen, J (1960). Coefficient Agreement Nominal Scales. Educational Psychological Measurement, 20(1), 37–46. https://doi.org/10.1177/001316446002000104 Courty, B., Schmidt, V., Goyal-Kamal, Coutarel, M., Feld, B., Lecourt, J., & … (2023). mlco2/codecarbon: v2.2.7. https://doi.org/10.5281/zenodo.8181237 Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer: Filtering Sequential Redundancy Efficient Language Processing. https://doi.org/10.48550/arXiv.2006.03236 Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. https://doi.org/10.18653/v1/N19-1423 Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378–382. https://doi.org/10.1037/h0031619 Gruber, N., & Kreuzpointner, L. (2013). Measuring reliability picture story exercises like TAT. PloS One, 8(11), e79450. https://doi.org/10.1371/journal.pone.0079450 Gwet, K. L. (2014). Handbook inter-rater reliability: definitive guide measuring extent agreement among raters (Fourth edition). STATAXIS. Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., & Bing, G. (2017). Learning class-imbalanced data: Review methods applications. Expert Systems Applications, 73, 220–239. https://doi.org/10.1016/j.eswa.2016.12.035 , H., Bai, Y., Garcia, E. ., & Li, S. (2008). ADASYN: Adaptive synthetic sampling approach imbalanced learning. 2008 IEEE International Joint Conference Neural Networks (IEEE World Congress Computational Intelligence) (pp. 1322–1328). IEEE. https://doi.org/10.1109/IJCNN.2008.4633969 , P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT Disentangled Attention. https://doi.org/10.48550/arXiv.2006.03654 Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th Ed.). SAGE. Lee, D.‑H. (2013). Pseudo-Label: Simple Efficient Semi-Supervised Learning Method Deep Neural Networks. CML 2013 Workshop: Challenges Representation Learning. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 Luan, H., Geczy, P., Lai, H., Gobert, J., Yang, S. J. H., Ogata, H., Baltes, J., Guerra, R., Li, P., & Tsai, C.‑C. (2020). Challenges Future Directions Big Data Artificial Intelligence Education. Frontiers Psychology, 11, 1–11. https://doi.org/10.3389/fpsyg.2020.580820 Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.‑Y. (2020). MPNet: Masked Permuted Pre-training Language Understanding. https://doi.org/10.48550/arXiv.2004.09297 Stütz, S., Berding, F., Reincke, S., & Scheper, L. (2022). Characteristics learning tasks accounting textbooks: AI assisted analysis. Empirical Research Vocational Education Training, 14(1). https://doi.org/10.1186/s40461-022-00138-2 Wong, J., Baars, M., Koning, B. B. de, van der Zee, T., Davis, D., Khalil, M., Houben, G.‑J., & Paas, F. (2019). Educational Theories Learning Analytics: Data Knowledge. D. Ifenthaler, D.-K. Mah, & J. Y.-K. Yau (Eds.), Utilizing Learning Analytics Support Study Success (pp. 3–25). Springer. https://doi.org/10.1007/978-3-319-64792-0_1","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Base class for models using neural nets — AIFEBaseModel","title":"Base class for models using neural nets — AIFEBaseModel","text":"Abstract class models rely python library 'transformers'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base class for models using neural nets — AIFEBaseModel","text":"Objects containing fields methods used several classes 'ai education'. class designed direct application used developers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Base class for models using neural nets — AIFEBaseModel","text":"model ('tensorflow_model' 'pytorch_model') Field storing 'tensorflow' 'pytorch' model loading. model_config ('list()') List storing information configuration model. last_training ('list()') List storing history, configuration, results last training. information overwritten new training started. last_training$start_time: Time point training started. last_training$learning_time: Duration training process. last_training$finish_time: Time last training finished. last_training$history: History last training. last_training$data: Object class table storing initial frequencies passed data. last_training$config: List storing configuration used last training.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Base class for models using neural nets — AIFEBaseModel","text":"AIFEBaseModel$get_model_info() AIFEBaseModel$get_text_embedding_model() AIFEBaseModel$set_publication_info() AIFEBaseModel$get_publication_info() AIFEBaseModel$set_model_license() AIFEBaseModel$get_model_license() AIFEBaseModel$set_documentation_license() AIFEBaseModel$get_documentation_license() AIFEBaseModel$set_model_description() AIFEBaseModel$get_model_description() AIFEBaseModel$save() AIFEBaseModel$load() AIFEBaseModel$get_package_versions() AIFEBaseModel$get_sustainability_data() AIFEBaseModel$get_ml_framework() AIFEBaseModel$get_text_embedding_model_name() AIFEBaseModel$check_embedding_model() AIFEBaseModel$count_parameter() AIFEBaseModel$is_configured() AIFEBaseModel$get_private() AIFEBaseModel$get_all_fields() AIFEBaseModel$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting model information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_model_info()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"list relevant model information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-text-embedding-model-","dir":"Reference","previous_headings":"","what":"Method get_text_embedding_model()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting text embedding model information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_text_embedding_model()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"list relevant model information text embedding model underlying model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method setting publication information model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$set_publication_info(authors, citation, url = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"authors List authors. citation Free text citation. url URL corresponding homepage.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. used setting private members publication information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting bibliographic information model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_publication_info()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"list saved bibliographic information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-set-model-license-","dir":"Reference","previous_headings":"","what":"Method set_model_license()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method setting license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$set_model_license(license = \"CC BY\")"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. used setting private member software license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-model-license-","dir":"Reference","previous_headings":"","what":"Method get_model_license()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method getting license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_model_license()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"string representing license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-set-documentation-license-","dir":"Reference","previous_headings":"","what":"Method set_documentation_license()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method setting license model's documentation.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$set_documentation_license(license = \"CC BY\")"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. used setting private member documentation license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-documentation-license-","dir":"Reference","previous_headings":"","what":"Method get_documentation_license()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method getting license model's documentation.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_documentation_license()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns license string.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method setting description model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$set_model_description(   eng = NULL,   native = NULL,   abstract_eng = NULL,   abstract_native = NULL,   keywords_eng = NULL,   keywords_native = NULL )"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"eng string text describing training, theoretical empirical background, output English. native string text describing training , theoretical empirical background, output native language model. abstract_eng string text providing summary description English. abstract_native string text providing summary description native language model. keywords_eng vector keyword English. keywords_native vector keyword native language model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. used setting private members description model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting model description.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_model_description()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"list description classifier English native language.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-save-","dir":"Reference","previous_headings":"","what":"Method save()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method saving model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$save(dir_path, folder_name)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"dir_path string Path directory model saved. folder_name string Name folder created within directory.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. saves model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-load-","dir":"Reference","previous_headings":"","what":"Method load()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method importing model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$load(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"dir_path string Path directory model saved.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Function return value. used load weights model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-package-versions-","dir":"Reference","previous_headings":"","what":"Method get_package_versions()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting summary R python packages' versions used creating model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_package_versions()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns list containing versions relevant R python packages.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-sustainability-data-","dir":"Reference","previous_headings":"","what":"Method get_sustainability_data()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting summary tracked energy consumption training estimate resulting CO2 equivalents kg.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_sustainability_data()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns list containing tracked energy consumption, CO2 equivalents kg, information tracker used, technical information training infrastructure.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-ml-framework-","dir":"Reference","previous_headings":"","what":"Method get_ml_framework()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting machine learning framework used model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_ml_framework()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns string describing machine learning framework used classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-text-embedding-model-name-","dir":"Reference","previous_headings":"","what":"Method get_text_embedding_model_name()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting name (unique id) underlying text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_text_embedding_model_name()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns string describing name text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-check-embedding-model-","dir":"Reference","previous_headings":"","what":"Method check_embedding_model()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method checking provided text embeddings created TextEmbeddingModel model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$check_embedding_model(text_embeddings)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"text_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"TRUE underlying TextEmbeddingModel . FALSE models differ.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-count-parameter-","dir":"Reference","previous_headings":"","what":"Method count_parameter()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method counting trainable parameters model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$count_parameter()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns number trainable parameters model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-is-configured-","dir":"Reference","previous_headings":"","what":"Method is_configured()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method checking model successfully configured. object can used value TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$is_configured()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"bool TRUE model fully configured. FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-private-","dir":"Reference","previous_headings":"","what":"Method get_private()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method requesting private fields methods. Used loading updating object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_private()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-19","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Returns list private fields methods.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-get-all-fields-","dir":"Reference","previous_headings":"","what":"Method get_all_fields()","title":"Base class for models using neural nets — AIFEBaseModel","text":"Return fields.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$get_all_fields()"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"returns-20","dir":"Reference","previous_headings":"","what":"Returns","title":"Base class for models using neural nets — AIFEBaseModel","text":"Method returns list containing public private fields object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Base class for models using neural nets — AIFEBaseModel","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for models using neural nets — AIFEBaseModel","text":"","code":"AIFEBaseModel$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFEBaseModel.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for models using neural nets — AIFEBaseModel","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETrType.html","id":null,"dir":"Reference","previous_headings":"","what":"Transformer types — AIFETrType","title":"Transformer types — AIFETrType","text":"list contains transformer types. Elements list can used public make AIFETransformerMaker R6 class input parameter type. following elements: bert = 'bert' roberta = 'roberta' deberta_v2 = 'deberta_v2' funnel = 'funnel' longformer = 'longformer' mpnet = 'mpnet' Elements can used like AIFETrType$bert, AIFETrType$deberta_v2, AIFETrType$funnel, etc.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETrType.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transformer types — AIFETrType","text":"","code":"AIFETrType"},{"path":"https://fberding.github.io/aifeducation/reference/AIFETrType.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Transformer types — AIFETrType","text":"object class list length 6.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":null,"dir":"Reference","previous_headings":"","what":"R6 class for transformer creation — AIFETransformerMaker","title":"R6 class for transformer creation — AIFETransformerMaker","text":"class developed make creation transformers easier users. Pass transformer's type make method get desired transformer. Now run create /train methods new transformer. already created aife_transformer_maker object class can used. See p.3 Transformer Maker Transformers Developers details. See .AIFEBaseTransformer class details.","code":""},{"path":[]},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"R6 class for transformer creation — AIFETransformerMaker","text":"AIFETransformerMaker$make() AIFETransformerMaker$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"method-make-","dir":"Reference","previous_headings":"","what":"Method make()","title":"R6 class for transformer creation — AIFETransformerMaker","text":"Creates new transformer passed type.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"R6 class for transformer creation — AIFETransformerMaker","text":"","code":"AIFETransformerMaker$make(type)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"R6 class for transformer creation — AIFETransformerMaker","text":"type string type new transformer. Allowed types bert, roberta, deberta_v2, funnel, longformer, mpnet. See AIFETrType list.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"R6 class for transformer creation — AIFETransformerMaker","text":"success - new transformer, otherwise - error (passed type invalid).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"R6 class for transformer creation — AIFETransformerMaker","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"R6 class for transformer creation — AIFETransformerMaker","text":"","code":"AIFETransformerMaker$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"R6 class for transformer creation — AIFETransformerMaker","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/AIFETransformerMaker.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"R6 class for transformer creation — AIFETransformerMaker","text":"","code":"# Create transformer maker tr_maker <- AIFETransformerMaker$new()  # Use 'make' method of the 'tr_maker' object # Pass string with the type of transformers # Allowed types are \"bert\", \"deberta_v2\", \"funnel\", etc. See aifeducation::AIFETrType list my_bert <- tr_maker$make(\"bert\") #> [1] \"BERT Model has been initialized.\"  # Or use elements of the 'aifeducation::AIFETrType' list my_longformer <- tr_maker$make(AIFETrType$longformer) #> [1] \"Longformer Model has been initialized.\"  # Run 'create' or 'train' methods of the transformer in order to create a # new transformer or train the newly created one, respectively # my_bert$create(...) # my_bert$train(...)  # my_longformer$create(...) # my_longformer$train(...)"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Data manager for classification tasks — DataManagerClassifier","title":"Data manager for classification tasks — DataManagerClassifier","text":"Abstract class managing data samples training classifier. DataManagerClassifier used TEClassifierRegular TEClassifierProtoNet.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data manager for classification tasks — DataManagerClassifier","text":"Objects class used ensuring correct data management training different types classifiers. Objects class also used data augmentation creating synthetic cases different techniques.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Data manager for classification tasks — DataManagerClassifier","text":"config ('list') Field storing configuration DataManagerClassifier. state ('list') Field storing current state DataManagerClassifier. datasets ('list') Field storing data sets used training. elements list data sets class datasets.arrow_dataset.Dataset. following data sets available: data_labeled: cases label. data_unlabeled: cases label. data_labeled_synthetic: synthetic cases corresponding labels. data_labeled_pseudo: subset data_unlabeled pseudo labels estimated classifier. name_idx ('named vector') Field storing pairs indexes names every case. pairs labeled unlabeled data separated. samples ('list') Field storing assignment every cases train, validation test data set depending concrete fold. indexes names stored. addition, list contains assignment final training excludes test data set. DataManagerClassifier uses folds sample final training can requested +1.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Data manager for classification tasks — DataManagerClassifier","text":"DataManagerClassifier$new() DataManagerClassifier$get_config() DataManagerClassifier$get_labeled_data() DataManagerClassifier$get_unlabeled_data() DataManagerClassifier$get_samples() DataManagerClassifier$set_state() DataManagerClassifier$get_n_folds() DataManagerClassifier$get_n_classes() DataManagerClassifier$get_statistics() DataManagerClassifier$get_dataset() DataManagerClassifier$get_val_dataset() DataManagerClassifier$get_test_dataset() DataManagerClassifier$create_synthetic() DataManagerClassifier$add_replace_pseudo_data() DataManagerClassifier$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Creating new instance class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$new(   data_embeddings,   data_targets,   folds = 5,   val_size = 0.25,   class_levels,   one_hot_encoding = TRUE,   add_matrix_map = TRUE,   sc_methods = \"dbsmote\",   sc_min_k = 1,   sc_max_k = 10,   trace = TRUE,   n_cores = auto_n_cores() )"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"data_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings DataManagerClassifier created. data_targets factor containing labels cases stored data_embeddings. Factor must named use names used data_embeddings. Missing values supported supplied (e.g., pseudo labeling). folds int determining number cross-fold samples. Value must least 2. val_size double 0 1, indicating proportion cases class used validation sample. remaining cases part training data. class_levels vector containing possible levels labels. one_hot_encoding bool TRUE labels converted one hot encoding. add_matrix_map bool TRUE embeddings transformed two dimensional matrix. number rows equals number cases. number columns equals times*features. sc_methods string determining technique used creating synthetic cases. sc_min_k int determining minimal number neighbors creating synthetic cases. sc_max_k int determining minimal number neighbors creating synthetic cases. trace bool TRUE information process printed console. n_cores int Number cores used calculation synthetic cases.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method returns initialized object class DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-config-","dir":"Reference","previous_headings":"","what":"Method get_config()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting configuration DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_config()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns list storing configuration DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-labeled-data-","dir":"Reference","previous_headings":"","what":"Method get_labeled_data()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting complete labeled data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_labeled_data()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns object class datasets.arrow_dataset.Dataset containing cases labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-unlabeled-data-","dir":"Reference","previous_headings":"","what":"Method get_unlabeled_data()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting complete unlabeled data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_unlabeled_data()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns object class datasets.arrow_dataset.Dataset containing cases without labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-samples-","dir":"Reference","previous_headings":"","what":"Method get_samples()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting assignments train, validation, test data sets every fold final training.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_samples()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns list storing assignments train, validation, test data set every fold. case sample final training test data set always empty (NULL).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-set-state-","dir":"Reference","previous_headings":"","what":"Method set_state()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method setting current state DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$set_state(iteration, step = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"iteration int determining current iteration training. iteration determines fold use training, validation, testing. number fold +1 request sample final training. requesting sample final training iteration can take string \"final\". step int determining step estimating using pseudo labels training. relevant training requested pseudo labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method return anything. used setting internal state DataManager.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-n-folds-","dir":"Reference","previous_headings":"","what":"Method get_n_folds()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting number folds DataManagerClassifier can use current data.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_n_folds()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns number folds DataManagerClassifier uses.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-n-classes-","dir":"Reference","previous_headings":"","what":"Method get_n_classes()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting number classes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_n_classes()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns number classes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-statistics-","dir":"Reference","previous_headings":"","what":"Method get_statistics()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting descriptive sample statistics.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_statistics()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns table describing absolute frequencies labeled unlabeled data. rows contain length sequences columns contain labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-dataset-","dir":"Reference","previous_headings":"","what":"Method get_dataset()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting data set training depending current state DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_dataset(   inc_labeled = TRUE,   inc_unlabeled = FALSE,   inc_synthetic = FALSE,   inc_pseudo_data = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"inc_labeled bool TRUE data set includes cases labels. inc_unlabeled bool TRUE data set includes cases labels. inc_synthetic bool TRUE data set includes synthetic cases corresponding labels. inc_pseudo_data bool TRUE data set includes cases pseudo labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns object class datasets.arrow_dataset.Dataset containing requested kind data along requested transformations training. Please note method returns data sets designed training . corresponding validation data set requested get_val_dataset corresponding test data set get_test_dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-val-dataset-","dir":"Reference","previous_headings":"","what":"Method get_val_dataset()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting data set validation depending current state DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_val_dataset()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns object class datasets.arrow_dataset.Dataset containing requested kind data along requested transformations validation. corresponding data set training can requested get_dataset corresponding data set testing get_test_dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-get-test-dataset-","dir":"Reference","previous_headings":"","what":"Method get_test_dataset()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method requesting data set testing depending current state DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$get_test_dataset()"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"Returns object class datasets.arrow_dataset.Dataset containing requested kind data along requested transformations validation. corresponding data set training can requested get_dataset corresponding data set validation get_val_dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-create-synthetic-","dir":"Reference","previous_headings":"","what":"Method create_synthetic()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method generating synthetic data used training. process uses labeled data belonging current state DataManagerClassifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$create_synthetic(trace = TRUE, inc_pseudo_data = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"trace bool TRUE information process printed console. inc_pseudo_data bool TRUE data pseudo labels used addition labeled data generating synthetic cases.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"method nothing return. generates new data set synthetic cases stored object class datasets.arrow_dataset.Dataset field datasets$data_labeled_synthetic. Please note call method override existing data set corresponding field.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-add-replace-pseudo-data-","dir":"Reference","previous_headings":"","what":"Method add_replace_pseudo_data()","title":"Data manager for classification tasks — DataManagerClassifier","text":"Method adding data pseudo labels generated classifier","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$add_replace_pseudo_data(inputs, labels)"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"inputs array matrix representing input data. labels factor containing corresponding pseudo labels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Data manager for classification tasks — DataManagerClassifier","text":"method nothing return. generates new data set synthetic cases stored object class datasets.arrow_dataset.Dataset field datasets$data_labeled_pseudo. Please note call method override existing data set corresponding field.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Data manager for classification tasks — DataManagerClassifier","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Data manager for classification tasks — DataManagerClassifier","text":"","code":"DataManagerClassifier$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/DataManagerClassifier.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data manager for classification tasks — DataManagerClassifier","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedded text — EmbeddedText","title":"Embedded text — EmbeddedText","text":"Object class R6 stores text embeddings generated object class TextEmbeddingModel. text embeddings stored within memory/RAM. case high number documents data may fit memory/RAM. Thus, please use object small sample texts. general, recommended use object class LargeDataSetForTextEmbeddings can deal number texts.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embedded text — EmbeddedText","text":"Returns object class EmbeddedText. objects used storing managing text embeddings created objects class TextEmbeddingModel. Objects class EmbeddedText serve input objects class TEClassifierRegular, TEClassifierProtoNet, TEFeatureExtractor. main aim class provide structured link embedding models classifiers. Since objects class save information text embedding model created text embedding ensures embedding generated embedding model combined. Furthermore, stored information allows objects check embeddings correct text embedding model used training predicting.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Embedded text — EmbeddedText","text":"embeddings ('data.frame()') data.frame containing text embeddings chunks. Documents rows. Embedding dimensions columns.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Embedded text — EmbeddedText","text":"EmbeddedText$configure() EmbeddedText$save() EmbeddedText$is_configured() EmbeddedText$load_from_disk() EmbeddedText$get_model_info() EmbeddedText$get_model_label() EmbeddedText$get_times() EmbeddedText$get_features() EmbeddedText$get_original_features() EmbeddedText$is_compressed() EmbeddedText$add_feature_extractor_info() EmbeddedText$get_feature_extractor_info() EmbeddedText$convert_to_LargeDataSetForTextEmbeddings() EmbeddedText$n_rows() EmbeddedText$get_all_fields() EmbeddedText$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Embedded text — EmbeddedText","text":"Creates new object representing text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$configure(   model_name = NA,   model_label = NA,   model_date = NA,   model_method = NA,   model_version = NA,   model_language = NA,   param_seq_length = NA,   param_chunks = NULL,   param_features = NULL,   param_overlap = NULL,   param_emb_layer_min = NULL,   param_emb_layer_max = NULL,   param_emb_pool_type = NULL,   param_aggregation = NULL,   embeddings )"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"model_name string Name model generates embedding. model_label string Label model generates embedding. model_date string Date embedding generating model created. model_method string Method underlying embedding model. model_version string Version model generated embedding. model_language string Language model generated embedding. param_seq_length int Maximum number tokens processes generating model chunk. param_chunks int Maximum number chunks supported generating model. param_features int Number dimensions text embeddings. param_overlap int Number tokens added beginning sequence next chunk model.    #' param_emb_layer_min int string determining first layer included creation embeddings. param_emb_layer_max int string determining last layer included creation embeddings. param_emb_pool_type string determining method pooling token embeddings within layer. param_aggregation string Aggregation method hidden states. Deprecated. included backward compatibility. embeddings data.frame containing text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns object class EmbeddedText stores text embeddings produced objects class TextEmbeddingModel.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-save-","dir":"Reference","previous_headings":"","what":"Method save()","title":"Embedded text — EmbeddedText","text":"Saves data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$save(dir_path, folder_name, create_dir = TRUE)"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"dir_path Path store data set. folder_name string Name folder storing data set. create_dir bool True directory created exist.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Method return anything. write data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-is-configured-","dir":"Reference","previous_headings":"","what":"Method is_configured()","title":"Embedded text — EmbeddedText","text":"Method checking model successfully configured. object can used value TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$is_configured()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"bool TRUE model fully configured. FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Embedded text — EmbeddedText","text":"loads object class EmbeddedText disk updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"dir_path Path data set set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Embedded text — EmbeddedText","text":"Method retrieving information model generated embedding.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_model_info()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"list contains saved information underlying text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-model-label-","dir":"Reference","previous_headings":"","what":"Method get_model_label()","title":"Embedded text — EmbeddedText","text":"Method retrieving label model generated embedding.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_model_label()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"string Label corresponding text embedding model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-times-","dir":"Reference","previous_headings":"","what":"Method get_times()","title":"Embedded text — EmbeddedText","text":"Number chunks/times text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_times()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns int describing number chunks/times text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-features-","dir":"Reference","previous_headings":"","what":"Method get_features()","title":"Embedded text — EmbeddedText","text":"Number actual features/dimensions text embeddings.case feature extractor used number features smaller original number features. receive original number features (number features applying feature extractor) can use method get_original_features class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_features()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns int describing number features/dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-original-features-","dir":"Reference","previous_headings":"","what":"Method get_original_features()","title":"Embedded text — EmbeddedText","text":"Number original features/dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_original_features()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns int describing number features/dimensions feature extractor) used feature extractor) applied.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-is-compressed-","dir":"Reference","previous_headings":"","what":"Method is_compressed()","title":"Embedded text — EmbeddedText","text":"Checks text embedding reduced feature extractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$is_compressed()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns TRUE number dimensions reduced feature extractor. return FALSE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-add-feature-extractor-info-","dir":"Reference","previous_headings":"","what":"Method add_feature_extractor_info()","title":"Embedded text — EmbeddedText","text":"Method setting information feature extractor used reduce number dimensions text embeddings. information used feature extractor applied.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$add_feature_extractor_info(   model_name,   model_label = NA,   features = NA,   method = NA,   noise_factor = NA,   optimizer = NA )"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"model_name string Name underlying TextEmbeddingModel. model_label string Label underlying TextEmbeddingModel. features int Number dimension (features) compressed text embeddings. method string Method TEFeatureExtractor applies genereating compressed text embeddings. noise_factor double Noise factor TEFeatureExtractor. optimizer string Optimizer used training TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Method nothing return. sets information feature extractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-feature-extractor-info-","dir":"Reference","previous_headings":"","what":"Method get_feature_extractor_info()","title":"Embedded text — EmbeddedText","text":"Method receiving information feature extractor used reduce number dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_feature_extractor_info()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns list information feature extractor. feature extractor used returns NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-convert-to-largedatasetfortextembeddings-","dir":"Reference","previous_headings":"","what":"Method convert_to_LargeDataSetForTextEmbeddings()","title":"Embedded text — EmbeddedText","text":"Method converting object object class LargeDataSetForTextEmbeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$convert_to_LargeDataSetForTextEmbeddings()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns object class LargeDataSetForTextEmbeddings uses memory mapping allowing work large data sets.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-n-rows-","dir":"Reference","previous_headings":"","what":"Method n_rows()","title":"Embedded text — EmbeddedText","text":"Number rows.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$n_rows()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Returns number rows text embeddings represent number cases.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-get-all-fields-","dir":"Reference","previous_headings":"","what":"Method get_all_fields()","title":"Embedded text — EmbeddedText","text":"Return fields.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$get_all_fields()"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Embedded text — EmbeddedText","text":"Method returns list containing public private fields object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Embedded text — EmbeddedText","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedded text — EmbeddedText","text":"","code":"EmbeddedText$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/EmbeddedText.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedded text — EmbeddedText","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":null,"dir":"Reference","previous_headings":"","what":"Abstract base class for large data sets — LargeDataSetBase","title":"Abstract base class for large data sets — LargeDataSetBase","text":"object contains public private methods may useful every large data sets. Objects class intended used directly. LargeDataSetForTextEmbeddings LargeDataSetForText.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Returns new object class.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Abstract base class for large data sets — LargeDataSetBase","text":"LargeDataSetBase$n_cols() LargeDataSetBase$n_rows() LargeDataSetBase$get_colnames() LargeDataSetBase$get_dataset() LargeDataSetBase$reduce_to_unique_ids() LargeDataSetBase$select() LargeDataSetBase$get_ids() LargeDataSetBase$save() LargeDataSetBase$load_from_disk() LargeDataSetBase$load() LargeDataSetBase$get_all_fields() LargeDataSetBase$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-n-cols-","dir":"Reference","previous_headings":"","what":"Method n_cols()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Number columns data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$n_cols()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"int describing number columns data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-n-rows-","dir":"Reference","previous_headings":"","what":"Method n_rows()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Number rows data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$n_rows()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"int describing number rows data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-get-colnames-","dir":"Reference","previous_headings":"","what":"Method get_colnames()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Get names columns data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$get_colnames()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"vector containing names columns strings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-get-dataset-","dir":"Reference","previous_headings":"","what":"Method get_dataset()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Get data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$get_dataset()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Returns data set object object class datasets.arrow_dataset.Dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-reduce-to-unique-ids-","dir":"Reference","previous_headings":"","what":"Method reduce_to_unique_ids()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Reduces data set data set containing unique ids. case id exists multiple times data set first case remains data set. cases dropped. Attention Calling method change data set place.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$reduce_to_unique_ids()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Method return anything. changes data set object place.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-select-","dir":"Reference","previous_headings":"","what":"Method select()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Returns data set contains cases belonging specific indices.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$select(indicies)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract base class for large data sets — LargeDataSetBase","text":"indicies vector int selecting rows data set. Attention indices zero-based.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Returns data set class datasets.arrow_dataset.Dataset selected rows.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-get-ids-","dir":"Reference","previous_headings":"","what":"Method get_ids()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Get ids","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$get_ids()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Returns vector containing ids every row strings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-save-","dir":"Reference","previous_headings":"","what":"Method save()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Saves data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$save(dir_path, folder_name, create_dir = TRUE)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract base class for large data sets — LargeDataSetBase","text":"dir_path Path store data set. folder_name string Name folder storing data set. create_dir bool True directory created exist.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Method return anything. write data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"loads object class LargeDataSetBase disk 'updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract base class for large data sets — LargeDataSetBase","text":"dir_path Path data set set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-load-","dir":"Reference","previous_headings":"","what":"Method load()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Loads data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$load(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract base class for large data sets — LargeDataSetBase","text":"dir_path Path data set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Method return anything. loads data set disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-get-all-fields-","dir":"Reference","previous_headings":"","what":"Method get_all_fields()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Return fields.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$get_all_fields()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract base class for large data sets — LargeDataSetBase","text":"Method returns list containing public private fields object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Abstract base class for large data sets — LargeDataSetBase","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract base class for large data sets — LargeDataSetBase","text":"","code":"LargeDataSetBase$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetBase.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract base class for large data sets — LargeDataSetBase","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":null,"dir":"Reference","previous_headings":"","what":"Abstract class for large data sets containing raw texts — LargeDataSetForText","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"object stores raw texts. data objects stored memory directly. using memory mapping objects allow work data sets fit memory/RAM.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Returns new object class.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"aifeducation::LargeDataSetBase -> LargeDataSetForText","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"aifeducation::LargeDataSetBase$get_all_fields() aifeducation::LargeDataSetBase$get_colnames() aifeducation::LargeDataSetBase$get_dataset() aifeducation::LargeDataSetBase$get_ids() aifeducation::LargeDataSetBase$load() aifeducation::LargeDataSetBase$load_from_disk() aifeducation::LargeDataSetBase$n_cols() aifeducation::LargeDataSetBase$n_rows() aifeducation::LargeDataSetBase$reduce_to_unique_ids() aifeducation::LargeDataSetBase$save() aifeducation::LargeDataSetBase$select()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"LargeDataSetForText$new() LargeDataSetForText$add_from_files_txt() LargeDataSetForText$add_from_files_pdf() LargeDataSetForText$add_from_files_xlsx() LargeDataSetForText$add_from_data.frame() LargeDataSetForText$get_private() LargeDataSetForText$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method creation LargeDataSetForText instance. can initialized init_data parameter passed (Uses add_from_data.frame() method init_data data.frame).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$new(init_data = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"init_data Initial data.frame dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"new instance class initialized init_data passed.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-add-from-files-txt-","dir":"Reference","previous_headings":"","what":"Method add_from_files_txt()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method adding raw texts saved within .txt files data set. Please note directory contain one folder .txt file. order create informative data set every folder can contain following additional files: bib_entry.txt: containing text version bibliographic information raw text. license.txt: containing statement license use raw text \"CC \". url_license.txt: containing url/link license internet. text_license.txt: containing license raw text. url_source.txt: containing url/link source internet. id every .txt file file name without file extension. Please aware provide unique file names. Id raw texts mandatory, bibliographic license information optional.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$add_from_files_txt(   dir_path,   batch_size = 500,   log_file = NULL,   log_write_interval = 2,   log_top_value = 0,   log_top_total = 1,   log_top_message = NA,   trace = TRUE )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"dir_path Path directory files stored. batch_size int determining number files process . log_file string Path file log saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_file NULL. log_top_value int indicating current iteration process. log_top_total int determining maximal number iterations. log_top_message string providing additional information process. trace bool TRUE information progress printed console.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"method return anything. adds new raw texts data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-add-from-files-pdf-","dir":"Reference","previous_headings":"","what":"Method add_from_files_pdf()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method adding raw texts saved within .pdf files data set. Please note directory contain one folder .pdf file. order create informative data set every folder can contain following additional files: bib_entry.txt: containing text version bibliographic information raw text. license.txt: containing statement license use raw text \"CC \". url_license.txt: containing url/link license internet. text_license.txt: containing license raw text. url_source.txt: containing url/link source internet. id every .pdf file file name without file extension. Please aware provide unique file names. Id raw texts mandatory, bibliographic license information optional.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$add_from_files_pdf(   dir_path,   batch_size = 500,   log_file = NULL,   log_write_interval = 2,   log_top_value = 0,   log_top_total = 1,   log_top_message = NA,   trace = TRUE )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"dir_path Path directory files stored. batch_size int determining number files process . log_file string Path file log saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_file NULL. log_top_value int indicating current iteration process. log_top_total int determining maximal number iterations. log_top_message string providing additional information process. trace bool TRUE information progress printed console.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"method return anything. adds new raw texts data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-add-from-files-xlsx-","dir":"Reference","previous_headings":"","what":"Method add_from_files_xlsx()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method adding raw texts saved within .xlsx files data set. method assumes texts saved rows columns store id raw texts columns. addition, column bibliography information license can added. column names rows must specified following arguments. must .xlsx files chosen directory. Id raw texts mandatory, bibliographic, license, license's url, license's text, source's url optional. Additional columns dropped.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$add_from_files_xlsx(   dir_path,   trace = TRUE,   id_column = \"id\",   text_column = \"text\",   bib_entry_column = \"bib_entry\",   license_column = \"license\",   url_license_column = \"url_license\",   text_license_column = \"text_license\",   url_source_column = \"url_source\",   log_file = NULL,   log_write_interval = 2,   log_top_value = 0,   log_top_total = 1,   log_top_message = NA )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"dir_path Path directory files stored. trace bool TRUE prints information progress console. id_column string Name column storing ids texts. text_column string Name column storing raw text. bib_entry_column string Name column storing bibliographic information texts. license_column string Name column storing information licenses. url_license_column string Name column storing information url license internet. text_license_column string Name column storing license text. url_source_column string Name column storing information url source internet. log_file string Path file log saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_file NULL. log_top_value int indicating current iteration process. log_top_total int determining maximal number iterations. log_top_message string providing additional information process.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"method return anything. adds new raw texts data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-add-from-data-frame-","dir":"Reference","previous_headings":"","what":"Method add_from_data.frame()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method adding raw texts data.frame","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$add_from_data.frame(data_frame)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"data_frame Object class data.frame least following columns \"id\",\"text\",\"bib_entry\", \"license\", \"url_license\", \"text_license\", \"url_source\". \"id\" and7or \"text\" missing error occurs. columns present data.frame added empty values(NA). Additional columns dropped.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"method return anything. adds new raw texts data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-get-private-","dir":"Reference","previous_headings":"","what":"Method get_private()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Method requesting private fields methods. Used loading updating object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$get_private()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"Returns list private fields methods.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"","code":"LargeDataSetForText$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForText.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing raw texts — LargeDataSetForText","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"object stores text embeddings usually produced object class TextEmbeddingModel. data objects stored memory directly. using memory mapping objects allow work data sets fit memory/RAM. LargeDataSetForTextEmbeddings used storing managing text embeddings created objects class TextEmbeddingModel. Objects class LargeDataSetForTextEmbeddings serve input objects class TEClassifierRegular, TEClassifierProtoNet, TEFeatureExtractor. main aim class provide structured link embedding models classifiers. Since objects class save information text embedding model created text embedding ensures embedding generated embedding model combined. Furthermore, stored information allows objects check embeddings correct text embedding model used training predicting.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns new object class.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"aifeducation::LargeDataSetBase -> LargeDataSetForTextEmbeddings","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"aifeducation::LargeDataSetBase$get_all_fields() aifeducation::LargeDataSetBase$get_colnames() aifeducation::LargeDataSetBase$get_dataset() aifeducation::LargeDataSetBase$get_ids() aifeducation::LargeDataSetBase$load() aifeducation::LargeDataSetBase$n_cols() aifeducation::LargeDataSetBase$n_rows() aifeducation::LargeDataSetBase$reduce_to_unique_ids() aifeducation::LargeDataSetBase$save() aifeducation::LargeDataSetBase$select()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"LargeDataSetForTextEmbeddings$configure() LargeDataSetForTextEmbeddings$is_configured() LargeDataSetForTextEmbeddings$get_text_embedding_model_name() LargeDataSetForTextEmbeddings$get_model_info() LargeDataSetForTextEmbeddings$load_from_disk() LargeDataSetForTextEmbeddings$get_model_label() LargeDataSetForTextEmbeddings$add_feature_extractor_info() LargeDataSetForTextEmbeddings$get_feature_extractor_info() LargeDataSetForTextEmbeddings$is_compressed() LargeDataSetForTextEmbeddings$get_times() LargeDataSetForTextEmbeddings$get_features() LargeDataSetForTextEmbeddings$get_original_features() LargeDataSetForTextEmbeddings$add_embeddings_from_array() LargeDataSetForTextEmbeddings$add_embeddings_from_EmbeddedText() LargeDataSetForTextEmbeddings$add_embeddings_from_LargeDataSetForTextEmbeddings() LargeDataSetForTextEmbeddings$convert_to_EmbeddedText() LargeDataSetForTextEmbeddings$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Creates new object representing text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$configure(   model_name = NA,   model_label = NA,   model_date = NA,   model_method = NA,   model_version = NA,   model_language = NA,   param_seq_length = NA,   param_chunks = NULL,   param_features = NULL,   param_overlap = NULL,   param_emb_layer_min = NULL,   param_emb_layer_max = NULL,   param_emb_pool_type = NULL,   param_aggregation = NULL )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"model_name string Name model generates embedding. model_label string Label model generates embedding. model_date string Date embedding generating model created. model_method string Method underlying embedding model. model_version string Version model generated embedding. model_language string Language model generated embedding. param_seq_length int Maximum number tokens processes generating model chunk. param_chunks int Maximum number chunks supported generating model. param_features int Number dimensions text embeddings. param_overlap int Number tokens added beginning sequence next chunk model. param_emb_layer_min int string determining first layer included creation embeddings. param_emb_layer_max int string determining last layer included creation embeddings. param_emb_pool_type string determining method pooling token embeddings within layer. param_aggregation string Aggregation method hidden states. Deprecated. included backward compatibility.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"method returns new object class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-is-configured-","dir":"Reference","previous_headings":"","what":"Method is_configured()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method checking model successfully configured. object can used value TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$is_configured()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"bool TRUE model fully configured. FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-text-embedding-model-name-","dir":"Reference","previous_headings":"","what":"Method get_text_embedding_model_name()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method requesting name (unique id) underlying text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_text_embedding_model_name()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns string describing name text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method retrieving information model generated embedding.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_model_info()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"list containing saved information underlying text embedding model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"loads object class LargeDataSetForTextEmbeddings disk updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"dir_path Path data set set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-model-label-","dir":"Reference","previous_headings":"","what":"Method get_model_label()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method retrieving label model generated embedding.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_model_label()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"string Label corresponding text embedding model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-add-feature-extractor-info-","dir":"Reference","previous_headings":"","what":"Method add_feature_extractor_info()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method setting information TEFeatureExtractor used reduce number dimensions text embeddings. information used TEFeatureExtractor applied.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$add_feature_extractor_info(   model_name,   model_label = NA,   features = NA,   method = NA,   noise_factor = NA,   optimizer = NA )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"model_name string Name underlying TextEmbeddingModel. model_label string Label underlying TextEmbeddingModel. features int Number dimension (features) compressed text embeddings. method string Method TEFeatureExtractor applies genereating compressed text embeddings. noise_factor double Noise factor TEFeatureExtractor. optimizer string Optimizer used training TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method nothing return. sets information TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-feature-extractor-info-","dir":"Reference","previous_headings":"","what":"Method get_feature_extractor_info()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method receiving information TEFeatureExtractor used reduce number dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_feature_extractor_info()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns list information TEFeatureExtractor. TEFeatureExtractor used returns NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-is-compressed-","dir":"Reference","previous_headings":"","what":"Method is_compressed()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Checks text embedding reduced TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$is_compressed()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns TRUE number dimensions reduced TEFeatureExtractor. return FALSE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-times-","dir":"Reference","previous_headings":"","what":"Method get_times()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Number chunks/times text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_times()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns int describing number chunks/times text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-features-","dir":"Reference","previous_headings":"","what":"Method get_features()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Number actual features/dimensions text embeddings.case TEFeatureExtractor used number features smaller original number features. receive original number features (number features applying TEFeatureExtractor) can use method get_original_features class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_features()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns int describing number features/dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-get-original-features-","dir":"Reference","previous_headings":"","what":"Method get_original_features()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Number original features/dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$get_original_features()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Returns int describing number features/dimensions TEFeatureExtractor) used TEFeatureExtractor) applied.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-add-embeddings-from-array-","dir":"Reference","previous_headings":"","what":"Method add_embeddings_from_array()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method adding new data data set array. Please note method check cases already exist data set. reduce data set unique cases call method reduce_to_unique_ids.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$add_embeddings_from_array(embedding_array)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"embedding_array array containing text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"method return anything. adds new data data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-add-embeddings-from-embeddedtext-","dir":"Reference","previous_headings":"","what":"Method add_embeddings_from_EmbeddedText()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method adding new data data set EmbeddedText. Please note method check cases already exist data set. reduce data set unique cases call method reduce_to_unique_ids.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$add_embeddings_from_EmbeddedText(EmbeddedText)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"EmbeddedText Object class EmbeddedText.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"method return anything. adds new data data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-add-embeddings-from-largedatasetfortextembeddings-","dir":"Reference","previous_headings":"","what":"Method add_embeddings_from_LargeDataSetForTextEmbeddings()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method adding new data data set LargeDataSetForTextEmbeddings. Please note method check cases already exist data set. reduce data set unique cases call method reduce_to_unique_ids.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$add_embeddings_from_LargeDataSetForTextEmbeddings(   dataset )"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"dataset Object class LargeDataSetForTextEmbeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"method return anything. adds new data data set.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-convert-to-embeddedtext-","dir":"Reference","previous_headings":"","what":"Method convert_to_EmbeddedText()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"Method converting object object class EmbeddedText. Attention object uses memory mapping allow usage data sets fit memory. calling method data set loaded stored memory/RAM. may lead --memory error.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$convert_to_EmbeddedText()"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"LargeDataSetForTextEmbeddings object class EmbeddedText stored memory/RAM.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"","code":"LargeDataSetForTextEmbeddings$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/LargeDataSetForTextEmbeddings.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract class for large data sets containing text embeddings — LargeDataSetForTextEmbeddings","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/License_Server.html","id":null,"dir":"Reference","previous_headings":"","what":"Server function for: graphical user interface for showing the license. — License_Server","title":"Server function for: graphical user interface for showing the license. — License_Server","text":"Functions generates functionality page server.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/License_Server.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Server function for: graphical user interface for showing the license. — License_Server","text":"","code":"License_Server(id)"},{"path":"https://fberding.github.io/aifeducation/reference/License_Server.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Server function for: graphical user interface for showing the license. — License_Server","text":"id string determining id namespace.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/License_Server.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Server function for: graphical user interface for showing the license. — License_Server","text":"function nothing return. used create functionality page shiny app.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_Server.html","id":null,"dir":"Reference","previous_headings":"","what":"Server function for: graphical user interface for displaying the reliability of classifiers. — Reliability_Server","title":"Server function for: graphical user interface for displaying the reliability of classifiers. — Reliability_Server","text":"Functions generates functionality page server.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_Server.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Server function for: graphical user interface for displaying the reliability of classifiers. — Reliability_Server","text":"","code":"Reliability_Server(id, model)"},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_Server.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Server function for: graphical user interface for displaying the reliability of classifiers. — Reliability_Server","text":"id string determining id namespace. model Model used inference.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_Server.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Server function for: graphical user interface for displaying the reliability of classifiers. — Reliability_Server","text":"function nothing return. used create functionality page shiny app.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_UI.html","id":null,"dir":"Reference","previous_headings":"","what":"Graphical user interface for displaying the reliability of classifiers. — Reliability_UI","title":"Graphical user interface for displaying the reliability of classifiers. — Reliability_UI","text":"Functions generates tab within page displaying infomration reliability classifiers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_UI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graphical user interface for displaying the reliability of classifiers. — Reliability_UI","text":"","code":"Reliability_UI(id)"},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_UI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graphical user interface for displaying the reliability of classifiers. — Reliability_UI","text":"id string determining id namespace.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/Reliability_UI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Graphical user interface for displaying the reliability of classifiers. — Reliability_UI","text":"function nothing return. used build page shiny app.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":null,"dir":"Reference","previous_headings":"","what":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Abstract class neural nets 'keras'/'tensorflow' 'pytorch'. object represents implementation prototypical network -shot learning described Snell, Swersky, Zemel (2017). network uses multi way contrastive loss described Zhang et al. (2019). network learns scale metric described Oreshkin, Rodriguez, Lacoste (2018)","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Objects class used assigning texts classes/categories. creation training classifier object class EmbeddedText LargeDataSetForTextEmbeddings factor necessary. object class EmbeddedText LargeDataSetForTextEmbeddings contains numerical text representations (text embeddings) raw texts generated object class TextEmbeddingModel. factor contains classes/categories every text. Missing values (unlabeled cases) supported. predictions object class EmbeddedText LargeDataSetForTextEmbeddings used created TextEmbeddingModel training.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Oreshkin, B. N., Rodriguez, P. & Lacoste, . (2018). TADAM: Task dependent adaptive metric improved -shot learning. https://doi.org/10.48550/arXiv.1805.10123 Snell, J., Swersky, K. & Zemel, R. S. (2017). Prototypical Networks -shot Learning. https://doi.org/10.48550/arXiv.1703.05175 Zhang, X., Nie, J., Zong, L., Yu, H. & Liang, W. (2019). One Shot Learning Margin. Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang & S.-J. Huang (Eds.), Lecture Notes Computer Science. Advances Knowledge Discovery Data Mining (Vol. 11440, pp. 305–317). Springer International Publishing. https://doi.org/10.1007/978-3-030-16145-3_24","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"aifeducation::AIFEBaseModel -> aifeducation::TEClassifierRegular -> TEClassifierProtoNet","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"aifeducation::AIFEBaseModel$count_parameter() aifeducation::AIFEBaseModel$get_all_fields() aifeducation::AIFEBaseModel$get_documentation_license() aifeducation::AIFEBaseModel$get_ml_framework() aifeducation::AIFEBaseModel$get_model_description() aifeducation::AIFEBaseModel$get_model_info() aifeducation::AIFEBaseModel$get_model_license() aifeducation::AIFEBaseModel$get_package_versions() aifeducation::AIFEBaseModel$get_private() aifeducation::AIFEBaseModel$get_publication_info() aifeducation::AIFEBaseModel$get_sustainability_data() aifeducation::AIFEBaseModel$get_text_embedding_model() aifeducation::AIFEBaseModel$get_text_embedding_model_name() aifeducation::AIFEBaseModel$is_configured() aifeducation::AIFEBaseModel$load() aifeducation::AIFEBaseModel$set_documentation_license() aifeducation::AIFEBaseModel$set_model_description() aifeducation::AIFEBaseModel$set_model_license() aifeducation::AIFEBaseModel$set_publication_info() aifeducation::TEClassifierRegular$check_embedding_model() aifeducation::TEClassifierRegular$check_feature_extractor_object_type() aifeducation::TEClassifierRegular$load_from_disk() aifeducation::TEClassifierRegular$predict() aifeducation::TEClassifierRegular$requires_compression() aifeducation::TEClassifierRegular$save()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"TEClassifierProtoNet$configure() TEClassifierProtoNet$train() TEClassifierProtoNet$embed() TEClassifierProtoNet$plot_embeddings() TEClassifierProtoNet$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Creating new instance class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"","code":"TEClassifierProtoNet$configure(   ml_framework = \"pytorch\",   name = NULL,   label = NULL,   text_embeddings = NULL,   feature_extractor = NULL,   target_levels = NULL,   dense_size = 4,   dense_layers = 0,   rec_size = 4,   rec_layers = 2,   rec_type = \"gru\",   rec_bidirectional = FALSE,   embedding_dim = 2,   self_attention_heads = 0,   intermediate_size = NULL,   attention_type = \"fourier\",   add_pos_embedding = TRUE,   rec_dropout = 0.1,   repeat_encoder = 1,   dense_dropout = 0.4,   recurrent_dropout = 0.4,   encoder_dropout = 0.1,   optimizer = \"adam\" )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"ml_framework string Currently pytorch supported (ml_framework=\"pytorch\"). name string Name new classifier. Please refer common name conventions. Free text can used parameter label. label string Label new classifier. can use free text. text_embeddings object class TextEmbeddingModel LargeDataSetForTextEmbeddings. feature_extractor Object class TEFeatureExtractor used order reduce number dimensions text embeddings. feature extractor applied set NULL. target_levels vector containing levels (categories classes) within target data. Please order matters. ordinal data please ensure levels sorted correctly later levels indicating higher category/class. nominal data order matter. dense_size int Number neurons dense layer. dense_layers int Number dense layers. rec_size int Number neurons recurrent layer. rec_layers int Number recurrent layers. rec_type string Type recurrent layers.rec_type=\"gru\" Gated Recurrent Unit rec_type=\"lstm\" Long Short-Term Memory. rec_bidirectional bool TRUE bidirectional version recurrent layers used. embedding_dim int determining number dimensions text embedding. self_attention_heads int determining number attention heads self-attention layer. relevant attention_type=\"multihead\". intermediate_size int determining size projection layer within transformer encoder. attention_type string Choose relevant attention type. Possible values \"fourier\" \"multihead\". Please note may see different values case different input orders choose fourier linux. add_pos_embedding bool TRUE positional embedding used. rec_dropout double ranging 0 lower 1, determining dropout bidirectional recurrent layers. repeat_encoder int determining many times encoder added network. dense_dropout double ranging 0 lower 1, determining dropout dense layers. recurrent_dropout double ranging 0 lower 1, determining recurrent dropout recurrent layer. relevant keras models. encoder_dropout double ranging 0 lower 1, determining dropout dense projection within encoder layers. optimizer string \"adam\" \"rmsprop\" .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Returns object class TEClassifierProtoNet ready training.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Method training neural net. Training includes routine early stopping. case loss<0.0001 Accuracy=1.00 Average Iota=1.00 training stops. history uses values last trained epoch remaining epochs. training model best values Average Iota, Accuracy, Loss validation data set used final model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"","code":"TEClassifierProtoNet$train(   data_embeddings,   data_targets,   data_folds = 5,   data_val_size = 0.25,   use_sc = TRUE,   sc_method = \"dbsmote\",   sc_min_k = 1,   sc_max_k = 10,   use_pl = TRUE,   pl_max_steps = 3,   pl_max = 1,   pl_anchor = 1,   pl_min = 0,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   epochs = 40,   batch_size = 35,   Ns = 5,   Nq = 3,   loss_alpha = 0.5,   loss_margin = 0.5,   sampling_separate = FALSE,   sampling_shuffle = TRUE,   dir_checkpoint,   trace = TRUE,   ml_trace = 1,   log_dir = NULL,   log_write_interval = 10,   n_cores = auto_n_cores() )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"data_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings. data_targets factor containing labels cases stored data_embeddings. Factor must named use names used data_embeddings. data_folds int determining number cross-fold samples. data_val_size double 0 1, indicating proportion cases class used validation sample estimation model. remaining cases part training data. use_sc bool TRUE estimation integrate synthetic cases. FALSE . sc_method vector containing method generating synthetic cases. Possible sc_method=\"adas\", sc_method=\"smote\", sc_method=\"dbsmote\". sc_min_k int determining minimal number k used creating synthetic units. sc_max_k int determining maximal number k used creating synthetic units. use_pl bool TRUE estimation integrate pseudo-labeling. FALSE . pl_max_steps int determining maximum number steps pseudo-labeling. pl_max double 0 1, setting maximal level confidence considering case pseudo-labeling. pl_anchor double 0 1 indicating reference point sorting new cases every label. See notes details. pl_min double 0 1, setting minimal level confidence considering case pseudo-labeling. sustain_track bool TRUE energy consumption tracked training via python library 'codecarbon'. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region Region within country. available USA Canada See documentation codecarbon information. https://mlco2.github.io/codecarbon/parameters.html sustain_interval int Interval seconds measuring power usage. epochs int Number training epochs. batch_size int Size batches training. Ns int Number cases every class sample. Nq int Number cases every class query. loss_alpha double Value 0 1 indicating strong loss focus pulling cases corresponding prototypes pushing cases away prototypes. higher value loss concentrates pulling cases corresponding prototypes. loss_margin double Value greater 0 indicating minimal distance every case prototypes classes sampling_separate bool TRUE cases every class divided data set sample query. never mixed. TRUE sample query cases drawn data pool. , case can part sample one epoch another epoch can part query. ensured case never part sample query time. addition, ensured every cases exists training step. sampling_shuffle bool TRUE cases randomly drawn data every step. FALSE cases shuffled. dir_checkpoint string Path directory checkpoint training saved. directory exist, created. trace bool TRUE, information estimation phase printed console. ml_trace int ml_trace=0 print information training process pytorch console. log_dir string Path directory log files saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL. n_cores int Number cores used calculation synthetic cases. relevant use_sc=TRUE. balance_class_weights bool TRUE class weights generated based frequencies training data method Inverse Class Frequency'. FALSE class weight 1. balance_sequence_length bool TRUE sample weights generated length sequences based frequencies training data method Inverse Class Frequency'. FALSE sequences length weight 1.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"sc_max_k: values sc_min_k sc_max_k successively used. number sc_max_k high, value reduced number allows calculating synthetic units. pl_anchor: help value, new cases sorted. aim, distance anchor calculated cases arranged ascending order.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Function return value. changes object trained classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"method-embed-","dir":"Reference","previous_headings":"","what":"Method embed()","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Method embedding documents. Please confuse type embeddings embeddings texts created object class TextEmbeddingModel. embeddings embed documents according similarity specific classes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"","code":"TEClassifierProtoNet$embed(embeddings_q = NULL, batch_size = 32)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"embeddings_q Object class EmbeddedText LargeDataSetForTextEmbeddings containing text embeddings cases embedded classification space. batch_size int batch size.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Returns list containing following elements embeddings_q: embeddings cases (query sample). embeddings_prototypes: embeddings prototypes learned training. represents center different classes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"method-plot-embeddings-","dir":"Reference","previous_headings":"","what":"Method plot_embeddings()","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Method creating plot visualize embeddings corresponding centers (prototypes).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"","code":"TEClassifierProtoNet$plot_embeddings(   embeddings_q,   classes_q = NULL,   batch_size = 12,   alpha = 0.5,   size_points = 3,   size_points_prototypes = 8,   inc_unlabeled = TRUE )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"embeddings_q Object class EmbeddedText LargeDataSetForTextEmbeddings containing text embeddings cases embedded classification space. classes_q Named factor containg true classes every case. Please note names must match names/ids embeddings_q. batch_size int batch size. alpha float Value indicating transparent points (important many points overlap). apply points representing prototypes. size_points int Size points excluding points prototypes. size_points_prototypes int Size points representing prototypes. inc_unlabeled bool TRUE plot includes unlabeled cases data points.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"Returns plot class ggplotvisualizing embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"","code":"TEClassifierProtoNet$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierProtoNet.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a ProtoNet — TEClassifierProtoNet","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":null,"dir":"Reference","previous_headings":"","what":"Text embedding classifier with a neural net — TEClassifierRegular","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Abstract class neural nets 'keras'/'tensorflow' ' pytorch'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Objects class used assigning texts classes/categories. creation training classifier object class EmbeddedText LargeDataSetForTextEmbeddings one hand factor hand necessary. object class EmbeddedText LargeDataSetForTextEmbeddings  contains numerical text representations (text embeddings) raw texts generated object class TextEmbeddingModel. supporting large data sets recommended use LargeDataSetForTextEmbeddings instead EmbeddedText. factor contains classes/categories every text. Missing values (unlabeled cases) supported can used pseudo labeling. predictions object class EmbeddedText LargeDataSetForTextEmbeddings used created TextEmbeddingModel training.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"aifeducation::AIFEBaseModel -> TEClassifierRegular","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"feature_extractor ('list()') List storing information objects feature_extractor. reliability ('list()') List storing central reliability measures last training. reliability$test_metric: Array containing reliability measures test data every fold step (case pseudo-labeling). reliability$test_metric_mean: Array containing reliability measures test data. values represent mean values every fold. reliability$raw_iota_objects: List containing iota_object generated package iotarelr every fold end last training test data. reliability$raw_iota_objects$iota_objects_end: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold test data. reliability$raw_iota_objects$iota_objects_end_free: List objects class iotarelr_iota2 containing estimated iota reliability second generation final model every fold test data. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$iota_object_end: Object class iotarelr_iota2 mean individual objects every fold test data. reliability$iota_object_end_free: Object class iotarelr_iota2 mean individual objects every fold. Please note model estimated without forcing Assignment Error Matrix line assumption weak superiority. reliability$standard_measures_end: Object class list containing final measures precision, recall, f1 every fold. reliability$standard_measures_mean: matrix containing mean measures precision, recall, f1.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"aifeducation::AIFEBaseModel$count_parameter() aifeducation::AIFEBaseModel$get_all_fields() aifeducation::AIFEBaseModel$get_documentation_license() aifeducation::AIFEBaseModel$get_ml_framework() aifeducation::AIFEBaseModel$get_model_description() aifeducation::AIFEBaseModel$get_model_info() aifeducation::AIFEBaseModel$get_model_license() aifeducation::AIFEBaseModel$get_package_versions() aifeducation::AIFEBaseModel$get_private() aifeducation::AIFEBaseModel$get_publication_info() aifeducation::AIFEBaseModel$get_sustainability_data() aifeducation::AIFEBaseModel$get_text_embedding_model() aifeducation::AIFEBaseModel$get_text_embedding_model_name() aifeducation::AIFEBaseModel$is_configured() aifeducation::AIFEBaseModel$load() aifeducation::AIFEBaseModel$set_documentation_license() aifeducation::AIFEBaseModel$set_model_description() aifeducation::AIFEBaseModel$set_model_license() aifeducation::AIFEBaseModel$set_publication_info()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"TEClassifierRegular$configure() TEClassifierRegular$train() TEClassifierRegular$predict() TEClassifierRegular$check_embedding_model() TEClassifierRegular$check_feature_extractor_object_type() TEClassifierRegular$requires_compression() TEClassifierRegular$save() TEClassifierRegular$load_from_disk() TEClassifierRegular$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Creating new instance class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$configure(   ml_framework = \"pytorch\",   name = NULL,   label = NULL,   text_embeddings = NULL,   feature_extractor = NULL,   target_levels = NULL,   dense_size = 4,   dense_layers = 0,   rec_size = 4,   rec_layers = 2,   rec_type = \"gru\",   rec_bidirectional = FALSE,   self_attention_heads = 0,   intermediate_size = NULL,   attention_type = \"fourier\",   add_pos_embedding = TRUE,   rec_dropout = 0.1,   repeat_encoder = 1,   dense_dropout = 0.4,   recurrent_dropout = 0.4,   encoder_dropout = 0.1,   optimizer = \"adam\" )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"ml_framework string Framework use training inference. ml_framework=\"tensorflow\" 'tensorflow' ml_framework=\"pytorch\" 'pytorch' name string Name new classifier. Please refer common name conventions. Free text can used parameter label. label string Label new classifier. can use free text. text_embeddings object class EmbeddedText LargeDataSetForTextEmbeddings. feature_extractor Object class TEFeatureExtractor used order reduce number dimensions text embeddings. feature extractor applied set NULL. target_levels vector containing levels (categories classes) within target data. Please order matters. ordinal data please ensure levels sorted correctly later levels indicating higher category/class. nominal data order matter. dense_size int Number neurons dense layer. dense_layers int Number dense layers. rec_size int Number neurons recurrent layer. rec_layers int Number recurrent layers. rec_type string Type recurrent layers. rec_type=\"gru\" Gated Recurrent Unit rec_type=\"lstm\" Long Short-Term Memory. rec_bidirectional bool TRUE bidirectional version recurrent layers used. self_attention_heads int determining number attention heads self-attention layer. relevant attention_type=\"multihead\" intermediate_size int determining size projection layer within transformer encoder. attention_type string Choose relevant attention type. Possible values fourier multihead. Please note may see different values case different input orders choose fourier linux. add_pos_embedding bool TRUE positional embedding used. rec_dropout int ranging 0 lower 1, determining dropout bidirectional recurrent layers. repeat_encoder int determining many times encoder added network. dense_dropout int ranging 0 lower 1, determining dropout dense layers. recurrent_dropout int ranging 0 lower 1, determining recurrent dropout recurrent layer. relevant keras models. encoder_dropout int ranging 0 lower 1, determining dropout dense projection within encoder layers. optimizer string \"adam\" \"rmsprop\" .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Returns object class TEClassifierRegular ready training.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method training neural net. Training includes routine early stopping. case loss<0.0001 Accuracy=1.00 Average Iota=1.00 training stops. history uses values last trained epoch remaining epochs. training model best values Average Iota, Accuracy, Loss validation data set used final model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$train(   data_embeddings,   data_targets,   data_folds = 5,   data_val_size = 0.25,   balance_class_weights = TRUE,   balance_sequence_length = TRUE,   use_sc = TRUE,   sc_method = \"dbsmote\",   sc_min_k = 1,   sc_max_k = 10,   use_pl = TRUE,   pl_max_steps = 3,   pl_max = 1,   pl_anchor = 1,   pl_min = 0,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   epochs = 40,   batch_size = 32,   dir_checkpoint,   trace = TRUE,   ml_trace = 1,   log_dir = NULL,   log_write_interval = 10,   n_cores = auto_n_cores() )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"data_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings. data_targets factor containing labels cases stored data_embeddings. Factor must named use names used data_embeddings. data_folds int determining number cross-fold samples. data_val_size double 0 1, indicating proportion cases class used validation sample estimation model. remaining cases part training data. balance_class_weights bool TRUE class weights generated based frequencies training data method Inverse Class Frequency'. FALSE class weight 1. balance_sequence_length bool TRUE sample weights generated length sequences based frequencies training data method Inverse Class Frequency'. FALSE sequences length weight 1. use_sc bool TRUE estimation integrate synthetic cases. FALSE . sc_method vector containing method generating synthetic cases. Possible sc_method=\"adas\", sc_method=\"smote\", sc_method=\"dbsmote\". sc_min_k int determining minimal number k used creating synthetic units. sc_max_k int determining maximal number k used creating synthetic units. use_pl bool TRUE estimation integrate pseudo-labeling. FALSE . pl_max_steps int determining maximum number steps pseudo-labeling. pl_max double 0 1, setting maximal level confidence considering case pseudo-labeling. pl_anchor double 0 1 indicating reference point sorting new cases every label. See notes details. pl_min double 0 1, setting minimal level confidence considering case pseudo-labeling. sustain_track bool TRUE energy consumption tracked training via python library 'codecarbon'. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region Region within country. available USA Canada See documentation codecarbon information. https://mlco2.github.io/codecarbon/parameters.html sustain_interval int Interval seconds measuring power usage. epochs int Number training epochs. batch_size int Size batches training. dir_checkpoint string Path directory checkpoint training saved. directory exist, created. trace bool TRUE, information estimation phase printed console. ml_trace int ml_trace=0 print information training process pytorch console. log_dir string Path directory log files saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL. n_cores int Number cores used calculation synthetic cases. relevant use_sc=TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"sc_max_k: values sc_min_k sc_max_k successively used. number sc_max_k high, value reduced number allows calculating synthetic units. pl_anchor: help value, new cases sorted. aim, distance anchor calculated cases arranged ascending order.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Function return value. changes object trained classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method predicting new data trained neural net.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$predict(newdata, batch_size = 32, ml_trace = 1)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"newdata Object class TextEmbeddingModel LargeDataSetForTextEmbeddings predictions made. addition, method allows use objects class array datasets.arrow_dataset.Dataset. However, used developers. batch_size int Size batches. ml_trace int ml_trace=0 print information process machine learning framework.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Returns data.frame containing predictions probabilities different labels case.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-check-embedding-model-","dir":"Reference","previous_headings":"","what":"Method check_embedding_model()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method checking provided text embeddings created TextEmbeddingModel classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$check_embedding_model(   text_embeddings,   require_compressed = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"text_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings. require_compressed TRUE compressed version embeddings necessary. Compressed embeddings created object class TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"TRUE underlying TextEmbeddingModel . FALSE models differ.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-check-feature-extractor-object-type-","dir":"Reference","previous_headings":"","what":"Method check_feature_extractor_object_type()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method checking object class TEFeatureExtractor.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$check_feature_extractor_object_type(feature_extractor)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"feature_extractor Object class TEFeatureExtractor","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"method nothing returns. raises error object NULL object rely machine learning framework classifier object trained.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-requires-compression-","dir":"Reference","previous_headings":"","what":"Method requires_compression()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method checking provided text embeddings must compressed via TEFeatureExtractor processing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$requires_compression(text_embeddings)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"text_embeddings Object class EmbeddedText, LargeDataSetForTextEmbeddings, array datasets.arrow_dataset.Dataset.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Return TRUE compression necessary FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-save-","dir":"Reference","previous_headings":"","what":"Method save()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method saving model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$save(dir_path, folder_name)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"dir_path string Path directory model saved. folder_name string Name folder created within directory.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Function return value. saves model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"loads object disk updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"dir_path Path object set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"","code":"TEClassifierRegular$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TEClassifierRegular.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding classifier with a neural net — TEClassifierRegular","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Abstract class auto encoders 'pytorch'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Objects class used reducing number dimensions text embeddings created object class TextEmbeddingModel. training object class EmbeddedText LargeDataSetForTextEmbeddings generated object class TextEmbeddingModel necessary. Passing raw texts supported. prediction ob object class EmbeddedText LargeDataSetForTextEmbeddings necessary generated TextEmbeddingModel training. Prediction outputs new object class EmbeddedText LargeDataSetForTextEmbeddings contains text embedding lower number dimensions. models use tied weights encoder decoder layers (except method=\"lstm\") apply estimation orthogonal weights. addition, training tries train model achieve uncorrelated features. Objects class TEFeatureExtractor designed used classifiers TEClassifierRegular TEClassifierProtoNet.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"aifeducation::AIFEBaseModel -> TEFeatureExtractor","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"aifeducation::AIFEBaseModel$check_embedding_model() aifeducation::AIFEBaseModel$count_parameter() aifeducation::AIFEBaseModel$get_all_fields() aifeducation::AIFEBaseModel$get_documentation_license() aifeducation::AIFEBaseModel$get_ml_framework() aifeducation::AIFEBaseModel$get_model_description() aifeducation::AIFEBaseModel$get_model_info() aifeducation::AIFEBaseModel$get_model_license() aifeducation::AIFEBaseModel$get_package_versions() aifeducation::AIFEBaseModel$get_private() aifeducation::AIFEBaseModel$get_publication_info() aifeducation::AIFEBaseModel$get_sustainability_data() aifeducation::AIFEBaseModel$get_text_embedding_model() aifeducation::AIFEBaseModel$get_text_embedding_model_name() aifeducation::AIFEBaseModel$is_configured() aifeducation::AIFEBaseModel$load() aifeducation::AIFEBaseModel$save() aifeducation::AIFEBaseModel$set_documentation_license() aifeducation::AIFEBaseModel$set_model_description() aifeducation::AIFEBaseModel$set_model_license() aifeducation::AIFEBaseModel$set_publication_info()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"TEFeatureExtractor$configure() TEFeatureExtractor$train() TEFeatureExtractor$load_from_disk() TEFeatureExtractor$extract_features() TEFeatureExtractor$extract_features_large() TEFeatureExtractor$is_trained() TEFeatureExtractor$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Creating new instance class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$configure(   ml_framework = \"pytorch\",   name = NULL,   label = NULL,   text_embeddings = NULL,   features = 128,   method = \"lstm\",   noise_factor = 0.2,   optimizer = \"adam\" )"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"ml_framework string Framework use training inference. Currently ml_framework=\"pytorch\" supported. name string Name new classifier. Please refer common name conventions. Free text can used parameter label. label string Label new classifier. can use free text. text_embeddings object class EmbeddedText LargeDataSetForTextEmbeddings. features int determining number dimensions dimension text embedding reduced. method string Method use feature extraction. \"lstm\" extractor based LSTM-layers \"dense\" dense layers. noise_factor double 0 value lower 1 indicating much noise added training feature extractor. optimizer string \"adam\" \"rmsprop\" .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Returns object class TEFeatureExtractor ready training.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Method training neural net.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$train(   data_embeddings,   data_val_size = 0.25,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   epochs = 40,   batch_size = 32,   dir_checkpoint,   trace = TRUE,   ml_trace = 1,   log_dir = NULL,   log_write_interval = 10 )"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"data_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings. data_val_size double 0 1, indicating proportion cases used validation sample. sustain_track bool TRUE energy consumption tracked training via python library 'codecarbon'. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region Region within country. available USA Canada See documentation 'codecarbon' information. https://mlco2.github.io/codecarbon/parameters.html sustain_interval int Interval seconds measuring power usage. epochs int Number training epochs. batch_size int Size batches. dir_checkpoint string Path directory checkpoint training saved. directory exist, created. trace bool TRUE, information estimation phase printed console. ml_trace int ml_trace=0 print information training process pytorch console. ml_trace=1 prints progress bar. log_dir string Path directory log files saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Function return value. changes object trained classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"loads object disk updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"dir_path Path object set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-extract-features-","dir":"Reference","previous_headings":"","what":"Method extract_features()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Method extracting features. Applying method reduces number dimensions text embeddings. Please note method used small number cases compressed since data loaded completely memory. high number cases please use method extract_features_large.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$extract_features(data_embeddings, batch_size)"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"data_embeddings Object class EmbeddedText,LargeDataSetForTextEmbeddings, datasets.arrow_dataset.Dataset array containing text embeddings reduced dimensions. batch_size int batch size.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Returns object class EmbeddedText containing compressed embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-extract-features-large-","dir":"Reference","previous_headings":"","what":"Method extract_features_large()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Method extracting features large number cases. Applying method reduces number dimensions text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$extract_features_large(   data_embeddings,   batch_size,   trace = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"data_embeddings Object class EmbeddedText LargeDataSetForTextEmbeddings containing text embeddings reduced dimensions. batch_size int batch size. trace bool TRUE information progress printed console.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Returns object class LargeDataSetForTextEmbeddings containing compressed embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-is-trained-","dir":"Reference","previous_headings":"","what":"Method is_trained()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Check TEFeatureExtractor trained.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$is_trained()"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"Returns TRUE object trained FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"","code":"TEFeatureExtractor$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TEFeatureExtractor.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature extractor for reducing the number for dimensions of text embeddings. — TEFeatureExtractor","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Text embedding model — TextEmbeddingModel","title":"Text embedding model — TextEmbeddingModel","text":"R6 class stores text embedding model can used tokenize, encode, decode, embed raw texts. object provides unique interface different text processing methods.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Text embedding model — TextEmbeddingModel","text":"Objects class TextEmbeddingModel transform raw texts numerical representations can used downstream tasks. aim objects class allow tokenize raw texts, encode tokens sequences integers, decode sequences integers back tokens.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Text embedding model — TextEmbeddingModel","text":"last_training ('list()') List storing history results last training. information overwritten new training started. tokenizer_statistics ('matrix()') Matrix containing tokenizer statistics creation tokenizer training runs according Kaya & Tantuğ (2024). Kaya, Y. B., & Tantuğ, . C. (2024). Effect tokenization granularity Turkish large language models. Intelligent Systems Applications, 21, 200335. https://doi.org/10.1016/j.iswa.2024.200335","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Text embedding model — TextEmbeddingModel","text":"TextEmbeddingModel$configure() TextEmbeddingModel$load_from_disk() TextEmbeddingModel$load() TextEmbeddingModel$save() TextEmbeddingModel$encode() TextEmbeddingModel$decode() TextEmbeddingModel$get_special_tokens() TextEmbeddingModel$embed() TextEmbeddingModel$embed_large() TextEmbeddingModel$fill_mask() TextEmbeddingModel$set_publication_info() TextEmbeddingModel$get_publication_info() TextEmbeddingModel$set_model_license() TextEmbeddingModel$get_model_license() TextEmbeddingModel$set_documentation_license() TextEmbeddingModel$get_documentation_license() TextEmbeddingModel$set_model_description() TextEmbeddingModel$get_model_description() TextEmbeddingModel$get_model_info() TextEmbeddingModel$get_package_versions() TextEmbeddingModel$get_basic_components() TextEmbeddingModel$get_transformer_components() TextEmbeddingModel$get_sustainability_data() TextEmbeddingModel$get_ml_framework() TextEmbeddingModel$count_parameter() TextEmbeddingModel$is_configured() TextEmbeddingModel$get_private() TextEmbeddingModel$get_all_fields() TextEmbeddingModel$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-configure-","dir":"Reference","previous_headings":"","what":"Method configure()","title":"Text embedding model — TextEmbeddingModel","text":"Method creating new text embedding model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$configure(   model_name = NULL,   model_label = NULL,   model_language = NULL,   method = NULL,   ml_framework = \"pytorch\",   max_length = 0,   chunks = 2,   overlap = 0,   emb_layer_min = \"middle\",   emb_layer_max = \"2_3_layer\",   emb_pool_type = \"average\",   model_dir = NULL,   trace = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"model_name string containing name new model. model_label string containing label/title new model. model_language string containing language model represents (e.g., English). method string determining kind embedding model. Currently following models supported: method=\"bert\" Bidirectional Encoder Representations Transformers (BERT), method=\"roberta\" Robustly Optimized BERT Pretraining Approach (RoBERTa), method=\"longformer\" Long-Document Transformer, method=\"funnel\" Funnel-Transformer, method=\"deberta_v2\" Decoding-enhanced BERT Disentangled Attention (DeBERTa V2), method=\"glove\"`` GlobalVector Clusters, method=\"lda\"` topic modeling. See details information. ml_framework string Framework use model. ml_framework=\"tensorflow\" 'tensorflow' ml_framework=\"pytorch\" 'pytorch'. relevant transformer models. request bag--words model set ml_framework=NULL. max_length int determining maximum length token sequences used transformer models. relevant methods. chunks int Maximum number chunks. Must least 2. overlap int determining number tokens added beginning next chunk. relevant transformer models. emb_layer_min int string determining first layer included creation embeddings. integer correspondents layer number. first layer number 1. Instead integer following strings possible: \"start\" first layer, \"middle\" middle layer, \"2_3_layer\" layer two-third layer, \"last\" last layer. emb_layer_max int string determining last layer included creation embeddings. integer correspondents layer number. first layer number 1. Instead integer following strings possible: \"start\" first layer, \"middle\" middle layer, \"2_3_layer\" layer two-third layer, \"last\" last layer. emb_pool_type string determining method pooling token embeddings within layer. \"cls\" embedding CLS token used. \"average\" token embedding tokens averaged (excluding padding tokens). \"cls supported method=\"funnel\". model_dir string path directory BERT model stored. trace bool TRUE prints information progress. FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Text embedding model — TextEmbeddingModel","text":"case transformer (e.g.method=\"bert\", method=\"roberta\", method=\"longformer\"), pretrained transformer model must supplied via model_dir.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns object class TextEmbeddingModel.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-load-from-disk-","dir":"Reference","previous_headings":"","what":"Method load_from_disk()","title":"Text embedding model — TextEmbeddingModel","text":"loads object disk updates object current version package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"dir_path Path object set stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Method return anything. loads object disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-load-","dir":"Reference","previous_headings":"","what":"Method load()","title":"Text embedding model — TextEmbeddingModel","text":"Method loading transformers model R.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$load(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"dir_path string containing path relevant model directory.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used loading saved transformer model R interface.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-save-","dir":"Reference","previous_headings":"","what":"Method save()","title":"Text embedding model — TextEmbeddingModel","text":"Method saving transformer model disk.Relevant transformer models.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$save(dir_path, folder_name)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"dir_path string containing path relevant model directory. folder_name string Name folder created within directory. folder contains model files.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used saving transformer model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-encode-","dir":"Reference","previous_headings":"","what":"Method encode()","title":"Text embedding model — TextEmbeddingModel","text":"Method encoding words raw texts integers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$encode(   raw_text,   token_encodings_only = FALSE,   to_int = TRUE,   trace = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"raw_text vectorcontaining raw texts. token_encodings_only bool TRUE, token encodings returned. FALSE, complete encoding returned important transformer models. to_int bool TRUE integer ids tokens returned. FALSE tokens returned. Argument applies transformer models token_encodings_only=TRUE. trace bool TRUE, information progress printed. FALSE requested.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list containing integer token sequences raw texts special tokens.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-decode-","dir":"Reference","previous_headings":"","what":"Method decode()","title":"Text embedding model — TextEmbeddingModel","text":"Method decoding sequence integers tokens","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$decode(int_seqence, to_token = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"int_seqence list containing integer sequences transformed tokens plain text. to_token bool FALSE plain text returned. TRUE sequence tokens returned. Argument relevant model based transformer.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list token sequences","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-special-tokens-","dir":"Reference","previous_headings":"","what":"Method get_special_tokens()","title":"Text embedding model — TextEmbeddingModel","text":"Method receiving special tokens model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_special_tokens()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns matrix containing special tokens rows type, token, id columns.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-embed-","dir":"Reference","previous_headings":"","what":"Method embed()","title":"Text embedding model — TextEmbeddingModel","text":"Method creating text embeddings raw texts. method used small number texts transformed text embeddings. large number texts please use method embed_large. case using GPU running memory using 'tensorflow'  reduce batch size restart R switch use cpu via set_config_cpu_only. general, relevant 'pytorch'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$embed(   raw_text = NULL,   doc_id = NULL,   batch_size = 8,   trace = FALSE,   return_large_dataset = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"raw_text vector containing raw texts. doc_id vector containing corresponding IDs every text. batch_size int determining maximal size every batch. trace bool TRUE, information progression printed console. return_large_dataset 'bool' TRUE retuned object class LargeDataSetForTextEmbeddings. FALSE class EmbeddedText","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Method returns object class EmbeddedText LargeDataSetForTextEmbeddings. object contains embeddings data.frame information model creating embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-embed-large-","dir":"Reference","previous_headings":"","what":"Method embed_large()","title":"Text embedding model — TextEmbeddingModel","text":"Method creating text embeddings raw texts.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$embed_large(   large_datas_set,   batch_size = 32,   trace = FALSE,   log_file = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"large_datas_set Object class LargeDataSetForText containing raw texts. batch_size int determining maximal size every batch. trace bool TRUE, information progression printed console. log_file string Path file log saved. logging desired set argument NULL. log_write_interval int Time seconds determining interval logger try update log files. relevant log_file NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Method returns object class LargeDataSetForTextEmbeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-fill-mask-","dir":"Reference","previous_headings":"","what":"Method fill_mask()","title":"Text embedding model — TextEmbeddingModel","text":"Method calculating tokens behind mask tokens.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$fill_mask(text, n_solutions = 5)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"text string Text containing mask tokens. n_solutions int Number estimated tokens every mask.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns list containing data.frame every mask. data.frame contains solutions rows reports score, token id, token string columns.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-set-publication-info-","dir":"Reference","previous_headings":"","what":"Method set_publication_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting bibliographic information model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_publication_info(type, authors, citation, url = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"type string Type information changed/added. developer, modifier possible. authors List people. citation string Citation free text. url string Corresponding URL applicable.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used set private members publication information model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-publication-info-","dir":"Reference","previous_headings":"","what":"Method get_publication_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method getting bibliographic information model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_publication_info()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list bibliographic information.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-set-model-license-","dir":"Reference","previous_headings":"","what":"Method set_model_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting license model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_model_license(license = \"CC BY\")"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used setting private member software license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-model-license-","dir":"Reference","previous_headings":"","what":"Method get_model_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting license model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_license()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"string License model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-set-documentation-license-","dir":"Reference","previous_headings":"","what":"Method set_documentation_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting license models' documentation.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_documentation_license(license = \"CC BY\")"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used set private member documentation license model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-documentation-license-","dir":"Reference","previous_headings":"","what":"Method get_documentation_license()","title":"Text embedding model — TextEmbeddingModel","text":"Method getting license models' documentation.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_documentation_license()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"license string containing abbreviation license license text.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-set-model-description-","dir":"Reference","previous_headings":"","what":"Method set_model_description()","title":"Text embedding model — TextEmbeddingModel","text":"Method setting description model","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$set_model_description(   eng = NULL,   native = NULL,   abstract_eng = NULL,   abstract_native = NULL,   keywords_eng = NULL,   keywords_native = NULL )"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"eng string text describing training classifier, theoretical empirical background, different output labels English. native string text describing training classifier, theoretical empirical background, different output labels native language model. abstract_eng string text providing summary description English. abstract_native string text providing summary description native language classifier. keywords_eng vectorof keywords English. keywords_native vectorof keywords native language classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Function return value. used set private members description model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-model-description-","dir":"Reference","previous_headings":"","what":"Method get_model_description()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting model description.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_description()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list description model English native language.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting model information","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_model_info()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"list relevant model information","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-package-versions-","dir":"Reference","previous_headings":"","what":"Method get_package_versions()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting summary R python packages' versions used creating model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_package_versions()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns list containing versions relevant R python packages.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-basic-components-","dir":"Reference","previous_headings":"","what":"Method get_basic_components()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting part interface's configuration necessary models.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_basic_components()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-19","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns list.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-transformer-components-","dir":"Reference","previous_headings":"","what":"Method get_transformer_components()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting part interface's configuration necessary transformer models.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_transformer_components()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-20","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns list.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-sustainability-data-","dir":"Reference","previous_headings":"","what":"Method get_sustainability_data()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting log tracked energy consumption training estimate resulting CO2 equivalents kg.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_sustainability_data()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-21","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns matrix containing tracked energy consumption, CO2 equivalents kg, information tracker used, technical information training infrastructure every training run.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-ml-framework-","dir":"Reference","previous_headings":"","what":"Method get_ml_framework()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting machine learning framework used classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_ml_framework()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-22","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns string describing machine learning framework used classifier.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-count-parameter-","dir":"Reference","previous_headings":"","what":"Method count_parameter()","title":"Text embedding model — TextEmbeddingModel","text":"Method counting trainable parameters model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-24","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$count_parameter(with_head = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"with_head bool TRUE number parameters returned including language modeling head model. FALSE number parameters core model returned.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-23","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns number trainable parameters model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-is-configured-","dir":"Reference","previous_headings":"","what":"Method is_configured()","title":"Text embedding model — TextEmbeddingModel","text":"Method checking model successfully configured. object can used value TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-25","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$is_configured()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-24","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"bool TRUE model fully configured. FALSE .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-private-","dir":"Reference","previous_headings":"","what":"Method get_private()","title":"Text embedding model — TextEmbeddingModel","text":"Method requesting private fields methods. Used loading updating object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-26","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_private()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-25","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Returns list private fields methods.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-get-all-fields-","dir":"Reference","previous_headings":"","what":"Method get_all_fields()","title":"Text embedding model — TextEmbeddingModel","text":"Return fields.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-27","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$get_all_fields()"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"returns-26","dir":"Reference","previous_headings":"","what":"Returns","title":"Text embedding model — TextEmbeddingModel","text":"Method returns list containing public private fields object.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Text embedding model — TextEmbeddingModel","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"usage-28","dir":"Reference","previous_headings":"","what":"Usage","title":"Text embedding model — TextEmbeddingModel","text":"","code":"TextEmbeddingModel$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/TextEmbeddingModel.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Text embedding model — TextEmbeddingModel","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/aife_transformer_maker.html","id":null,"dir":"Reference","previous_headings":"","what":"R6 object of the AIFETransformerMaker class — aife_transformer_maker","title":"R6 object of the AIFETransformerMaker class — aife_transformer_maker","text":"Object creating transformers different types. See AIFETransformerMaker class details.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/aife_transformer_maker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"R6 object of the AIFETransformerMaker class — aife_transformer_maker","text":"","code":"aife_transformer_maker"},{"path":"https://fberding.github.io/aifeducation/reference/aife_transformer_maker.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"R6 object of the AIFETransformerMaker class — aife_transformer_maker","text":"object class AIFETransformerMaker (inherits R6) length 3.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/aife_transformer_maker.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"R6 object of the AIFETransformerMaker class — aife_transformer_maker","text":"","code":"# Use 'make' method of the 'aifeducation::aife_transformer_maker' object # Pass string with the type of transformers # Allowed types are \"bert\", \"deberta_v2\", \"funnel\", etc. See aifeducation::AIFETrType list my_bert <- aife_transformer_maker$make(\"bert\") #> [1] \"BERT Model has been initialized.\"  # Or use elements of the 'aifeducation::AIFETrType' list my_longformer <- aife_transformer_maker$make(AIFETrType$longformer) #> [1] \"Longformer Model has been initialized.\"  # Run 'create' or 'train' methods of the transformer in order to create a # new transformer or train the newly created one, respectively # my_bert$create(...) # my_bert$train(...)  # my_longformer$create(...) # my_longformer$train(...)"},{"path":"https://fberding.github.io/aifeducation/reference/auto_n_cores.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of cores for multiple tasks — auto_n_cores","title":"Number of cores for multiple tasks — auto_n_cores","text":"Function getting number cores used parallel processing tasks. number cores set 75 % available cores. environment variable CI set \"true\" process running cran 2 returned.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/auto_n_cores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of cores for multiple tasks — auto_n_cores","text":"","code":"auto_n_cores()"},{"path":"https://fberding.github.io/aifeducation/reference/auto_n_cores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of cores for multiple tasks — auto_n_cores","text":"Returns int number cores.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/calc_standard_classification_measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate standard classification measures — calc_standard_classification_measures","title":"Calculate standard classification measures — calc_standard_classification_measures","text":"Function calculating recall, precision, f1.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/calc_standard_classification_measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate standard classification measures — calc_standard_classification_measures","text":"","code":"calc_standard_classification_measures(true_values, predicted_values)"},{"path":"https://fberding.github.io/aifeducation/reference/calc_standard_classification_measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate standard classification measures — calc_standard_classification_measures","text":"true_values factor containing true labels/categories. predicted_values factor containing predicted labels/categories.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/calc_standard_classification_measures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate standard classification measures — calc_standard_classification_measures","text":"Returns matrix contains cases categories rows measures (precision, recall, f1) columns.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/check_aif_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if all necessary python modules are available — check_aif_py_modules","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function checks  python modules necessary package aifeducation work available.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/check_aif_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"","code":"check_aif_py_modules(trace = TRUE, check = \"pytorch\")"},{"path":"https://fberding.github.io/aifeducation/reference/check_aif_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"trace bool TRUE list modules availability printed console. check string determining machine learning framework check . check = \"pytorch\": 'pytorch'. check = \"tensorflow\": 'tensorflow'. check = \"\": frameworks.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/check_aif_py_modules.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if all necessary python modules are available — check_aif_py_modules","text":"function prints table relevant packages shows modules available unavailable. relevant modules available, functions returns TRUE. cases returns FALSE","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/clean_pytorch_log_transformers.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean pytorch log of transformers — clean_pytorch_log_transformers","title":"Clean pytorch log of transformers — clean_pytorch_log_transformers","text":"Function preparing cleaning log created object class Trainer python library 'transformer's.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/clean_pytorch_log_transformers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean pytorch log of transformers — clean_pytorch_log_transformers","text":"","code":"clean_pytorch_log_transformers(log)"},{"path":"https://fberding.github.io/aifeducation/reference/clean_pytorch_log_transformers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean pytorch log of transformers — clean_pytorch_log_transformers","text":"log data.frame containing log.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/clean_pytorch_log_transformers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean pytorch log of transformers — clean_pytorch_log_transformers","text":"Returns data.frame containing epochs, loss, val_loss.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/cohens_kappa.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cohen's Kappa — cohens_kappa","title":"Calculate Cohen's Kappa — cohens_kappa","text":"function calculates different version Cohen's Kappa.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/cohens_kappa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cohen's Kappa — cohens_kappa","text":"","code":"cohens_kappa(rater_one, rater_two)"},{"path":"https://fberding.github.io/aifeducation/reference/cohens_kappa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cohen's Kappa — cohens_kappa","text":"rater_one factor rating first coder. rater_two factor ratings second coder.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/cohens_kappa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cohen's Kappa — cohens_kappa","text":"Returns list containing results Cohen' Kappa weights applied (kappa_unweighted), weights applied weights increase linear (kappa_linear), weights applied weights increase quadratic (kappa_squared).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/cohens_kappa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Cohen's Kappa — cohens_kappa","text":"Cohen, J (1968). Weighted kappa: Nominal scale agreement provision scaled disagreement partial credit. Psychological Bulletin, 70(4), 213–220. doi:10.1037/h0026256 Cohen, J (1960). Coefficient Agreement Nominal Scales. Educational Psychological Measurement, 20(1), 37–46. doi:10.1177/001316446002000104","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/create_config_state.html","id":null,"dir":"Reference","previous_headings":"","what":"Create config for R interfaces — create_config_state","title":"Create config for R interfaces — create_config_state","text":"Function creates config can saved disk. used loading object disk order set correct configuration.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_config_state.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create config for R interfaces — create_config_state","text":"","code":"create_config_state(object)"},{"path":"https://fberding.github.io/aifeducation/reference/create_config_state.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create config for R interfaces — create_config_state","text":"object Object class \"TEClassifierRegular\", \"TEClassifierProtoNet\", \"TEFeatureExtractor\", \"TextEmbeddingModel\", \"LargeDataSetForTextEmbeddings\", \"LargeDataSetForText\", \"EmbeddedText\".","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_config_state.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create config for R interfaces — create_config_state","text":"Returns list contains class object, public, private fields.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/create_data_embeddings_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate description for text embeddings — create_data_embeddings_description","title":"Generate description for text embeddings — create_data_embeddings_description","text":"Function generates description underling TextEmbeddingModel give text embeddings.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_data_embeddings_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate description for text embeddings — create_data_embeddings_description","text":"","code":"create_data_embeddings_description(embeddings)"},{"path":"https://fberding.github.io/aifeducation/reference/create_data_embeddings_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate description for text embeddings — create_data_embeddings_description","text":"embeddings Object class LargeDataSetForTextEmbeddings EmbeddedText.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_data_embeddings_description.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate description for text embeddings — create_data_embeddings_description","text":"Returns shiny::tagList containing html elements user interface.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/create_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Create directory if not exists — create_dir","title":"Create directory if not exists — create_dir","text":"Check whether passed dir_path directory exists. , creates new directory prints msg message trace TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create directory if not exists — create_dir","text":"","code":"create_dir(dir_path, trace, msg = \"Creating Directory\", msg_fun = TRUE)"},{"path":"https://fberding.github.io/aifeducation/reference/create_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create directory if not exists — create_dir","text":"dir_path string new directory path created. trace bool Whether msg message printed. msg string message printed trace TRUE. msg_fun func Function used printing message.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create directory if not exists — create_dir","text":"TRUE FALSE depending whether shiny app active.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/create_synthetic_units_from_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create synthetic units — create_synthetic_units_from_matrix","title":"Create synthetic units — create_synthetic_units_from_matrix","text":"Function creating synthetic cases order balance data training TEClassifierRegular TEClassifierProtoNet]. auxiliary function use get_synthetic_cases_from_matrix allow parallel computations.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_synthetic_units_from_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create synthetic units — create_synthetic_units_from_matrix","text":"","code":"create_synthetic_units_from_matrix(   matrix_form,   target,   required_cases,   k,   method,   cat,   k_s,   max_k )"},{"path":"https://fberding.github.io/aifeducation/reference/create_synthetic_units_from_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create synthetic units — create_synthetic_units_from_matrix","text":"matrix_form Named matrix containing text embeddings matrix form. cases object taken EmbeddedText$embeddings. target Named factor containing labels/categories corresponding cases. required_cases int Number cases necessary fill gab frequency class investigation major class. k int number nearest neighbors sampling process. method vector containing strings requested methods generating new cases. Currently \"smote\",\"dbsmote\", \"adas\" package smotefamily available. cat string category new cases created. k_s int Number ks complete generation process. max_k int maximum number nearest neighbors sampling process.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/create_synthetic_units_from_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create synthetic units — create_synthetic_units_from_matrix","text":"Returns list contains text embeddings new synthetic cases named data.frame labels named factor.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"base class used create define .AIFE*Transformer-like classes. serves skeleton future concrete transformer used create object (attempt call new-method produce error). See p.1 Base Transformer Class Transformers Developers details.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"create-method basic algorithm used create new transformer, called directly.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"train-method basic algorithm used train tune transformer called directly.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"concrete-transformer-implementation","dir":"Reference","previous_headings":"","what":"Concrete transformer implementation","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"already implemented concrete (child) transformers (e.g. BERT, DeBERTa-V2, etc.), implement new one see p.4 Implement Custom Transformer Transformers Developers","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Hugging Face transformers documantation: BERT DeBERTa Funnel Longformer RoBERTa MPNet","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"params list containing transformer's parameters ('static', 'dynamic' 'dependent' parameters) list() containing transformer parameters. Can set set_model_param(). temp list containing temporary transformer's parameters list() containing temporary local variables need accessed step functions. Can set set_model_temp(). example, can variable tok_new stores tokenizer steps_for_creation$create_tokenizer_draft. train tokenizer, access variable tok_new steps_for_creation$calculate_vocab temp list class.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"-static-parameters","dir":"Reference","previous_headings":"","what":"'Static' parameters","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Regardless transformer, following parameters always included: ml_framework text_dataset sustain_track sustain_iso_code sustain_region sustain_interval trace pytorch_safetensors log_dir log_write_interval","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"-dynamic-parameters","dir":"Reference","previous_headings":"","what":"'Dynamic' parameters","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"case create also contains (see create-method details): model_dir vocab_size max_position_embeddings hidden_size hidden_act hidden_dropout_prob attention_probs_dropout_prob intermediate_size num_attention_heads case train also contains (see train-method details): output_dir model_dir_path p_mask whole_word val_size n_epoch batch_size chunk_size min_seq_len full_sequences_only learning_rate n_workers multi_process keras_trace pytorch_trace","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"-dependent-parameters","dir":"Reference","previous_headings":"","what":"'Dependent' parameters","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Depending transformer method used class may contain different parameters: vocab_do_lower_case num_hidden_layer add_prefix_space etc.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":".AIFEBaseTransformer$new() .AIFEBaseTransformer$set_title() .AIFEBaseTransformer$set_model_param() .AIFEBaseTransformer$set_model_temp() .AIFEBaseTransformer$set_SFC_check_max_pos_emb() .AIFEBaseTransformer$set_SFC_create_tokenizer_draft() .AIFEBaseTransformer$set_SFC_calculate_vocab() .AIFEBaseTransformer$set_SFC_save_tokenizer_draft() .AIFEBaseTransformer$set_SFC_create_final_tokenizer() .AIFEBaseTransformer$set_SFC_create_transformer_model() .AIFEBaseTransformer$set_required_SFC() .AIFEBaseTransformer$set_SFT_load_existing_model() .AIFEBaseTransformer$set_SFT_cuda_empty_cache() .AIFEBaseTransformer$set_SFT_create_data_collator() .AIFEBaseTransformer$create() .AIFEBaseTransformer$train() .AIFEBaseTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"object class created. Thus, method's call produce error.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns error.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-title-","dir":"Reference","previous_headings":"","what":"Method set_title()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter title. Sets new value title private attribute.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_title(title)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"title string new title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-model-param-","dir":"Reference","previous_headings":"","what":"Method set_model_param()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter parameters. Adds new parameter value params list.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_model_param(param_name, param_value)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"param_name string Parameter's name. param_value Parameter's value.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-model-temp-","dir":"Reference","previous_headings":"","what":"Method set_model_temp()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter temporary model's parameters. Adds new temporary parameter value temp list.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_model_temp(temp_name, temp_value)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"temp_name string Parameter's name. temp_value Parameter's value.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-check-max-pos-emb-","dir":"Reference","previous_headings":"","what":"Method set_SFC_check_max_pos_emb()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter check_max_pos_emb element private steps_for_creation list. Sets new fun function check_max_pos_emb step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_check_max_pos_emb(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-create-tokenizer-draft-","dir":"Reference","previous_headings":"","what":"Method set_SFC_create_tokenizer_draft()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter create_tokenizer_draft element  private steps_for_creation list. Sets new fun function create_tokenizer_draft step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_create_tokenizer_draft(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-calculate-vocab-","dir":"Reference","previous_headings":"","what":"Method set_SFC_calculate_vocab()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter calculate_vocab element private steps_for_creation list. Sets new fun function calculate_vocab step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_calculate_vocab(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-save-tokenizer-draft-","dir":"Reference","previous_headings":"","what":"Method set_SFC_save_tokenizer_draft()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter save_tokenizer_draft element private steps_for_creation list. Sets new fun function save_tokenizer_draft step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_save_tokenizer_draft(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-create-final-tokenizer-","dir":"Reference","previous_headings":"","what":"Method set_SFC_create_final_tokenizer()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter create_final_tokenizer element private steps_for_creation list. Sets new fun function create_final_tokenizer step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_create_final_tokenizer(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sfc-create-transformer-model-","dir":"Reference","previous_headings":"","what":"Method set_SFC_create_transformer_model()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter create_transformer_model element private steps_for_creation list. Sets new fun function create_transformer_model step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFC_create_transformer_model(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-required-sfc-","dir":"Reference","previous_headings":"","what":"Method set_required_SFC()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter required elements private steps_for_creation list. Executes setters required creation steps.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_required_SFC(required_SFC)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"required_SFC list() list new required steps.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sft-load-existing-model-","dir":"Reference","previous_headings":"","what":"Method set_SFT_load_existing_model()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter load_existing_model element private steps_for_training list. Sets new fun function load_existing_model step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFT_load_existing_model(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sft-cuda-empty-cache-","dir":"Reference","previous_headings":"","what":"Method set_SFT_cuda_empty_cache()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter cuda_empty_cache element private steps_for_training list. Sets new fun function cuda_empty_cache step.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFT_cuda_empty_cache(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-set-sft-create-data-collator-","dir":"Reference","previous_headings":"","what":"Method set_SFT_create_data_collator()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"Setter create_data_collator element private steps_for_training list. Sets new fun function create_data_collator step. Use method make custom data collator transformer.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$set_SFT_create_data_collator(fun)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"fun function() new function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method creates transformer configuration based child-transformer architecture vocabulary using python libraries transformers tokenizers. method adds following parameters temp list: log_file raw_text_dataset pt_safe_save value_top total_top message_top method uses following parameters temp list: log_file raw_text_dataset tokenizer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$create(   ml_framework,   model_dir,   text_dataset,   vocab_size,   max_position_embeddings,   hidden_size,   num_attention_heads,   intermediate_size,   hidden_act,   hidden_dropout_prob,   attention_probs_dropout_prob,   sustain_track,   sustain_iso_code,   sustain_region,   sustain_interval,   trace,   pytorch_safetensors,   log_dir,   log_write_interval )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method can used train fine-tune transformer based BERT architecture help python libraries transformers, datasets, tokenizers. method adds following parameters temp list: log_file loss_file from_pt from_tf load_safe raw_text_dataset pt_safe_save value_top total_top message_top method uses following parameters temp list: log_file raw_text_dataset tokenized_dataset tokenizer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$train(   ml_framework,   output_dir,   model_dir_path,   text_dataset,   p_mask,   whole_word,   val_size,   n_epoch,   batch_size,   chunk_size,   full_sequences_only,   min_seq_len,   learning_rate,   n_workers,   multi_process,   sustain_track,   sustain_iso_code,   sustain_region,   sustain_interval,   trace,   keras_trace,   pytorch_trace,   pytorch_safetensors,   log_dir,   log_write_interval )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. whole_word bool TRUE: whole word masking applied. FALSE: token masking used. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"","code":".AIFEBaseTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBaseTransformer.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base R6 class for creation and definition of .AIFE*Transformer-like classes — .AIFEBaseTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"class following methods: create: creates new transformer based BERT. train: trains fine-tunes BERT model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"model uses WordPiece tokenizer like BERT can trained whole word masking. transformer library may display warning, can ignored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"New models can created using .AIFEBertTransformer$create method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"train model, pass directory model method .AIFEBertTransformer$train. Pre-Trained models can fine-tuned using method available https://huggingface.co/. model trained using dynamic masking, opposed original paper, used static masking.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"Devlin, J., Chang, M.‑W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training Deep Bidirectional Transformers Language Understanding. J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings 2019 Conference North (pp. 4171–4186). Association Computational Linguistics. doi:10.18653/v1/N19-1423 Hugging Face documentation https://huggingface.co/docs/transformers/model_doc/bert https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForMaskedLM https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFEBertTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":".AIFEBertTransformer$new() .AIFEBertTransformer$create() .AIFEBertTransformer$train() .AIFEBertTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"Creates new transformer based BERT sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"","code":".AIFEBertTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"method creates transformer configuration based BERT base architecture vocabulary based WordPiece using python libraries transformers tokenizers. method adds following 'dependent' parameters base class's inherited params list: vocab_do_lower_case num_hidden_layer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"","code":".AIFEBertTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   sustain_track = FALSE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"method can used train fine-tune transformer based BERT architecture help python libraries transformers, datasets, tokenizers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"","code":".AIFEBertTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.003,   n_workers = 1,   multi_process = FALSE,   sustain_track = FALSE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. whole_word bool TRUE: whole word masking applied. FALSE: token masking used. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"","code":".AIFEBertTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEBertTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of BERT transformers — .AIFEBertTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"class following methods: create: creates new transformer based DeBERTa-V2. train: trains fine-tunes DeBERTa-V2 model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"model WordPiece tokenizer created. standard implementation DeBERTa version 2 HuggingFace uses SentencePiece tokenizer. Thus, please use AutoTokenizer transformers library work model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"New models can created using .AIFEDebertaTransformer$create method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"train model, pass directory model method .AIFEDebertaTransformer$train. Pre-Trained models can fine-tuned function available https://huggingface.co/. Training model makes use dynamic masking.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":", P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT Disentangled Attention. doi:10.48550/arXiv.2006.03654 Hugging Face documentatio https://huggingface.co/docs/transformers/model_doc/deberta-v2 https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFEDebertaTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":".AIFEDebertaTransformer$new() .AIFEDebertaTransformer$create() .AIFEDebertaTransformer$train() .AIFEDebertaTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"Creates new transformer based DeBERTa-V2 sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"","code":".AIFEDebertaTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"method creates transformer configuration based DeBERTa-V2 base architecture vocabulary based SentencePiece tokenizer using python transformers tokenizers libraries. method adds following 'dependent' parameters base class's inherited params list: vocab_do_lower_case num_hidden_layer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"","code":".AIFEDebertaTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 128100,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 1536,   num_hidden_layer = 24,   num_attention_heads = 24,   intermediate_size = 6144,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"method can used train fine-tune transformer based DeBERTa-V2 architecture help python libraries transformers, datasets, tokenizers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"","code":".AIFEDebertaTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.03,   n_workers = 1,   multi_process = FALSE,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. whole_word bool TRUE: whole word masking applied. FALSE: token masking used. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"","code":".AIFEDebertaTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEDebertaTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of DeBERTa-V2 transformers — .AIFEDebertaTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"class following methods: create: creates new transformer based Funnel. train: trains fine-tunes Funnel model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"model uses configuration truncate_seq = TRUE avoid implementation problems tensorflow. model uses WordPiece tokenizer like BERT can trained whole word masking. transformer library may display warning, can ignored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"New models can created using .AIFEFunnelTransformer$create method. Model created separete_cls = TRUE, truncate_seq = TRUE, pool_q_only = TRUE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"train model, pass directory model method .AIFEFunnelTransformer$train. Pre-Trained models can fine-tuned function available https://huggingface.co/. Training model makes use dynamic masking.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer: Filtering Sequential Redundancy Efficient Language Processing. doi:10.48550/arXiv.2006.03236 Hugging Face documentation https://huggingface.co/docs/transformers/model_doc/funnel#funnel-transformer https://huggingface.co/docs/transformers/model_doc/funnel#transformers.FunnelModel https://huggingface.co/docs/transformers/model_doc/funnel#transformers.TFFunnelModel","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFEFunnelTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":".AIFEFunnelTransformer$new() .AIFEFunnelTransformer$create() .AIFEFunnelTransformer$train() .AIFEFunnelTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"Creates new transformer based Funnel sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"","code":".AIFEFunnelTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"method creates transformer configuration based Funnel transformer base architecture vocabulary based WordPiece using python transformers tokenizers libraries. method adds following 'dependent' parameters base class's inherited params list: vocab_do_lower_case target_hidden_size block_sizes num_decoder_layers pooling_type activation_dropout","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"","code":".AIFEFunnelTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   target_hidden_size = 64,   block_sizes = c(4, 4, 4),   num_attention_heads = 12,   intermediate_size = 3072,   num_decoder_layers = 2,   pooling_type = \"mean\",   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   activation_dropout = 0,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. target_hidden_size int Number neurons final layer. parameter determines dimensionality resulting text embedding. block_sizes vector int determining number sizes block. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. num_decoder_layers int Number decoding layers. pooling_type string Type pooling. \"mean\" pooling mean. \"max\" pooling maximum values. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. activation_dropout float Dropout probability layers feed-forward blocks. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"method can used train fine-tune transformer based Funnel Transformer architecture help python libraries transformers, datasets, tokenizers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"","code":".AIFEFunnelTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.003,   n_workers = 1,   multi_process = FALSE,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. whole_word bool TRUE: whole word masking applied. FALSE: token masking used. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"","code":".AIFEFunnelTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEFunnelTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Funnel transformers — .AIFEFunnelTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"class following methods: create: creates new transformer based Longformer. train: trains fine-tunes Longformer model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"New models can created using .AIFELongformerTransformer$create method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"train model, pass directory model method .AIFELongformerTransformer$train. Pre-Trained models can fine-tuned function available https://huggingface.co/. Training model makes use dynamic masking.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"Beltagy, ., Peters, M. E., & Cohan, . (2020). Longformer: Long-Document Transformer. doi:10.48550/arXiv.2004.05150 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/longformer https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerModel https://huggingface.co/docs/transformers/model_doc/longformer#transformers.TFLongformerModel","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFELongformerTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":".AIFELongformerTransformer$new() .AIFELongformerTransformer$create() .AIFELongformerTransformer$train() .AIFELongformerTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"Creates new transformer based Longformer sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"","code":".AIFELongformerTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"method returns nothing","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"method creates transformer configuration based Longformer base architecture vocabulary based Byte-Pair Encoding (BPE) tokenizer using python transformers tokenizers libraries. method adds following 'dependent' parameters base class's inherited params list: add_prefix_space trim_offsets num_hidden_layer attention_window","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"","code":".AIFELongformerTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 30522,   add_prefix_space = FALSE,   trim_offsets = TRUE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   attention_window = 512,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. add_prefix_space bool TRUE additional space inserted leading words. trim_offsets bool TRUE trims whitespaces produced offsets. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. attention_window int Size window around token attention mechanism every layer. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"method can used train fine-tune transformer based Longformer Transformer architecture help python libraries transformers, datasets, tokenizers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"","code":".AIFELongformerTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.03,   n_workers = 1,   multi_process = FALSE,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"","code":".AIFELongformerTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFELongformerTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of Longformer transformers — .AIFELongformerTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"class following methods: create: creates new transformer based MPNet. train: trains fine-tunes MPNet model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"Using class tensorflow supported. Supported framework pytorch.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"New models can created using .AIFEMpnetTransformer$create method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"train model, pass directory model method .AIFEMpnetTransformer$train.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"Song,K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. (2020). MPNet: Masked Permuted Pre-training Language Understanding. doi:10.48550/arXiv.2004.09297 Hugging Face documentation https://huggingface.co/docs/transformers/model_doc/mpnet https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.MPNetForMaskedLM https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.TFMPNetForMaskedLM","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFEMpnetTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"special_tokens_list list List special tokens following elements: cls - CLS token representation (<s>) pad - pad token representation (<pad>) sep - sep token representation (<\/s>) unk - unk token representation (<unk>) mask - mask token representation (<mask>)","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":".AIFEMpnetTransformer$new() .AIFEMpnetTransformer$create() .AIFEMpnetTransformer$train() .AIFEMpnetTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"Creates new transformer based MPNet sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"","code":".AIFEMpnetTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"method creates transformer configuration based MPNet base architecture. method adds following 'dependent' parameters base class's inherited params list: vocab_do_lower_case num_hidden_layer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"","code":".AIFEMpnetTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 30522,   vocab_do_lower_case = FALSE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   sustain_track = FALSE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. vocab_do_lower_case bool TRUE words/tokens lower case. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"method can used train fine-tune transformer based MPNet architecture help python libraries transformers, datasets, tokenizers. method adds following 'dependent' parameter base class's inherited params list: p_perm","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"","code":".AIFEMpnetTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   p_perm = 0.15,   whole_word = TRUE,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.003,   n_workers = 1,   multi_process = FALSE,   sustain_track = FALSE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. p_perm double Ratio determines number words/tokens used permutation. whole_word bool TRUE: whole word masking applied. FALSE: token masking used. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"","code":".AIFEMpnetTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFEMpnetTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of MPNet transformers — .AIFEMpnetTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"class following methods: create: creates new transformer based RoBERTa. train: trains fine-tunes RoBERTa model.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"create","dir":"Reference","previous_headings":"","what":"Create","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"New models can created using .AIFERobertaTransformer$create method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"train","dir":"Reference","previous_headings":"","what":"Train","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"train model, pass directory model method .AIFERobertaTransformer$train. Pre-Trained models can fine-tuned function available https://huggingface.co/. Training model makes use dynamic masking.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa: Robustly Optimized BERT Pretraining Approach. doi:10.48550/arXiv.1907.11692 Hugging Face Documentation https://huggingface.co/docs/transformers/model_doc/roberta https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel https://huggingface.co/docs/transformers/model_doc/roberta#transformers.TFRobertaModel","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"aifeducation::.AIFEBaseTransformer -> .AIFERobertaTransformer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab() aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb() aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer() aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model() aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft() aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator() aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache() aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model() aifeducation::.AIFEBaseTransformer$set_model_param() aifeducation::.AIFEBaseTransformer$set_model_temp() aifeducation::.AIFEBaseTransformer$set_required_SFC() aifeducation::.AIFEBaseTransformer$set_title()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":".AIFERobertaTransformer$new() .AIFERobertaTransformer$create() .AIFERobertaTransformer$train() .AIFERobertaTransformer$clone()","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"Creates new transformer based RoBERTa sets title.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"","code":".AIFERobertaTransformer$new()"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"method returns nothing.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"method-create-","dir":"Reference","previous_headings":"","what":"Method create()","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"method creates transformer configuration based RoBERTa base architecture vocabulary based Byte-Pair Encoding (BPE) tokenizer using python transformers tokenizers libraries. method adds following 'dependent' parameters base class' inherited params list: add_prefix_space trim_offsets num_hidden_layer","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"","code":".AIFERobertaTransformer$create(   ml_framework = \"pytorch\",   model_dir,   text_dataset,   vocab_size = 30522,   add_prefix_space = FALSE,   trim_offsets = TRUE,   max_position_embeddings = 512,   hidden_size = 768,   num_hidden_layer = 12,   num_attention_heads = 12,   intermediate_size = 3072,   hidden_act = \"gelu\",   hidden_dropout_prob = 0.1,   attention_probs_dropout_prob = 0.1,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. model_dir string Path directory model saved. text_dataset Object class LargeDataSetForText. vocab_size int Size vocabulary. add_prefix_space bool TRUE additional space inserted leading words. trim_offsets bool TRUE trims whitespaces produced offsets. max_position_embeddings int Number maximum position embeddings. parameter also determines maximum length sequence can processed model. hidden_size int Number neurons layer. parameter determines dimensionality resulting text embedding. num_hidden_layer int Number hidden layers. num_attention_heads int Number attention heads. intermediate_size int Number neurons intermediate layer attention mechanism. hidden_act string Name activation function. hidden_dropout_prob double Ratio dropout. attention_probs_dropout_prob double Ratio dropout attention probabilities. sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"method return object. Instead, saves configuration vocabulary new model disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"method can used train fine-tune transformer based RoBERTa Transformer architecture help python libraries transformers, datasets, tokenizers.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"","code":".AIFERobertaTransformer$train(   ml_framework = \"pytorch\",   output_dir,   model_dir_path,   text_dataset,   p_mask = 0.15,   val_size = 0.1,   n_epoch = 1,   batch_size = 12,   chunk_size = 250,   full_sequences_only = FALSE,   min_seq_len = 50,   learning_rate = 0.03,   n_workers = 1,   multi_process = FALSE,   sustain_track = TRUE,   sustain_iso_code = NULL,   sustain_region = NULL,   sustain_interval = 15,   trace = TRUE,   keras_trace = 1,   pytorch_trace = 1,   pytorch_safetensors = TRUE,   log_dir = NULL,   log_write_interval = 2 )"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"ml_framework string Framework use training inference. ml_framework = \"tensorflow\": 'tensorflow'. ml_framework = \"pytorch\": 'pytorch'. output_dir string Path directory final model saved. directory exist, created. model_dir_path string Path directory original model stored. text_dataset Object class LargeDataSetForText. p_mask double Ratio determines number words/tokens used masking. val_size double Ratio determines amount token chunks used validation. n_epoch int Number epochs training. batch_size int Size batches. chunk_size int Size every chunk training. full_sequences_only bool TRUE using chunks sequence length equal chunk_size. min_seq_len int relevant full_sequences_only = FALSE. Value determines minimal sequence length included training process. learning_rate double Learning rate adam optimizer. n_workers int Number workers. relevant ml_framework = \"tensorflow\". multi_process bool TRUE multiple processes activated. relevant ml_framework = \"tensorflow\". sustain_track bool TRUE energy consumption tracked training via python library codecarbon. sustain_iso_code string ISO code (Alpha-3-Code) country. variable must set sustainability tracked. list can found Wikipedia: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes. sustain_region string Region within country. available USA Canada. See documentation codecarbon information https://mlco2.github.io/codecarbon/parameters.html. sustain_interval integer Interval seconds measuring power usage. trace bool TRUE information progress printed console. keras_trace int keras_trace = 0: print information training process keras console. keras_trace = 1: prints progress bar. keras_trace = 2: prints one line information every epoch. relevant ml_framework = \"tensorflow\". pytorch_trace int pytorch_trace = 0: print information training process pytorch console. pytorch_trace = 1: prints progress bar. pytorch_safetensors bool relevant pytorch models. TRUE: 'pytorch' model saved safetensors format. FALSE ('safetensors' available): model saved standard pytorch format (.bin). log_dir Path directory log files saved. log_write_interval int Time seconds determining interval logger try update log files. relevant log_dir NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"method return object. Instead trained fine-tuned model saved disk.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"objects class cloneable method.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"","code":".AIFERobertaTransformer$clone(deep = FALSE)"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFERobertaTransformer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Child R6 class for creation and training of RoBERTa transformers — .AIFERobertaTransformer","text":"deep Whether make deep clone.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFETrObj.html","id":null,"dir":"Reference","previous_headings":"","what":"Transformer objects — .AIFETrObj","title":"Transformer objects — .AIFETrObj","text":"list contains transformer objects. Elements list used public make AIFETransformerMaker R6 class. list designed used directly. following elements: bert, roberta, deberta_v2, funnel, longformer, mpnet","code":""},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFETrObj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transformer objects — .AIFETrObj","text":"","code":".AIFETrObj"},{"path":"https://fberding.github.io/aifeducation/reference/dot-AIFETrObj.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Transformer objects — .AIFETrObj","text":"object class list length 6.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/fleiss_kappa.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Fleiss' Kappa — fleiss_kappa","title":"Calculate Fleiss' Kappa — fleiss_kappa","text":"function calculates Fleiss' Kappa.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/fleiss_kappa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Fleiss' Kappa — fleiss_kappa","text":"","code":"fleiss_kappa(rater_one, rater_two, additional_raters = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/fleiss_kappa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Fleiss' Kappa — fleiss_kappa","text":"rater_one factor rating first coder. rater_two factor ratings second coder. additional_raters list Additional raters requirements rater_one rater_two. additional raters set NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/fleiss_kappa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Fleiss' Kappa — fleiss_kappa","text":"Retuns value Fleiss' Kappa.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/fleiss_kappa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Fleiss' Kappa — fleiss_kappa","text":"Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378–382. doi:10.1037/h0031619","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/generate_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate ID suffix for objects — generate_id","title":"Generate ID suffix for objects — generate_id","text":"Function generating ID suffix objects class TextEmbeddingModel, TEClassifierRegular, TEClassifierProtoNet.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/generate_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate ID suffix for objects — generate_id","text":"","code":"generate_id(length = 16)"},{"path":"https://fberding.github.io/aifeducation/reference/generate_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate ID suffix for objects — generate_id","text":"length int determining length id suffix.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/generate_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate ID suffix for objects — generate_id","text":"Returns string requested length.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_alpha_3_codes.html","id":null,"dir":"Reference","previous_headings":"","what":"Country Alpha 3 Codes — get_alpha_3_codes","title":"Country Alpha 3 Codes — get_alpha_3_codes","text":"Function requesting vector containing alpha-3 codes countries.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_alpha_3_codes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Country Alpha 3 Codes — get_alpha_3_codes","text":"","code":"get_alpha_3_codes()"},{"path":"https://fberding.github.io/aifeducation/reference/get_alpha_3_codes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Country Alpha 3 Codes — get_alpha_3_codes","text":"Returns vector containing alpha-3 codes countries.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_coder_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate reliability measures based on content analysis — get_coder_metrics","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"function calculates different reliability measures based empirical research method content analysis.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_coder_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"","code":"get_coder_metrics(   true_values = NULL,   predicted_values = NULL,   return_names_only = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/get_coder_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"true_values factor containing true labels/categories. predicted_values factor containing predicted labels/categories. return_names_only bool TRUE returns names resulting vector. Use FALSE request computation values.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_coder_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate reliability measures based on content analysis — get_coder_metrics","text":"return_names_only = FALSE returns vector following reliability measures: iota_index: Iota Index Iota Reliability Concept Version 2. min_iota2: Minimal Iota Iota Reliability Concept Version 2. avg_iota2: Average Iota Iota Reliability Concept Version 2. max_iota2: Maximum Iota Iota Reliability Concept Version 2. min_alpha: Minmal Alpha Reliability Iota Reliability Concept Version 2. avg_alpha: Average Alpha Reliability Iota Reliability Concept Version 2. max_alpha: Maximum Alpha Reliability Iota Reliability Concept Version 2. static_iota_index: Static Iota Index Iota Reliability Concept Version 2. dynamic_iota_index: Dynamic Iota Index Iota Reliability Concept Version 2. kalpha_nominal: Krippendorff's Alpha nominal variables. kalpha_ordinal: Krippendorff's Alpha ordinal variables. kendall: Kendall's coefficient concordance W correction ties. c_kappa_unweighted: Cohen's Kappa unweighted. c_kappa_linear: Weighted Cohen's Kappa linear increasing weights. c_kappa_squared: Weighted Cohen's Kappa quadratic increasing weights. kappa_fleiss: Fleiss' Kappa multiple raters without exact estimation. percentage_agreement: Percentage Agreement. balanced_accuracy: Average accuracy within class. gwet_ac: Gwet's AC1/AC2 agreement coefficient. return_names_only = TRUE returns names vector elements.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_file_extension.html","id":null,"dir":"Reference","previous_headings":"","what":"Get file extension — get_file_extension","title":"Get file extension — get_file_extension","text":"Function requesting file extension","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_file_extension.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get file extension — get_file_extension","text":"","code":"get_file_extension(file_path)"},{"path":"https://fberding.github.io/aifeducation/reference/get_file_extension.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get file extension — get_file_extension","text":"file_path string Path file.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_file_extension.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get file extension — get_file_extension","text":"Returns extension file string.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_n_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the number of chunks/sequences for each case — get_n_chunks","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"Function calculating number chunks/sequences every case.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_n_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"","code":"get_n_chunks(text_embeddings, features, times)"},{"path":"https://fberding.github.io/aifeducation/reference/get_n_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"text_embeddings data.frame array containing text embeddings. features int Number features within sequence. times int Number sequences.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_n_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the number of chunks/sequences for each case — get_n_chunks","text":"Namedvector integers representing number chunks/sequences every case.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_py_package_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get versions of python components — get_py_package_versions","title":"Get versions of python components — get_py_package_versions","text":"Function requesting summary versions critical python components.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_py_package_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get versions of python components — get_py_package_versions","text":"","code":"get_py_package_versions()"},{"path":"https://fberding.github.io/aifeducation/reference/get_py_package_versions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get versions of python components — get_py_package_versions","text":"Returns list contains version number python versions critical python packages. package available version set NA.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/get_synthetic_cases_from_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create synthetic cases for balancing training data — get_synthetic_cases_from_matrix","title":"Create synthetic cases for balancing training data — get_synthetic_cases_from_matrix","text":"function creates synthetic cases balancing training object class TEClassifierRegular TEClassifierProtoNet.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_synthetic_cases_from_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create synthetic cases for balancing training data — get_synthetic_cases_from_matrix","text":"","code":"get_synthetic_cases_from_matrix(   matrix_form,   times,   features,   target,   sequence_length,   method = c(\"smote\"),   min_k = 1,   max_k = 6 )"},{"path":"https://fberding.github.io/aifeducation/reference/get_synthetic_cases_from_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create synthetic cases for balancing training data — get_synthetic_cases_from_matrix","text":"matrix_form Named matrix containing text embeddings matrix form. times int number sequences/times. features int number features within sequence. target Named factor containing labels corresponding embeddings. sequence_length int Length text embedding sequences. method vector containing strings requested methods generating new cases. Currently \"smote\", \"dbsmote\", \"adas\" package smotefamily available. min_k int minimal number nearest neighbors sampling process. max_k int maximum number nearest neighbors sampling process.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/get_synthetic_cases_from_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create synthetic cases for balancing training data — get_synthetic_cases_from_matrix","text":"list following components: syntetic_embeddings: Named data.frame containing text embeddings synthetic cases. syntetic_targets: Named factor containing labels corresponding synthetic cases. n_syntetic_units: table showing number synthetic cases every label/category.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/imdb_movie_reviews.html","id":null,"dir":"Reference","previous_headings":"","what":"Standford Movie Review Dataset — imdb_movie_reviews","title":"Standford Movie Review Dataset — imdb_movie_reviews","text":"data.frame consisting subset 100 negative 200 positive movie reviews dataset provided Maas et al. (2011). data.frame consists three columns. first column 'text' stores movie review. second stores labels (0 = negative, 1 = positive). last column stores id. purpose data illustration vignettes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/imdb_movie_reviews.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standford Movie Review Dataset — imdb_movie_reviews","text":"","code":"imdb_movie_reviews"},{"path":"https://fberding.github.io/aifeducation/reference/imdb_movie_reviews.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Standford Movie Review Dataset — imdb_movie_reviews","text":"data.frame","code":""},{"path":"https://fberding.github.io/aifeducation/reference/imdb_movie_reviews.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Standford Movie Review Dataset — imdb_movie_reviews","text":"Maas, . L., Daly, R. E., Pham, P. T., Huang, D., Ng, . Y., & Potts, C. (2011). Learning Word Vectors Sentiment Analysis. D. Lin, Y. Matsumoto, & R. Mihalcea (Eds.), Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies (pp. 142–150). Association Computational Linguistics. https://aclanthology.org/P11-1015","code":""},{"path":"https://fberding.github.io/aifeducation/reference/install_aifeducation.html","id":null,"dir":"Reference","previous_headings":"","what":"Install aifeducation on a machine — install_aifeducation","title":"Install aifeducation on a machine — install_aifeducation","text":"Function installing 'aifeducation' machine. functions assumes 'python' 'miniconda' installed. 'pytorch' installed.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/install_aifeducation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install aifeducation on a machine — install_aifeducation","text":"","code":"install_aifeducation(install_aifeducation_studio = TRUE)"},{"path":"https://fberding.github.io/aifeducation/reference/install_aifeducation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install aifeducation on a machine — install_aifeducation","text":"install_aifeducation_studio bool TRUE necessary R packages installed using AI Education Studio.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/install_aifeducation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install aifeducation on a machine — install_aifeducation","text":"Function nothing return. installs python, optional R packages, necessary 'python' packages machine.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/install_py_modules.html","id":null,"dir":"Reference","previous_headings":"","what":"Installing necessary python modules to an environment — install_py_modules","title":"Installing necessary python modules to an environment — install_py_modules","text":"Function installing necessary python modules.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/install_py_modules.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Installing necessary python modules to an environment — install_py_modules","text":"","code":"install_py_modules(   envname = \"aifeducation\",   install = \"pytorch\",   transformer_version = \"<=4.46\",   tokenizers_version = \"<=0.20.4\",   pandas_version = \"<=2.2.3\",   datasets_version = \"<=3.1.0\",   codecarbon_version = \"<=2.8.2\",   safetensors_version = \"<=0.4.5\",   torcheval_version = \"<=0.0.7\",   accelerate_version = \"<=1.1.1\",   pytorch_cuda_version = \"12.1\",   python_version = \"3.9\",   remove_first = FALSE )"},{"path":"https://fberding.github.io/aifeducation/reference/install_py_modules.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Installing necessary python modules to an environment — install_py_modules","text":"envname string Name environment packages installed. install character determining machine learning frameworks installed. install = \"\": 'pytorch' 'tensorflow'. install = \"pytorch\": 'pytorch'. install = \"tensorflow\": 'tensorflow'. transformer_version string determining desired version python library 'transformers'. tokenizers_version string determining desired version python library 'tokenizers'. pandas_version string determining desired version python library 'pandas'. datasets_version string determining desired version python library 'datasets'. codecarbon_version string determining desired version python library 'codecarbon'. safetensors_version string determining desired version python library 'safetensors'. torcheval_version string determining desired version python library 'torcheval'. accelerate_version string determining desired version python library 'accelerate'. pytorch_cuda_version string determining desired version 'cuda' ' PyTorch'. python_version string Python version use. remove_first bool TRUE removes environment completely recreating environment installing packages. FALSE packages installed existing environment without prior changes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/install_py_modules.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Installing necessary python modules to an environment — install_py_modules","text":"Returns values objects. Function used installing necessary python libraries conda environment.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/is.null_or_na.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if NULL or NA — is.null_or_na","title":"Check if NULL or NA — is.null_or_na","text":"Function checking object NULL .","code":""},{"path":"https://fberding.github.io/aifeducation/reference/is.null_or_na.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if NULL or NA — is.null_or_na","text":"","code":"is.null_or_na(object)"},{"path":"https://fberding.github.io/aifeducation/reference/is.null_or_na.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if NULL or NA — is.null_or_na","text":"object object test.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/is.null_or_na.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if NULL or NA — is.null_or_na","text":"Returns FALSE object NULL NA. Returns TRUE cases.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/kendalls_w.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Kendall's coefficient of concordance w — kendalls_w","title":"Calculate Kendall's coefficient of concordance w — kendalls_w","text":"function calculates Kendall's coefficient concordance w without correction.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/kendalls_w.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Kendall's coefficient of concordance w — kendalls_w","text":"","code":"kendalls_w(rater_one, rater_two, additional_raters = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/kendalls_w.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Kendall's coefficient of concordance w — kendalls_w","text":"rater_one factor rating first coder. rater_two factor ratings second coder. additional_raters list Additional raters requirements rater_one rater_two. additional raters set NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/kendalls_w.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Kendall's coefficient of concordance w — kendalls_w","text":"Returns list containing results Kendall's coefficient concordance w without correction.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/kripp_alpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Krippendorff's Alpha — kripp_alpha","title":"Calculate Krippendorff's Alpha — kripp_alpha","text":"function calculates different Krippendorff's Alpha nominal ordinal variables.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/kripp_alpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Krippendorff's Alpha — kripp_alpha","text":"","code":"kripp_alpha(rater_one, rater_two, additional_raters = NULL)"},{"path":"https://fberding.github.io/aifeducation/reference/kripp_alpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Krippendorff's Alpha — kripp_alpha","text":"rater_one factor rating first coder. rater_two factor ratings second coder. additional_raters list Additional raters requirements rater_one rater_two. additional raters set NULL.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/kripp_alpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Krippendorff's Alpha — kripp_alpha","text":"Returns list containing results Krippendorff's Alpha nominal ordinal data.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/kripp_alpha.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Krippendorff's Alpha — kripp_alpha","text":"Krippendorff, K. (2019). Content Analysis: Introduction Methodology (4th Ed.). SAGE","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/load_from_disk.html","id":null,"dir":"Reference","previous_headings":"","what":"Loading objects created with 'aifeducation' — load_from_disk","title":"Loading objects created with 'aifeducation' — load_from_disk","text":"Function loading objects created 'aifeducation'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/load_from_disk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loading objects created with 'aifeducation' — load_from_disk","text":"","code":"load_from_disk(dir_path)"},{"path":"https://fberding.github.io/aifeducation/reference/load_from_disk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loading objects created with 'aifeducation' — load_from_disk","text":"dir_path string Path directory model stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/load_from_disk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loading objects created with 'aifeducation' — load_from_disk","text":"Returns object class TEClassifierRegular, TEClassifierProtoNet,  TEFeatureExtractor, TextEmbeddingModel, LargeDataSetForTextEmbeddings, LargeDataSetForText EmbeddedText.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/long_load_target_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load target data for long running tasks — long_load_target_data","title":"Load target data for long running tasks — long_load_target_data","text":"Function loads target data long running task.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/long_load_target_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load target data for long running tasks — long_load_target_data","text":"","code":"long_load_target_data(file_path, selectet_column)"},{"path":"https://fberding.github.io/aifeducation/reference/long_load_target_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load target data for long running tasks — long_load_target_data","text":"file_path string Path file storing target data. selectet_column string Name column containing target data.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/long_load_target_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load target data for long running tasks — long_load_target_data","text":"Returns named factor containing target data.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/long_load_target_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load target data for long running tasks — long_load_target_data","text":"function assumes target data stored columns cases rows categories columns. ids cases must stored column called \"id\".","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/matrix_to_array_c.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape matrix to array — matrix_to_array_c","title":"Reshape matrix to array — matrix_to_array_c","text":"Function written C++ reshaping matrix containing sequential data array use keras.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/matrix_to_array_c.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape matrix to array — matrix_to_array_c","text":"","code":"matrix_to_array_c(matrix, times, features)"},{"path":"https://fberding.github.io/aifeducation/reference/matrix_to_array_c.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape matrix to array — matrix_to_array_c","text":"matrix matrix containing sequential data. times uword Number sequences. features uword Number features within sequence.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/matrix_to_array_c.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape matrix to array — matrix_to_array_c","text":"Returns array. first dimension corresponds cases, second times, third features.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/output_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Print message — output_message","title":"Print message — output_message","text":"Prints message msg trace parameter TRUE current date message() cat() function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/output_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print message — output_message","text":"","code":"output_message(msg, trace, msg_fun)"},{"path":"https://fberding.github.io/aifeducation/reference/output_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print message — output_message","text":"msg string Message printed. trace bool Silent printing (FALSE) (TRUE). msg_fun bool value determines function used. TRUE message(), FALSE cat().","code":""},{"path":"https://fberding.github.io/aifeducation/reference/output_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print message — output_message","text":"function returns nothing.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/print_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Print message (message()) — print_message","title":"Print message (message()) — print_message","text":"Prints message msg trace parameter TRUE current date message() function.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/print_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print message (message()) — print_message","text":"","code":"print_message(msg, trace)"},{"path":"https://fberding.github.io/aifeducation/reference/print_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print message (message()) — print_message","text":"msg string Message printed. trace bool Silent printing (FALSE) (TRUE).","code":""},{"path":"https://fberding.github.io/aifeducation/reference/print_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print message (message()) — print_message","text":"function returns nothing.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/run_py_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Run python file — run_py_file","title":"Run python file — run_py_file","text":"Used run python files reticulate::py_run_file() folder python.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/run_py_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run python file — run_py_file","text":"","code":"run_py_file(py_file_name)"},{"path":"https://fberding.github.io/aifeducation/reference/run_py_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run python file — run_py_file","text":"py_file_name string Name python file run. file must python folder aifeducation package.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/run_py_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run python file — run_py_file","text":"function returns nothing.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/save_to_disk.html","id":null,"dir":"Reference","previous_headings":"","what":"Saving objects created with 'aifeducation' — save_to_disk","title":"Saving objects created with 'aifeducation' — save_to_disk","text":"Function saving objects created 'aifeducation'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/save_to_disk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saving objects created with 'aifeducation' — save_to_disk","text":"","code":"save_to_disk(object, dir_path, folder_name)"},{"path":"https://fberding.github.io/aifeducation/reference/save_to_disk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saving objects created with 'aifeducation' — save_to_disk","text":"object Object class TEClassifierRegular, TEClassifierProtoNet,  TEFeatureExtractor, TextEmbeddingModel, LargeDataSetForTextEmbeddings, LargeDataSetForText EmbeddedText saved. dir_path string Path directory model stored. folder_name string Name folder files stored.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/save_to_disk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Saving objects created with 'aifeducation' — save_to_disk","text":"Function return value. saves model disk. return value, called side effects.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/set_config_cpu_only.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting cpu only for 'tensorflow' — set_config_cpu_only","title":"Setting cpu only for 'tensorflow' — set_config_cpu_only","text":"functions configurates 'tensorflow' use cpus.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_cpu_only.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting cpu only for 'tensorflow' — set_config_cpu_only","text":"","code":"set_config_cpu_only()"},{"path":"https://fberding.github.io/aifeducation/reference/set_config_cpu_only.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setting cpu only for 'tensorflow' — set_config_cpu_only","text":"function return anything. used side effects.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_cpu_only.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting cpu only for 'tensorflow' — set_config_cpu_only","text":"os$environ$setdefault(\"CUDA_VISIBLE_DEVICES\",\"-1\")","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/set_config_gpu_low_memory.html","id":null,"dir":"Reference","previous_headings":"","what":"Setting gpus' memory usage — set_config_gpu_low_memory","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function changes memory usage gpus allow computations machines small memory. function, computations large models may possible speed computation decreases.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_gpu_low_memory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"","code":"set_config_gpu_low_memory()"},{"path":"https://fberding.github.io/aifeducation/reference/set_config_gpu_low_memory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function return anything. used side effects.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_gpu_low_memory.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setting gpus' memory usage — set_config_gpu_low_memory","text":"function sets TF_GPU_ALLOCATOR \"cuda_malloc_async\" sets memory growth TRUE.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/set_config_os_environ_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information in tensorflow — set_config_os_environ_logger","title":"Sets the level for logging information in tensorflow — set_config_os_environ_logger","text":"function changes level logging information 'tensorflow' via os environment. function must called importing 'tensorflow'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_os_environ_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information in tensorflow — set_config_os_environ_logger","text":"","code":"set_config_os_environ_logger(level = \"ERROR\")"},{"path":"https://fberding.github.io/aifeducation/reference/set_config_os_environ_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information in tensorflow — set_config_os_environ_logger","text":"level string Minimal level printed console. Four levels available: INFO, WARNING, ERROR NONE.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_os_environ_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information in tensorflow — set_config_os_environ_logger","text":"function return anything. used side effects.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/set_config_tf_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information in tensorflow — set_config_tf_logger","title":"Sets the level for logging information in tensorflow — set_config_tf_logger","text":"function changes level logging information 'tensorflow'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_tf_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information in tensorflow — set_config_tf_logger","text":"","code":"set_config_tf_logger(level = \"ERROR\")"},{"path":"https://fberding.github.io/aifeducation/reference/set_config_tf_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information in tensorflow — set_config_tf_logger","text":"level string Minimal level printed console. Five levels available: FATAL, ERROR, WARN, INFO, DEBUG.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_config_tf_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information in tensorflow — set_config_tf_logger","text":"function return anything. used side effects.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/set_transformers_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets the level for logging information of the 'transformers' library — set_transformers_logger","title":"Sets the level for logging information of the 'transformers' library — set_transformers_logger","text":"function changes level logging information 'transformers' library. influences output printed console creating training transformer models well TextEmbeddingModels.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_transformers_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets the level for logging information of the 'transformers' library — set_transformers_logger","text":"","code":"set_transformers_logger(level = \"ERROR\")"},{"path":"https://fberding.github.io/aifeducation/reference/set_transformers_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets the level for logging information of the 'transformers' library — set_transformers_logger","text":"level string Minimal level printed console. Four levels available: INFO, WARNING, ERROR DEBUG","code":""},{"path":"https://fberding.github.io/aifeducation/reference/set_transformers_logger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets the level for logging information of the 'transformers' library — set_transformers_logger","text":"function return anything. used side effects.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/start_aifeducation_studio.html","id":null,"dir":"Reference","previous_headings":"","what":"Aifeducation Studio — start_aifeducation_studio","title":"Aifeducation Studio — start_aifeducation_studio","text":"Functions starts shiny app represents Aifeducation Studio.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/start_aifeducation_studio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aifeducation Studio — start_aifeducation_studio","text":"","code":"start_aifeducation_studio()"},{"path":"https://fberding.github.io/aifeducation/reference/start_aifeducation_studio.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aifeducation Studio — start_aifeducation_studio","text":"function nothing return. used start shiny app.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/summarize_tracked_sustainability.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizing tracked sustainability data — summarize_tracked_sustainability","title":"Summarizing tracked sustainability data — summarize_tracked_sustainability","text":"Function summarizing tracked sustainability data tracker python library 'codecarbon'.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/summarize_tracked_sustainability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizing tracked sustainability data — summarize_tracked_sustainability","text":"","code":"summarize_tracked_sustainability(sustainability_tracker)"},{"path":"https://fberding.github.io/aifeducation/reference/summarize_tracked_sustainability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizing tracked sustainability data — summarize_tracked_sustainability","text":"sustainability_tracker Object class codecarbon.emissions_tracker.OfflineEmissionsTracker python library codecarbon.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/summarize_tracked_sustainability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarizing tracked sustainability data — summarize_tracked_sustainability","text":"Returns list contains tracked sustainability data.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/to_categorical_c.html","id":null,"dir":"Reference","previous_headings":"","what":"Transforming classes to one-hot encoding — to_categorical_c","title":"Transforming classes to one-hot encoding — to_categorical_c","text":"Function written C++ transforming vector classes (int) binary class matrix.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/to_categorical_c.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transforming classes to one-hot encoding — to_categorical_c","text":"","code":"to_categorical_c(class_vector, n_classes)"},{"path":"https://fberding.github.io/aifeducation/reference/to_categorical_c.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transforming classes to one-hot encoding — to_categorical_c","text":"class_vector vector containing integers every class. integers must range 0 n_classes-1. n_classes int Total number classes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/to_categorical_c.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transforming classes to one-hot encoding — to_categorical_c","text":"Returns matrix containing binary representation every class.","code":""},{"path":[]},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Vignette classifier — vignette_classifier","title":"Vignette classifier — vignette_classifier","text":"object class TEClassifierRegular trained subset Standford Movie Review Dataset. purpose classifier illustration vignettes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vignette classifier — vignette_classifier","text":"","code":"vignette_classifier"},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Vignette classifier — vignette_classifier","text":"R6","code":""},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_ProtoNet.html","id":null,"dir":"Reference","previous_headings":"","what":"Vignette classifier ProtoNet — vignette_classifier_ProtoNet","title":"Vignette classifier ProtoNet — vignette_classifier_ProtoNet","text":"object class TEClassifierProtoNet trained subset Standford Movie Review Dataset. purpose classifier illustration vignettes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_ProtoNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vignette classifier ProtoNet — vignette_classifier_ProtoNet","text":"","code":"vignette_classifier_ProtoNet"},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_ProtoNet.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Vignette classifier ProtoNet — vignette_classifier_ProtoNet","text":"R6","code":""},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_sc_pl.html","id":null,"dir":"Reference","previous_headings":"","what":"Vignette classifier trained with Synthetic Cases and Pseudo Labeling — vignette_classifier_sc_pl","title":"Vignette classifier trained with Synthetic Cases and Pseudo Labeling — vignette_classifier_sc_pl","text":"object class TEClassifierProtoNet trained subset Standford Movie Review Dataset. purpose classifier illustration vignettes.","code":""},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_sc_pl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vignette classifier trained with Synthetic Cases and Pseudo Labeling — vignette_classifier_sc_pl","text":"","code":"vignette_classifier_sc_pl"},{"path":"https://fberding.github.io/aifeducation/reference/vignette_classifier_sc_pl.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Vignette classifier trained with Synthetic Cases and Pseudo Labeling — vignette_classifier_sc_pl","text":"R6","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-102","dir":"Changelog","previous_headings":"","what":"aifeducation 1.0.2","title":"aifeducation 1.0.2","text":"Fixed bug alpha 3 codes sustainability tracking preventing Ai Education Studio start. source urls entries LargeDataSetsForTexts now displayed correctly within Ai Education Studio.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-101","dir":"Changelog","previous_headings":"","what":"aifeducation 1.0.1","title":"aifeducation 1.0.1","text":"CRAN release: 2025-01-28 Fixed bug initialization ‘codecarbon’ linux.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-100","dir":"Changelog","previous_headings":"","what":"aifeducation 1.0.0","title":"aifeducation 1.0.0","text":"CRAN release: 2024-12-20 First complete release package including major changes, bug fixes, new features, objects. important change decided use ‘PyTorch’ several reasons. First, ‘PyTorch’ flexible stable machine learning framework. moment, new architectures based ‘PyTorch’ can seen Hugging Face. Currently (11th November 2024) 190,237 models framework compared 13,346 models ‘tensorflow’. Second, ‘PyTorch’ provides easy installation supports native GPU acceleration Linux Windows tensorflow supports native GPU support Linux Windows version 2.10 lower. Fourth, keras, important element ‘tensorflow’, changed multi-back-end framework. However, keras 3.0 native Windows support. Since assume many educational researchers use either Windows Mac familiar complex system configurations (using Windows subsystem Linux (WSL)), problematic. addition, changed algorithm saving loading models, data, objects ensure models trained package working within future versions aifeducation can updated new developments. also necessary allow reproducibility models research based models. achieve goal make changes models created version 0.3.3 lower. still need models, please install older version aifeducation. following changes made: Major Changes core machine learning framework now ‘PyTorch’. ‘Tensorflow’ still supported models limited version 2.15. implementation support ‘tensorflow’ models currently planned. decided base package ‘PyTorch’ framework widely used research, flexible, provides broad GPU support, offers stable code across versions. Implemented new mechanic new methods objects allowing objects created older version package update current version loading. Removed bag--words models package order focus package approaches use AI. Installation Configuration Added new function convenient installation ‘python’ ‘pytorch’. Transformer Models Complete rewrite transformer functions modern object-oriented approach R6 classes (AIFETransformerMaker). Functions type create_xxx_model train_xxx_model now deprecated. Added support MPNet ‘pytorch’ ‘tensorflow’. TEFeatureExtractor Adding TEFeatureExtractor new class ‘pytorch’ . TEFeatureExtractor auto-encoders can used reduce number features text embeddings passing onto classifiers. aim reduce computational time /increase performance classifiers. TextEmbeddingClassifiers TEClassifierRegular replaces TextEmbeddingClassifierNeuralNet. new class provides additional methods fixes bug pytorch models used predict two classes. TextEmbeddingClassifierNeuralNet now deprecated. Added TEClassifierProtoNet classifier applys methods meta-learning based ProtoNets. comparison TextEmbeddingClassifierNeuralNet, training loop new classes altered reduced complexity users. example, type pseudo-labeling described Cascante-Bonilla et al. (2020) now implemented type technique described Lee (2013) removed. addition, now possible add synthetic cases within every step pseudo-labeling. See vignettes details. Graphical User Interface Aifeducation Studio Complete rewrite user interface based bslib removing dependencies shinydashboard. User interface supports pytorch longer tensorflow. Implemented long running tasks training transformer shiny ExtendedTask. allows computation task background shiny app stay responsive. , turn, avoids “greying ” app. Implemented new reporting system providing feedback user computations. Data Management Introduced two new classes LargeDataSetForTextEmbeddings LargeDataSetForText based python libraries ‘arrow’ ‘datasets’ allowing store use data fit memory. LargeDataSetForText stores raw texts LargeDataSetForTextEmbeddings contain text embeddings. Added support AI models new kinds objects allow training large data sets. Added new methods objects class EmbeddedTexts (e.g. converting EmbeddedTexts LargeDataSetForTextEmbeddings). See corresponding documentation details. function combine_embeddings now deprecated. Please use corresponding method EmbeddedTexts. Saving Loading Introduced save_to_disk load_from_disk new core functions saving loading objects models package. Functions load_ai_model save_ai_model now deprecated. Please use functions models created version 0.3.3 lower. Changes Removed dependencies package abind irr. Updated vignettes.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-033","dir":"Changelog","previous_headings":"","what":"aifeducation 0.3.3","title":"aifeducation 0.3.3","text":"CRAN release: 2024-04-22 Graphical User Interface Aifeducation Studio Fixed bug concerning IDs .pdf .csv files. Now IDs correctly saved within text collection file. Fixed bug checking selection least one file type creation text collection. TextEmbeddingClassifiers Fixed process checking TextEmbeddingModels compatible. Python Installation Fixed bug caused installation incompatible versions keras Tensorflow. Changes Removed quanteda.textmodels necessary library testing package. Added dataset testing package based Maas et al. (2011).","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-032","dir":"Changelog","previous_headings":"","what":"aifeducation 0.3.2","title":"aifeducation 0.3.2","text":"CRAN release: 2024-03-15 TextEmbeddingClassifiers Fixed bug GlobalAveragePooling1D_PT. Now layer makes correct pooling. change effect PyTorch models trained version 0.3.1. TextEmbeddingModel Replaced parameter ‘aggregation’ three new parameters allowing explicitly choose start end layer included creation embeddings. Furthermore, two options pooling method within layer added (“cls” “average”). Added support reporting training validation loss training corresponding base model. Transformer Models Fixed bug creation transformer models except funnel. Now choosing number layers working. file ‘history.log’ now saved within model’s folder reporting loss validation loss training epoch. EmbeddedText Changed process validating EmbeddedTexts compatible. Now model’s unique name used validation. Added new fields updated methods account new options creating embeddings (layer selection pooling type). Graphical User Interface Aifeducation Studio Adapted interface according changes made version. Improved read raw texts. Reading now reduces multiple spaces characters one single space character. Hyphenation removed. Python Installation Updated installation account new version keras.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-031","dir":"Changelog","previous_headings":"","what":"aifeducation 0.3.1","title":"aifeducation 0.3.1","text":"CRAN release: 2024-02-18 Graphical User Interface Aifeducation Studio Added shiny app package serves graphical user interface. Transformer Models Fixed bug transformers except BERT concerning unk_token. Switched SentencePiece tokenizer WordPiece tokenizer DeBERTa_V2. Add possibility train DeBERTa_V2 FunnelTransformer models Whole Word Masking. TextEmbeddingModel Added method ‘fill-mask’. Added new argument method ‘encode’, allowing chose encoding token ids token strings. Added new argument method ‘decode’, allowing chose decoding single tokens plain text. Fixed bug embedding texts using pytorch. fix decrease computational time enables gpu support (available machine). Fixed two missing columns saving results sustainability tracking machines without gpu. Implemented advantages datasets python library ‘datasets’ increasing computational speed allowing use large datasets. TextEmbeddingClassifiers Adding support pytorch without need kerasV3 keras-core. Classifiers pytorch now implemented native pytorch. Changed architecture new classifiers extended abilities neural nets adding possibility add positional embedding. Changed architecture new classifiers extended abilities neural nets adding alternative method self-attention mechanism via fourier transformation (similar FNet). Added balanced_accuracy new metric determining state model predicts classes best. Fixed error training history saved correctly. Added record metric test dataset training history pytorch. Added option balance class weights calculating training loss according Inverse Frequency method. Balance class weights activated default. Added method checking compatibility underlying TextEmbeddingModels classifier object class EmbeddedText. Added precision, recall, f1-score new metrics. Python Installation Added argument ‘install_py_modules’, allowing choose machine learning framework installed. Updated ‘check_aif_py_modules’. Changes Setting machine learning framework start session longer necessary. function setting global ml_framework remains active convenience. ml_framework can now switched time session. Updated documentation.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-030","dir":"Changelog","previous_headings":"","what":"aifeducation 0.3.0","title":"aifeducation 0.3.0","text":"CRAN release: 2023-10-10 Added DeBERTa Funnel-Transformer support. Fixed issues installing required python packages. Fixed issues training transformer models. Fixed issue calculating final iota values classifiers pseudo labeling active. Added support PyTorch Tensorflow transformer models. Added support PyTorch classifier objects via keras 3 future. Removed augmentation vocabulary training BERT models. Updated documentation. Changed reported values kappa.","code":""},{"path":"https://fberding.github.io/aifeducation/news/index.html","id":"aifeducation-020","dir":"Changelog","previous_headings":"","what":"aifeducation 0.2.0","title":"aifeducation 0.2.0","text":"CRAN release: 2023-08-15 First release CRAN","code":""}]
